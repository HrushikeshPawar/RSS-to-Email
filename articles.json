{
    "DeepMind Blog": {
        "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents": {
            "url": "https://www.deepmind.com/blog/spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents",
            "description": "In our recent paper we explore how multi-agent deep reinforcement learning can serve as a model of complex social interactions, like the formation of social norms. This new class of models could provide a path to create richer, more detailed simulations of the world.",
            "pubdate": "Tue, 18 Jan 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                18
            ],
            "email_sent": true
        },
        "DeepMind: The Podcast returns for Season 2": {
            "url": "https://www.deepmind.com/blog/deepmind-the-podcast-returns-for-season-2",
            "description": "We believe artificial intelligence (AI) is one of the most significant technologies of our age and we want to help people understand its potential and how it\u2019s being created.",
            "pubdate": "Tue, 25 Jan 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                25
            ],
            "email_sent": true
        },
        "Competitive programming with AlphaCode": {
            "url": "https://www.deepmind.com/blog/competitive-programming-with-alphacode",
            "description": "Solving novel problems and setting a new milestone in competitive programming.",
            "pubdate": "Wed, 02 Feb 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                2
            ],
            "email_sent": true
        },
        "Red Teaming Language Models with Language Models": {
            "url": "https://www.deepmind.com/blog/red-teaming-language-models-with-language-models",
            "description": "In our recent paper, we show that it is possible to automatically find inputs that elicit harmful text from language models by generating inputs using language models themselves. Our approach provides one tool for finding harmful model behaviours before users are impacted, though we emphasize that it should be viewed as one component alongside many other techniques that will be needed to find harms and mitigate them once found.",
            "pubdate": "Mon, 07 Feb 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                7
            ],
            "email_sent": true
        },
        "MuZeros first step from research into the real world": {
            "url": "https://www.deepmind.com/blog/muzeros-first-step-from-research-into-the-real-world",
            "description": "Collaborating with YouTube to optimise video compression in the open source VP9 codec.",
            "pubdate": "Fri, 11 Feb 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                11
            ],
            "email_sent": true
        },
        "Accelerating fusion science through learned plasma control": {
            "url": "https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control",
            "description": "Successfully controlling the nuclear fusion plasma in a tokamak with deep reinforcement learning",
            "pubdate": "Wed, 16 Feb 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                16
            ],
            "email_sent": true
        },
        "Probing Image-Language Transformers for Verb Understanding": {
            "url": "https://www.deepmind.com/blog/probing-image-language-transformers-for-verb-understanding",
            "description": "Multimodal Image-Language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations--in particular, if these models can distinguish verbs or they only use the nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs consisting of 447 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate the pretrained models in a zero-shot way.",
            "pubdate": "Wed, 23 Feb 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                23
            ],
            "email_sent": true
        },
        "Learning Robust Real-Time Cultural Transmission without Human Data": {
            "url": "https://www.deepmind.com/blog/learning-robust-real-time-cultural-transmission-without-human-data",
            "description": "In this work, we use deep reinforcement learning to generate artificial agents capable of test-time cultural transmission. Once trained, our agents can infer and recall navigational knowledge demonstrated by experts. This knowledge transfer happens in real time and generalises across a vast space of previously unseen tasks.",
            "pubdate": "Thu, 03 Mar 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                3
            ],
            "email_sent": true
        },
        "Predicting the past with Ithaca": {
            "url": "https://www.deepmind.com/blog/predicting-the-past-with-ithaca",
            "description": "The birth of human writing marked the dawn of History and is crucial to our understanding of past civilisations and the world we live in today. For example, more than 2,500 years ago, the Greeks began writing on stone, pottery, and metal to document everything from leases and laws to calendars and oracles, giving a detailed insight into the Mediterranean region. Unfortunately, it\u2019s an incomplete record. Many of the surviving inscriptions have been damaged over the centuries or moved from their original location. In addition, modern dating techniques, such as radiocarbon dating, cannot be used on these materials, making inscriptions difficult and time-consuming to interpret.",
            "pubdate": "Wed, 09 Mar 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                9
            ],
            "email_sent": true
        },
        "GopherCite: Teaching language models to support answers with verified quotes": {
            "url": "https://www.deepmind.com/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes",
            "description": "Language models like Gopher can \u201challucinate\u201d facts that appear plausible but are actually fake. Those who are familiar with this problem know to do their own fact-checking, rather than trusting what language models say. Those who are not, may end up believing something that isn\u2019t true. This paper describes GopherCite, a model which aims to address the problem of language model hallucination. GopherCite attempts to back up all of its factual claims with evidence from the web.",
            "pubdate": "Wed, 16 Mar 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                16
            ],
            "email_sent": true
        },
        "An empirical analysis of compute-optimal large language model training": {
            "url": "https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training",
            "description": "We ask the question: \u201cWhat is the optimal model size and number of training tokens for a given compute budget?\u201d To answer this question, we train models of various sizes and with various numbers of tokens, and estimate this trade-off empirically. Our main finding is that the current large language models are far too large for their compute budget and are not being trained on enough data.",
            "pubdate": "Tue, 12 Apr 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                12
            ],
            "email_sent": true
        },
        "DeepMinds latest research at ICLR 2022": {
            "url": "https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2022",
            "description": "Beyond supporting the event as sponsors and regular workshop organisers, our research teams are presenting 29 papers, including 10 collaborations this year. Here\u2019s a brief glimpse into our upcoming oral, spotlight, and poster presentations.",
            "pubdate": "Mon, 25 Apr 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                25
            ],
            "email_sent": true
        },
        "Tackling multiple tasks with a single visual language model": {
            "url": "https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model",
            "description": "We introduce Flamingo, a single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks.",
            "pubdate": "Thu, 28 Apr 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                28
            ],
            "email_sent": true
        },
        "When a passion for bass and brass help build better tools": {
            "url": "https://www.deepmind.com/blog/when-a-passion-for-bass-and-brass-help-build-better-tools",
            "description": "We caught up with Kevin Millikin, a software engineer on the DevTools team. He\u2019s in Salt Lake City this week to present at PyCon US, the largest annual gathering for those using and developing the open-source Python programming language.",
            "pubdate": "Thu, 28 Apr 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                28
            ],
            "email_sent": true
        },
        "Active offline policy selection": {
            "url": "https://www.deepmind.com/blog/active-offline-policy-selection",
            "description": "To make RL more applicable to real-world applications like robotics, we propose using an intelligent evaluation procedure to select the policy for deployment, called active offline policy selection (A-OPS). In A-OPS, we make use of the prerecorded dataset and allow limited interactions with the real environment to boost the selection quality.",
            "pubdate": "Fri, 06 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                6
            ],
            "email_sent": true
        },
        "A Generalist Agent": {
            "url": "https://www.deepmind.com/blog/a-generalist-agent",
            "description": "Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.",
            "pubdate": "Thu, 12 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning": {
            "url": "https://www.deepmind.com/blog/emergent-bartering-behaviour-in-multi-agent-reinforcement-learning",
            "description": "In our recent paper, we explore how populations of deep reinforcement learning (deep RL) agents can learn microeconomic behaviours, such as production, consumption, and trading of goods. We find that artificial agents learn to make economically rational decisions about production, consumption, and prices, and react appropriately to supply and demand changes.",
            "pubdate": "Mon, 16 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                16
            ],
            "email_sent": true
        },
        "From LEGO competitions to DeepMind's robotics lab": {
            "url": "https://www.deepmind.com/blog/from-lego-competitions-to-deepminds-robotics-lab",
            "description": "If you want to be at DeepMind, go for it. Apply, interview, and just try. You might not get it the first time but that doesn\u2019t mean you can\u2019t try again. I never thought DeepMind would accept me, and when they did, I thought it was a mistake. Everyone doubts themselves \u2013 I\u2019ve never felt like the smartest person in the room. I\u2019ve often felt the opposite. But I\u2019ve learned that, despite those feelings, I do belong and I do deserve to work at a place like this. And that journey, for me, started with just trying.",
            "pubdate": "Thu, 19 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                19
            ],
            "email_sent": true
        },
        "Open-sourcing MuJoCo": {
            "url": "https://www.deepmind.com/blog/open-sourcing-mujoco",
            "description": "In October 2021, we announced that we acquired the MuJoCo physics simulator, and made it freely available for everyone to support research everywhere. We also committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. Today, we\u2019re thrilled to report that open sourcing is complete and the entire codebase is on GitHub!\u00a0Here, we explain why MuJoCo is a great platform for open-source collaboration and share a preview of our roadmap going forward.",
            "pubdate": "Mon, 23 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                23
            ],
            "email_sent": true
        },
        "Building a culture of pioneering responsibly": {
            "url": "https://www.deepmind.com/blog/building-a-culture-of-pioneering-responsibly",
            "description": "When I joined DeepMind as COO, I did so in large part because I could tell that the founders and team had the same focus on positive social impact. In fact, at DeepMind, we now champion a term that perfectly captures my own values and hopes for integrating technology into people\u2019s daily lives: pioneering responsibly. I believe pioneering responsibly should be a priority for anyone working in tech. But I also recognise that it\u2019s especially important when it comes to powerful, widespread technologies like artificial intelligence. AI is arguably the most impactful technology being developed today. It has the potential to benefit humanity in innumerable ways \u2013 from combating climate change to preventing and treating disease. But it\u2019s essential that we account for both its positive and negative downstream impacts.",
            "pubdate": "Tue, 24 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                24
            ],
            "email_sent": true
        },
        "Kyrgyzstan to Kings Cross: the star baker cooking up code": {
            "url": "https://www.deepmind.com/blog/kyrgyzstan-to-kings-cross-the-star-baker-cooking-up-code",
            "description": "My day can vary, it really depends on which phase of the project I'm on. Let\u2019s say we want to add a feature to our product \u2013 my tasks could range from designing solutions and working with the team to find the best one, to deploying new features into production and doing maintenance. Along the way, I\u2019ll communicate changes to our stakeholders, write docs, code and test solutions, build analytics dashboards, clean-up old code, and fix bugs.",
            "pubdate": "Thu, 26 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                26
            ],
            "email_sent": true
        },
        "Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models": {
            "url": "https://www.deepmind.com/blog/dynamic-language-understanding-adaptation-to-new-knowledge-in-parametric-and-semi-parametric-models",
            "description": "To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting.",
            "pubdate": "Thu, 26 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                26
            ],
            "email_sent": true
        },
        "Evaluating Multimodal Interactive Agents": {
            "url": "https://www.deepmind.com/blog/evaluating-multimodal-interactive-agents",
            "description": "In this paper, we assess the merits of these existing evaluation metrics and present a novel approach to evaluation called the Standardised Test Suite (STS). The STS uses behavioural scenarios mined from real human interaction data.",
            "pubdate": "Fri, 27 May 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                27
            ],
            "email_sent": true
        },
        "Advocating for the LGBTQ+ community in AI research": {
            "url": "https://www.deepmind.com/blog/advocating-for-the-lgbtq-community-in-ai-research",
            "description": "Research scientist, Kevin McKee, tells how his early love of science fiction and social psychology inspired his career, and how he\u2019s helping advance research in \u2018queer fairness\u2019, support human-AI collaboration, and study the effects of AI on the LGBTQ+ community.",
            "pubdate": "Wed, 01 Jun 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                1
            ],
            "email_sent": true
        },
        "Bridging DeepMind research with Alphabet products": {
            "url": "https://www.deepmind.com/blog/bridging-deepmind-research-with-alphabet-products",
            "description": "Today we caught up with Gemma Jennings, a product manager on the Applied team, who led a session on vision language models at the AI Summit, one of the world\u2019s largest AI events for business.",
            "pubdate": "Wed, 15 Jun 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                15
            ],
            "email_sent": true
        },
        "Unlocking High-Accuracy Differentially Private Image Classification through Scale": {
            "url": "https://www.deepmind.com/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale",
            "description": "According to empirical evidence from prior works, utility degradation in DP-SGD becomes more severe on larger neural network models \u2013 including the ones regularly used to achieve the best performance on challenging image classification benchmarks. Our work investigates this phenomenon and proposes a series of simple modifications to both the training procedure and model architecture, yielding a significant improvement on the accuracy of DP training on standard image classification benchmarks.",
            "pubdate": "Fri, 17 Jun 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                17
            ],
            "email_sent": true
        },
        "BYOL-Explore: Exploration with Bootstrapped Prediction": {
            "url": "https://www.deepmind.com/blog/byol-explore-exploration-with-bootstrapped-prediction",
            "description": "We present BYOL-Explore, a conceptually simple yet general approach for curiosity-driven exploration in visually-complex environments. BYOL-Explore learns a world representation, the world dynamics, and an exploration policy all-together by optimizing a single prediction loss in the latent space with no additional auxiliary objective. We show that BYOL-Explore is effective in DM-HARD-8, a challenging partially-observable continuous-action hard-exploration benchmark with visually-rich 3-D environments.",
            "pubdate": "Mon, 20 Jun 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                20
            ],
            "email_sent": true
        },
        "Leading a movement to strengthen machine learning in Africa": {
            "url": "https://www.deepmind.com/blog/leading-a-movement-to-strengthen-machine-learning-in-africa",
            "description": "",
            "pubdate": "Thu, 23 Jun 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "Human-centred mechanism design with Democratic AI": {
            "url": "https://www.deepmind.com/blog/human-centred-mechanism-design-with-democratic-ai",
            "description": "In our recent paper, published in Nature Human Behaviour, we provide a proof-of-concept demonstration that deep reinforcement learning (RL) can be used to find economic policies that people will vote for by majority in a simple game. The paper thus addresses a key challenge in AI research - how to train AI systems that align with human values.",
            "pubdate": "Mon, 04 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                4
            ],
            "email_sent": true
        },
        "Intuitive physics learning in a deep-learning model inspired by developmental psychology": {
            "url": "https://www.deepmind.com/blog/intuitive-physics-learning-in-a-deep-learning-model-inspired-by-developmental-psychology",
            "description": "Despite significant effort, current AI systems pale in their understanding of intuitive physics, in comparison to even very young children. In the present work, we address this AI problem, specifically by drawing on the field of developmental psychology.",
            "pubdate": "Mon, 11 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                11
            ],
            "email_sent": true
        },
        "Working together with YouTube": {
            "url": "https://www.deepmind.com/blog/working-together-with-youtube",
            "description": "",
            "pubdate": "Thu, 14 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "DeepMinds latest research at ICML 2022": {
            "url": "https://www.deepmind.com/blog/deepminds-latest-research-at-icml-2022",
            "description": "Starting this weekend, the thirty-ninth International Conference on Machine Learning (ICML 2022) is meeting from 17-23 July, 2022 at the Baltimore Convention Center in Maryland, USA, and will be running as a hybrid event. Researchers working across artificial intelligence, data science, machine vision, computational biology, speech recognition, and more are presenting and publishing their cutting-edge work in machine learning.",
            "pubdate": "Fri, 15 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                15
            ],
            "email_sent": true
        },
        "Perceiver AR: general-purpose, long-context autoregressive generation": {
            "url": "https://www.deepmind.com/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation",
            "description": "We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.",
            "pubdate": "Sat, 16 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                16
            ],
            "email_sent": true
        },
        "The virtuous cycle of AI research": {
            "url": "https://www.deepmind.com/blog/the-virtuous-cycle-of-ai-research",
            "description": "We recently caught up with Petar Veli\u010dkovi\u0107, a research scientist at DeepMind. Along with his co-authors, Petar is presenting his paper The CLRS Algorithmic Reasoning Benchmark at ICML 2022 in Baltimore, Maryland, USA.",
            "pubdate": "Tue, 19 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                19
            ],
            "email_sent": true
        },
        "AlphaFold reveals the structure of the protein universe": {
            "url": "https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe",
            "description": "Today, in partnership with EMBL\u2019s European Bioinformatics Institute (EMBL-EBI), we\u2019re now releasing predicted structures for nearly all catalogued proteins known to science, which will expand the AlphaFold DB by over 200x - from nearly 1 million structures to over 200 million structures - with the potential to dramatically increase our understanding of biology.",
            "pubdate": "Thu, 28 Jul 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                28
            ],
            "email_sent": true
        },
        "Realising scientists are the real superheroes": {
            "url": "https://www.deepmind.com/blog/realising-scientists-are-the-real-superheroes",
            "description": "Meet Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, a research engineer on our Multi-Agent Research team who\u2019s drawing on knowledge of game theory, computer science, and social evolution to get AI agents working better together.",
            "pubdate": "Thu, 11 Aug 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                11
            ],
            "email_sent": true
        },
        "Discovering when an agent is present in a system": {
            "url": "https://www.deepmind.com/blog/discovering-when-an-agent-is-present-in-a-system",
            "description": "We want to build safe, aligned artificial general intelligence (AGI) systems that pursue the intended goals of its designers. Causal influence diagrams (CIDs) are a way to model decision-making situations that allow us to reason about agent incentives. By relating training setups to the incentives that shape agent behaviour, CIDs help illuminate potential risks before training an agent and can inspire better agent designs. But how do we know when a CID is an accurate model of a training setup?",
            "pubdate": "Thu, 18 Aug 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Advancing conservation with AI-based facial recognition of turtles": {
            "url": "https://www.deepmind.com/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles",
            "description": "We came across Zindi \u2013 a dedicated partner with complementary goals \u2013 who are the largest community of African data scientists and host competitions that focus on solving Africa\u2019s most pressing problems. Our Science team\u2019s Diversity, Equity, and Inclusion (DE&amp;I) team worked with Zindi to identify a scientific challenge that could help advance conservation efforts and grow involvement in AI. Inspired by Zindi\u2019s bounding box turtle challenge, we landed on a project with the potential for real impact: turtle facial recognition.",
            "pubdate": "Thu, 25 Aug 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "In conversation with artificial intelligence: aligning language models with human values": {
            "url": "https://www.deepmind.com/blog/in-conversation-with-artificial-intelligence-aligning-language-models-with-human-values",
            "description": "Our new paper, In conversation with AI: aligning language models with human values, explores a different approach, asking what successful communication between humans and an artificial conversational agent might look like and what values should guide conversation in these contexts.",
            "pubdate": "Tue, 06 Sep 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                9,
                6
            ],
            "email_sent": true
        },
        "How our principles helped define AlphaFolds release": {
            "url": "https://www.deepmind.com/blog/how-our-principles-helped-define-alphafolds-release",
            "description": "Our Operating Principles have come to define both our commitment to prioritising widespread benefit, as well as the areas of research and applications we refuse to pursue. These principles have been at the heart of our decision making since DeepMind was founded, and continue to be refined as the AI landscape changes and grows. They are designed for our role as a research-driven science company and consistent with Google\u2019s AI principles.",
            "pubdate": "Wed, 14 Sep 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                9,
                14
            ],
            "email_sent": true
        },
        "Building safer dialogue agents": {
            "url": "https://www.deepmind.com/blog/building-safer-dialogue-agents",
            "description": "In our latest paper, we introduce Sparrow \u2013 a dialogue agent that\u2019s useful and reduces the risk of unsafe and inappropriate answers. Our agent is designed to talk with a user, answer questions, and search the internet using Google when it\u2019s helpful to look up evidence to inform its responses.",
            "pubdate": "Thu, 22 Sep 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "How undesired goals can arise with correct rewards": {
            "url": "https://www.deepmind.com/blog/how-undesired-goals-can-arise-with-correct-rewards",
            "description": "As we build increasingly advanced artificial intelligence (AI) systems, we want to make sure they don\u2019t pursue undesired goals. Such behaviour in an AI agent is often the result of specification gaming \u2013 exploiting a poor choice of what they are rewarded for. In our latest paper, we explore a more subtle mechanism by which AI systems may unintentionally learn to pursue undesired goals: goal misgeneralisation (GMG).\u00a0GMG occurs when a system's capabilities generalise successfully but its goal does not generalise as desired, so the system competently pursues the wrong goal. Crucially, in contrast to specification gaming, GMG can occur even when the AI system is trained with a correct specification.",
            "pubdate": "Fri, 07 Oct 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                10,
                7
            ],
            "email_sent": true
        },
        "Measuring perception in AI models": {
            "url": "https://www.deepmind.com/blog/measuring-perception-in-ai-models",
            "description": "Perception \u2013 the process of experiencing the world through senses \u2013 is a significant part of intelligence. And building agents with human-level perceptual understanding of the world is a central but challenging task, which is becoming increasingly important in robotics, self-driving cars, personal assistants, medical imaging, and more. So today, we\u2019re introducing the Perception Test, a multimodal benchmark using real-world videos to help evaluate the perception capabilities of a model.",
            "pubdate": "Wed, 12 Oct 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                10,
                12
            ],
            "email_sent": true
        },
        "The pursuit of AI education - past, present and future": {
            "url": "https://www.deepmind.com/blog/the-pursuit-of-ai-education-past-present-and-future",
            "description": "Meet Sylvia Christie, our education partnerships manager who\u2019s played a leading role in expanding our scholarship programme, which is marking its five-year anniversary.",
            "pubdate": "Tue, 08 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                8
            ],
            "email_sent": true
        }
    },
    "Nvidia Blog": {
        "Smart Devices, Smart Manufacturing: Pegatron Taps AI, Digital Twins": {
            "url": "https://blogs.nvidia.com/blog/2022/08/16/pegatron-ai-omniverse/",
            "description": "<p>In the fast-paced field of making the world\u2019s tech devices, Pegatron Corp. initially harnessed AI to gain an edge. Now, it\u2019s on the cusp of creating digital twins to further streamline its efficiency. Whether or not they\u2019re familiar with the name, most people have probably used smartphones, tablets, Wi-Fi routers or other products that Taiwan-based <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/16/pegatron-ai-omniverse/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/16/pegatron-ai-omniverse/\" rel=\"nofollow\">Smart Devices, Smart Manufacturing: Pegatron Taps AI, Digital Twins</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 16 Aug 2022 15:00:59 +0000",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "AI Shows the Way: Seoul Robotics Helps Cars Move, Park on Their Own": {
            "url": "https://blogs.nvidia.com/blog/2022/08/16/seoul-robotics-autonomy-through-infrastructure/",
            "description": "<p>Imagine driving a car \u2014 one without self-driving capabilities \u2014 to a mall, airport or parking garage, and using an app to have the car drive off to park itself. Software company Seoul Robotics is using NVIDIA technology to make this possible \u2014 turning non-autonomous cars into self-driving vehicles. Headquartered in Korea, the company\u2019s initial <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/16/seoul-robotics-autonomy-through-infrastructure/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/16/seoul-robotics-autonomy-through-infrastructure/\" rel=\"nofollow\">AI Shows the Way: Seoul Robotics Helps Cars Move, Park on Their Own</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 16 Aug 2022 15:00:43 +0000",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "Digital Art Professor Kate Parsons Inspires Next Generation of Creators This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/08/16/in-the-nvidia-studio-august-16/",
            "description": "<p>Many artists can edit a video, paint a picture or build a model \u2014 but transforming one\u2019s imagination into stunning creations can now involve breakthrough design technologies. Kate Parsons, a digital art professor at Pepperdine University and this week\u2019s featured In the NVIDIA Studio artist, helped bring a music video for How Do I Get to Invincible to life using virtual reality and NVIDIA GeForce RTX GPUs. </p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/16/in-the-nvidia-studio-august-16/\" rel=\"nofollow\">Digital Art Professor Kate Parsons Inspires Next Generation of Creators This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 16 Aug 2022 13:00:48 +0000",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "Immunai Co-Founder Luis Voloch on Using Deep Learning to Develop New Drugs": {
            "url": "https://blogs.nvidia.com/blog/2022/08/17/immunai-luis-voloch/",
            "description": "<p>Mapping the immune system could lead to the creation of drugs that help our bodies win the fight against cancer and other diseases. That\u2019s the big idea behind immunotherapy. The problem: the immune system is incredibly complex. Enter Immunai, a biotech company that\u2019s using cutting-edge genomics &#38; ML technology to map the human immune system <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/17/immunai-luis-voloch/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/17/immunai-luis-voloch/\" rel=\"nofollow\">Immunai Co-Founder Luis Voloch on Using Deep Learning to Develop New Drugs</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 17 Aug 2022 13:00:41 +0000",
            "pubdate_parsed": [
                2022,
                8,
                17
            ],
            "email_sent": true
        },
        "Startup Digs Into Public Filings With GPU-Driven Machine Learning to Serve Up Alternative Financial Data Services": {
            "url": "https://blogs.nvidia.com/blog/2022/08/18/gpu-driven-machine-learning-alternative-financial-data-services/",
            "description": "<p>When Rachel Carpenter and Joseph French founded Intrinio a decade ago, the fintech revolution had only just begun. But they saw an opportunity to apply machine learning to vast amounts of financial filings to create an alternative data provider among the giants. The startup, based in St. Petersburg, Fla., delivers financial data to hedge funds, <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/18/gpu-driven-machine-learning-alternative-financial-data-services/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/18/gpu-driven-machine-learning-alternative-financial-data-services/\" rel=\"nofollow\">Startup Digs Into Public Filings With GPU-Driven Machine Learning to Serve Up Alternative Financial Data Services</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 16:06:46 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Boldly Go: Discover New Frontiers in AI-Powered Transportation at GTC": {
            "url": "https://blogs.nvidia.com/blog/2022/08/18/discover-frontiers-ai-autonomous-vehicles-gtc/",
            "description": "<p>AI and the metaverse are revolutionizing every aspect of the way we live, work and play \u2014 including how we move. Leaders in the automotive and technology industries will come together at NVIDIA GTC to discuss the newest breakthroughs driving intelligent vehicles, whether in the real world or in simulation. The virtual conference, which runs <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/18/discover-frontiers-ai-autonomous-vehicles-gtc/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/18/discover-frontiers-ai-autonomous-vehicles-gtc/\" rel=\"nofollow\">Boldly Go: Discover New Frontiers in AI-Powered Transportation at GTC</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 15:32:48 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Startups Vision AI Software Trains Itself  in One Hour  to Detect Manufacturing Defects in Real Time": {
            "url": "https://blogs.nvidia.com/blog/2022/08/18/covision-visual-inspection-for-manufacturing/",
            "description": "<p>Cameras have been deployed in factories for over a decade \u2014 so why, Franz Tschimben wondered, hasn\u2019t automated visual inspection yet become the worldwide standard? This question motivated Tschimben and his colleagues to found Covision Quality, an AI-based visual-inspection software startup that uses NVIDIA technology to transform end-of-line defect detection for the manufacturing industry. \u201cThe <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/18/covision-visual-inspection-for-manufacturing/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/18/covision-visual-inspection-for-manufacturing/\" rel=\"nofollow\">Startup\u2019s Vision AI Software Trains Itself \u2014 in One Hour \u2014 to Detect Manufacturing Defects in Real Time</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 15:00:39 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Easy A: GeForce NOW Brings Higher Resolution and Frame Rates for Browser Streaming on PC": {
            "url": "https://blogs.nvidia.com/blog/2022/08/18/geforce-now-thursday-august-18/",
            "description": "<p>Class is in session this GFN Thursday as GeForce NOW makes the up-grade with support for higher resolutions and frame rates in Chrome browser on PC. It\u2019s the easiest way to spice up a boring study session. When the lecture is over, dive into the six games joining the GeForce NOW library this week, where <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/18/geforce-now-thursday-august-18/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/18/geforce-now-thursday-august-18/\" rel=\"nofollow\">Easy A: GeForce NOW Brings Higher Resolution and Frame Rates for Browser Streaming on PC</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 13:00:55 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "NVIDIA to Share New Details on Grace CPU, Hopper GPU, NVLink Switch, Jetson Orin Module at Hot Chips": {
            "url": "https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/",
            "description": "<p>In four talks over two days, senior NVIDIA engineers will describe innovations in accelerated computing for modern data centers and systems at the edge of the network. Speaking at a virtual Hot Chips event, an annual gathering of processor and system architects, they\u2019ll disclose performance numbers and other technical details for NVIDIA\u2019s first server CPU, <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/19/grace-hopper-nvswitch-hot-chips/\" rel=\"nofollow\">NVIDIA to Share New Details on Grace CPU, Hopper GPU, NVLink Switch, Jetson Orin Module at Hot Chips</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Fri, 19 Aug 2022 15:00:33 +0000",
            "pubdate_parsed": [
                2022,
                8,
                19
            ],
            "email_sent": true
        },
        "Meet the Omnivore: Startup in3D Turns Selfies Into Talking, Dancing Avatars With NVIDIA Omniverse": {
            "url": "https://blogs.nvidia.com/blog/2022/08/19/omniverse-developer-in3d/",
            "description": "<p>Imagine taking a selfie and using it to get a moving, talking, customizable 3D avatar of yourself in just seconds.\u00a0</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/19/omniverse-developer-in3d/\" rel=\"nofollow\">Meet the Omnivore: Startup in3D Turns Selfies Into Talking, Dancing Avatars With NVIDIA Omniverse</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Fri, 19 Aug 2022 15:00:02 +0000",
            "pubdate_parsed": [
                2022,
                8,
                19
            ],
            "email_sent": true
        },
        "An AI-Enabled Drone Could Soon Become Every Rhino Poachers Horn Enemy": {
            "url": "https://blogs.nvidia.com/blog/2022/08/22/ai-drone-rhino-poachers/",
            "description": "<p>Watching out for the nearly-extinct two-ton beasts may be the ultimate example of a job best done remotely.</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/22/ai-drone-rhino-poachers/\" rel=\"nofollow\">An AI-Enabled Drone Could Soon Become Every Rhino Poacher&#8217;s\u2026 Horn Enemy</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Mon, 22 Aug 2022 13:00:59 +0000",
            "pubdate_parsed": [
                2022,
                8,
                22
            ],
            "email_sent": true
        },
        "Learn How Leading Companies Are Building AI Centers of Excellence, at NVIDIA GTC": {
            "url": "https://blogs.nvidia.com/blog/2022/08/23/gtc-ai-centers-of-excellence/",
            "description": "<p>AI Centers of Excellence are organizational units dedicated to implementing a company-wide AI vision. They help identify business use cases, create an implementation roadmap, accelerate adoption, assess impact and more. NVIDIA GTC, a global conference on AI and the metaverse, brings together the world\u2019s top business and technology leaders who\u2019ve embraced artificial intelligence to transform <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/23/gtc-ai-centers-of-excellence/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/23/gtc-ai-centers-of-excellence/\" rel=\"nofollow\">Learn How Leading Companies Are Building AI Centers of Excellence, at NVIDIA GTC</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 23 Aug 2022 16:00:59 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Shelter From the Storm: AI Helps Gauge Catastrophe Risks": {
            "url": "https://blogs.nvidia.com/blog/2022/08/23/ai-catastrophe-risk-masterful/",
            "description": "<p>Floods in Kentucky and wildfires in California are the kinds of disasters companies of all sorts are trying to address with AI. Tom Rikert, co-founder and CEO of San Francisco-based startup Masterful AI, is one of many experts helping them manage catastrophe risk. In the U.S. alone, the National Association of Insurance Commissioners estimates that <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/23/ai-catastrophe-risk-masterful/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/23/ai-catastrophe-risk-masterful/\" rel=\"nofollow\">Shelter From the Storm: AI Helps Gauge Catastrophe Risks</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 23 Aug 2022 15:00:55 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Predict, Detect, Mitigate: AI for Climate Science Takes the Stage at NVIDIA GTC": {
            "url": "https://blogs.nvidia.com/blog/2022/08/23/ai-for-climate-science-gtc/",
            "description": "<p>Recent AI advances enable modeling of weather forecasting 4-5 magnitudes faster than traditional computing methods. The brightest leaders, researchers and developers in climate science, high performance computing and AI will discuss such technology breakthroughs \u2014 and how they can help foster a greener Earth \u2014 at NVIDIA GTC. The virtual conference, running Sept. 19-22, also <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/23/ai-for-climate-science-gtc/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/23/ai-for-climate-science-gtc/\" rel=\"nofollow\">Predict, Detect, Mitigate: AI for Climate Science Takes the Stage at NVIDIA GTC</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 23 Aug 2022 15:00:18 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "3D Artists Reimagine, Remaster Iconic European Architecture This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/08/23/in-the-nvidia-studio-august-23/",
            "description": "<p>A triple threat steps In the NVIDIA Studio this week: a tantalizing trio of talented 3D artists who each reimagined and remastered classic European buildings with individualistic flair. </p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/23/in-the-nvidia-studio-august-23/\" rel=\"nofollow\">3D Artists Reimagine, Remaster Iconic European Architecture This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 23 Aug 2022 13:00:35 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "3D Artist Creates Blooming, Generative Sculptures With NVIDIA RTX and AI": {
            "url": "https://blogs.nvidia.com/blog/2022/08/25/generative-art-rtx-ai/",
            "description": "<p>Looking for a change of art? Try using AI \u2014 that\u2019s what 3D artist Nikola Damjanov is doing. Based in Serbia, Damjanov has over 15 years of experience in the graphics industry, from making 3D models and animations to creating high-quality visual effects for music videos and movies. Now an artist at game developer company <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/25/generative-art-rtx-ai/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/25/generative-art-rtx-ai/\" rel=\"nofollow\">3D Artist Creates Blooming, Generative Sculptures With NVIDIA RTX and AI</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 25 Aug 2022 16:00:28 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "Fintech Company Blocks Fraud Attacks for Financial Institutions With AI and NVIDIA GPUs": {
            "url": "https://blogs.nvidia.com/blog/2022/08/25/featurespace-blocks-financial-fraud/",
            "description": "<p>E-commerce sales have skyrocketed as more people shop remotely, spurred by the pandemic. But this surge has also led fraudsters to use the opportunity to scam retailers and customers, according to David Sutton, director of analytical technology at fintech company Featurespace. The company, headquartered in the U.K., has developed AI-powered technology to increase the speed <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/25/featurespace-blocks-financial-fraud/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/25/featurespace-blocks-financial-fraud/\" rel=\"nofollow\">Fintech Company Blocks Fraud Attacks for Financial Institutions With AI and NVIDIA GPUs</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 25 Aug 2022 15:00:22 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "GFN Thursday Adds Saints Row, Genshin Impact on Mobile With Touch Controls": {
            "url": "https://blogs.nvidia.com/blog/2022/08/25/geforce-now-thursday-august-25/",
            "description": "<p>Some weeks, GFN Thursday reveals new or unique features. Other weeks, it\u2019s a cool reward. And every week, it offers its members new games. This week, it\u2019s all of the above. First, Saints Row marches into GeForce NOW. Be your own boss in the new reboot of the classic open-world criminal adventure series, now available <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/25/geforce-now-thursday-august-25/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/25/geforce-now-thursday-august-25/\" rel=\"nofollow\">GFN Thursday Adds \u2018Saints Row,\u2019 \u2018Genshin Impact\u2019 on Mobile With Touch Controls</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 25 Aug 2022 13:00:02 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "OBS Studio to Release Software Update 28.0 With NVIDIA Broadcast Features In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/08/31/in-the-nvidia-studio-august-31/",
            "description": "<p>In the NVIDIA Studio celebrates the Open Broadcaster Software (OBS) Studio\u2019s 10th anniversary and its 28.0 software release. Plus, popular streamer WATCHHOLLIE shares how she uses OBS and a GeForce RTX 3080 GPU in a single-PC setup to elevate her livestreams.</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/31/in-the-nvidia-studio-august-31/\" rel=\"nofollow\">OBS Studio to Release Software Update 28.0 With NVIDIA Broadcast Features \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 31 Aug 2022 13:00:44 +0000",
            "pubdate_parsed": [
                2022,
                8,
                31
            ],
            "email_sent": true
        },
        "Rendered.ai Founder and CEO Nathan Kundtz on Using AI to Build Better AI": {
            "url": "https://blogs.nvidia.com/blog/2022/08/31/rendered-ai/",
            "description": "<p>Data is the fuel that makes artificial intelligence run. Training machine learning and AI systems requires data. And the quality of datasets has a big impact on the systems\u2019 results. But compiling quality real-world data for AI and ML can be difficult and expensive. That\u2019s where synthetic data comes in. The guest for this week\u2019s <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/08/31/rendered-ai/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/08/31/rendered-ai/\" rel=\"nofollow\">Rendered.ai Founder and CEO Nathan Kundtz on Using AI to Build Better AI</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 31 Aug 2022 13:00:41 +0000",
            "pubdate_parsed": [
                2022,
                8,
                31
            ],
            "email_sent": true
        },
        "GFN Thursday Slides Into September With 22 New Games": {
            "url": "https://blogs.nvidia.com/blog/2022/09/01/geforce-now-thursday-september-1/",
            "description": "<p>We\u2019d wake you up when September ends, but then you\u2019d miss out on a whole new set of games coming to GeForce NOW. Gear up for 22 games joining the GeForce NOW library, with 19 day-and-date releases including action role-playing game Steelrising. Playing them all will take some serious strategy. And build the perfect Minifigure <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/01/geforce-now-thursday-september-1/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/01/geforce-now-thursday-september-1/\" rel=\"nofollow\">GFN Thursday Slides Into September With 22 New Games</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 01 Sep 2022 13:00:47 +0000",
            "pubdate_parsed": [
                2022,
                9,
                1
            ],
            "email_sent": true
        },
        "Ridiculously Realistic Renders Rule This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/09/06/in-the-nvidia-studio-september-6/",
            "description": "<p>Viral creator turned NVIDIA 3D artist Lorenzo Drago takes viewers on a jaw-dropping journey through Toyama, Japan\u2019s Etch\u016b-Daimon Station this week In the NVIDIA Studio. </p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/06/in-the-nvidia-studio-september-6/\" rel=\"nofollow\">Ridiculously Realistic Renders Rule This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 06 Sep 2022 13:00:27 +0000",
            "pubdate_parsed": [
                2022,
                9,
                6
            ],
            "email_sent": true
        },
        "GeForce NOW Supports Over 1,400 Games Streaming Instantly": {
            "url": "https://blogs.nvidia.com/blog/2022/09/08/geforce-now-thursday-september-8/",
            "description": "<p>This GFN Thursday marks a milestone: With the addition of six new titles this week, more than 1,400 games are now available to stream from the GeForce NOW library. Plus, GeForce NOW members streaming to supported Smart TVs from Samsung and LG can get into their games faster with an improved user interface. Your Games, <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/08/geforce-now-thursday-september-8/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/08/geforce-now-thursday-september-8/\" rel=\"nofollow\">GeForce NOW Supports Over 1,400 Games Streaming Instantly</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 08 Sep 2022 13:00:55 +0000",
            "pubdate_parsed": [
                2022,
                9,
                8
            ],
            "email_sent": true
        },
        "Concept Designer Ben Mauro Delivers Epic 3D Trailer Huxley This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/09/13/in-the-nvidia-studio-september-13/",
            "description": "<p>The gripping sci-fi comic Huxley was brought to life in an action-packed 3D trailer full of excitement and intrigue this week In the NVIDIA Studio.</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/13/in-the-nvidia-studio-september-13/\" rel=\"nofollow\">Concept Designer Ben Mauro Delivers Epic 3D Trailer \u2018Huxley\u2019 This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 13 Sep 2022 13:00:23 +0000",
            "pubdate_parsed": [
                2022,
                9,
                13
            ],
            "email_sent": true
        },
        "GFN Thursday Delivers Seven New Games This Week": {
            "url": "https://blogs.nvidia.com/blog/2022/09/15/geforce-now-thursday-september-15/",
            "description": "<p>TGIGFNT: thank goodness it\u2019s GFN Thursday. Start your weekend early with seven new games joining the GeForce NOW library of over 1,400 titles. Whether it\u2019s streaming on an older-than-the-dinosaurs PC, a Mac that normally couldn\u2019t dream of playing PC titles, or mobile devices \u2013 it\u2019s all possible to play your way thanks to GeForce NOW. <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/15/geforce-now-thursday-september-15/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/15/geforce-now-thursday-september-15/\" rel=\"nofollow\">GFN Thursday Delivers Seven New Games This Week</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 15 Sep 2022 13:00:24 +0000",
            "pubdate_parsed": [
                2022,
                9,
                15
            ],
            "email_sent": true
        },
        "A Podcast With Teeth: How Overjet Brings AI to Dentists Offices": {
            "url": "https://blogs.nvidia.com/blog/2022/09/21/ai-podcast-overjet/",
            "description": "<p>Dentists get a bad rap. Dentists also get more people out of more aggravating pain than just about anyone. Which is why the more technology dentists have, the better. Overjet, a member of the NVIDIA Inception program for startups, is moving fast to bring AI to dentists\u2019 offices. On this episode of the NVIDIA AI <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/21/ai-podcast-overjet/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/21/ai-podcast-overjet/\" rel=\"nofollow\">A Podcast With Teeth: How Overjet Brings AI to Dentists\u2019 Offices</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 21 Sep 2022 13:00:46 +0000",
            "pubdate_parsed": [
                2022,
                9,
                21
            ],
            "email_sent": true
        },
        "Go Hands On: Logitech G CLOUD Launches With Support for GeForce NOW": {
            "url": "https://blogs.nvidia.com/blog/2022/09/22/geforce-now-thursday-september-22/",
            "description": "<p>When it rains, it pours. And this GFN Thursday brings a downpour of news for GeForce NOW members. The Logitech G CLOUD is the latest gaming handheld device to support GeForce NOW, giving members a brand new way to keep the gaming going. But that\u2019s not all: Portal with RTX joins GeForce NOW in November, <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/22/geforce-now-thursday-september-22/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/22/geforce-now-thursday-september-22/\" rel=\"nofollow\">Go Hands On: Logitech G CLOUD Launches With Support for GeForce NOW</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 22 Sep 2022 13:00:48 +0000",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "Continental and AEye Join NVIDIA DRIVE Sim Sensor Ecosystem, Providing Rich Capabilities for AV Development": {
            "url": "https://blogs.nvidia.com/blog/2022/09/22/continental-aeye-drive-sim-sensor-ecosystem/",
            "description": "<p>Autonomous vehicle sensors require the same rigorous testing and validation as the car itself, and one simulation platform is up to the task. Global tier-1 supplier Continental and software-defined lidar maker AEye announced this week at NVIDIA GTC that they will migrate their intelligent lidar sensor model into NVIDIA DRIVE Sim. The companies are the <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/22/continental-aeye-drive-sim-sensor-ecosystem/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/22/continental-aeye-drive-sim-sensor-ecosystem/\" rel=\"nofollow\">Continental and AEye Join NVIDIA DRIVE Sim Sensor Ecosystem, Providing Rich Capabilities for AV Development</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 22 Sep 2022 12:41:01 +0000",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "World-Class: NVIDIA Research Builds AI Model to Populate Virtual Worlds With 3D Objects, Characters": {
            "url": "https://blogs.nvidia.com/blog/2022/09/23/3d-generative-ai-research-virtual-worlds/",
            "description": "<p>The massive virtual worlds created by growing numbers of companies and creators could be more easily populated with a diverse array of 3D buildings, vehicles, characters and more \u2014 thanks to a new AI model from NVIDIA Research. Trained using only 2D images, NVIDIA GET3D generates 3D shapes with high-fidelity textures and complex geometric details. <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/23/3d-generative-ai-research-virtual-worlds/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/23/3d-generative-ai-research-virtual-worlds/\" rel=\"nofollow\">World-Class: NVIDIA Research Builds AI Model to Populate Virtual Worlds With 3D Objects, Characters</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Fri, 23 Sep 2022 13:00:29 +0000",
            "pubdate_parsed": [
                2022,
                9,
                23
            ],
            "email_sent": true
        },
        "Video Virtuoso Sabour Amirazodi Shares AI-Powered Editing Tips This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/09/28/in-the-nvidia-studio-september-28/",
            "description": "<p>NVIDIA artist Sabour Amirazodi demonstrates his video editing workflows featuring AI this week in a special edition of In the NVIDIA Studio.</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/28/in-the-nvidia-studio-september-28/\" rel=\"nofollow\">Video Virtuoso Sabour Amirazodi Shares AI-Powered Editing Tips This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 28 Sep 2022 13:00:28 +0000",
            "pubdate_parsed": [
                2022,
                9,
                28
            ],
            "email_sent": true
        },
        "All This and Mor-a Are Yours With Exclusive Genshin Impact GeForce NOW Membership Reward": {
            "url": "https://blogs.nvidia.com/blog/2022/09/29/geforce-now-thursday-september-29/",
            "description": "<p>It\u2019s good to be a GeForce NOW member. Genshin Impact\u2019s new Version 3.1 update launches this GFN Thursday, just in time for the game\u2019s second anniversary. Even better: GeForce NOW members can get an exclusive starter pack reward, perfect for their first steps in HoYoverse\u2019s open-world adventure, action role-playing game. And don\u2019t forget the nine <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/09/29/geforce-now-thursday-september-29/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/09/29/geforce-now-thursday-september-29/\" rel=\"nofollow\">All This and Mor-a Are Yours With Exclusive \u2018Genshin Impact\u2019 GeForce NOW Membership Reward</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 29 Sep 2022 13:00:51 +0000",
            "pubdate_parsed": [
                2022,
                9,
                29
            ],
            "email_sent": true
        },
        "Creator EposVox Shares Streaming Lessons, Successes This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/10/04/in-the-nvidia-studio-october-04/",
            "description": "<p>TwitchCon \u2014 the world\u2019s top gathering of live streamers \u2013 kicks off Friday with the new line of GeForce RTX 40 Series GPUs bringing incredible new technology \u2014 from AV1 to AI \u2014 to elevate live streams for aspiring and professional Twitch creators alike.  </p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/04/in-the-nvidia-studio-october-04/\" rel=\"nofollow\">Creator EposVox Shares Streaming Lessons, Successes This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 04 Oct 2022 13:00:06 +0000",
            "pubdate_parsed": [
                2022,
                10,
                4
            ],
            "email_sent": true
        },
        "Fall Into October With 25 New Games Streaming on GeForce NOW": {
            "url": "https://blogs.nvidia.com/blog/2022/10/06/geforce-now-thursday-octobter-6/",
            "description": "<p>Cooler weather, the changing colors of the leaves, the needless addition of pumpkin spice to just about everything, and discount Halloween candy are just some things to look forward to in the fall. GeForce NOW members can add one more thing to the list \u2014 25 games joining the cloud gaming library in October, including <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/06/geforce-now-thursday-octobter-6/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/06/geforce-now-thursday-octobter-6/\" rel=\"nofollow\">Fall Into October With 25 New Games Streaming on GeForce NOW</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 06 Oct 2022 13:00:04 +0000",
            "pubdate_parsed": [
                2022,
                10,
                6
            ],
            "email_sent": true
        },
        "GeForce RTX 4090 GPU Arrives, Enabling New World-Building Possibilities for 3D Artists This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/10/12/in-the-nvidia-studio-october-12/",
            "description": "<p>This week 'In the NVIDIA Studio' creators can now pick up the GeForce RTX 4090 GPU, available from top add-in card providers including ASUS, Colorful, Gainward, Galaxy, GIGABYTE, INNO3D, MSI, Palit, PNY and ZOTAC, as well as from system integrators and builders worldwide. </p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/12/in-the-nvidia-studio-october-12/\" rel=\"nofollow\">GeForce RTX 4090 GPU Arrives, Enabling New World-Building Possibilities for 3D Artists This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 12 Oct 2022 13:00:24 +0000",
            "pubdate_parsed": [
                2022,
                10,
                12
            ],
            "email_sent": true
        },
        "GeForce NOW Streams High-Res, 120-FPS PC Gaming to Worlds First Cloud Gaming Chromebooks": {
            "url": "https://blogs.nvidia.com/blog/2022/10/13/geforce-now-thursday-oct-13/",
            "description": "<p>High-end PC gaming arrives on more devices this GFN Thursday. GeForce NOW RTX 3080 members can now stream their favorite PC games at up to 1600p and 120 frames per second in a Chrome browser. No downloads, no installs, just victory. Even better, NVIDIA has worked with Google to support the newest Chromebooks, which are <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/13/geforce-now-thursday-oct-13/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/13/geforce-now-thursday-oct-13/\" rel=\"nofollow\">GeForce NOW Streams High-Res, 120-FPS PC Gaming to World\u2019s First Cloud Gaming Chromebooks</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 13 Oct 2022 13:00:02 +0000",
            "pubdate_parsed": [
                2022,
                10,
                13
            ],
            "email_sent": true
        },
        "AI Supercomputer to Power $200 Million Oregon State University Innovation Complex": {
            "url": "https://blogs.nvidia.com/blog/2022/10/14/ai-supercomputer-oregon-state/",
            "description": "<p>As a civil engineer, Scott Ashford used explosives to make the ground under Japan\u2019s Sendai airport safer in an earthquake. Now, as the dean of the engineering college at Oregon State University, he\u2019s at ground zero of another seismic event. In its biggest fundraising celebration in nearly a decade, Oregon State announced plans today for <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/14/ai-supercomputer-oregon-state/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/14/ai-supercomputer-oregon-state/\" rel=\"nofollow\">AI Supercomputer to Power $200 Million Oregon State University Innovation Complex</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Sat, 15 Oct 2022 04:30:04 +0000",
            "pubdate_parsed": [
                2022,
                10,
                15
            ],
            "email_sent": true
        },
        "Souped-Up Auto Quotes: ProovStation Delivers GPU-Driven AI Appraisals": {
            "url": "https://blogs.nvidia.com/blog/2022/10/17/proovstation-gpu-ai-appraisals/",
            "description": "<p>Vehicle appraisals are getting souped up with a GPU-accelerated AI overhaul. ProovStation, a four-year-old startup based in Lyon, France, is taking on the ambitious computer-vision quest of automating vehicle inspection and repair estimates, aiming AI-driven super-high-resolution stations at businesses worldwide. It recently launched three of its state-of-the-art vehicle inspection scanners at French retail giant Carrefour\u2019s <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/17/proovstation-gpu-ai-appraisals/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/17/proovstation-gpu-ai-appraisals/\" rel=\"nofollow\">Souped-Up Auto Quotes: ProovStation Delivers GPU-Driven AI Appraisals</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 18 Oct 2022 03:00:22 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Adobe MAX Kicks Off With Creative App Updates and 3D Artist Anna Natter Impresses This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/10/18/in-the-nvidia-studio-october-18/",
            "description": "<p>Editor\u2019s note: This post is part of our weekly In the NVIDIA Studio series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how NVIDIA Studio technology improves creative workflows. In the coming weeks, we\u2019ll be deep diving on new GeForce RTX 40 Series GPU features, technologies and resources, and how they dramatically <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/18/in-the-nvidia-studio-october-18/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/18/in-the-nvidia-studio-october-18/\" rel=\"nofollow\">Adobe MAX Kicks Off With Creative App Updates and 3D Artist Anna Natter Impresses This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 18 Oct 2022 13:00:27 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Get in Touch With New Mobile Gaming Controls on GeForce NOW": {
            "url": "https://blogs.nvidia.com/blog/2022/10/20/geforce-now-thursday-oct-20/",
            "description": "<p>GeForce NOW expands touch control support to 13 more games this GFN Thursday. That means it\u2019s easier than ever to take PC gaming on the go using mobile devices and tablets. The new \u201cMobile Touch Controls\u201d row in the GeForce NOW app is the easiest way for members to find which games put the action <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/20/geforce-now-thursday-oct-20/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/20/geforce-now-thursday-oct-20/\" rel=\"nofollow\">Get in Touch With New Mobile Gaming Controls on GeForce NOW</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 20 Oct 2022 13:00:43 +0000",
            "pubdate_parsed": [
                2022,
                10,
                20
            ],
            "email_sent": true
        },
        "3D Artist SouthernShotty Creates Wholesome Characters This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/10/25/in-the-nvidia-studio-october-25/",
            "description": "<p>This week 'In the NVIDIA Studio,' we\u2019re highlighting 3D and motion graphics artist SouthernShotty \u2014 and scenes from his soon-to-be released short film, Watermelon Girl.\u00a0</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/25/in-the-nvidia-studio-october-25/\" rel=\"nofollow\">3D Artist SouthernShotty Creates Wholesome Characters This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Tue, 25 Oct 2022 13:00:08 +0000",
            "pubdate_parsed": [
                2022,
                10,
                25
            ],
            "email_sent": true
        },
        "Make Gaming a Priority: Special Membership Discount Hits GeForce NOW for Limited Time": {
            "url": "https://blogs.nvidia.com/blog/2022/10/27/geforce-now-thursday-oct-27/",
            "description": "<p>This spook-tacular Halloween edition of GFN Thursday features a special treat: 40% off a six-month GeForce NOW Priority Membership \u2014 get it for just $29.99 for a limited time. Several sweet new games are also joining the GeForce NOW library. Creatures of the night can now stream vampire survival game V Rising from the cloud. <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/27/geforce-now-thursday-oct-27/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/27/geforce-now-thursday-oct-27/\" rel=\"nofollow\">Make Gaming a Priority: Special Membership Discount Hits GeForce NOW for Limited Time</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 27 Oct 2022 13:00:06 +0000",
            "pubdate_parsed": [
                2022,
                10,
                27
            ],
            "email_sent": true
        },
        "GeForce RTX 40 Series Receives Massive Creator App Benefits This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2022/10/31/in-the-nvidia-studio-october-31/",
            "description": "<p>Artists deploying the critically acclaimed GeForce RTX 4090 GPUs are primed to receive significant performance boosts in key creative apps. Plus, a special spook-tober edition of In the NVIDIA Studio features two talented 3D artists and their Halloween-themed creations this week.</p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/31/in-the-nvidia-studio-october-31/\" rel=\"nofollow\">GeForce RTX 40 Series Receives Massive Creator App Benefits This Week \u2018In the NVIDIA Studio\u2019</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Mon, 31 Oct 2022 13:00:03 +0000",
            "pubdate_parsed": [
                2022,
                10,
                31
            ],
            "email_sent": true
        },
        "Think Fast: Lotus Eletre Tops Charts in Driving and AI Compute Speeds, Powered by NVIDIA DRIVE Orin": {
            "url": "https://blogs.nvidia.com/blog/2022/10/31/lotus-eletre-ai-drive-orin/",
            "description": "<p>One of the biggest names in racing is going even bigger. Performance automaker Lotus launched its first SUV, the Eletre, earlier this week. The fully electric vehicle sacrifices little in terms of speed and outperforms when it comes to technology. It features an immersive digital cockpit, lengthy battery range of up to 370 miles and <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/10/31/lotus-eletre-ai-drive-orin/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/10/31/lotus-eletre-ai-drive-orin/\" rel=\"nofollow\">Think Fast: Lotus Eletre Tops Charts in Driving and AI Compute Speeds, Powered by NVIDIA DRIVE Orin</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Mon, 31 Oct 2022 12:38:05 +0000",
            "pubdate_parsed": [
                2022,
                10,
                31
            ],
            "email_sent": true
        },
        "Check Out 26 New Games Streaming on GeForce NOW in November": {
            "url": "https://blogs.nvidia.com/blog/2022/11/03/geforce-now-thursday-nov-3/",
            "description": "<p>It\u2019s a brand new month, which means this GFN Thursday is all about the new games streaming from the cloud. In November, 26 titles will join the GeForce NOW library. Kick off with 11 additions this week, like Total War: THREE KINGDOMS and new content updates for Genshin Impact and Apex Legends. Plus, leading 5G <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/11/03/geforce-now-thursday-nov-3/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/11/03/geforce-now-thursday-nov-3/\" rel=\"nofollow\">Check Out 26 New Games Streaming on GeForce NOW in November</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 03 Nov 2022 13:00:15 +0000",
            "pubdate_parsed": [
                2022,
                11,
                3
            ],
            "email_sent": true
        },
        "Give the Gift of Gaming With GeForce NOW Gift Cards": {
            "url": "https://blogs.nvidia.com/blog/2022/11/10/geforce-now-thursday-nov-10/",
            "description": "<p>The holiday season is approaching, and GeForce NOW has everyone covered. This GFN Thursday brings an easy way to give the gift of gaming with GeForce NOW gift cards, for yourself or for a gamer in your life. Plus, stream 10 new games from the cloud this week, including the first story downloadable content (DLC) <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/11/10/geforce-now-thursday-nov-10/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/11/10/geforce-now-thursday-nov-10/\" rel=\"nofollow\">Give the Gift of Gaming With GeForce NOW Gift Cards</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 10 Nov 2022 14:00:33 +0000",
            "pubdate_parsed": [
                2022,
                11,
                10
            ],
            "email_sent": true
        },
        "Qubit Pharmaceuticals Accelerates Drug Discovery With Hybrid Quantum Computing": {
            "url": "https://blogs.nvidia.com/blog/2022/11/30/qubit-pharmaceuticals-accelerates-drug-discovery-quantum-computing/",
            "description": "<p>The promise of quantum computing is to solve unsolvable problems. And companies are already making headway with hybrid approaches \u2014 those that combine classical and quantum computing \u2014 to tackle challenges like drug discovery for incurable diseases. By accelerating drug molecule simulation and modeling with hybrid quantum computing, startup Qubit Pharmaceuticals is significantly reducing the <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/11/30/qubit-pharmaceuticals-accelerates-drug-discovery-quantum-computing/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/11/30/qubit-pharmaceuticals-accelerates-drug-discovery-quantum-computing/\" rel=\"nofollow\">Qubit Pharmaceuticals Accelerates Drug Discovery With Hybrid Quantum Computing</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 30 Nov 2022 08:01:00 +0000",
            "pubdate_parsed": [
                2022,
                11,
                30
            ],
            "email_sent": true
        },
        "Hittin the Sim: NVIDIAs Matt Cragun on Conditioning Autonomous Vehicles in Simulation": {
            "url": "https://blogs.nvidia.com/blog/2022/12/07/autonomous-vehicles-simulation/",
            "description": "<p>Training, testing and validating autonomous vehicles requires a continuous pipeline \u2014 or data factory \u2014 to introduce new scenarios and refine deep neural networks. A key component of this process is simulation. AV developers can test a virtually limitless number of scenarios, repeatably and at scale, with high-fidelity, physically based simulation. And like much of <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/12/07/autonomous-vehicles-simulation/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/12/07/autonomous-vehicles-simulation/\" rel=\"nofollow\">Hittin\u2019 the Sim: NVIDIA\u2019s Matt Cragun on Conditioning Autonomous Vehicles in Simulation</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Wed, 07 Dec 2022 13:00:03 +0000",
            "pubdate_parsed": [
                2022,
                12,
                7
            ],
            "email_sent": true
        },
        "23 and AV: Transportation Industry to Drive Into Metaverse, Cloud Technologies": {
            "url": "https://blogs.nvidia.com/blog/2022/12/08/2023-av-transportation-industry-metaverse-cloud/",
            "description": "<p>As the autonomous vehicle industry enters the next year, it will start navigating into even greater technology frontiers. Next-generation vehicles won\u2019t just be defined by autonomous driving capabilities. Everything from the design and production process to the in-vehicle experience is entering a new era of digitization, efficiency, safety and intelligence. These trends arrive after a <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2022/12/08/2023-av-transportation-industry-metaverse-cloud/\">Read article &#62;</a></p>\n<p>The post <a href=\"https://blogs.nvidia.com/blog/2022/12/08/2023-av-transportation-industry-metaverse-cloud/\" rel=\"nofollow\">\u201823 and AV: Transportation Industry to Drive Into Metaverse, Cloud Technologies</a> appeared first on <a href=\"https://blogs.nvidia.com\" rel=\"nofollow\">NVIDIA Blog</a>.</p>",
            "pubdate": "Thu, 08 Dec 2022 12:00:28 +0000",
            "pubdate_parsed": [
                2022,
                12,
                8
            ],
            "email_sent": true
        },
        "Booked for Brilliance: Swedens National Library Turns Page to AI to Parse Centuries of Data": {
            "url": "https://blogs.nvidia.com/blog/2023/01/23/sweden-library-ai-open-source/",
            "description": "For the past 500 years, the National Library of Sweden has collected virtually every word published in Swedish, from priceless medieval manuscripts to present-day pizza menus. Thanks to a centuries-old law that requires a copy of everything published in Swedish to be submitted to the library \u2014 also known as Kungliga biblioteket, or KB \u2014 <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/01/23/sweden-library-ai-open-source/\">Read article &#62;</a>",
            "pubdate": "Mon, 23 Jan 2023 08:01:40 +0000",
            "pubdate_parsed": [
                2023,
                1,
                23
            ],
            "email_sent": true
        },
        "NVIDIA CEO Ignites AI Conversation in Stockholm": {
            "url": "https://blogs.nvidia.com/blog/2023/01/24/nvidia-ceo-stockholm/",
            "description": "Jensen Huang headlines Stockholm AI confab, Berzelius supercomputer upgraded to 94 NVIDIA DGX A100 systems.",
            "pubdate": "Wed, 25 Jan 2023 00:36:49 +0000",
            "pubdate_parsed": [
                2023,
                1,
                25
            ],
            "email_sent": true
        },
        "NVIDIA Unveils GPU-Accelerated AI-on-5G System for Edge AI, 5G and Omniverse Digital Twins": {
            "url": "https://blogs.nvidia.com/blog/2023/02/27/mwc-ai-on-5g-system/",
            "description": "Telcos are seeking industry-standard solutions that can run 5G, AI applications and immersive graphics workloads on the same server \u2014 including for computer vision and the metaverse. To meet this need, NVIDIA is developing a new AI-on-5G solution that combines 5G vRAN, edge AI and digital twin workloads on an all-in-one, hyperconverged and GPU-accelerated system. <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/02/27/mwc-ai-on-5g-system/\">Read article &#62;</a>",
            "pubdate": "Mon, 27 Feb 2023 13:00:46 +0000",
            "pubdate_parsed": [
                2023,
                2,
                27
            ],
            "email_sent": true
        },
        "GeForce NOW Springs Into March With 19 New Games in the Cloud, Including Disney Dreamlight Valley": {
            "url": "https://blogs.nvidia.com/blog/2023/03/02/geforce-now-thursday-march-2/",
            "description": "March is already here and a new month always means new games, with a total of 19 joining the GeForce NOW library. Set off on a magical journey to restore Disney magic when Disney Dreamlight Valley joins the cloud later this month. Plus, the hunt is on with Capcom\u2019s Monster Hunter Rise now available for <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/02/geforce-now-thursday-march-2/\">Read article &#62;</a>",
            "pubdate": "Thu, 02 Mar 2023 14:00:43 +0000",
            "pubdate_parsed": [
                2023,
                3,
                2
            ],
            "email_sent": true
        },
        "NVIDIA Canvas 1.4 Available With Panorama Beta This Week In the NVIDIA Studio": {
            "url": "https://blogs.nvidia.com/blog/2023/03/16/in-the-nvidia-studio-march-16/",
            "description": "An update is now available for NVIDIA Canvas, the free beta app that harnesses the power of AI to help artists quickly turn simple brushstrokes into realistic landscapes.",
            "pubdate": "Thu, 16 Mar 2023 13:05:17 +0000",
            "pubdate_parsed": [
                2023,
                3,
                16
            ],
            "email_sent": true
        },
        "Game Like a PC: GeForce NOW Breaks Boundaries Transforming Macs Into Ultimate Gaming PCs": {
            "url": "https://blogs.nvidia.com/blog/2023/03/16/geforce-now-thursday-march-16/",
            "description": "Disney Dreamlight Valley is streaming from Steam and Epic Games Store on GeForce NOW starting today. It\u2019s one of two new games this week that members can stream with beyond-fast performance using a GeForce NOW Ultimate membership. Game as if using a PC on any device \u2014 at up to 4K resolution and 120 frames <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/16/geforce-now-thursday-march-16/\">Read article &#62;</a>",
            "pubdate": "Thu, 16 Mar 2023 13:00:56 +0000",
            "pubdate_parsed": [
                2023,
                3,
                16
            ],
            "email_sent": true
        },
        "Peter Ma on How Hes Using AI to Found 8 Promising Signals for Alien Life": {
            "url": "https://blogs.nvidia.com/blog/2023/03/16/peter-ma-podcast/",
            "description": "Peter Ma was bored in his high school computer science class. So he decided to teach himself something new: how to use artificial intelligence to find alien life. That\u2019s how he eventually became the lead author of a groundbreaking study published in Nature Astronomy. The study reveals how Ma and his co-authors used AI to <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/16/peter-ma-podcast/\">Read article &#62;</a>",
            "pubdate": "Thu, 16 Mar 2023 13:00:33 +0000",
            "pubdate_parsed": [
                2023,
                3,
                16
            ],
            "email_sent": true
        },
        "NVIDIA CEO to Reveal Whats Next for AI at GTC": {
            "url": "https://blogs.nvidia.com/blog/2023/03/16/next-ai-gtc/",
            "description": "The secret\u2019s out. Thanks to ChatGPT, everyone knows about the power of modern AI. To find out what\u2019s coming next, tune in to NVIDIA founder and CEO Jensen Huang\u2019s keynote address at NVIDIA GTC on Tuesday, March 21, at 8 a.m. Pacific. Huang will share his vision for the future of AI and how NVIDIA <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/16/next-ai-gtc/\">Read article &#62;</a>",
            "pubdate": "Fri, 17 Mar 2023 00:46:40 +0000",
            "pubdate_parsed": [
                2023,
                3,
                17
            ],
            "email_sent": true
        },
        "GFN Thursday Celebrates 1,500+ Games and Their Journey to GeForce NOW": {
            "url": "https://blogs.nvidia.com/blog/2023/03/23/geforce-now-thursday-march-23/",
            "description": "Gamers love games \u2014 as do the people who make them. GeForce NOW streams over 1,500 games from the cloud, and with the Game Developers Conference in full swing this week, today\u2019s GFN Thursday celebrates all things games: the tech behind them, the tools that bring them to the cloud, the ways to play them <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/23/geforce-now-thursday-march-23/\">Read article &#62;</a>",
            "pubdate": "Thu, 23 Mar 2023 13:00:27 +0000",
            "pubdate_parsed": [
                2023,
                3,
                23
            ],
            "email_sent": true
        },
        "Blender Update 3.5 Fuels 3D Content Creation, Powered by NVIDIA GeForce RTX GPUs": {
            "url": "https://blogs.nvidia.com/blog/2023/03/29/blender-3-5-release/",
            "description": "Blender, the world\u2019s most popular 3D creation suite \u2014 free and open source \u2014 released its major version 3.5 update. Expected to have a profound impact on 3D creative workflows, this latest release features support for Open Shading Language (OSL) shaders with the NVIDIA OptiX ray-tracing engine.",
            "pubdate": "Wed, 29 Mar 2023 13:00:45 +0000",
            "pubdate_parsed": [
                2023,
                3,
                29
            ],
            "email_sent": true
        },
        "April Showers Bring 23 New GeForce NOW Games Including Have a Nice Death": {
            "url": "https://blogs.nvidia.com/blog/2023/03/30/geforce-now-thursday-march-30/",
            "description": "It\u2019s another rewarding GFN Thursday, with 23 new games for April on top of 11 joining the cloud this week and a new Marvel\u2019s Midnight Suns reward now available first for GeForce NOW Premium members. Newark, N.J., is next to complete its upgrade to RTX 4080 SuperPODs, making it the 12th region worldwide to bring <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/03/30/geforce-now-thursday-march-30/\">Read article &#62;</a>",
            "pubdate": "Thu, 30 Mar 2023 13:00:10 +0000",
            "pubdate_parsed": [
                2023,
                3,
                30
            ],
            "email_sent": true
        },
        "Video Editor Patrick Stirling Invents Custom Effect for DaVinci Resolve Software": {
            "url": "https://blogs.nvidia.com/blog/2023/04/04/davinci-resolve-fusion-ai-effects/",
            "description": "Video editor Patrick Stirling used the Magic Mask feature in Blackmagic Design\u2019s DaVinci Resolve software to create a custom effect that creates textured animations of people, this week In the NVIDIA Studio.",
            "pubdate": "Tue, 04 Apr 2023 13:00:31 +0000",
            "pubdate_parsed": [
                2023,
                4,
                4
            ],
            "email_sent": true
        },
        "Gaming on the Go: GeForce NOW Gives Members More Ways to Play": {
            "url": "https://blogs.nvidia.com/blog/2023/04/06/geforce-now-thursday-april-6/",
            "description": "This GFN Thursday explores the many ways GeForce NOW members can play their favorite PC games across the devices they know and love. Plus, seven new games join the GeForce NOW library this week. More Ways to Play GeForce NOW is the ultimate platform for gamers who want to play across more devices than their <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/04/06/geforce-now-thursday-april-6/\">Read article &#62;</a>",
            "pubdate": "Thu, 06 Apr 2023 13:00:12 +0000",
            "pubdate_parsed": [
                2023,
                4,
                6
            ],
            "email_sent": true
        },
        "The New Standard in Gaming: GeForce RTX Gamers Embrace Ray Tracing, DLSS in Record Numbers": {
            "url": "https://blogs.nvidia.com/blog/2023/04/12/ray-tracing-dlss/",
            "description": "Creating a map requires masterful geographical knowledge, artistic skill and evolving technologies that have taken people from using hand-drawn sketches to satellite imagery. Just as important, changes need to be navigated in the way people consume maps, from paper charts to GPS navigation and interactive online charts. The way people think about video games is <a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/2023/04/12/ray-tracing-dlss/\">Read article &#62;</a>",
            "pubdate": "Wed, 12 Apr 2023 13:00:33 +0000",
            "pubdate_parsed": [
                2023,
                4,
                12
            ],
            "email_sent": true
        }
    },
    "CMU Machine Learning Blog": {
        "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs": {
            "url": "https://blog.ml.cmu.edu/2022/08/26/recurrent-model-free-rl-can-be-a-strong-baseline-for-many-pomdps-2/",
            "description": "Figure 1. Our implementation of recurrent model-free RL outperforms the on-policy version (PPO/A2C-GRU), and a recent model-based POMDP algorithm (VRM) on most tasks of a POMDP benchmark where VRM was evaluated in their paper. While algorithms for decision-making typically focus on relatively easy problems where everything is known, most realistic problems involve noise and incomplete information. Complex algorithms have been proposed to tackle these complex problems, but there&#8217;s a simple approach that (in theory) works on both the easy and the complex problems. We show how to make this simple approach work in practice. Why POMDPs? Decision-making tasks in the real world are messy, with noise, occlusions, and uncertainty that are typically missing from their canonical problem formulation as a Markov decision process (MDP; Bellman, 1957). In contrast, Partially Observable MDPs (POMDPs; \u00c5str\u00f6m, 1965) can capture the uncertainty in the states, rewards, and dynamics. Such uncertainty arises in applications such as robotics, healthcare, NLP and finance. Apart from being realistic, POMDPs are a general framework that contains many subareas in RL, including: Meta-RL (Schmidhuber, 1987, Thrun and Pratt, 2012, Duan et al., 2016, Wang et al., 2017): assume the hidden states (task variables) of a POMDP do not vary through [&#8230;]",
            "pubdate": "Fri, 26 Aug 2022 17:38:40 +0000",
            "pubdate_parsed": [
                2022,
                8,
                26
            ],
            "email_sent": true
        },
        "Tracking Any Pixel in a Video": {
            "url": "https://blog.ml.cmu.edu/2022/09/09/tracking-any-pixel-in-a-video/",
            "description": "We upgrade pixels into PIPs: &#8220;Persistent Independent Particles&#8221;. With this representation, we track any pixel over time, and overcome visibility issues with a learned temporal prior. Motion estimation is a fundamental task of computer vision, with extremely broad applications. By tracking something, you can build models of its various properties: shape, texture, articulation, dynamics, affordances, and so on. More fine-grained tracking allows more fine-grained understanding. For robots, fine-grained tracking also enables fine-grained manipulation. Even setting aside downstream AI-related applications, motion tracks are directly useful for video editing applications &#8212; making realistic edits to a person or object in a video demands precise-as-possible tracking of the pixels, across an indefinite timespan. There are a variety of methods for tracking objects (at the level of segmentation masks or bounding boxes), or for tracking certain points in certain categories (e.g., the joints of a person), but there are actually very few options for general-purpose fine-grained tracking. In this domain, the dominant approaches are feature matching and optical flow. The feature matching approach is: compute a feature for the target on the first frame, then compute features for pixels in other frames, and then compute &#8220;matches&#8221; using feature similarity (i.e., nearest neighbors). This often [&#8230;]",
            "pubdate": "Sat, 10 Sep 2022 00:54:53 +0000",
            "pubdate_parsed": [
                2022,
                9,
                10
            ],
            "email_sent": true
        },
        "Are Model Explanations Useful in Practice? Rethinking How to Support Human-ML Interactions.": {
            "url": "https://blog.ml.cmu.edu/2023/03/30/are-model-explanations-useful-in-practice-rethinking-how-to-support-human-ml-interactions/",
            "description": "Model explanations have been touted as crucial information to facilitate human-ML interactions in many real-world applications where end users make decisions informed by ML predictions. For example, explanations are thought to assist model developers in identifying when models rely on spurious artifacts and to aid domain experts in determining whether to follow a model\u2019s prediction. However, while numerous explainable AI (XAI) methods have been developed, XAI has yet to deliver on this promise. XAI methods are typically optimized for diverse but narrow technical objectives disconnected from their claimed use cases. To connect methods to concrete use cases, we argued in our Communications of ACM paper [1] for researchers to rigorously evaluate how well proposed methods can help real users in their real-world applications.&#160; Towards bridging this gap, our group has since completed two collaborative projects where we worked with domain experts in e-commerce fraud detection and paper matching for peer review. Through these efforts, we\u2019ve gleaned the following two insights: Existing XAI methods are not useful for decision-making. Presenting humans with popular, general-purpose XAI methods does not improve their performance on real-world use cases that motivated the development of these methods. Our negative findings align with those of contemporaneous works. [&#8230;]",
            "pubdate": "Fri, 31 Mar 2023 03:32:01 +0000",
            "pubdate_parsed": [
                2023,
                3,
                31
            ],
            "email_sent": true
        }
    },
    "TensorFlow Blog": {
        "Highlights from TensorFlows 2021 exploreCSR awards": {
            "url": "https://blog.tensorflow.org/2022/02/exploreCSR-awards-highlights.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjDPu7hWKseCInuSNKBkdkHEmHMRHIv2WoQjqs8fVMrvtglukxO4hKtDOAnMKFpLn1VslC37Y4qf-mDzO13fQHkwZoYf_fT7cL4cb0E_6XZHtm_VkuLesY83xdyli9zNoiPg79DbPZi97zCMVb4J9Logkm5O7UxG3IY2mYpKXm9Z5lWskPAAnyu86Kb\" style=\"display: none;\" /> <p><em>Posted by <a href=\"https://twitter.com/random_forests\">Josh Gordon</a>, Jocelyn Becker, and Sloan Davis for the TensorFlow team</em></p><a name=\"more\"></a> <p>Increasing the number of students pursuing computer science research is a priority at Google, especially for students from historically marginalized groups in the field. Since 2018, Google\u2019s <a href=\"https://research.google/outreach/explore-csr/\">exploreCSR</a> awards have aided higher education efforts that support students interested in pursuing graduate studies and research careers in computing.  </p><p>The TensorFlow team is proud to provide additional funding to support this important program. To date, we have awarded more than 20 professors with funding to support their education and outreach work in machine learning.  </p><p>We\u2019d like to highlight examples of the many (and often, unique) outreach programs the 2021 award recipients have created so far. These range from research experiences with robotics, aquatic vehicles, federated learning, and offline digital libraries to mentored small group workshops on data science and programming skills. They\u2019re sorted alphabetically by university below. </p><p>If you\u2019re interested in creating your own programs like these with support from Google, keep an eye on the <a href=\"https://research.google/outreach/explore-csr/\">exploreCSR</a> website for the next round of applications opening in June 2022. </p><p><strong><em>Laura Hosman and Courtney Finkbeiner, Arizona State University</em></strong></p><p>The <a href=\"https://solarspell.org/\">SolarSPELL</a> initiative at Arizona State University will host a workshop series thanks to support from exploreCSR to encourage students underrepresented in computer science research in their academic journey. The SolarSPELL initiative produces an offline, solar-powered digital library designed to bring educational content to resource-constrained locations that may lack electricity, internet connectivity, and/or traditional libraries.  </p><p>The exploreCSR workshop series, titled \u201cSolarSPELL exploreCSR: Computing for Good\u201d, involves 6 weeks of sessions using SolarSPELL as a case study for how students can apply machine learning to tackle real-world problems and develop solutions for social good. Students will meet SolarSPELL\u2019s co-director and learn about the history of the SolarSPELL initiative; learn about graduate programs available at ASU; and hear from guest panelists from industry.  </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgUvY3hc5BkaB57WmKy-mVerTCQ9rHSD03STkw8TfUKR81xxJDAgyj90w7QC6FxH80J5-BvWdb979RL91fjHQuNihJ_2lhrXpClFCnAFRQJIhVE2PGA7DEebEyLv6pCQRRpCXyciF-xJsES6_5UsQq9DUZCOcnIn57qtTFPcNatd4GwY0xmoCa-_liY\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A solar-powered, offline digital library.</td></tr></tbody></table> <p>Aside from the information sessions, students will also gain hands-on experience working in teams and problem solving for real-world topics. The SolarSPELL team will present the students with three different challenges for student teams to develop a proposed solution using machine learning. Students will then be eligible to apply for paid summer fellowship positions with SolarSPELL to develop and integrate one of the proposed machine learning models into SolarSPELL\u2019s technology.  </p><p>SolarSPELL is a student-driven initiative, so the solutions that the exploreCSR students develop will be implemented in our digital libraries to improve hundreds of library users\u2019 experiences around the world. With libraries in 10 countries in the Pacific Islands and East Africa, and plans to expand to Latin America and the Middle East, these students will have a far-reaching impact. </p><p><strong><em>Daehan Kwak, Kean University</em></strong></p><p>My colleague Xudong Zhang and I created an undergraduate research study group centered on computer vision, with projects underway on student attention detection, mask and social distancing detection, and pill recognition for healthcare scenarios. As one example, a student created a pill detection application using data from the National Library of Medicine pillbox. This can be used, for example, by high-volume distribution pharmacies to be more efficient and accurate, or by retirement homes to verify the pills a resident is taking. We\u2019re pleased to share that the pill recognition project won third place in the Kean Business Plan Competition and was accepted to be presented at <a href=\"https://www.cur.org/what/events/students/poh/\">Posters on the Hill 2022</a>. </p><p><strong><em>Matthew Roberts, Macquarie University</em></strong></p><p>The School of Computing at Macquarie University is working to lower the barrier to entry for students who are new to experimenting with ML by employing real-world examples. This month, around fifty students will spend the week testing their ideas for solving autonomous aquatic vehicles challenges (for example, navigation) under guidance from Macquarie University researchers. They will be developing their ideas with a sophisticated simulation environment, and the best solutions will be ready for deployment to real hardware testing in the water later in the year. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiAeBoCCXY-rZSeoQLS3OcukXLsZzTz2-yjz3YwvFFTUJiFpcTPl4WKHIAWQB7I_bJwaT2R6-j6VJ3JxSF-adjRYGchmd352jmUqPKbqFJbqyhNMG4oYTLVxD1oFobbn30Qjn5nfd2vDpJbAANAeoO6EoVnuQ46yM32O0CaK-A-KQO84vzMQ3d5RzxC\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A MacSim simulation of the Sydney Regatta Center (created by VRX), a placeholder for a machine learning model, is making random predictions, ready for improvements the students come up with.</td></tr></tbody></table> <p>Accurately simulated sensors like cameras and LIDAR can be subjected to various models, allowing people to experiment with even more sophisticated ideas to solve complex problems. After our first year in exploreCSR, the adjustments we made to our simulator and the workshop will generate new ideas and light a spark for machine learning research early in students' careers. </p><p><strong><em>Pooyan Fazli, San Francisco State University</em></strong></p><p>60+ students from 10 universities and colleges attended our<a href=\"https://democratizeai.org/\"> 2-day virtual exploreCSR workshop</a>. Participants were from San Francisco State University, CSU East Bay, CSU San Marcos, CSU Stanislaus, Foothill College, Northwestern University, San Diego State University, Sonoma State University, UC San Diego, and the University of San Francisco. </p><p>We had two invited speakers and two panels on mentorship and career pathways with 10 panelists from Google Research, Stanford, Emory University, Virginia Tech, and the University of Copenhagen. </p><p>As part of this workshop, we organized hands-on activities to introduce students to different aspects of AI and its applications for social good, such as with climate change. We also had mini-presentations and discussions on AI fairness, accountability, transparency and ethics in different areas, such as robotics, educational data mining, and impacts on underserved communities. </p><p>Following the workshop, selected students will participate in a research project under the guidance of graduate students and faculty during the spring semester. Through the research projects, we have a two-fold aim: to help students develop a sense of belonging in the AI and machine learning research community, and to illuminate a pathway for them to pursue graduate studies in AI/ML that explores the potential of developing responsible AI toward social good. </p><p>The research projects will begin with eight weekly meetups and hands-on training on Python programming with open-source publicly available materials. Then, students will engage in applied research projects that focus on AI applications for social good, such as health, environment, safety, education, climate change, and accessibility. </p><p><strong><em>Farzana Rahman, Syracuse University</em></strong></p><p>Earlier this year, the Electrical Engineering and Computer Science department of Syracuse University hosted RESORC (REsearch Exposure in Socially Relevant Computing), an exploreCSR program, for the second time. This program provided research exposure to 78 undergraduate students from SU and nearby institutions targeting populations historically underrepresented in computing. The goal of these two workshops was to give students an opportunity to learn machine learning using open-source tools, and to gain experience with data science workflows including collecting and labeling data, training a model, and carefully evaluating it. The ML workshops were the mostly highly rated sessions of the RESORC program. </p><p><strong><em>Erin Hestir and Leigh Bernacchi, University of California, Merced</em></strong></p><p>  </p><p>Since 2019, University of California, Merced has partnered with Merced College and California State University Stanislaus on the Google exploreCSR program <a href=\"https://citris.ucmerced.edu/valle\">\u00a1Valle! Get Your Start in Tech!</a>, serving 32 Central Valley of California undergraduates in STEM annually to build a sense of belonging, practice professional networking, and develop technical skills. Participants convene on Zoom and in-person this semester. Valle students typically come from historically underrepresented groups, and the program is designed to support their pursuits of computational research, graduate school and computer science related careers. Many have gone on to achieve just that! </p><p>  </p><p>This year we added additional training thanks to Google Research to support machine learning applications for social good. This program is open to all Valle participants as well as partner schools, inclusive of graduate and undergraduate students in all STEM fields, and will be taught by creative graduate students in computer science from UC Merced. Each workshop will be taught by a near-peer mentor\u2014a practice that supports mutual success in academics\u2014and the mentor will coach teams to develop ML projects for social good. </p><p>  </p><p>The goal of the program is to overcome some of the trepidation scientists and students may have about computational science and machine learning through teamwork, fun and a higher purpose. Students will be able to develop their skills and interest, focusing on ML applications to climate, sustainability, agriculture and food, and diversity in tech and aviation. </p><p><strong><em>Basak Guler, University of California, Riverside</em></strong></p><p>At the University of California, Riverside, we created an undergraduate research study group focused on federated and distributed machine learning. Federated learning has become widely popular in recent years due to its communication efficiency and on-device learning architecture. Our study group meets on a weekly basis, and students learn about the principles of federated and distributed learning, state-of-the-art federated learning algorithms, recent applications from financial services to healthcare, as well as recent challenges and advances in privacy, security, and fairness. Student projects provide opportunities for undergraduate students to be involved in machine learning research, and learn from the experiences of both faculty and graduate students. This program can facilitate their transition from undergraduate to graduate degrees, and prepare them for positions of leadership in industry, government, public service, and academia. </p><p><strong><em>Gonzalo A. Bello, University of Illinois at Chicago</em></strong></p><p>The computer science department is hosting a series of exploreCSR workshops, including <a href=\"https://engineering.uic.edu/news-stories/workshop-allows-students-to-explore-data-science/\">exploreCSR: Exploring Data Science Research</a>, to introduce students to data science and machine learning research. These workshops aim to encourage students from historically underrepresented groups to pursue graduate studies and careers in research in the field of computer science. UIC students from all majors were encouraged to apply, including those who haven\u2019t taken any computer science courses. Each semester, 60 students were selected out of more than 120 who applied, and 10 teaching assistants and a professor mentored students. In addition to lectures, students work on hands-on projects together where they explore, visualize, and build models using real-world data from the city of Chicago. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhve9mJivIXHkAKKR1QTY9fYuYNlaLlkDNxMCDZ26NHr-ARueVqF7NvA_JmcRdSfEMZxcnzkkzL5bGT5cbpiyVB-ZReaOa966J_XZxF8Hn6fj7sQjX9WX_rA8KKr3dyjWcAJTYVbpWULd_VqikMgLOk-CGnapkOphlzNvpt94928-2NHoo7xVctJ67r\" style=\"width: 50%;\" /></center></td></tr>  </tbody></table>  <p><strong><em>Melanie Moses and Humayra Tasnim, The University of New Mexico</em></strong></p><p>The UNM Google exploreCSR activity for 2021-2022 is a semester-long course called Swarmathon: The Next Generation. The students will learn technical skills like developing machine learning models for object recognition in robots, and soft skills including team building, research skills, and discussions with faculty and external speakers. The UNM exploreCSR program builds on 5 years of training students in a NASA-sponsored robotics competition called the Swarmathon (2014-2019). In 2019/2020 we developed a series of exploreCSR Swarmathon: TNG workshops which included a faculty panel, an industry mentor, an open-source tutorial, and a day-long workshop to enable \u201cSwarmie\u201d robots to classify and automatically retrieve objects.  </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgn4do72xEklUYQADYsXZM277ifC9VmM83kaQmSpIM7C4cKJlFdeW9t2XPMWSooDLJkElGH5qg13B82CvNqVjiO2jxsp1HNXffOwu0ZCocGR0kXi8KiETUwnRlaqTNJZZdQQeQPuxuO06LHXzhTnq27ViMzJZg4PBaFOayFN4G4DRot0q0X5VNljbJx\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A glimpse of our robots in action.</td></tr></tbody></table> <p>  This year, in our exploreCSR Swarmathon: TNG course, students will have additional time to actively engage in developing and tuning their own machine learning models to test in the Swarmie robots. They will develop object detection models using convolutional neural networks (CNNs). They will be provided with a dataset of images of objects (shown below) taken from the robot camera and a simple model. The students will further develop the model and training data and then test their models on actual robots in real-time to see how much they can improve object recognition models to classify and retrieve the proper specified objects.</p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhoBYEZIk106bijUo1_u5hWOQyW9VM8YKy6kznxXK1UX3jpwJr5k1vi12kncOEj5XSL3E1WgWcOFDVvZr5-wcuYbhToBZ9ZoeVxruzQ0IMzhMop0toDFfqWxcvhvW0UrELxm5tY1F7kWc7VOO7nGsejmJuOW0ZuEfpk_I6q0B9Oz5fZtHlDegZcTOBK\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Different shaped cubes for detection.</td></tr></tbody></table> <p>  Students will learn first-hand the reality gap between simulations and real-world experiments. This will encourage them to develop their own mini-research projects to enhance their model performance to resolve that gap. The exploreCSR-funded Swarmathon: TNG course will provide students with the opportunity to actively engage in hands-on robotics research. We hope the experience of defining a research objective, conducting a set of experiments, testing a model, and seeing results play out in our robotics arena will motivate students to attend graduate school and consider research careers.</p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiM_aKpMKE40fa0ALpPQ-_oWDZByNX_noVuneLKmYQtyjQYsNMzAtOxEox6PKgzVpxy7hxbpiZEj0v6Kq4qQtB585VasDWVc-7_P2rZT7Q9qf0a7b9zfBHofVFfcUvpSPcC90lEDlbEnVYWmknGmkm_fC-BTurl-RwmhWoCjz6K7lz4Sv04gqEQ2RP_\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Swarmie with a cube in its gripper.</td></tr></tbody></table> <p><strong><em>Daniel Mej\u00eda, The University of Texas at El Paso</em></strong></p><p>We\u2019re building a series of workshops open to undergraduate students of all majors to introduce them to applied machine learning and research topics, starting with foundational concepts in math and a newfound way of approaching a problem through the eyes of a data scientist. These workshops are open to all students, including those who do not have any prior experience. We hope to encourage students to consider pursuing graduate studies, especially those who may have not previously considered it. I believe that the earlier students are exposed, the more likely that they will pursue a graduate degree. </p><p><strong><em>Henry Griffith, The University of Texas at San Antonio</em></strong></p><p>At the University of Texas at San Antonio, we\u2019re creating a portfolio of programs to enhance the persistence of first year Electrical and Computer Engineering students into research computing pathways. By integrating our programming with our Introduction to Electrical and Computer Engineering course, which has a total annual enrollment of approximately 200 students, we have the opportunity to achieve tremendous scale with our efforts. Our programs include an undergraduate research experience, a near-peer mentoring program, and group study projects - all designed to develop students' professional and technical skills and to accelerate their progression into research opportunities. </p><p><strong><em>John Akers, University of Washington</em></strong></p><p>Our exploreCSR workshop, <a href=\"https://realitylab.uw.edu/components/graduate-prep-workshop.html\">CSNext</a>, is scheduled to begin this April. It\u2019s a 4-week online program of workshops, seminars, and project work designed to encourage undergraduate students - particularly those from historically underrepresented groups - to consider and successfully apply to graduate schools in computer science. Participants will hear presentations from several University of Washington labs, such as Computer Vision/Graphics (GRAIL), Security and Privacy, and Human-Computer Interaction. There will be presentations on deep learning and on current graduate-level research, a panel discussion from current UW CSE grad students from varying backgrounds, opportunities to meet current graduate students from several UW CSE labs, and participants will be led through small-group exercises learning about active research from graduate student mentors. Participants will also learn about graduate school application processes and resources, led by staff from UW CSE Graduate Student Services. </p><h3>Learning more</h3>  <p>If you\u2019re interested in creating your own programs like these with support from Google, keep an eye on the <a href=\"https://research.google/outreach/explore-csr/\">exploreCSR</a> website for the next round of applications opening in June 2022.  </p>",
            "pubdate": "Wed, 23 Feb 2022 16:58:00 +0000",
            "pubdate_parsed": [
                2022,
                2,
                23
            ],
            "email_sent": true
        },
        "Accelerating TensorFlow Lite Micro on Cadence Audio Digital Signal Processors": {
            "url": "https://blog.tensorflow.org/2022/03/Accelerating-TFLite-Micro-On-Cadence.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmewLNKzKRZm7tPlPsm3AzC8g47bwqeo7EsJwCruRRi9s1gY2gRhfdNUPU6-SZhGq-AGTEW3FrcQpVWgF48w3OaAxZ7If3kYHuROVuqdWN1x5J3AJx43WVorfIsYsb2ulFJIJGjOfP4zR0P1KQ8cp1ajaHsGB1w5vfrvZrsfubTIh9Ju13vo4p1ry/s1600/image1.jpg\" style=\"display: none;\" />  <p><em>Posted by Raj Pawate (Cadence) and Advait Jain (Google)</em></p><a name=\"more\"></a><p>Digital Signal Processors (DSPs) are a key part of any battery-powered device offering a way to process audio data with a very low power consumption. These chips run signal processing algorithms such as audio codecs, noise canceling and beam forming. </p><p>Increasingly these DSPs are also being used to run neural networks such as wake-word detection, speech recognition, and noise suppression. A key part of enabling such applications is the ability to execute these neural networks as efficiently as possible. </p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmewLNKzKRZm7tPlPsm3AzC8g47bwqeo7EsJwCruRRi9s1gY2gRhfdNUPU6-SZhGq-AGTEW3FrcQpVWgF48w3OaAxZ7If3kYHuROVuqdWN1x5J3AJx43WVorfIsYsb2ulFJIJGjOfP4zR0P1KQ8cp1ajaHsGB1w5vfrvZrsfubTIh9Ju13vo4p1ry/s1600/image1.jpg\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmewLNKzKRZm7tPlPsm3AzC8g47bwqeo7EsJwCruRRi9s1gY2gRhfdNUPU6-SZhGq-AGTEW3FrcQpVWgF48w3OaAxZ7If3kYHuROVuqdWN1x5J3AJx43WVorfIsYsb2ulFJIJGjOfP4zR0P1KQ8cp1ajaHsGB1w5vfrvZrsfubTIh9Ju13vo4p1ry/s1600/image1.jpg\" /></a></div>  <p>However, productization paths for machine learning on DSPs can often be ad-hoc.\u00a0In contrast, speech, audio, and video codecs have\u00a0worldwide standards bodies such as ITU and 3GPP creating algorithms for compression and decompression addressing several aspects of quality measurement, fixed point arithmetic considerations and interoperability. </p><p>TensorFlow Lite Micro (TFLM) is a generic open-sourced inference framework that runs machine learning models on embedded targets, including DSPs.\u00a0Similarly, <a href=\"https://www.cadence.com/en_US/home.html\">Cadence</a> has invested heavily in PPA-optimized hardware-software platforms such as Cadence Tensilica HiFi DSP family for audio and Cadence Tensilica Vision DSP family for vision. </p><h3>Google and Cadence \u2013 A Multi-Year Partnership for Enabling AI at the Edge</h3>  <p>This was the genesis of the collaboration between the TFLM team and the Audio DSP teams at Cadence, starting in 2019.\u00a0The TFLM team is focusing on leveraging the broad TensorFlow framework and developing a smooth path from training to embedded and DSP deployment via an interpreter and reference kernels.\u00a0Cadence is developing a highly optimized software library, called NeuralNet library (NNLIB), that leverages the SIMD and VLIW capabilities of their low-power HiFi DSPs. This collaboration started with three optimized kernels for one Xtensa DSP, and now encompasses over 50 kernels across a variety of platforms such as HiFi 5, HiFi 4, HiFi 3z, Fusion F1 as well as Vision DSPs such as P6, and includes the ability to offload to an accelerator, if available. </p><p>Additionally, we have collaborated to add continuous integration for all the optimized code targeted for the Cadence DSPs. This includes infrastructure that tests that every pull request to the TFLM repository passes all the unit tests for the Tensilica toolchain with various HiFix and Vision P6 cores. As such, we ensure that the combined TFLM and NNLIB open source software is both tightly integrated and has good <a href=\"https://github.com/tensorflow/tflite-micro/actions/workflows/xtensa.yml?query=event%3Aschedule\">automated test coverage</a>. </p><h3>Performance Improvements</h3>  <p>Most recently, we have collaborated on adding optimizations for models that are <a href=\"https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8\">quantized with int16 activations</a>. Specifically in the domain of audio, int16 activations can be critical for the quality of quantized generative models. We expect that these optimized kernels will enable a new class of ML-powered audio signal processing.  The table below shows a few operators that are required for implementing a noise suppression neural net.  We show a 267x improvement in cycle count for a variant of <a href=\"https://arxiv.org/abs/2008.02027\">SEANet</a>, an example noise suppression neural net. </p><p>The following table shows the improvement with the optimized kernels relative to the reference implementations as measured with the Xtensa instruction set simulation tool. </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 11.5pt;\">    <table>        <tbody>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; background-color: rgb(67, 67, 67); padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #ffffff; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Operator</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; background-color: rgb(67, 67, 67); padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #ffffff; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Improvement</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Transpose Conv</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">458x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Conv2D</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">287x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Sub</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">39x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Add</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">24x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Leaky ReLU</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">18x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Srided_Slice</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">10x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Pad</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">6x</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; background-color: rgb(204, 204, 204); padding: 5pt; overflow: hidden; width: 49.74%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Overall Network</span></p>                </td>                <td style=\"border-width: 1pt; border-style: solid; border-color: rgb(0, 0, 0); vertical-align: top; background-color: rgb(204, 204, 204); padding: 5pt; overflow: hidden; width: 50.0849%;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">267x</span></p>                </td>            </tr>        </tbody>    </table></div> <h3>How to use these optimizations</h3>  <p>All of the code can be used from the <a href=\"https://github.com/tensorflow/tflite-micro\">TFLite Micro</a> GitHub repository. </p><p>To use HiFi 3z targeted TFLM optimizations, the following conditions need to be met: </p><ul> <li>the TensorFlow Lite (TFLite) flatbuffer model is quantized with int16 activations and int8 weights  <li>it uses one or more of the operators listed in the table above  <li>TFLM is compiled with <code>OPTIMIZED_KERNEL_DIR=xtensa</code></li></ul><p>For example, you can run Conv2D kernel integration tests with reference C++ code with: </p>   <pre><code class=\"\">make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=hifi4 XTENSA_CORE= test_integration_tests_seanet_conv</code></pre>  <p>And compare that to the optimized kernels by adding <code>OPTIMIZED_KERNEL_DIR=xtensa</code>: </p>   <pre><code class=\"\">make -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=hifi4 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE= test_integration_tests_seanet_conv</code></pre>  <h3>Looking Ahead</h3>  <p>While the work thus far has been primarily focused on convolutional neural networks, Google and Cadence are also working together to develop an optimized LSTM operator and have released a <a href=\"https://github.com/tensorflow/tflite-micro/tree/1c6b50ef8b9dac1bb31dfe79679aec781666a86b/third_party/xtensa/examples/micro_speech_lstm\">first example</a> of an LSTM-based key-word recognizer. We expect to expand on this and continue to bring optimized and production-ready implementations of the latest developments in AI/ML to Tensilica Xtensa DSPs. </p><h3>Acknowledgements</h3>  <p>We would like to acknowledge a number of our colleagues who have contributed to making this collaboration successful. </p><p>Cadence: Int16 optimizations: Manjunath CP, Bhanu Prakash Venkata, Anirban Mandal LSTM implementation: Niranjan Yadla, Lukman Rahumathulla, Manjunath CP, Pramodkumar Surana, Arjun Medinakere NNLIB optimizations: Vijay Pawar, Prasad Nikam, Harshavardhan, Mayur Jagtap, Raj Pawate </p><p>Google: Advait Jain, Deqiang Chen, Lawrence Chan, Marco Tagliasacchi, Nat Jeffries, Nick Kreeger, Pete Warden, Rocky Rhodes, Ting Yan, Yunpeng Li, Victor Ungureanu </p>",
            "pubdate": "Thu, 24 Mar 2022 16:18:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                24
            ],
            "email_sent": true
        },
        "Intro mPOD DxTrack: A low-cost healthcare device using TensorFlow Lite Micro": {
            "url": "https://blog.tensorflow.org/2022/03/intro-mpod-dxtrack-low-cost-healthcare.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOzl4x5OR9H0hf81VYobIMLILeu81Y_BEP5KWbveG3Evh5ieqO2NioYqa4hRTgFGp-IymRrZ4ioBKDCj_vaz0j_oWdS_34OteYZSXbGro0WBB7mQDqKsdqkw9ELVL7HdApsm06Lf-JKRnLNNYOcebEH0XbRauiZWhYLago3T1zwaJeubDmkj1Fp7k3/s1600/image2.png\" style=\"display: none;\" />  <p><em>A guest post by <a href=\"https://www.linkedin.com/in/jeffreyly/\">Jeffrey Ly</a>, CEO &amp; <a href=\"https://www.linkedin.com/in/joannaashby\">Joanna Ashby</a>, CMO of mPOD, Inc.</em></p><p>mPOD is a NIH-funded pre-seed startup headquartered out of Johnson &amp; Johnson\u2019s Innovation (JLABS) in New York City. In this article, we\u2019d like to share with you a hardware device we have developed independently at mPOD leveraging TensorFlow Lite Micro (TFLM) as a core technology, called DxTrack.  </p><a name=\"more\"></a><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEim0Z2oe1rhtzURFByHCs2DPyzak4ogcVGGqw7EugzlBzgQl5TuUVM-mOEdYf2ifS4g8zdN0iUSO4VSRpRFeUBnUAI-dqmieMEQHoiAUhDgIHsN1Nl6Y5AMgaogDOqp_TW3pZevgSOg3ZaNPYDPtoMFk-u1gU98wFpOXAs9Nr87BLERUPD0yDbLWf8h/s1600/image8.gif\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEim0Z2oe1rhtzURFByHCs2DPyzak4ogcVGGqw7EugzlBzgQl5TuUVM-mOEdYf2ifS4g8zdN0iUSO4VSRpRFeUBnUAI-dqmieMEQHoiAUhDgIHsN1Nl6Y5AMgaogDOqp_TW3pZevgSOg3ZaNPYDPtoMFk-u1gU98wFpOXAs9Nr87BLERUPD0yDbLWf8h/s1600/image8.gif\" /></a></div>  <p>mPOD DxTrack leverages TFLM and low cost hardware to enable accurate, rapid and objective interpretation of currently available lateral flow assays (LFAs) in less than 10 seconds. LFAs serve as diagnostic tools because they are low-cost and simple to use without specialized skills or equipment. Most recently popularized by COVID-19 rapid antigen tests, LFAs are also used extensively testing for pregnancy, disease tracking, STDs, food intolerances, and therapeutic drugs along with an extensive array of biomarkers totaling billions of tests sold each year. The mPOD Dxtrack is applicable to use with any type of visually read lateral flow assay, demonstrating a healthcare use case for TFLM that can directly impact our everyday lives.  </p><p>The LFA begins with a sample (nasal swab,  saliva, urine, blood, etc) loaded at (1) in the figure below. Once the sample has flowed to the green conjugate zone (2), it is labeled with a signaling moiety. Through capillary action, the sample will continue flowing until it is immobilized at (3), with these LFA tests, two lines indicate a positive result, one line indicates a negative result.  </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOzl4x5OR9H0hf81VYobIMLILeu81Y_BEP5KWbveG3Evh5ieqO2NioYqa4hRTgFGp-IymRrZ4ioBKDCj_vaz0j_oWdS_34OteYZSXbGro0WBB7mQDqKsdqkw9ELVL7HdApsm06Lf-JKRnLNNYOcebEH0XbRauiZWhYLago3T1zwaJeubDmkj1Fp7k3/s1600/image2.png\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 1. Side (A) &amp; Top (B) view of a lateral flow assay (LFA) sample where at (1) the sample (nasal swab,  saliva, urine, blood, etc) is loaded before flowing to the green zone (2), where the target is labeled with a signaling moiety. Through capillary action, the sample will continue flowing until it is immobilized at (3) to form the test line. Excess material is absorbed at (4).</td></tr>  </tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXj0FI77LgORp7QhzSI07oqN26vCBLQDPnJDd88IEY0Wp92rhluonloN5KkaSHT9EJmwbQdA_7_Or6FeAB9Z4wciBa0VxtBrdaSuIMZTJ08IOow85id8m-bRQI_zfNIMKDCRk5S40FmWVmkYnoCaanKmexvEbTsxWkHM6ChW5cHmoneYgEESvKyAok/s1600/image9.png\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 2. These are the 3 possible classes results for a lateral flow assay (LFA) test. </td></tr>  </tbody></table>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5BdMrQvgGHkPNqTtBaw4a3CUzPr23uReiYl0mUoV0Og4TSmRy7jhv8xmkZoknkvbRwjuxwZ5RrlbqoVBe8xAgXDLejTO6yczSIjDdTwuRqenlUCFUGi4I_mZV3zrPSulDmACckWn4sb30PyLyEinBO9Zrb1X3ZdlHmlJFq1C_CqM3x55_KV7CLx1K/s1600/image5.png\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 3. This is a diagram NOWDiagnostics ADEXUSDx lateral flow assay (LFA) designed to collect and run saliva sample in point-of-care (POC) and over-the-counter (OTC) settings.</td></tr>  </tbody></table> <p>When used correctly, these tests are very effective; however self-testing presents challenges for the lay user to interpret. Significant variability is present between devices, making it difficult to tell if the test line you see is negative \u2026or a faint positive? </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW41dgw5CN-QxeG6F4a4KEeXPDEjCOt5dFo7Sr_LMvofWnmgbcH7h-HLu_fLyuOjZZoJqnISvKkLKm6eW31vzK_rk_oXx9dtHiuQCweRw8zHFTvPhrTuozm7jtV-grpx5aYSqBRn80HotRUBYCfoYCl-IkBwamFkCEMotPgcbupr589V3lufpRpEin/s1600/image1.gif\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 4. A visualization of how the TinyML model on the mPOD DxTrack break interprets and classifies different lateral flow assay (LFA) results.</td></tr>  </tbody></table> <p>To address this challenge, we developed mPOD DxTrack, an over-the-counter (OTC) LFA reader that improves the utility of lateral flow assays by enabling rapid and objective readings with a simple, under $5 (Cost-of-Goods) globally-deployable device. The mPOD DxTrack aims to read lateral flow assay  tests using ML to accomplish two goals: 1) enable rapid and objective readings of LFAs and 2) streamline digital reporting.  Critically, TinyML allows for the software on the mPOD DxTrack to be deployed on low-cost (less-than $5) hardware that can be widely distributed - which is difficult with existing LFA readers which rely on high-cost/high complexity hardware that cost hundreds to thousands of dollars per unit.  Ultimately, we believe that TinyML will enable the mPOD DxTrack to catch missed positive test results by removing human bias and increasing confidence in lateral flow device testing, reducing user error, and increasing overall result accuracy. </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwdPvQ1s9GmXPqK0chIpdJ4DjRqTYM4jRf_dS8G19aDvRajapFKwt0oIREC-IWXM9mJhNZOtV5yxafLrRQjZFPrmhkDpWVHGY3cv0fh-8xuz5Dyq6YHA2LuXohoQeglfzbiVj9_u_kKhyBq11NSu4aO9vd-UvF4Ap0HaInsWa1DLayUCSTVLLm7-nO/s1600/image6.png\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 5. Assembly view of the mPOD DxTrack with lateral flow assay (LFA) cassette.</td></tr>  </tbody></table>  <p>Technical Dive </p><p><span style=\"text-decoration: underline;\">Key Considerations</span></p><ul> <li>Achieving high accuracy 99% overall accuracy, (99% sensitivity, 99% specificity) for model performance when interpreting live-run LFA strips.  <li>Ensuring the model can maintain that level of performance while fitting the hardware constraints. </li></ul><p><strong><span style=\"text-decoration: underline;\">Model size constraints for TinyML</span></strong></p><p>Deployment of the DxTrack TinyML model on the Pico4ML Dev kit is constrained by 2 pieces of hardware: Flash memory and SRAM. The Pico4ML Dev kit has 2MB of flash memory to host the .uf2 file and 264kb of SRAM that accommodate the intermediate arrays (among other things) of the model. Ensuring the model size stays within these bounds is critical because while the code can successfully compile, run on the host machine and even successfully flash on the Pico4Ml Dev Kit, it will hang during set-up and not execute the main loop. </p><p>Rather than guess and check the size of intermediate arrays (a process we initially took with little reproducible success), we ended up developing a workflow that enabled us to quantify the model\u2019s arena size by first using the interpreter function. See below, where this function was called during setup: </p> <pre><code class=\"\">TfLiteStatus setup_status = ScreenInit(error_reporter);<br />if (setup_status != kTfLiteOk){<br />while(1){TF_LITE_REPORT_ERROR(error_reporter, \"Set up failed\\n\");};<br /> }<br />arena_size = interpreter->arena_used_bytes();<br />printf(\"Arena_Size Used: %zu \\n\", arena_size);</code></pre> <p>When printed out, this is what the value from the interpreter function should look during Pico4ML Dev kit boot-up:</p> <pre><code class=\"\">DEV_Module_Init OK                                                              <br />Arena_Size Used: 93500                                                         <br /> sd_spi_go_low_frequency: Actual frequency: 122070                               <br />V2-Version Card                                                                 <br />R3/R7: 0x1aa                                                                    <br />R3/R7: 0xff8000                                                                 <br />R3/R7: 0xc0ff8000                                                               <br />Card Initialized: High Capacity Card                                           <br />SD card initialized<br />SDHC/SDXC Card: hc_c_size: 15237                                                <br />Sectors: 15603712                                                               <br />Capacity: 7619 MB                                                           <br />sd_spi_go_high_frequency: Actual frequency: 12500000<br /></code></pre> <p>With this value available to us, we are then able to set the appropriate TensorArenaSize. As you can see from above, the model uses 93500 bytes of SRAM. By setting the TensorArenaSize to just above that amount 99x1024 = 101376 bytes, we are able to allocate enough memory to host the model without going over the hardware limits (which also causes the Pico4ML Dev Kit to freeze).</p> <pre><code class=\"\">// An area of memory to use for input, output, and intermediate arrays.<br />constexpr int  kTensorArenaSize =  99* 1024; // 136 * 1024; //81 * 1024;<br />static uint8_t tensor_arena[kTensorArenaSize];</code></pre>  <p><span style=\"text-decoration: underline;\">Transforming from Unquantized to Quantized Models</span></p><p>Now that we have a reproducible methodology to quantify and deploy the model onto the Pico4ML Dev Kit, our next challenge is ensuring that the model can achieve the accuracy we require while still fitting with the size constrained by the hardware. For reference, the mPOD DxTrack platform is designed to interpret a 96x96 image. In the original model design, we were able to achieve > 99.999% accuracy with our model, but  the intermediate layer is 96x96x32 at fp32 which requires over 1 MB of memory - it would never fit on the Pico4ML Dev Kit\u2019s 264KB of SRAM. In order to achieve the size requirement for the model, we needed to take the model from unquantized to quantized; our best option was to utilize full int8 quantization. In essence, instead of treating the tensor values as floating points (float32), we correlate those values to integers (int8). Unfortunately,  this decreased the model size 4-fold, allowing it to fit onto the Pico4ML Dev Kit's rounding error from fp32 to int8 compounded, resulting in dramatically reduced model performance.  </p>  <p><img alt=\"ALT TEXT\" height=\"300\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgUFYKQMeyius4mZLk8qm7rVEYDGnle51G1rOtdwFtmNaVOS2HdFwFl8jTt40bCzBAYHjYsXNPASjnYnZ3E_QHYWuh-mVcvzhpm5gQjfMMz40guMzPcH2AGpYu8R_RQEyEyHF0HZaH1-4kLVXvClukEID5xyKvVx1b04LewRCn1uwVQy8p0wWvhope3/s1600/image7.png\" style=\"float: right; margin-bottom: 0.3px; margin-right: 10px;\" width=\"300px\" /></p><p>To combat this drop in model performance, we examined the effect of two different quantization strategies to improve performance: Post-training quantization (PTQ) and Quantization-aware training (QAT).  </p><p>Below, we compare 3 different models to understand which quantization strategy is best. For reference:  </p><ul> <li>Model 1:  2-layer convolutional network  <li>Model 2:  3-layer convolutional network   <li>Model 3:  4-layer convolutional network </li></ul><p>As we can see, Quantization-aware training (QAT) uniformly beats the post-training quantization (PTQ) method and it became part of our workflow moving forward. </p><p><strong><span style=\"text-decoration: underline;\">What performance can we achieve now?</span></strong></p><p>Tested across over 800 real-world test runs, the mPOD DxTrack can preliminary achieve an overall accuracy of 98.7%. This version of the model is currently being evaluated by our network of manufacturing partners who we work closely with. Currently we are assembling a unique dataset of images as part of a patient-focused data pipeline to learn from each manufacturing partnership and building bespoke models. </p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnlX3AOb4V8qMOt9UfNU-izH4eIh_XOtEbYOkk9YRh-TlZdlKBaJEwb36-zV9tM93UMJOUlWN6ib3QdtO1ndAvMiQTpIWp1PgrbKLsydmFeJ92bFVULkwXRpksuyJRQEoV2-NIm63dmCgug2MinW5Dx_4GjpOdrWa9MEOKSxLD-QI_JAJHOh2laoY-/s1600/image4.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnlX3AOb4V8qMOt9UfNU-izH4eIh_XOtEbYOkk9YRh-TlZdlKBaJEwb36-zV9tM93UMJOUlWN6ib3QdtO1ndAvMiQTpIWp1PgrbKLsydmFeJ92bFVULkwXRpksuyJRQEoV2-NIm63dmCgug2MinW5Dx_4GjpOdrWa9MEOKSxLD-QI_JAJHOh2laoY-/s1600/image4.png\" /></a></div> <p>Our preliminary work has also helped us correlate model performance with appropriately large dataset size to achieve the performance high enough accuracy for our healthcare application. Per the figure attached, the model needs to be trained on a quality dataset of at least 15,000 images. Our commercial-ready target is likely to require datasets that are greater than 100,000 images.</p> <p><img alt=\"ALT TEXT\" height=\"300\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiHP5hy5z6OchM1Xs8QoXV_KWccZoff2z6HYraEQcoZ3OWxtOIjtm112_qIeH0uf4w7jw5QK0MiPaRh21Cfetp5OpO4TG2QXnZglPiXz6tn_5HZQHz23Z_AHrMRISvIw5bsRQUtDkVdDWK2oox8utNIKktzQ304hXNrN5pp9Faa-N8q9FiBXtm6Q4W6/s1600/image3.png\" style=\"float: right; margin-bottom: 0.3px; margin-right: 10px;\" width=\"300px\" /></p> <p>To learn more about mPOD Inc, please visit our website at <a href=\"http://www.mpod.io\">www.mpod.io</a>. If you\u2019re interested in learning more about TinyML, we recommend checking out this <a href=\"https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Power-Microcontrollers/dp/1492052043\">book</a> and this <a href=\"https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning\">course</a>. </p>",
            "pubdate": "Mon, 28 Mar 2022 18:01:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                28
            ],
            "email_sent": true
        },
        "How LinkedIn Personalized Performance for Millions of Members using TensorFlow.js": {
            "url": "https://blog.tensorflow.org/2022/03/how-linkedin-personalized-performance.html",
            "description": "<p><em>A guest post by LinkedIn</em></p><p><a href=\"https://www.linkedin.com/in/markepascual/\">Mark Pascual</a>, Sr. Staff Engineer </p><p><a href=\"https://www.linkedin.com/in/nitinpasumarthy/\">Nitin Pasumarthy</a>, Staff Engineer </p><a name=\"more\"></a><h3>Introduction</h3>  <p>The Performance team at LinkedIn optimizes latency to load web and mobile pages. Faster sites improve customer engagement and eventually revenue to LinkedIn. This concept is <a href=\"https://wpostats.com/\">well documented by many other companies too</a> who have had similar experiences but how do you define the optimal trade off between page load times and engagement? </p><p>The relationship between speed and engagement is non-linear. Fast loading sites, after a point, may not increase engagement by further reducing their load times. At LinkedIn we have used this relationship between engagement and speed to selectively customize the features on <a href=\"https://engineering.linkedin.com/blog/2018/03/linkedin-lite--a-lightweight-mobile-web-experience\">LinkedIn Lite</a> - a lighter, faster version of LinkedIn, specifically built for mobile web browsers.  </p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAEQ2KGrdiRdXqw46tw3E2as7NaxLPfJK_mKjqMOwpn47Hz0hjvMUefZXvzV4KdtJ-ndPH_wDw8Y0v4gQ0Cgu3gnPo8MZL7MbK9-4p2G6ore_NT9U3e3dhsmisHWBdBFoSqrAK1PcpExiSFfy10jeeA7emPKBsmoIezLMNBIDyWFC2On5JuAtSaEw/s1600/image1.gif\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiMAEQ2KGrdiRdXqw46tw3E2as7NaxLPfJK_mKjqMOwpn47Hz0hjvMUefZXvzV4KdtJ-ndPH_wDw8Y0v4gQ0Cgu3gnPo8MZL7MbK9-4p2G6ore_NT9U3e3dhsmisHWBdBFoSqrAK1PcpExiSFfy10jeeA7emPKBsmoIezLMNBIDyWFC2On5JuAtSaEw/s1600/image1.gif\" /></a></div>  <p>To do this, we trained a deep neural network to identify if a request to LinkedIn would result in a fast page load in real time. Based on the performance quality result predicted by this model we change the resolution of all images on a given user\u2019s news feed before the resulting webpage was sent to the client. This led to an increase in the magnitude of billions for extra Feed Viral Actions (<strong>+0.23%</strong>) taken, millions more Engaged Feed Users (<strong>+0.16%</strong>) and Sponsored Revenue increased significantly for us too (<strong>+0.76%</strong>). </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6ZVoFb8j7Iwy5AP_g_06WzvABGAxkh9pGj-KMlQdYljjwxKNnJVZsGsgCosn-kuAD2AcJgHdPrMWGQUdnvZ9zb6vjnTXtajv-3V5Ah5XD4bKVkXW5kK7-D1SOeb72gQkIHJEbUv5SHyolNGImBJRpGzLnCVWSguFO54VaM0cRtmmYDpM7FCvA9-bu/s1600/image2.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><strong><em>Image Quality Comparison:  Image on the left uses 4x more memory than the one on the right which is less than ideal to send to users on slow network connections or when the device may be low on resources. Prior to using an ML model, we only showed the low resolution image which was not great for users that had capacity for higher quality images on newer devices.</em></strong></td></tr></tbody></table>   <p>We described in great detail why many of our performance optimization experiments failed back in 2017 and how we used those learnings to build a Performance Quality Model (PQM) in our <a href=\"https://www.linkedin.com/pulse/personalizing-performance-adapting-application-real-time-pasumarthy\">Personalizing Performance: Adapting Application in real time to member environments</a> blog. <br /><br />PQM\u2019s bold goal is to predict various performance metrics (e.g. page load time) of any web / mobile page using both device and network characteristics of end users to empower (web) developers to build impactful application features that are otherwise tricky to implement (like the one we described above). <br /></p><p><center><em>We are happy to announce that we are open sourcing our first performance quality model  that is trained on millions of <a href=\"https://en.wikipedia.org/wiki/Real_user_monitoring\">RUM</a> data samples from around the world free to use for your own website performance optimizations! <a href=\"https://github.com/linkedin/performance-quality-models/tree/main/ssr-mobile-web/mweb-jan-2022-v1\">Learn more and get started here</a>.</em>  </center></p><p>In the rest of this blog, we will go over how our team of full stack developers deployed this PQM in production that works at Linkedin scale! We wish to prove that deploying TensorFlow.js ML models today is both easy and beneficial for those working on the Node.js stack. </p><h3>TensorFlow.js: Model Deployment in Node.js</h3>  <p>At the time of our production deployment, LinkedIn\u2019s TensorFlow model deployment machinery was still being developed. Furthermore, using TensorFlow Serving was not yet a feasible option for us. So even though we had a model ready for use, we needed to figure out a way to deploy it.  </p><p>As LinkedIn is primarily a Java/JVM stack for serving external traffic, it might seem like TensorFlow Java would be ideal, but it was still experimental and didn\u2019t have the API stability guarantees that we require. </p><p>We looked at our options and realized that we already use Node.js (behind the JVM) as part of our frontend web serving stack in order to perform server side optimizations when serving HTML pages. The architecture for this is unique in that we use the JVM to manage an external pool of Node.js processes to perform \u201cwork,\u201d e.g., the previously mentioned server side optimizations. The \u201cwork\u201d can really be anything that Node.js can perform. In our use case, this enables us to use TensorFlow.js in an architecture that was already proven. </p><p>We repurposed our frontend stack to use Node.js to deploy our custom model and ended up with great results. In terms of performance, our mixed stack of Java and Node.js easily met our SLAs. The 50th and 90th percentile production latencies as measured (a) from a client (within the datacenter), (b) from on host instrumentation, and (c) in terms of only Node.js performing inference using TensorFlow.js are shown in the table below. </p>  <div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border: none; border-collapse: collapse; width: 468pt;\">        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\"><br /></td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">50th Percentile</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">90th Percentile</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">From client (within datacenter)</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">10 ms</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">12 ms</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">On host</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">8 ms</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">10 ms</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Inference in Node.js</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">2 ms</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">3 ms</span></p>                </td>            </tr>        </tbody>    </table></div> <p>The resulting architecture is shown above in Figure 1 below.</p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3ARPZvycGXHKTn7Cz3RjtGR0cOE1inKHoe-91xNOq8eONMkoi8gLg2yJkVgV7h-eIUtRBnp4ohPlvlvo5td3uElKQHTAgTUcFIgLyFM95VmQhYhrd5poAQCAEkR1Uuoq2mizh5li_qJkZGHS91B4VSYSNTqelazUsWmS_I6SWC6Wq-bv5Se9G7yyg/s1600/image4.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"></td></tr></tbody></table>   <p>The API request that requires a prediction is received by the JVM server and is routed to our <a href=\"https://linkedin.github.io/rest.li/\">Rest.li</a> infrastructure which in turn routes the request to our performance prediction resource. To handle the request, the PaaS resource performs some feature generation based on the inputs and then makes an RPC call out to the Node.js process for the prediction. </p><p>The N Node.js processes are long-lived. They are started upon JVM startup and have already loaded the desired model using <code>tf.node.loadSavedModel()</code>. When a process receives a request for a prediction, it simply takes the input features, calls <code>tf_model.predict()</code>, and returns the result. Here is a simplified version of the Node.js code: </p> <pre><code class=\"\">const tf = require(\u2018@tensorflow/tfjs-node\u2019);<br /><br />async function main() {<br />  // load the model when the process starts so it\u2019s always ready<br />  const model = await tf.node.loadSavedModel(\u2018model_dir\u2019);<br /><br />  function predict(rawInput) {<br />    return tf.tidy(() => {<br />      // prepare the inputs as tensor arrays<br />      const x = {}<br />      for (const feature of Object.keys(predictionInput)) {<br />        x[feature] = tf.tensor([input[feature]], [1, 1]);<br />      }<br /><br />      const output = model.predict(x, {});<br />      const probs = Array.from(output.probabilities.dataSync());<br />      const classes = Array.from(output.all_class_ids.dataSync());<br />      const result = Object.fromEntries(classes.map((classId, i) => [classId, probs[i]]));<br />      return result; // {0: 0.8, 1: 0.15, 2: 0.05} probability of each performance quality<br />    });<br />  }<br /><br />  // Register our \u2018predict\u2019 RPC handler (pseudo-code)<br />  // process is an abstraction of the Node.js side of the communication channel<br />  // with the JVM<br />  process.registerHandler(\u2018predict\u2019, input => {    <br />    const result = predict(input);<br />    return Promise.resolve(result);<br />  });<br />}<br /><br />main();<br /><br /></code></pre>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh_LSM44GUKmnol9a7PawWI7jtkqcrll8eeU35Ms0_wU0FZ913ecCHkyfBI1AHY9ZIqGoxIkIzp6NwumZSsKBmW3hlUPPbfy1cxFVZDCLhxavQ4FBMX970sLpzO5qmPZ7zvcHDiDAIM3GmPUT3ojcZE1HzU7g5BAkBEmJ8Idw6nga8bb-9r7LxztMiK/s1600/image3.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"></td></tr></tbody></table>  <p><a href=\"https://expressjs.com/\">Express</a> could replace Rest.li\u2019s role and the feature generation pieces would need to be ported to Node.js, but everything else remains the same. As you can see, the architecture is cleaner and requires less mental hoops to manage both Java and Node.js in the same stack. </p><h3>An Unexpected Win</h3>  <p>In the architecture we described above, the external processes do not have to be Node.js.  The library that we use to manage the external process is pretty straightforward to implement in most technologies. In fact, we could have chosen Python for the external processes as it\u2019s popular for this ML use case. So what are the reasons we stuck with Node.js? Well, there\u2019s two: (1) we already had a Node.js implementation for the external process infrastructure and would have had to develop a new one for Python, and (2) it turns out that Node.js is also slightly faster at making the predictions due to the pre/post processing benefitting from the JIT compiler of JavaScript. </p><p>In order to prove this to ourselves, we took samples (~100k) from our real world prediction inputs and ran them against both Node.js and Python. The test bed was not exactly our production stack (we didn\u2019t include the JVM side), but it was close enough for our purposes. The results are shown below: </p> <div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border: none; border-collapse: collapse; width: 468pt;\">        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Stack</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">50th percentile</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Delta (from Python)</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Python</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">1.872 ms</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">0%</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 11pt; font-family: 'Proxima Nova',sans-serif; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Node.js</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">1.713 ms</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">-8.47%</span></p>                </td>            </tr>        </tbody>    </table></div>  <p><br />The results show that Node.js is almost 10% faster at performing inference for our specific model. Of course, performance may vary based on model architectures and the amount of pre and post processing being performed in Node. These results were from our model running on a typical production machine. Results may also vary due to model complexity, machine differences, and so on. <br /></p><p><a href=\"https://github.com/linkedin/performance-quality-models\">Checkout our README in the open source repo</a> to find out how we tested the model in Python and Node.js. </p><h3>Looking to the future</h3>  <p>Our current unique architecture does have some areas for improvement.  Probably the biggest opportunity is to address the uniqueness of this multi stack architecture itself. The mix of both Java and Node.js technologies adds additional cognitive overhead and complexity during design, development, debugging, operations, maintenance - however as previously stated you could move the whole stack to Node to simplify matters, so this is a solvable problem. </p><p>Another potential area for improvement comes from currently using a single threaded architecture on the Node.js side. Because of this, only a single prediction currently occurs at a time so latency sometimes includes some amount of queueing time. This can potentially be worked around by using Node <a href=\"https://nodejs.org/api/worker_threads.html\">worker threads</a> for parallel execution that may be considered in future versions of this implementation. In our particular case, however, prediction is very fast even as it stands, so we do not feel the need to explore this right now.  </p><h3>Summary</h3>  <p>The availability of TensorFlow.js gave us an easy option to deploy our model to serve production use cases when other options were not quite suitable or available to us. While our unique requirements resulted in using non-standard architecture (the mixture of the JVM and Node.js), TensorFlow.js can be used to an even greater effect in a homogeneous Node.js serving stack resulting in a very clean and performant architecture. With our open source performance quality model, a full stack JS engineer can personalize performance and improve their user engagement and we look forward to seeing how others use our open sourced model to do just that on their own websites.<br /></p><h3>Acknowledgements</h3>  <p>This success would not be possible without the tremendous work by <a href=\"https://www.linkedin.com/in/pvijayanathan/\">Prasanna Vijayanathan</a> and <a href=\"https://www.linkedin.com/in/niranjan-sharma-4a235850/\">Niranjan Sharma</a>. A huge thank you to <a href=\"https://www.linkedin.com/in/ACoAAABvvfsBDUf12dGVDTAEKoH8Q-SmA9DgvI8/\">Ritesh Maheshwari</a> and <a href=\"https://www.linkedin.com/in/rahulkumar14/\">Rahul Kumar</a> for their support and encouragement. Special thanks to <a href=\"https://www.linkedin.com/in/pyu10055/\">Ping Yu</a> (Google) and <a href=\"https://www.linkedin.com/in/creativetech/\">Jason Mayes</a> (Google) for their continued support and feedback. </p>",
            "pubdate": "Tue, 29 Mar 2022 18:01:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                29
            ],
            "email_sent": true
        },
        "How to migrate from BoostedTrees Estimators to TensorFlow Decision Forests": {
            "url": "https://blog.tensorflow.org/2022/04/how-to-migrate-from-boostedtrees.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW0Cq5zZzPAlNtIN1Fmb_Xxw-r_0c5_60QzOrJYVTJcvFxKjte2zA6kwQxgWbXOBKizFIWVneiaa2KnpP0cNdTWHxy5d3OApFMIO2WnojCRgrdnqVihqZW5DqztA3GkKuY-fv7Anwn8PpDfwPuCaNYjbaRwASdviQT2eei2jAf6xlF2xOEYohCaYEJ/s1600/image1.gif\" style=\"display: none;\" />  <p><em>Posted by <a href=\"https://twitter.com/mat_gb\">Mathieu Guillame-Bert</a> and <a href=\"https://twitter.com/random_forests\">Josh Gordon</a> for the TensorFlow team</em></p><a name=\"more\"></a><p>Decision forest models like <a href=\"https://developers.google.com/machine-learning/glossary#random-forest\">random forests</a> and <a href=\"https://developers.google.com/machine-learning/glossary#gradient-boosted-decision-trees-gbt\">gradient boosted trees</a> are often the most effective tools available for working with tabular data. They provide many advantages over neural networks, including being easier to configure, and faster to train. Using trees greatly reduces the amount of code required to prepare your dataset, as they natively handle numeric, categorical, and missing features. And they often give good results out-of-the-box, with interpretable properties.  </p><p>Although we usually think of TensorFlow as a library to train neural networks, a popular use case at Google is to use TensorFlow to create decision forests.  </p>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiW0Cq5zZzPAlNtIN1Fmb_Xxw-r_0c5_60QzOrJYVTJcvFxKjte2zA6kwQxgWbXOBKizFIWVneiaa2KnpP0cNdTWHxy5d3OApFMIO2WnojCRgrdnqVihqZW5DqztA3GkKuY-fv7Anwn8PpDfwPuCaNYjbaRwASdviQT2eei2jAf6xlF2xOEYohCaYEJ/s1600/image1.gif\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An animation of a decision tree classifying data.</td></tr></tbody></table>   <p>This article provides a migration guide if you were previously creating tree-based models using <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier\">tf.estimator.BoostedTrees</a>, which was <a href=\"https://blog.tensorflow.org/2019/03/how-to-train-boosted-trees-models-in-tensorflow.html\">introduced</a> in 2019. The Estimator API took care of much of the complexity of working with models in production, including distributed training and serialization. However, it is no longer recommended for new code. </p><p>If you are starting a new project, we recommend that you use <a href=\"https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\">TensorFlow Decision Forests</a> (TF-DF). This library provides state-of-the-art algorithms for training, serving and interpreting decision forest models, with many benefits over the previous approach, notably regarding quality, speed, and ease of use.  </p><p>To start, here are equivalent examples using the Estimator API and TF-DF to create a boosted tree model. </p><p><strong>Previously, this is how you would train a gradient boosted tree models with tf.estimator.BoostedTrees (no longer recommended)</strong></p> <pre><code class=\"\"><br />import tensorflow as tf<br /><br /># Dataset generators<br />def make_dataset_fn(dataset_path):<br />    def make_dataset():<br />        data = ... # read dataset<br />        return tf.data.Dataset.from_tensor_slices(...data...).repeat(10).batch(64)<br />    return make_dataset<br /><br /># List the possible values for the feature \"f_2\".<br />f_2_dictionary = [\"NA\", \"red\", \"blue\", \"green\"]<br /><br /># The feature columns define the input features of the model.<br />feature_columns = [<br />    tf.feature_column.numeric_column(\"f_1\"),<br />    tf.feature_column.indicator_column(<br />       tf.feature_column.categorical_column_with_vocabulary_list(\"f_2\",<br />         f_2_dictionary,<br />         # A special value \"missing\" is used to represent missing values.<br />         default_value=0)<br />       ),<br />    ]<br /><br /># Configure the estimator<br />estimator = boosted_trees.BoostedTreesClassifier(<br />          n_trees=1000,<br />          feature_columns=feature_columns,<br />          n_classes=3,<br />          # Rule of thumb proposed in the BoostedTreesClassifier documentation.<br />          n_batches_per_layer=max(2, int(len(train_df) / 2 / FLAGS.batch_size)),<br />      )<br /><br /># Stop the training is the validation loss stop decreasing.<br />early_stopping_hook = early_stopping.stop_if_no_decrease_hook(<br />      estimator,<br />      metric_name=\"loss\",<br />      max_steps_without_decrease=100,<br />      min_steps=50)<br /><br />tf.estimator.train_and_evaluate(<br />      estimator,<br />      train_spec=tf.estimator.TrainSpec(<br />          make_dataset_fn(train_path),<br />          hooks=[<br />              # Early stopping needs a CheckpointSaverHook.<br />              tf.train.CheckpointSaverHook(<br />                  checkpoint_dir=input_config.raw.temp_dir, save_steps=500),<br />              early_stopping_hook,<br />          ]),<br />      eval_spec=tf.estimator.EvalSpec(make_dataset_fn(valid_path)))<br /></code></pre>   <p><strong>How to train the same model using TensorFlow Decision Forests</strong></p> <pre><code class=\"\"><br />import tensorflow_decision_forests as tfdf<br /><br /># Load the datasets<br /># This code is similar to the estimator.<br />def make_dataset(dataset_path):<br />    data = ... # read dataset<br />    return tf.data.Dataset.from_tensor_slices(...data...).batch(64)<br /><br />train_dataset = make_dataset(train_path)<br />valid_dataset = make_dataset(valid_path)<br /><br /># List the input features of the model.<br />features = [<br />  tfdf.keras.FeatureUsage(\"f_1\", keras.FeatureSemantic.NUMERICAL),<br />  tfdf.keras.FeatureUsage(\"f_2\", keras.FeatureSemantic.CATEGORICAL),<br />]<br /><br />model = tfdf.keras.GradientBoostedTreesModel(<br />  task = tfdf.keras.Task.CLASSIFICATION,<br />  num_trees=1000,<br />  features=features,<br />  exclude_non_specified_features=True)<br /><br />model.fit(train_dataset, valid_dataset)<br /><br /># Export the model to a SavedModel.<br />model.save(\"project/model\")<br /></code></pre>   <p><strong>Remarks</strong></p><ul> <li>While not explicit in this example, early stopping is automatically enabled and configured.  <li>The dictionary of the \"f_2\" features is automatically built and optimized (e.g. rare values are merged into an out-of-vocabulary item).  <li>The number of classes (3 in this example) is automatically determined from the dataset.  <li>The batch size (64 in this example), has no impact on the model training. Larger values are often preferable as it makes reading the dataset more efficient. </li></ul><p>TF-DF is all about ease of use, and the previous example can be further simplified and improved, as shown next. </p><p><strong>How to train a TensorFlow Decision Forests (recommended solution)</strong></p> <pre><code class=\"\">import tensorflow_decision_forests as tfdf<br />import pandas as pd<br /><br /># Pandas dataset can be used easily with pd_dataframe_to_tf_dataset.<br />train_df = pd.read_csv(\"project/train.csv\")<br /><br /># Convert the Pandas dataframe into a TensorFlow dataset.<br />train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"my_label\")<br /><br />model = tfdf.keras.GradientBoostedTreeModel(num_trees=1000)<br />model.fit(train_dataset)<br /></code></pre>  <p><strong>Remarks</strong></p><ul> <li>We did not specify the semantics (e.g. numerical, or categorical) of the features. In this case, the semantics will be automatically inferred.  <li>We also didn\u2019t list which input features to use. In this case, all the columns (except for the label) will be used. The list and semantics of the input feature is visible in the training logs, or with the model inspector API.  <li>We did not specify any validation dataset. Each algorithm will optionally extract a validation dataset from the training examples as best for the algorithm. For example, by default, GradientBoostedTreeModel uses 10% of the training data for validation if no validation dataset is provided. </li></ul><p>Now, let\u2019s look at a couple differences between the Estimator API and TF-DF. </p><h3>Differences between the Estimator API and TF-DF</h3>  <p><strong>Type of algorithms</strong></p><p>TF-DF is a collection of decision forest algorithms. This includes (but is not limited to) the Gradient Boosted Trees available with the Estimator API. Notably, TF-DF also supports <a href=\"https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel\">Random Forest</a> (great for nosy datasets) and a <a href=\"https://g3doc.corp.google.com/third_party/tensorflow/google/g3doc/use_tensorflow/migration_decision_forests.md?cl=432890187#:~:text=nosy%20datasets)%20and%20a-,CART,-implementation%20(great%20for\">CART</a> implementation (great for model interpretation). </p><p>In addition, for each of those algorithms, TF-DF includes many variations found in the literature and validated experimentally [<a href=\"https://arxiv.org/abs/2009.09991\">1</a>, <a href=\"https://arxiv.org/abs/1603.02754\">2</a>, <a href=\"https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf\">3</a>]. </p><p><strong>Exact vs approximate splits</strong></p><p>The TF1 GBT Estimator is an approximated tree learning algorithm. Informally, the Estimator <a href=\"http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf\">builds</a> trees by only considering a random subset of examples and a random subset of the conditions at each step. </p><p>By default, TF-DF is an exact tree training algorithm. Informally, TF-DF considers all the training examples and all the possible splits at each step. This is a more common and often better performing solution. </p><p>While sometimes faster on larger datasets (>10B examples x features), the estimator approximation are often less accurate (as more trees need to be grown to reach the same quality). In a small dataset (&lt;100M examples x features), the form of approximated training implemented in the Estimator can even be slower than exact training. </p><p>TF-DF also supports various types of \"approximated\" tree training. The recommended approach is to use exact training, and optionally test approximated training on large datasets. </p><p><strong>Inference</strong></p><p>The Estimator runs model inference using the <a href=\"https://developers.google.com/machine-learning/decision-forests/decision-trees\">top-down tree routing algorithm</a>. TF-DF uses an extension of the <a href=\"http://ecmlpkdd2017.ijs.si/papers/paperID718.pdf\">QuickScorer</a> algorithm. </p><p>While both algorithms return the exact same results, the top-down algorithm is less efficient because of exceeding branching predictions and cache misses. TF-DF inference is generally 10x faster on the same model. </p><p>For latency critical applications TF-DF offers a <a href=\"https://g3doc.corp.google.com/third_party/tensorflow/google/g3doc/use_tensorflow/migration_decision_forests.md?cl=432890187#:~:text=TF%2DDF%20offers%20a-,C%2B%2B%20API,-.%20It%20provides%20often\">C++ API</a>. It provides often ~1\u00b5s/example/core inference time. This is often a 50x-1000x speed-up over TF SavedModel inference (especially on small batches). </p><p><strong>Multi-head models</strong></p><p>The Estimator supports multi-head models (a model that outputs multiple predictions). TF-DF (currently) does not support multi-head models directly, however, using the Keras Functional API, multiple TF-DF models trained in parallel can be assembled into a multi-head model. </p><h2>Learning more</h2>  <p>You can learn more about TensorFlow Decision Forests by visiting the <a href=\"https://www.tensorflow.org/decision_forests/\">website</a>. If you\u2019re new to this library, the <a href=\"https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\">beginner example</a> is a good place to start. Experienced TensorFlow users can visit this <a href=\"https://github.com/tensorflow/decision-forests/blob/main/documentation/migration.md\">guide</a> for important details about the difference between using decision forests and neural networks in TensorFlow, including how to configure your training pipeline, and tips on Dataset I/O. You can also see <a href=\"https://www.tensorflow.org/guide/migrate/migrating_estimator\">Migrate from Estimator to Keras APIs</a> for more info on migrating from Estimators to Keras in general.  </p>",
            "pubdate": "Mon, 04 Apr 2022 16:02:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                4
            ],
            "email_sent": true
        },
        "Video Classification on Edge Devices with TensorFlow Lite and MoViNet": {
            "url": "https://blog.tensorflow.org/2022/04/video-classification-on-edge-devices.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZEUGTtz5pQ1Oz86fmupeAjEoXcDZ9Ar4I4F5YhL6aNJbyAbz-AIJl9LRb-mzQ4Yh80VgEUM1HFhQaLWHeArM39HlOAwuVxHQw4lczTLqP0sSVloprJTx-blhannqi5tnsJksAQv7PpRQqj5IwgUdoxcrMpDjBNiwUJ32ljMMSPl9djYefKqpmnhWF/s1600/image3.gif\" style=\"display: none;\" /> <p><em>Posted by <a href=\"https://twitter.com/hyperparticle\">Dan Kondratyuk</a>, <a href=\"https://scholar.google.com/citations?user=1H9CkZgAAAAJ&amp;hl=en\">Liangzhe Yuan</a>, Google Research and <a href=\"https://twitter.com/khanhlvg\">Khanh LeViet</a>, TensorFlow Developer Relations</em></p><a name=\"more\"></a> <p>We are excited to announce <a href=\"https://github.com/tensorflow/models/tree/master/official/projects/movinet\">MoViNets</a> (pronounced \u201cmovie nets\u201d), a family of new mobile-optimized model architectures for video classification. The models are trained on the <a href=\"https://deepmind.com/research/open-source/kinetics\">Kinetics-600 dataset</a> to be able to recognize <a href=\"https://github.com/tensorflow/models/blob/e0ed507504b4efa6780e244677ded563b36b0ee7/official/projects/movinet/files/kinetics_600_labels.txt\">600</a> different human actions (such as playing trumpet, robot dancing, bowling, and more) and can classify video streams captured on a modern smartphone in real time. You can download the pre-trained TensorFlow Lite models from <a href=\"https://tfhub.dev/s?deployment-format=lite&amp;q=movinet\">TensorFlow Hub</a> or try it out using our <a href=\"https://github.com/tensorflow/examples/tree/master/lite/examples/video_classification/raspberry_pi\">Android</a> and <a href=\"https://github.com/tensorflow/examples/tree/master/lite/examples/video_classification/raspberry_pi\">Raspberry Pi</a> demo apps, as well as fine-tune your own MoViNets with the <a href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb\">Colab demo</a> and the <a href=\"https://github.com/tensorflow/models/tree/master/official/projects/movinet\">code in the TensorFlow Model Garden</a>. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZEUGTtz5pQ1Oz86fmupeAjEoXcDZ9Ar4I4F5YhL6aNJbyAbz-AIJl9LRb-mzQ4Yh80VgEUM1HFhQaLWHeArM39HlOAwuVxHQw4lczTLqP0sSVloprJTx-blhannqi5tnsJksAQv7PpRQqj5IwgUdoxcrMpDjBNiwUJ32ljMMSPl9djYefKqpmnhWF/s1600/image3.gif\" style=\"width: 400px;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Demo from the TensorFlow Lite video classification reference app</td></tr></tbody></table> <p>Video classification is a machine learning task that takes video frames as input and predicts a single class from a larger set of classes. Video action recognition is a type of video classification where the set of predicted classes consists of human actions that happened in the frames. Video action recognition is similar to image recognition in that both take input images and output the probabilities of the images belonging to each of the predefined classes. However, a video action recognition model has to look at both the content of each frame and the spatial relationships between adjacent frames to understand the actions in the video. For example, if you look at these still images, it\u2019s difficult to tell what the person is doing. </p> <table width=\"99%\">  <tr>    <td><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFmymdyfpjdWPYjs8kD-D_MV2WUZ3JCDWv9OHYFEuDW_jQNuGVjpJRAVP3EUoy_vYIcreeSuds0VAnxRi6XiU0fEDn0evaM2JyUtcKLDaNkit8YYMpTBgx71NEBwO0wM6vrsWFpb08Ea-31gUKSbazy4FTIe2pWs-Iw7jPxHa_HCsyxfoME4sfGem-/s1600/image1.jpg\" /></td>    <td> <p> <img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3pwXcRtZKm2p2rj-f4xQxZ3ZPP8_1Prp7lsvzvsrFLLRdTHQytxvxmS5UWF6RsN8qbKQik4zHu12ubEw8iE5wVPbfDf7P1ODkFd-hhUaf1kg0krVhzP1SlqJq1sFAcIh_pqy046KsdAMSZs2srGLPKQgo6tsrW-LR-86YOyXzNnUdK34I9SoltDTt/s1600/image2.jpg\" /></td>    <td align=\"right\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjp2vkqwPGn8Ge8eSM8ZYFOWde-e__ckXSdsnk9oQB98a1JyOLJiWAU_1Vv0NwXc7Z0baJXNSoz8WU-UqF_IKiWdgoXSLek-IoYg8HL057_nNhucvhC90Z4Qrqg_w-TjDLwoAkMnzXFI4NQP5Keh6kCgEdd0THMhiX4gpGclCZcQhLF7gEVBgjYpTzo/s1600/image8.jpg\" /></td></table> <p>But if you look at the full video, it becomes clear that the person is performing jumping jacks.</p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn0aKRhnGGZ0f7RafLO82vGefPVFDBbKT0oHXTL1esoaMFMUMnaiDW8Jt2aEXFK2rweNAeYVeKeu1failQaa99xOE0V1Uoj7XgmR_BFxXvsGaFR-lKXi06ZTxNdFhYBtSDUZbHVw7DaRXYC4dRL1lvu1rjXzZRjDs1WxevTXx_ay2SbmVULn5nykg0/s1600/image6.gif\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhn0aKRhnGGZ0f7RafLO82vGefPVFDBbKT0oHXTL1esoaMFMUMnaiDW8Jt2aEXFK2rweNAeYVeKeu1failQaa99xOE0V1Uoj7XgmR_BFxXvsGaFR-lKXi06ZTxNdFhYBtSDUZbHVw7DaRXYC4dRL1lvu1rjXzZRjDs1WxevTXx_ay2SbmVULn5nykg0/s1600/image6.gif\" /></a></div>  <h4>MoViNet Model Architecture</h4>  <p><strong>MoViNets</strong> are a family of convolutional neural networks which efficiently process video streams, outputting accurate predictions with a fraction of the latency of convolutional video classifiers like <a href=\"https://paperswithcode.com/model/resnet-3d?variant=resnet-3d-18\">3D ResNets</a> or transformer-based classifiers like <a href=\"https://arxiv.org/abs/2010.11929\">ViT</a>. </p><p>Frame-based classifiers output predictions on each 2D frame independently, resulting in sub-optimal performance due to their lack of temporal reasoning. On the other hand, 3D video classifiers offer high accuracy predictions by processing all frames in a video clip simultaneously, at a cost of significant memory and latency penalties as the number of input frames increases. MoViNets offer key advantages from both 2D frame-based classifiers and 3D video classifiers while mitigating their disadvantages.  </p><p>The following figure shows a typical approach to using 3D networks with multi-clip evaluation, where the predictions of multiple overlapping subclips are averaged together. Shorter subclips result in lower latency, but reduce the overall accuracy. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiZ73NcVJRnzVrcQs5DZpMho4F9Xg0SyoLCd6IgFWQ9iGxW3G_-fRGqY_O-wHPDrqjLW8oSAnOkM9ZynHIiWKYAwDNjOFekHzpvfuJ_0jZEm8adKNk0Z8eiC1KzvK0XXxO-xY8PK492OQ49FNUDoepuxUYe3CYGrGBSIovz3gGiIOiyS3SvDvAxnloc/s1600/image5.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Diagram illustrating Multi-Clip Evaluation for 3D Video Networks </td></tr></tbody></table>  <p>MoViNets take a hybrid approach, which proposes the use of <a href=\"https://paperswithcode.com/method/causal-convolution\">causal convolutions</a> in place of 3D convolutions, allowing intermediate activations to be cached across frames with a Stream Buffer. The Stream Buffer copies the input activations of all 3D operations, which is output by the model and then input back into the model on the next clip input. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQNM1sRHmOVQfExxkKVjiF81UcbtUpbw8phofsuuRa9x0Y5cvUks5qwODh6rbpHBq-0uQ-zsrOds7gBSfl0l65wulIjX9znqjzJbzAu2Zq5ROmgV2NKQEvkEF3_V3F_D2keZJ8yqJdxZws1Lr7NdlYRzRxtlyvAAMF3al0E7fac3cY6rvb6yzAl5rJ/s1600/image7.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Diagram illustrating Streaming Evaluation for MoViNets</td></tr></tbody></table>  <p>The result is that MoViNets can receive one frame input at a time, reducing peak memory usage while resulting in no loss of accuracy, with predictions equivalent to inputting all frames at once like a 3D video classifier. MoViNets additionally leverage <a href=\"https://paperswithcode.com/task/architecture-search\">Neural Architecture Search (NAS)</a> by searching for efficient configurations of models on video datasets (specifically Kinetics 600) across network width, depth, and resolution. </p><p>The result is a set of action classifiers that can output temporally-stable predictions that smoothly transition based on frame content. Below is an example plot of MoViNet-A2 making predictions on each frame on a video clip of skateboarding. Notice how the initial scene with a small amount of motion has relatively constant predictions, while the next scene with much larger motion causes a dramatic shift in predicted classes. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiX_uoXPIJrekwRHmlnPgVZSckNBwz5B1cBB-oeWPXIc-19jpLjyL9lRcL1ZETHtjdD15AkWR4PsOAyDUCsvJyQ_jBfMfQ3Y6aEX5rMn0mdmonRHKBpWG0QdZRIL3ZHOJrm2zDOK_c3hg5iZLElZUw2tn2yROi_ASL8oRwC3-B7pL_3MZW5nVALvJxJ/s1600/image4.gif\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A video plotting the top-5 predictions of MoViNet-A2 over time on an example 8-second (25 fps) skateboarding video clip. Create your own plots with <a href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/official/projects/movinet/tools/plot_movinet_video_stream_predictions.ipynb\">this Colab notebook</a>.</td></tr>    </tbody></table>   <p>MoViNets need a few modifications to be able to run effectively on edge devices. We start with <a href=\"https://tfhub.dev/tensorflow/movinet/a0/stream/kinetics-600/classification\">MoViNet-A0-Stream</a>, <a href=\"https://tfhub.dev/tensorflow/movinet/a1/stream/kinetics-600/classification\">MoViNet-A1-Stream</a>, and <a href=\"https://tfhub.dev/tensorflow/movinet/a2/stream/kinetics-600/classification\">MoViNet-A2-Stream</a>, which represent the smaller models that can feasibly run in real time (20 fps or higher). To effectively quantize MoViNet, we adapt a few modifications to the model architecture - the <a href=\"https://paperswithcode.com/method/hard-swish\">hard swish</a> activation is replaced by <a href=\"https://paperswithcode.com/method/relu6\">ReLU6</a>, and <a href=\"https://paperswithcode.com/method/squeeze-and-excitation-block\">Squeeze-and-Excitation</a> layers are removed in the original architectures, which results in 3-4 p.p accuracy drop on Kinetics-600. We then convert the models to <a href=\"https://www.tensorflow.org/lite\">TensorFlow Lite</a> and use <a href=\"https://www.tensorflow.org/lite/performance/post_training_quantization\">integer-based post-training quantization</a> (as well as <a href=\"https://www.tensorflow.org/lite/performance/post_training_float16_quant\">float16 quantization</a>) to reduce the model sizes and make them run faster on mobile CPUs. The integer-based post-training quantization process further introduces 2-3 p.p. accuracy loss. Compared to the original MoViNets, quantized MoViNets lag behind in accuracy on full 10-second Kinetics 600 clips (5-7 p.p. accuracy reduction in total), but in practice they are able to provide very accurate predictions on daily human actions, e.g., push ups, dancing, and playing piano. In the future, we plan to train with <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training\">quantization-aware training</a> to bridge this accuracy gap.  </p>     <p>We benchmark quantized A0, A1, and A2 on real hardware and the model inference time achieves 200, 120, and 60 fps respectively on Pixel 4 CPU. In practice, due to the input pipeline overhead, we see increased latency closer to 20-60 fps when running on Android with a camera as input. </p>  <div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border: none; border-collapse: collapse;\">        <tbody>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Model</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Quantization</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Top-1 Accuracy (%)</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Latency&nbsp;</span><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\"><br /></span><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">(ms, Pixel 4 CPU)</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Model Size (MB)</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Recommended Input</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A0-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">int8</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">65.0</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">4.80</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">3.1</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">172 x 172, 5 fps</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A1-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">int8</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">70.1</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">8.35</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">4.5</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">172 x 172, 5 fps</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A2-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">int8</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">72.2</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">15.76</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">5.1</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">224 x 224, 5 fps</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A0-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">float16</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">71.5</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">17.47</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">7.6</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">172 x 172, 5 fps</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A1-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">float16</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">76.0</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">34.82</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">13</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">172 x 172, 5 fps</span></p>                </td>            </tr>            <tr style=\"height: 26.25pt;\">                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 14.3891%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">A2-Stream</span></p>                </td>                <td style=\"border-width: 0.75pt; border-style: solid; border-color: rgb(204, 204, 204); vertical-align: bottom; padding: 2pt; overflow: hidden; width: 9.8874%;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">float16</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">77.5</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">76.31</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">15</span></p>                </td>                <td style=\"border-left: solid #cccccc 0.75pt; border-right: solid #cccccc 0.75pt; border-bottom: solid #cccccc 0.75pt; border-top: solid #cccccc 0.75pt; vertical-align: bottom; padding: 2pt 2pt 2pt 2pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">224 x 224, 5 fps</span></p>                </td>            </tr>        </tbody>    </table></div>  <p><strong>Train a Custom Model</strong></p><p>You can train your own video classifier model using the <a href=\"https://github.com/tensorflow/models/tree/master/official/projects/movinet\">MoViNet codebase</a> in the TensorFlow Model Garden. The provided <a href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/movinet/movinet_tutorial.ipynb\">Colab notebook</a> provides specific steps on how to fine-tune a pretrained video classifier on another dataset. </p><p><strong>Future Steps</strong></p><p>We are excited to see on-device online video action recognition powered by MoViNets, which demonstrate highly efficient performance. In the future, we plan to support quantize-aware training for MoViNets to mitigate the quantization accuracy loss. We also are interested in extending MoViNets as the backbone for more on-device video tasks, e.g. video object detection, video object segmentation, visual tracking, pose estimation, and more. </p><p><strong>Acknowledgement</strong></p><p>We would like to extend a big thanks to Yeqing Li for supporting MoViNets in TensorFlow Model Garden, Boqing Gong, Huisheng Wang, and Ting Liu for project guidance, Lu Wang for code reviews, and the TensorFlow Hub team for hosting our models. </p>",
            "pubdate": "Thu, 14 Apr 2022 16:04:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                14
            ],
            "email_sent": true
        },
        "Why Karrot Uses TFX, and How to Improve Productivity on ML Pipeline Development": {
            "url": "https://blog.tensorflow.org/2022/05/why-karrot-uses-tfx.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhL6IR5tPygfQ1vGwPYF8w9W5TjhAGUwNULyl8-laV4IzLH9iOYx5B8xk70btanduPLdpp8Oox5PKj5lXHAnBZUNICsPQqS04cggdRn6TUmaM6lYlaEOgRl5bX4ZSkzLZcJvPX_fYyTdiKKSWJTxHNJcc6v4UPH6SPz2_VnxhV88E3dQZ55kRAVVuIn/s1600/karrot.jpeg\" style=\"display: none;\" /><i>Posted by Ukjae Jeong, Gyoung-yoon Yoo, and Myeonghyeon Song from Karrot</i><a name=\"more\"></a><p></p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhL6IR5tPygfQ1vGwPYF8w9W5TjhAGUwNULyl8-laV4IzLH9iOYx5B8xk70btanduPLdpp8Oox5PKj5lXHAnBZUNICsPQqS04cggdRn6TUmaM6lYlaEOgRl5bX4ZSkzLZcJvPX_fYyTdiKKSWJTxHNJcc6v4UPH6SPz2_VnxhV88E3dQZ55kRAVVuIn/s1600/karrot.jpeg\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhL6IR5tPygfQ1vGwPYF8w9W5TjhAGUwNULyl8-laV4IzLH9iOYx5B8xk70btanduPLdpp8Oox5PKj5lXHAnBZUNICsPQqS04cggdRn6TUmaM6lYlaEOgRl5bX4ZSkzLZcJvPX_fYyTdiKKSWJTxHNJcc6v4UPH6SPz2_VnxhV88E3dQZ55kRAVVuIn/s1600/karrot.jpeg\" /></a></div><p><a href=\"https://us.karrotmarket.com\">Karrot</a> (the global service of <a href=\"https://www.daangn.com\">Danggeun Market</a> in Korea) is a local community service app that connects neighbors based on a secondhand marketplace. Danggeun Market was launched in 2015, and over 23 million people in Korea are using Danggeun Market in their local communities. Currently, Karrot is operated in 440 local communities in four countries: the U.K., the U.S., Canada, and Japan. In our service, scrolling through feeds to find inexpensive and useful items has become a daily pleasure for users. For better user experiences, we\u2019ve been applying several machine learning models such as recommendation models. </p><p>We are also working on ways to effectively and efficiently apply ML models. In particular, we\u2019re putting lots of effort into building machine learning pipelines for periodic deployment, rapid experiments, and continuous model improvement. </p><p>For the ML pipelines, we\u2019ve been using TFX (TensorFlow Extended) for production. So in this article, we will briefly introduce why we use TFX, and how we utilize TFX to improve productivity. </p><h4><strong>Machine Learning in Karrot</strong></h4>  <p>There are many ML projects inside Karrot. ML models are running inside the services. For example, we use automation models to detect fraud, and there are recommendation models to improve the user experience on our app feed. If you are interested in detailed descriptions of the models, please refer to <a href=\"https://medium.com/daangn\">our team blogs</a>, which are written in Korean. </p><p>As we\u2019ve been using Kubeflow for our ML models, we were able to periodically train, experiment, and deploy models but still, we had some pain points. As we started to use TFX with Kubeflow last year, TFX pushed this line further and let the team easily use our production pipelines. </p><h4><strong>How TFX helps us with production ML</strong></h4>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4t-wkbET2oIO6sJtP8Ig5R59tJRn28hF9wcSuelwOIddtZYLuDEiP7kKZNHl_azrtxaNDnkAio97duVXNAhn-H0SNkGPlUHT2H739AZS1Jh3LdywZMBw2p5Y0E9I8cEpCC9g8Jh6r2OqBemoKSNp9MvjsWVZYQ4MCj5D6ACNrVR_JKAN95WZ-clD5/s1600/productionMLtfx.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">TFX helps build and deploy production ML pipelines easily with open and extendable design.</td></tr></tbody></table> <p>TFX, completely open-sourced in 2019, is an end-to-end platform for production ML pipelines. It supports writing ML workflows in component units, which then can be run in multiple environments - <a href=\"https://beam.apache.org\">Apache Beam</a>, <a href=\"https://cloud.google.com/dataflow\">Dataflow</a>, <a href=\"https://www.kubeflow.org\">Kubeflow</a>, and <a href=\"https://airflow.apache.org\">Airflow</a>. It also comes with well-written standard components for data ingestion/transformation, training, and deployment. </p><h5>Standard Components</h5>  <p>TFX provides several standard components. If you are looking for components for data ingestion, there are <a href=\"https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/CsvExampleGen\">CsvExampleGen</a> based on local CSV files, <a href=\"https://github.com/tensorflow/tfx/tree/master/tfx/examples/custom_components/presto_example_gen/presto_component\">PrestoExampleGen</a>, and <a href=\"http://PrestoExampleGen\">BigQueryExampleGen</a> which can ingest data directly from Presto, BigQuery, and many other sources with some customization. So you can easily process data from multiple sources just by connecting pre-built components to your TFX pipelines. </p><p>It can also handle large-scale data processing smoothly. Since the <a href=\"https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Transform\">Transform</a> component that performs feature engineering is implemented on <a href=\"https://beam.apache.org\">Apache Beam</a>, you can execute it on GCP <a href=\"https://cloud.google.com/dataflow\">Dataflow</a> or another compute cluster in a distributed manner. </p><p>Of course, many other convenient components exist and are added constantly. </p><h5>Component Reusability</h5>  <p>In order to adapt TFX to our product, there is a need for custom components. TFX has a well-structured component design that enables us to create custom components naturally and easily connect them to existing TFX pipelines. A simple Python function or container can be transformed into a TFX component, or you can write the whole component in the exact same way as standard components are written. For more details, check out <a href=\"https://www.tensorflow.org/tfx/guide/understanding_custom_components\">the custom component guide</a>. </p><p>To enhance our productivity by delivering these advantages, we share custom components that have similar use cases among our ML pipelines as an internal library of Karrot Market. </p><h5>Various Runners are Supported</h5>  <p>TFX is compatible with a variety of environments. It can be run locally on your laptop or run on DataFlow, GCP\u2019s batch data processing service compatible with Apache Beam. You can visualize the output by manually running each component in a Jupyter Notebook. TFX also supports KubeFlow and Vertex AI, which have recently been released with new features as well. Therefore, the pipeline code is written once, and can then be run almost anywhere. We can simply create development, experiment, and deployment environments at once. For that reason, the burden of deploying models to production was significantly reduced by using TFX for our services. </p><h4><strong>Technical lessons</strong></h4>  <p>As we set up our ML pipelines with TFX, code qualities and our experiences in model development have increased. </p><p>However, there were difficulties. We didn't have a uniform project structure or best practices among our team. Maybe this is because TFX itself is relatively new and we've been using it before version 1. It became harder to understand codes and start to contribute. As the pipelines are becoming larger and more complex, it\u2019s getting harder to understand the meaning of custom components, corresponding config values, and dependencies. In addition, it was difficult to introduce some of the latest features to the team. </p><h4><strong>Improving the Development Experience</strong></h4>  <p>We decided to create and use a template for TFX pipelines to make it easier to understand each other's code, implement pipelines with the same pattern, and share know-how with each other. We merged components frequently used in Karrot and put them in a shared library so that ML pipelines can be developed very quickly. </p><p>It was expected that the template would accelerate the development of new projects. In addition, as mentioned above, we expected that each project would have a similar structure, making it easier to understand each other's projects. </p><p>So far, we have briefly introduced the template project. Here are some of our considerations to make better use of TFX in this project. </p><h5>Configuration first</h5>  <p>We prioritize our configuration first. It should be enough to understand how pipelines work by reading their configuration. If we can understand specific settings very easily, we can set up various experiments and proceed with them to AB testing. </p><p><code>example_gen_config.proto</code> written in Protocol Buffer(Protobuf), denotes the specification of config. <code>config.pbtxt</code> holds the values, and <code>pipeline.py</code> builds up the pipeline. </p>  <pre><code class=\"\">// config.pbtxt<br />example_gen_config {<br />    big_query_example_gen_config {<br />        query: \"# query for example gen\"<br />    }<br /><br /><br />    ...<br />}<br /><br />...<br /></code></pre>  <pre><code class=\"\">// example_gen_config.proto<br />message ExampleGenConfig {<br />    oneof config {<br />        BigQueryExampleGenConfig big_query_example_gen_config = 1;<br />        CsvExampleGenConfig csv_example_gen_config = 2;<br />    }<br /><br />    ...<br />}<br /><br />// When BigQueryExampleGen is used<br />message BigQueryExampleGenConfig {<br />    optional string query = 1;<br />}<br /><br />// When CsvExampleGenConfig is used<br />message CsvExampleGenConfig {<br />    optional string input_base = 1;<br />}<br /></code></pre>  <pre><code class=\"\"># pipeline.py<br />def create_pipeline(config):<br />   ...<br />   example_gen = _create_example_gen(config.example_gen_config)<br />   ...<br /><br /><br /><br /><br />def _create_example_gen(config: example_gen_config_pb2.ExampleGenConfig):<br />    ...<br /><br />    if config.HasField(\"big_query_example_gen_config\"):<br />        ...<br />        return ...<br /><br /><br />    if config.HasField(\"csv_example_gen_config\"):<br />        ...<br />        return ...<br /><br /><br />    raise ...<br /></code></pre> <p>All configurations of ExampleGen are determined by a single <code>ExampleGenConfig</code> message. Similarly, all pipeline components only depend on their configs and are created from them. This way, you can understand the structure of the pipeline just by looking at the configuration file. There is also the intention to make customization and code understanding easier by separating the part that defines each component. </p><p>For example, let's assume the following situation: In order to test the data transformation later, the Transform component needs to support various data processing methods. You might want to add a data augmentation process in the transform component. Then it should be done by adding a config related to the data augmentation function. Similarly, you can extend the predefined Protobuf specification to easily support multiple processing methods and make it easy to see which processing method to use. </p><h5>Managing Configs with Protobuf</h5>  <p>About the example code above, some people may wonder why they use Protobuf as a configuration tool. There are several reasons for this, and we will compare advantages with YAML, which is one of the common practices for configuration. </p><p>First, Protobuf has a robust interface, and validation such as type checking is convenient. There is no need to check whether any field is defined, as Protobuf defines the object structure in advance. In addition, it is useful to support backward/forward compatibility in a project under active development. </p><p>Also, you can easily check the pipeline structure. YAML has a hierarchical structure, but in the case of hydra, which is often used in the machine learning ecosystem, the stage (e.g. production, dev, alpha) settings are divided into several files, so we thought that Protobuf has better stability and visibility. </p><p>If you use Protobuf as your project setup tool, many of the Protobuf definitions defined in TFX can be reused. </p><h5>TensorFlow Ecosystem with Bazel</h5>  <p>Bazel is a language-independent build system that is easy to extend and supports a variety of languages and tools. From simple projects to large projects using multiple languages and tools, it can be used quickly and concisely in most situations. For more information, please refer to <a href=\"https://bazel.build/start/bazel-vision\">Bazel Vision</a> on the Bazel documentation page. </p><p>Using Bazel in a Python project is an uncommon setting, but we used Bazel as the project build system of the TFX template project. A brief introduction to the reason is as follows. </p><p>First of all, it works really well with Protobuf. Because Bazel is a language-independent build system, you can easily tie your Protobuf build artifacts as dependencies with other builds without worry. In addition, the Protocol Buffer repository itself uses Bazel, so it is easy to integrate it into the Bazel-based project. </p><p>The second reason is the special environment of the TensorFlow ecosystem. Many projects in the TensorFlow ecosystem use Bazel, and TFX also uses Bazel, so you can easily link builds with other projects (TensorFlow, TFX) using Bazel. </p><h5>Internal Custom TFX Modules</h5>  <p>As mentioned before, we\u2019ve been building an internal library for the custom TFX modules (especially the custom components) that are frequently used across multiple projects. Anyone in Karrot can add their components and share them with the team. </p><p>For example, we are using ArgoCD to manage applications(e.g. TF Serving) in Kubernetes clusters, so if someone develops a component for deploying with ArgoCD, we can easily share it via an internal library. The library now contains several custom modules for our team for productivity. </p><p>The reason why we can share our custom features as an internal shared library is probably thanks to the modular structure of TFX. Through this, we were able to improve the overall productivity of the team easily. We can reuse most of the components that were developed from several projects, and develop new projects very easily. </p><h4>Conclusion</h4>  <p>TFX provides lots of great features to develop production ML pipelines. We\u2019re using TFX on Kubeflow for ML pipelines to develop, experiment, and deploy in a better way, and it brings us many benefits. So we decided to introduce how we are using TFX in this blog post. </p><p>To learn more about Karrot, check out our website (<a href=\"https://www.daangn.com/\">Korea</a>, <a href=\"https://us.karrotmarket.com/\">US</a>, and <a href=\"https://ca.karrotmarket.com/\">Canada</a>). For the TFX, check out the <a href=\"https://www.tensorflow.org/tfx\">TFX documentation page</a>. </p>",
            "pubdate": "Fri, 06 May 2022 18:30:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                6
            ],
            "email_sent": true
        },
        "Portrait Depth API: Turning a Single Image into a 3D Photo with TensorFlow.js": {
            "url": "https://blog.tensorflow.org/2022/05/portrait-depth-api-turning-single-image.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8HBDNHcne_-2GHUqKFG0g2XmniU0vGySZRc3NPDEoqPqvK36cP-4vgvIawOJWNQzrLo90Xx7WAAOvY3V4HjFpXMGMcyWRvuI8e3JCzBEoyZv2P--8ldmF3_q3o2YgfjlCMZXOL_0YhWIOAKPCUTn4X3a5zkL9PMgctqPvGqJUohOFkjkmBMRbkiG/s1600/ezgif.com-gif-maker%20%2844%29.gif\" style=\"display: none;\" /> <p><em>Posted by <a href=\"https://www.duruofei.com\">Ruofei Du</a>, <a href=\"https://www.zhangyinda.com\">Yinda Zhang</a>, <a href=\"https://github.com/ahmedsabie\">Ahmed Sabie</a>, <a href=\"https://www.linkedin.com/in/creativetech\">Jason Mayes</a>, Google.</em><a name=\"more\"></a><p></p> <p>A <a href=\"https://en.wikipedia.org/wiki/Depth_map\">depth map</a> is essentially an image (or image channel) that contains information relating to the distance of the surfaces of objects in the scene from a given viewpoint (in this case, the camera itself) for every pixel in that image. Depth maps are a fundamental building block for a variety of computer graphics and computer vision applications, such as <a href=\"https://augmentedperception.github.io/depthlab/\">augmented reality</a>, <a href=\"https://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html\">portrait mode</a>, and <a href=\"https://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html\">3D reconstruction</a>. Despite the recent advances in depth sensing capabilities with <a href=\"https://developers.google.com/ar/develop/depth\">ARCore Depth API</a>, the majority of photographs on the web are still missing  associated depth maps. This, combined with users from the web community expressing a growing interest in having depth capabilities within JavaScript to enhance existing web apps such as to bring images to live, apply real time AR effects to a human face and body, or even reconstruct items for use in VR environments, helped shape the path for what you see today. </p><p>Today we are introducing the <a href=\"https://github.com/tensorflow/tfjs-models/blob/master/depth-estimation/README.md\">Depth API</a>, the first depth estimation API from TensorFlow.js. With this new API, we are also introducing the first depth model for portrait, ARPortraitDepth, which estimates a depth map for a single portrait image. To demonstrate one of many possible usages of depth information, we also present a computational photography application, <strong><a href=\"https://storage.googleapis.com/tfjs-models/demos/3dphoto/index.html\">3D photo</a></strong>, which utilizes the predicted depth and enables a 3D parallax effect on the given portrait image. Try the live demo below, everyone can easily make their social media profile photo 3D as shown below. </p>  <p><strong><center><a href=\"https://storage.googleapis.com/tfjs-models/demos/3dphoto/index.html\">Try out the 3D portrait demo for yourself</a>!</center></strong></p>  <a href=\"https://storage.googleapis.com/tfjs-models/demos/3dphoto/index.html\"><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEie8HBDNHcne_-2GHUqKFG0g2XmniU0vGySZRc3NPDEoqPqvK36cP-4vgvIawOJWNQzrLo90Xx7WAAOvY3V4HjFpXMGMcyWRvuI8e3JCzBEoyZv2P--8ldmF3_q3o2YgfjlCMZXOL_0YhWIOAKPCUTn4X3a5zkL9PMgctqPvGqJUohOFkjkmBMRbkiG/s1600/ezgif.com-gif-maker%20%2844%29.gif\" /></a> <h3>ARPortraitDepth: Single Image Depth Estimation</h3>  <p>At the core of the Portrait Depth API is a deep learning model, named ARPortraitDepth, that takes a single color portrait image as the input and produces a depth map. For the sake of computational efficiency, we adopt a light-weight U-Net architecture. As shown below, the encoder gradually downscales the image or feature map resolution by half, and the decoder increases the feature resolution to the same as the input. Deep learning features from the encoder are concatenated to the corresponding layers with the same spatial resolution in the decoders to bring high resolution signals for depth estimation. During training, we force the decoder to produce depth predictions with increasing resolutions at each layer, and add a loss for each of them with the ground truth. This empirically helps the decoder to predict accurate depth by gradually adding details. </p><p>Abundant and diverse training data is critical for the machine learning model to achieve overall decent performance, e.g. accuracy and robustness. We synthetically render pairs of color and depth images with various camera configurations, e.g. focal length, camera pose, from 3D digital humans captured by <a href=\"https://augmentedperception.github.io/therelightables/\">a high quality performance capture system</a>, and run <a href=\"https://augmentedperception.github.io/total_relighting/\">relighting augmentation</a> with High Dynamic Range environment illumination maps to increase the realism and diversity of the color images, e.g. shadows on the face. We also collect real data using mobile phones equipped with a front facing depth sensor, e.g. <a href=\"https://ai.googleblog.com/2020/04/udepth-real-time-3d-depth-sensing-on.html\">Google Pixel 4</a>, where the depth quality, as the training ground truth, is not as accurate and complete as that in our synthetic data, but the color images are effective in improving the performance of our model when running on images in the wild. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglwEQ44QOMuts9Z8dfnSFs6Ebs0VcDNrG4FOQq7mg5-jRsVrlhvNgVd624Uv6g1GlCaBfyxPhSm9ej7bubbooVvBw_WojUi2FlqAaUFrwTAscojrNXSBH__TZ5kAFEu3fcDMbvYOcIzGhm83mLPdWMVr1Tl9pFqPxsKu9ZiYpnUhs9tRYjSKmxg1vj/s1600/image5.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglwEQ44QOMuts9Z8dfnSFs6Ebs0VcDNrG4FOQq7mg5-jRsVrlhvNgVd624Uv6g1GlCaBfyxPhSm9ej7bubbooVvBw_WojUi2FlqAaUFrwTAscojrNXSBH__TZ5kAFEu3fcDMbvYOcIzGhm83mLPdWMVr1Tl9pFqPxsKu9ZiYpnUhs9tRYjSKmxg1vj/s1600/image5.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Single image depth estimation pipeline.</td></tr></tbody></table>   <p>To enhance the robustness against background variation, in practice, we run an off-the-shelf <a href=\"https://blog.tensorflow.org/2022/01/body-segmentation.html\">body segmentation model</a> with MediaPipe and TensorFlow.js before sending the image into the neural network of depth estimation. </p><p>The portrait depth model could enable a whole host of creative applications orientated around the human body that could drive next generation web apps. We refer readers to <a href=\"https://augmentedperception.github.io/depthlab/\">ARCore Depth Lab</a> for more inspirations.  </p><p>For the 3D photo application, we created a high-performance rendering pipeline. It first generates a segmented mask using the TensorFlow.js existing <a href=\"https://github.com/tensorflow/tfjs-models/tree/master/body-segmentation\">body segmentation API</a>. Next, we pass the masked portrait into the Portrait Depth API and obtain a depth map on the GPU. Eventually, we generate a depth mesh in <a href=\"https://threejs.org/\">three.js</a>, with vertices arranged in a regular grid and displaced by re-projecting corresponding depth values (see the figure below for generating the depth mesh). Finally, we apply texture projection to the depth mesh and rotate the camera around the z axis in a circle. Users can download the animations in GIF or WebM format.  </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju3TSCS6xv5jRleKw6kGGwKZ9jgInpgr2T-M0CcvDor1x0JKuozsqD9mDew-dPADY17US8lzLXx0FEwdOTvkO6ADJM4va33KBZa5QGeTZPQ_nXPy3EEGn5xTdfwtBeX8O9sepuWuJ5f9iNAtI-crx3l7cmOV2zrRos-K28X-qbHOdNg69cdTQ2tM3L/s1600/image11.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju3TSCS6xv5jRleKw6kGGwKZ9jgInpgr2T-M0CcvDor1x0JKuozsqD9mDew-dPADY17US8lzLXx0FEwdOTvkO6ADJM4va33KBZa5QGeTZPQ_nXPy3EEGn5xTdfwtBeX8O9sepuWuJ5f9iNAtI-crx3l7cmOV2zrRos-K28X-qbHOdNg69cdTQ2tM3L/s1600/image11.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Generating the depth mesh from the depth map for the 3D photo application.</td></tr></tbody></table> <h3>Portrait Depth API Installation</h3>  <p>The portrait depth API is currently offered as one variant of the new depth API. </p><p>To install the API and runtime library, you can either use the &lt;script> tag in your html file or use NPM.  </p><p>Through script tag: </p> <pre><code class=\"\">&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core\">&lt;/script><br />&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl\">&lt;/script><br />&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter\">&lt;/script><br />&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/body-segmentation\">&lt;/script><br />&lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation\">&lt;/script></code></pre> <p>Through NPM: </p> <pre><code class=\"\">yarn add @tensorflow/tfjs-core @tensorflow/tfjs-backend-webgl<br />yarn add @tensorflow/tfjs-converter<br />yarn add @tensorflow-models/body-segmentation<br />yarn add @tensorflow-models/depth-estimation</code></pre>  <p>To reference the API in your JS code, it depends on how you installed the library. </p><p>If installed through script tag, you can reference the library through the global namespace <code>depthEstimation</code>. </p><p>If installed through NPM, you need to import the libraries first:<br /></p> <pre><code class=\"\">import '@tensorflow/tfjs-backend-core';<br />import '@tensorflow/tfjs-backend-webgl';<br />import '@tensorflow/tfjs-converter';<br />import '@tensorflow-models/body-segmentation;<br />import * as depthEstimation from '@tensorflow-models/depth-estimation;</code></pre>   <h3>Try it yourself!</h3>  <p>First, you need to create an estimator:<br /></p> <pre><code class=\"\">const model = depthEstimation.SupportedModels.ARPortraitDepth;<br />    estimator = await depthEstimation.createEstimator(model);<br /><br /><br />    const video = document.getElementById('video');<br />    const depthMap = await estimator.estimateDepth(video);</code></pre> <p>Once you have an estimator, you can pass in a video stream, static image, or TensorFlow.js tensors to estimate depth: </p> <pre><code class=\"\">const video = document.getElementById('video');<br /><br />    const estimationConfig = {<br />      minDepth: 0, // The minimum depth value outputted by the estimator.<br />      maxDepth: 1, // The maximum depth value outputted by the estimator.<br />    };<br /><br />   const depthMap = await estimator.estimateDepth(video, estimationConfig);</code></pre>  <p><strong>How to use the output?</strong></p><p>The <code>depthMap</code> result above contains depth values for each pixel in the image. </p><p>The <code>depthMap</code> is an object which stores the underlying depth values. You can then utilize the provided asynchronous conversion functions such as <code>toCanvasImageSource</code>, <code>toArray</code>, and <code>toTensor</code> depending on the desired output type that you want for efficiency.  </p><p>It should be noted that different models have different internal representations of data. Therefore converting from one form to another may be expensive. In the name of efficiency, you can call <code>getUnderlyingType</code> to determine what form the depth map is in already so you may choose to keep it in the same form for faster results. </p><p>The semantics of the depthMap are as follows: the depth map is the same size as the input image. For array and tensor representations, there is one depth value per pixel. For <code>CanvasImageSource</code>, the green and blue channels are always set to 0, whereas the red channel stores the depth value.<br /><br />See below output snippet for example: </p>  <pre><code class=\"\">  {<br />    toCanvasImageSource(): ...<br />    toArray(): ...<br />    toTensor(): ...<br />    getUnderlyingType(): ...<br />  }</code></pre>  <h3>Browser Performance</h3>  <p><strong>Portrait Depth model</strong></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border: none; border-collapse: collapse;\">        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\"><br /></td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">MacBook M1 Pro 2021.&nbsp;</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 9pt; font-family: Arial; color: #3c4043; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">(FPS)</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">iPhone 13 Pro</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 9pt; font-family: Arial; color: #3c4043; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">(FPS)</span></p>                </td>                                 <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Desktop PC&nbsp;</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 9pt; font-family: Arial; color: #3c4043; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">Intel i9-10900K. Nvidia GTX 1070 GPU.</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 9pt; font-family: Arial; color: #3c4043; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">(FPS)</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: 700; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">TFJS Runtime</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 9pt; font-family: Arial; color: #3c4043; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">With WebGL backend.</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">51</span></p>                </td>                <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">22</span></p>                                 <td style=\"border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-bottom: solid #000000 1pt; border-top: solid #000000 1pt; vertical-align: top; padding: 5pt 5pt 5pt 5pt; overflow: hidden;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; text-align: center; margin-top: 0pt; margin-bottom: 0pt;\"><span style=\"font-size: 10pt; font-family: Arial; color: #202124; background-color: transparent; font-weight: 400; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline; white-space: pre; white-space: pre-wrap;\">47</span></p>                </td>            </tr>        </tbody>    </table></div><p><br /></p><h3>Acknowledgements</h3>  <p>We would like to acknowledge our colleagues who participated in or sponsored creating Portrait Depth API in TensorFlow.js: <a href=\"https://www.linkedin.com/in/na-li-02a39315\">Na Li</a>, <a href=\"https://www.linkedin.com/in/xiuxiuyuan/\">Xiuxiu Yuan</a>, <a href=\"https://research.google/people/106687/\">Rohit Pandey</a>, <a href=\"http://abhishekkar.info\">Abhishek Kar</a>, <a href=\"https://scholar.google.es/citations?user=dznX1DMAAAAJ&amp;hl=es\">Sergio Orts Escolano</a>, <a href=\"https://scholar.google.com/citations?user=5D0_pjcAAAAJ&amp;hl=en\">Christoph Rhemann</a>, <a href=\"https://www.linkedin.com/in/idrisaleem/\">Idris Aleem</a>, <a href=\"https://research.google/people/SeanFanello/\">Sean Fanello</a>, <a href=\"https://research.google/people/AdarshKowdle/\">Adarsh Kowdle</a>, <a href=\"https://www.linkedin.com/in/pyu10055\">Ping Yu</a>, <a href=\"https://www.olwal.com\">Alex Olwal</a>\u200e, <a href=\"https://www.linkedin.com/in/sarahheimlich/\">Sarah Heimlich</a>, <a href=\"https://www.linkedin.com/in/ceciliaabadie//\">Cecilia Abadie</a>. We would also like to acknowledge the <a href=\"https://blog.tensorflow.org/2022/01/body-segmentation.html\">body segmentation model</a> provided by MediaPipe, and <a href=\"https://augmentedperception.github.io/therelightables/\">The Relightables</a> for high quality synthetic data. </p>",
            "pubdate": "Tue, 10 May 2022 18:51:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                10
            ],
            "email_sent": true
        },
        "Using Machine Learning to Help Protect the Great Barrier Reef in Partnership with Australias CSIRO": {
            "url": "https://blog.tensorflow.org/2022/05/Kaggle-Great-Barrier-Reef-ML.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht3pf-aqNMrbwqRsqQ98eSfwAi-cpyPUTHKoYJgoeGMhB1uhw8Rt42TcbTkjhDF6v6f0fwiL6YfnMNp9ppOEBU6uw-Tt_PpP0akjV9vkZbNczEKo9XTxKQb0TLdtCX8UvMR-eVVKMdJKMvojlXv1Bv2nQsPohAl_-DpCfIv3FAJM_hWLLx1k9-RrhV/s1600/Screen%20Shot%202022-05-09%20at%201.06.30%20PM.png\" style=\"display: none;\" />  <p><em>Posted by Megha Malpani, Google Product Manager and Ard Oerlemans, Google Software Engineer</em><p> <a name=\"more\"></a><p></p> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht3pf-aqNMrbwqRsqQ98eSfwAi-cpyPUTHKoYJgoeGMhB1uhw8Rt42TcbTkjhDF6v6f0fwiL6YfnMNp9ppOEBU6uw-Tt_PpP0akjV9vkZbNczEKo9XTxKQb0TLdtCX8UvMR-eVVKMdJKMvojlXv1Bv2nQsPohAl_-DpCfIv3FAJM_hWLLx1k9-RrhV/s1600/Screen%20Shot%202022-05-09%20at%201.06.30%20PM.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht3pf-aqNMrbwqRsqQ98eSfwAi-cpyPUTHKoYJgoeGMhB1uhw8Rt42TcbTkjhDF6v6f0fwiL6YfnMNp9ppOEBU6uw-Tt_PpP0akjV9vkZbNczEKo9XTxKQb0TLdtCX8UvMR-eVVKMdJKMvojlXv1Bv2nQsPohAl_-DpCfIv3FAJM_hWLLx1k9-RrhV/s1600/Screen%20Shot%202022-05-09%20at%201.06.30%20PM.png\" /></a>    <p>Coral reefs are some of the most diverse and important ecosystems in the world, both for marine life and society more broadly. Not only are healthy reefs critical to fisheries and food security, they also protect coastlines from storm surge, support tourism-based economies, and advance drug discovery research, among other countless benefits. </p><p>Reefs face a number of rising threats, most notably climate change, pollution, and overfishing. In the past 30 years alone, there have been dramatic losses in coral cover and habitat in the Great Barrier Reef (GBR), with other reefs experiencing similar declines. In Australia, outbreaks of the coral-eating crown of thorns starfish (COTS) have been shown to cause major coral loss. While COTS naturally exist in the Indo-Pacific, reductions in the abundance of natural predators and excess run-off nutrients have led to massive outbreaks that are devastating already vulnerable coral communities. Controlling COTS populations is critical to promoting coral growth and resilience. </p><p>The <a href=\"https://www.barrierreef.org/\">Great Barrier Reef Foundation</a> established an <a href=\"https://www.barrierreef.org/what-we-do/reef-trust-partnership/crown-of-thorns-starfish-control\">innovation program</a> to develop new survey and intervention methods that radically improve COTS control. Google teamed up with <a href=\"https://www.csiro.au/en/about\">CSIRO</a>, Australia\u2019s national science agency, to develop innovative machine learning technology that can analyze video sequences accurately, efficiently, and in near real-time. The goal is to transform the underwater survey, monitoring and mapping reefs at scale to help rapidly identify and prioritize COTS outbreaks. This project is part of a broader partnership with CSIRO under Google\u2019s <a href=\"https://blog.google/intl/en-au/company-news/outreach-initiatives/digital-future-initiative/\">Digital Future Initiative</a> in Australia. </p><p>CSIRO developed an edge ML platform (built on top of the <a href=\"https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-agx-xavier/\">NVIDIA Jetson AGX Xavier</a>) that can analyze underwater image sequences and map out detections in near real-time. Our goal was to use the annotated dataset CSIRO had built over multiple field trips to develop the most accurate object detection model (across a variety of environments, weather conditions, and COTS populations) within a set of performance constraints, most notably, processing more than 10 frames per second (FPS) on a &lt;30 watt device. </p><p>We hosted a <a href=\"https://www.kaggle.com/c/tensorflow-great-barrier-reef\">Kaggle competition</a>, leveraging insights from the open source community to drive our experimentation plan. With over 2,000 teams and 61,000 submissions, we were able to learn from the successes and failures of far more experiments than we could hope to execute on our own. We used these insights to define our experimentation roadmap and ended up running hundreds of experiments on Google TPUs. </p><p>We used TensorFlow 2\u2019s <a href=\"https://github.com/tensorflow/models/tree/master/official\">Model Garden library</a> as our foundation, making use of its <a href=\"https://github.com/tensorflow/models/tree/master/official/vision/beta/projects/yolo\">scaled YOLOv4</a> model and corresponding training pipeline implementations. Our team of modeling experts then got to work, modifying the pipeline, experimenting with different image resolutions and model sizes, and applying various data augmentation and quantization techniques to create the most accurate model within our performance constraints. </p><p>Due to the limited amount of annotated data, a key part of this problem was figuring out the most effective data augmentation techniques. We ran hundreds of experiments based on what we learned from the Kaggle submissions to determine which techniques in combination were most effective in increasing our model\u2019s accuracy. </p><p>In parallel with our modeling workstream, we experimented with batching, XLA, and auto mixed precision (which converts parts of the model to fp16) to try and improve our performance, all of which resulted in increasing our FPS by 3x. We found however, that on the Jetson module, using <a href=\"https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.html\">TensorFlow-TensorRT </a>(converting the entire model to fp16) by itself actually resulted in a 4x total speed up, so we used TF-TRT exclusively moving forward. </p><p>After the starfish are detected in specific frames, a tracker is applied that links detections over time. This means that every detected starfish will be assigned a unique ID that it keeps as long as it stays visible in the video. We link detections in subsequent frames to each other by first using optical flow to predict where the starfish will be in the next frame, and then matching detections to predictions based on their Intersection over Union (IoU) score. </p><p>In a task like this where recall is more important than precision (i.e. we care more about not missing COTS than false positives), it is useful to consider the <a href=\"https://en.wikipedia.org/wiki/F-score#F%CE%B2\">F2 metric</a> to assess model accuracy. This metric can be used to evaluate a model's performance on individual frames. However, our ultimate goal was to determine the total number of COTS present in the video stream. Thus, we cared more about evaluating the entire pipeline\u2019s accuracy (model + tracker) than frame-by-frame performance (i.e. it\u2019s okay if the model has inaccurate predictions on a frame or two as long as the pipeline correctly identifies the starfish\u2019s overall existence and location). We ended up using a sequence-based F<sub>2</sub> metric that determines how many \u201ctracks\u201d are found at a certain average IoU threshold. </p><p>Our current 1080p model using TensorFlow TensorRT runs at 11 FPS on the Jetson AGX Xavier, reaching a sequence-based F<sub>2</sub> score of 0.80! We additionally trained a 720p model that runs at 22 FPS on the Jetson module, with a sequence-based F<sub>2</sub> score of 0.78. </p><p>Google & CSIRO are thrilled to announce that we are <strong>open-sourcing both COTS Object Detection models and have created a <a href=\"https://colab.sandbox.google.com/github/tensorflow/models/blob/master/official/projects/cots_detector/crown_of_thorns_starfish_detection_pipeline.ipynb\">Colab notebook</a> to demonstrate the server-side inference workflow</strong>. Our Colab tutorial allows students, marine researchers, or data scientists to evaluate our COTS ML models on image sequences with zero configuration/ML knowledge. Additionally, it provides a blueprint for implementing an optimized inference pipeline for edge ML platforms, such as the Jetson module. Please stay tuned as we plan to continue updating our models & trackers, ultimately open-sourcing a full TFX pipeline and dataset so that conservation organizations and other governments around the world can retrain and modify our model with their own datasets. Please reach out to us if you have a specific use case you\u2019d like to collaborate on! </p><p><strong>Acknowledgements</strong></p><p><em>A huge thank you to everyone who\u2019s hard work made this project possible!</em></p><p><em>We couldn\u2019t have done this without our partners at CSIRO \u2013 Brano Kusy, Jiajun Liu, Yang Li, Lachlan Tychsen-Smith, David Ahmedt-Aristizabal, Ross Marchant, Russ Babcock, Mick Haywood, Brendan Do, Jeremy Oorloff, Lars Andersson, and Joey Crosswell, the amazing Kaggle community, and last but not least, the team at Google \u2013 Glenn Cameron, Scott Riddle, Di Zhu, Abdullah Rashwan, Rebecca Borgo, Evan Rosen, Wolff Dobson, Tei Jeong, Addison Howard, Will Cukierski, Sohier Dane, Mark McDonald, Phil Culliton, Ryan Holbrook, Khanh LeViet, Mark Daoust, George Karpenkov, and Swati Singh.</em></p>",
            "pubdate": "Wed, 11 May 2022 20:19:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "On-device Text-to-Image Search with TensorFlow Lite Searcher Library": {
            "url": "https://blog.tensorflow.org/2022/05/on-device-text-to-image-search-with.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRKSVzRJ89LQ4GppKujCFnXR1F-8kmxpQwb7shUiaIPBPOvf3axOqEsZO54J4uzlYP0RVQ9VNlyeLMSIk8M_npJc70n7o0LqnfBQBzvxphU1vZmjwDCs47cpYMbd27Sh9VYKI9expP8lrpaxOJAmJrCgeRR5eQzTVxEcT5lfW1CMIKcEesUDJUGIuJ/s1600/image3.png\" style=\"display: none;\" />  <p><em>Posted by Zonglin Li, Lu Wang, Maxime Br\u00e9non, and Yuqi Li, Software Engineers</em></p><a name=\"more\"></a><p></p>  <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjw4uZh_Pvl7hamimWYoFo2moCBXww7L_J7wdunVVxNDj3dFpGU4u2klFvFfRPnvfe-jYIRUQup8bBSt0ZjWp5wRYc6YHMd3eLebmBWqscNeW4KsINcLH1XJhLLvOikkz_26_qwHjGqZhps3DnvaXyHfvwRT20XTqXIHiIetXoCpweZjcnVX6DE0yY7/s1600/Copy%20of%20I_O22_BlogBanners_RB_v09.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjw4uZh_Pvl7hamimWYoFo2moCBXww7L_J7wdunVVxNDj3dFpGU4u2klFvFfRPnvfe-jYIRUQup8bBSt0ZjWp5wRYc6YHMd3eLebmBWqscNeW4KsINcLH1XJhLLvOikkz_26_qwHjGqZhps3DnvaXyHfvwRT20XTqXIHiIetXoCpweZjcnVX6DE0yY7/s1600/Copy%20of%20I_O22_BlogBanners_RB_v09.png\" /></a></div>   <p>Today, we're excited to announce a new on-device embedding-based search library that allows you to quickly find similar images, text or audio from millions of data samples in a few milliseconds.  </p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaTXZDmasMaRs-z9tIu6R9a7zddIdDLv1IxyOI2OiAzgH0o6wHQVmRAJZfTdirnWYUbRE-5_G-rGk7SDXuFE4NnLuWgJco-tXvcZS39GI0Nk0hhSp-x3fH4V1B61alL8F9n8RV937GcLTGphNbwni8L_4tuWjeg5zriN1rPhox3O9XhvwVYpUuTiqe/s1600/image4.gif\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaTXZDmasMaRs-z9tIu6R9a7zddIdDLv1IxyOI2OiAzgH0o6wHQVmRAJZfTdirnWYUbRE-5_G-rGk7SDXuFE4NnLuWgJco-tXvcZS39GI0Nk0hhSp-x3fH4V1B61alL8F9n8RV937GcLTGphNbwni8L_4tuWjeg5zriN1rPhox3O9XhvwVYpUuTiqe/s1600/image4.gif\" /></a></div><p>It works by using a model to embed the search query into a high-dimensional vector representing the semantic meaning of the query. Then it uses <a href=\"https://github.com/google-research/google-research/tree/master/scann\">ScaNN</a> (Scalable Nearest Neighbors) to search for similar items from a predefined database. In order to apply it to your dataset, you need to use Model Maker Searcher API (<a href=\"https://www.tensorflow.org/lite/tutorials/model_maker_text_searcher\">tutorial</a><a href=\"https://www.tensorflow.org/lite/tutorials/model_maker_text_searcher\"></a>) to build a custom TFLite Searcher model, and then deploy it onto devices using Task Library Searcher API (<a href=\"https://www.tensorflow.org/lite/inference_with_metadata/task_library/image_searcher\">vision</a>/<a href=\"https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher\">text</a>). </p><p>For example, with the Searcher model trained on <a href=\"https://cocodataset.org/#home\">COCO</a>, searching the query, <code>A passenger plane on the runway</code>, will return the following images: </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooOxW8DRyg0f6TLCFjJJs1L1gWy7ic9vhDJSAomVaZQdxE_3-xlKsMcWi2b2bVwilK2ivmJPK7w3Q2jMwJvuRahVoO5WYlA9X4X0H111dReGI4Z7ckHel-Oux2bu--dfHw1-yKXcUzjl1h_TZxR2nbKH_csjQHfZtqUzxm4Ogc4aPP65NcaaNWCDa/s1600/image2.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhooOxW8DRyg0f6TLCFjJJs1L1gWy7ic9vhDJSAomVaZQdxE_3-xlKsMcWi2b2bVwilK2ivmJPK7w3Q2jMwJvuRahVoO5WYlA9X4X0H111dReGI4Z7ckHel-Oux2bu--dfHw1-yKXcUzjl1h_TZxR2nbKH_csjQHfZtqUzxm4Ogc4aPP65NcaaNWCDa/s1600/image2.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 1: All images are from COCO 2014 train and validation datasets. <a href=\"https://www.flickr.com/photos/multiplyleadership/6832329918/\">Image 1</a> by Mark Jones Jr. under <a href=\"http://creativecommons.org/licenses/by/2.0/\">Attribution License</a>. <a href=\"https://www.flickr.com/photos/bluehillranch/8613487675/\">Image 2</a> by 305 Seahill under <a href=\"http://creativecommons.org/licenses/by-nd/2.0/\">Attribution-NoDerivs License</a>. <a href=\"https://www.flickr.com/photos/25451699@N04/5942171839/\">Image 3</a> by tataquax under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. </td></tr></tbody></table><p>In this post, we will walk you through an end-to-end example of building a text-to-image search feature (retrieve the images given textual queries) using the new TensorFlow Lite Searcher Library. Here are the major steps: </p><ol> <li>Train a dual encoder model for image and text query encoding using the <a href=\"https://cocodataset.org/#home\">COCO</a> dataset.  <li>Create a text-to-image Searcher model using the Model Maker Searcher API.  <li>Retrieve images with text queries using the Task Library Searcher API.  </li></ol><h3>Train a Dual Encoder Model</h3> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRKSVzRJ89LQ4GppKujCFnXR1F-8kmxpQwb7shUiaIPBPOvf3axOqEsZO54J4uzlYP0RVQ9VNlyeLMSIk8M_npJc70n7o0LqnfBQBzvxphU1vZmjwDCs47cpYMbd27Sh9VYKI9expP8lrpaxOJAmJrCgeRR5eQzTVxEcT5lfW1CMIKcEesUDJUGIuJ/s1600/image3.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRKSVzRJ89LQ4GppKujCFnXR1F-8kmxpQwb7shUiaIPBPOvf3axOqEsZO54J4uzlYP0RVQ9VNlyeLMSIk8M_npJc70n7o0LqnfBQBzvxphU1vZmjwDCs47cpYMbd27Sh9VYKI9expP8lrpaxOJAmJrCgeRR5eQzTVxEcT5lfW1CMIKcEesUDJUGIuJ/s1600/image3.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 2: Train the dual encoder model with dot product similarity distance. The loss encourages related images and text to have larger dot products (the shaded green squares).</td></tr></tbody></table> <!-----  Yay, no errors, warnings, or alerts!  Conversion time: 0.579 seconds.   Using this HTML file:  1. Paste this output into your source file. 2. See the notes and action items below regarding this conversion run. 3. Check the rendered output (headings, lists, code blocks, tables) for proper    formatting and use a linkchecker before you publish this page.  Conversion notes:  * Docs to Markdown version 1.0\u03b233 * Tue May 10 2022 12:44:24 GMT-0700 (PDT) * Source doc: TensorFlow blog-On-device Text-to-Image Search with TensorFlow Lite Searcher Library * This is a partial selection. Check to make sure intra-doc links work. ----->  <p>The dual encoder model consists of an image encoder and a text encoder. The two encoders map the images and text, respectively, to embeddings in a high-dimensional space. The model computes the dot product between the image and text embeddings, and the loss encourages relevant image and text to have larger dot product (closer), and unrelated ones to have smaller dot product (farther apart).  </p><p>The training procedure is inspired by the <a href=\"https://arxiv.org/abs/2103.00020\">CLIP</a> paper and this <a href=\"https://keras.io/examples/nlp/nl_image_search/\">Keras example</a>. The image encoder is based on a pre-trained <a href=\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\">EfficientNet</a> model and the text encoder is based on a pre-trained <a href=\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\">Universal Sentence Encoder</a> model. The outputs from both encoders are then projected to a 128 dimensional space and are L2 normalized. For the dataset, we chose to use <a href=\"https://cocodataset.org/#home\">COCO</a>, as its train and validation splits have human generated captions for each image. Please take a look at the companion <a href=\"https://colab.sandbox.google.com/github/tensorflow/tflite-support/blob/master/tensorflow_lite_support/examples/colab/on_device_text_to_image_search_tflite.ipynb\">Colab notebook</a> for the details of the training process. </p><p>The dual encoder model makes it possible to retrieve images from a database without captions because once trained, the image embedder can directly extract the semantic meaning from the image without any need for human-generated captions. </p><h3>Create the text-to-image Searcher model using Model Maker</h3>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhImt-tBGiK8Z1W_JOAsuu3qlnDHrfko5bQ38e-njB3TAjxXk2lBtw_t2xxNMMrNt0BCu7uBarZtuMg0eCwNFkn7CoWEaKsUfl9f94b1EAICHmz2vXfMCDFhKpzK8tzsTv-KhS0l7AR3hC1RLQB4Fg5w2LAv0qYTrO7ea18j8Bnb0RhNItKqOK3oR8r/s1600/image1.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhImt-tBGiK8Z1W_JOAsuu3qlnDHrfko5bQ38e-njB3TAjxXk2lBtw_t2xxNMMrNt0BCu7uBarZtuMg0eCwNFkn7CoWEaKsUfl9f94b1EAICHmz2vXfMCDFhKpzK8tzsTv-KhS0l7AR3hC1RLQB4Fg5w2LAv0qYTrO7ea18j8Bnb0RhNItKqOK3oR8r/s1600/image1.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 3: Generate image embeddings using the image encoder and use Model Maker to create the TFLite Searcher model.</td></tr></tbody></table>  <p>Once the dual encoder model is trained, we can use it to create the TFLite Searcher model that searches for the most relevant images from an image dataset based on the text queries. This can be done by the following three steps: </p><ol> <li>Generate the embeddings of the image dataset using the TensorFlow image encoder. ScaNN is capable of searching through a very large dataset, hence we combined the train and validation splits of  COCO 2014 totaling 123K+ images to demonstrate its capabilities. See the code <a href=\"https://colab.sandbox.google.com/github/tensorflow/tflite-support/blob/master/tensorflow_lite_support/examples/colab/on_device_text_to_image_search_tflite.ipynb#scrollTo=Bp0qBKkyu4jA\">here</a>.  <li>Convert the TensorFlow text encoder model into TFLite format. See the code <a href=\"https://colab.research.google.com/github/tensorflow/tflite-support/blob/master/tensorflow_lite_support/examples/colab/on_device_text_to_image_search_tflite.ipynb#scrollTo=6Dzye66Xc8vE\">here</a>.  <li>Use <a href=\"https://www.tensorflow.org/lite/tutorials/model_maker_text_searcher\">Model Maker</a> to create the TFLite Searcher model from the TFLite text encoder and the image embeddings using the code below:</ol> <pre><code class=\"\"><br />#Configure ScaNN options. See the <a href=\"https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/searcher/ScaNNOptions\" target=\"_blank\">API doc</a> for how to configure ScaNN. <br />scann_options = searcher.ScaNNOptions(<br />      distance_measure='dot_product',<br />      tree=searcher.Tree(num_leaves=351, num_leaves_to_search=4),<br />      score_ah=searcher.ScoreAH(1, anisotropic_quantization_threshold=0.2))<br /><br /># Load the image embeddings and corresponding metadata if any.<br />data = searcher.DataLoader(tflite_embedder_path, image_embeddings, metadata)<br /><br /># Create the TFLite Searcher model.<br />model = searcher.Searcher.create_from_data(data, scann_options)<br /><br /># Export the TFLite Searcher model.<br />model.export(<br />      export_filename='searcher.tflite',<br />      userinfo='',<br />      export_format=searcher.ExportFormat.TFLITE)<br />     </code></pre>  <center><p id=\"imgCaption\"><a href=\"https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/searcher/ScaNNOptions\">API doc can be found here.</a></p></center> <p>When creating the Searcher model, Model Maker leverages ScaNN to index the embedding vectors. The embedding dataset is first partitioned into multiple subsets. In each of the subsets, ScaNN stores the quantized representation of the embedding vectors. At retrieval time, ScaNN selects a few most relevant partitions and scores the quantized representations with fast, approximate distances. This procedure saves both the model size (through quantization) and achieves speed up (through partition selection). See the <a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\">in-depth examination</a> to learn more about the ScaNN algorithm. </p><p>In the above example, we divide the dataset into 351 partitions (roughly the square root of the number of embeddings we have), and search 4 of them during retrieval, which is roughly 1% of the dataset. We also quantize the 128 dimensional float embeddings to 128 int8 values to save space.  </p><h3>Run inference using Task Library</h3>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhRfPCu6QCkMxXTJVV0Y7A2kWaHpImHFEuPIEHZTwgsxJ27JYvcI7ThzNCqLiVBB9u2P67F00FvGzdZzHvBBv0UO9PSyj4BmsUCjXjYAO4PSOUogKR4WNgBfLudNppCbNrnQgSWh6a3v-lWrFHiFUYBSmYd2A2LE9e9jNqsvk67XMvsicgOeJwYIcz/s1600/image1.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhRfPCu6QCkMxXTJVV0Y7A2kWaHpImHFEuPIEHZTwgsxJ27JYvcI7ThzNCqLiVBB9u2P67F00FvGzdZzHvBBv0UO9PSyj4BmsUCjXjYAO4PSOUogKR4WNgBfLudNppCbNrnQgSWh6a3v-lWrFHiFUYBSmYd2A2LE9e9jNqsvk67XMvsicgOeJwYIcz/s1600/image1.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 4: Run inference using Task Library with the TFLite Searcher model. It takes the query text and returns the top neighbor\u2019s metadata. From there we can find the corresponding images.</td></tr></tbody></table>   <p>To query images using the Searcher model, you only need a couple of lines of code like the following using <a href=\"https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher\">Task Library</a>: </p> <pre><code class=\"\">from tflite_support.task import text<br /><br /># Initialize a TextSearcher object<br />searcher = text.TextSearcher.create_from_file('searcher.tflite')<br /><br /># Search the input query<br />results = searcher.search(query_text)<br /><br /># Show the results<br />for rank in range(len(results.nearest_neighbors)):<br />  print('Rank #', rank, ':')<br />  image_id = results.nearest_neighbors[rank].metadata<br />  print('image_id: ', image_id)<br />  print('distance: ', results.nearest_neighbors[rank].distance)<br />  show_image_by_id(image_id)</code></pre>  <p>Try the code from the <a href=\"https://colab.sandbox.google.com/github/tensorflow/tflite-support/blob/master/tensorflow_lite_support/examples/colab/on_device_text_to_image_search_tflite.ipynb\">Colab</a>. Also, see <a href=\"https://www.tensorflow.org/lite/inference_with_metadata/task_library/text_searcher\">more information</a> on how to integrate the model using the Task Library Java and C++ API, especially on Android. Each query in general takes only 6 milliseconds on Pixel 6.  </p><p>Here are some example results: </p><p>Query: <code>A man riding a bike</code></p><p>Results are ranked according to the approximate similarity distance. Here is a sample of retrieved images. Note that we are only showing images if their licenses allow. </p> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdVewBVwxQOAORYm4Jvd0W6g2lLEtvFbLs-qwCctuYT1KKOKYeZwbDhL08E7Kz2YPdIl4Ts6zmo0hqr8L6zatbdnMThSLesESxv6AnfFP_-gmHU6covHOSdOFHKhmP6KNRmadEt1yGS-LCWJH6MWIyu3v8MPwWqCdymNkmzFIgs_BbIJSG7y6tdrOH/s1600/image6.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdVewBVwxQOAORYm4Jvd0W6g2lLEtvFbLs-qwCctuYT1KKOKYeZwbDhL08E7Kz2YPdIl4Ts6zmo0hqr8L6zatbdnMThSLesESxv6AnfFP_-gmHU6covHOSdOFHKhmP6KNRmadEt1yGS-LCWJH6MWIyu3v8MPwWqCdymNkmzFIgs_BbIJSG7y6tdrOH/s1600/image6.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Figure 5: All images are from COCO 2014 train and validation datasets. <a href=\"https://www.flickr.com/photos/kamalayan/4945223078/\">Image 1</a> by Reuel Mark Delez under <a href=\"http://creativecommons.org/licenses/by/2.0/\">Attribution License</a>. <a href=\"https://www.flickr.com/photos/bike/300626852/\">Image 2</a> by Richard Masoner / Cyclelicious under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. <a href=\"https://www.flickr.com/photos/jula_julz/3599983065/\">Image 3</a> by Julia under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. <a href=\"https://www.flickr.com/photos/roebot/3322126404/\">Image 4</a> by Aaron Fulkerson under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. <a href=\"https://www.flickr.com/photos/bike/2927188293/\">Image 5</a> by Richard Masoner / Cyclelicious under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. <a href=\"https://www.flickr.com/photos/bike/3018725318/\">Image 6</a> by Richard Masoner / Cyclelicious under <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Attribution-ShareAlike License</a>. </td></tr></tbody></table> <h3>Future work</h3>  <p>We\u2019ll be working on enabling more search types beyond image and text, such as audio clips. </p><p>Contact odml-pipelines-team@google.com if you want to leave any feedback. Our goal is to make on-device ML even easier for you and we value your input! </p>  <h3>Acknowledgements</h3>  <p><em>We would like to thank Khanh LeViet, Chuo-Ling Chang, Ruiqi Guo, Lawrence Chan, Laurence Moroney, Yu-Cheng Ling, Matthias Grundmann, as well as Robby Neale, Chung-Ching Chang\u200e, Tom Small and Khalid Salama for their active support of this work. We would also like to thank the entire ScaNN team: David Simcha, Erik Lindgren, Felix Chern, Phil Sun and Sanjiv Kumar. </em></p>",
            "pubdate": "Wed, 11 May 2022 17:44:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "AI and Machine Learning @ I/O Recap": {
            "url": "https://blog.tensorflow.org/2022/05/ai-and-machine-learning-io-recap.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhPVjGNKa1ihfr9scYARP3XU6-ZaDpyLfXU-drCOWhp_EwNqtywdPEl36uMxZdJHl7DcuK2njFEq3NZ4WEV13yK86S1OKsbkUzLr0D8WMeW1Rqf5Od1QQv7ehJXYNjFQ_wTNUU197M14C2cazmYACz3PA_h8Ir3vFMp-C67OzAPqz3S-JNRFm3COR2n/s1600/recapml.jpeg\" style=\"display: none;\" /> <p><em>Posted by TensorFlow Team</em><a name=\"more\"></a><p></p>  <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QgVznRcilVS55FVhwRrp3-NPi-PWOorlfhPzIm439ioiu-adRlL83c8HpqffiDu3uIwYp0FUBl0C55uCemSnxIchSyCQpObDiElEoruuCTTslG3xn2V0xqrXYq7hTQDbgteDLcTBy_y8nRUhtuoBQhPTBpAdxW2crFq4XbzPCWT3JHfo3PZtPGBS/s1600/Copy%20of%20Copy%20of%20I_O22_BlogBanners_RB_v09.png\" style=\"display: block; padding: 1em 0; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5QgVznRcilVS55FVhwRrp3-NPi-PWOorlfhPzIm439ioiu-adRlL83c8HpqffiDu3uIwYp0FUBl0C55uCemSnxIchSyCQpObDiElEoruuCTTslG3xn2V0xqrXYq7hTQDbgteDLcTBy_y8nRUhtuoBQhPTBpAdxW2crFq4XbzPCWT3JHfo3PZtPGBS/s1600/Copy%20of%20Copy%20of%20I_O22_BlogBanners_RB_v09.png\" /></a></div> <p>Google I/O 2022 was a major milestone in the evolution of AI and Machine Learning for developers. We\u2019re really excited about the potential for developers using our technologies and Machine Learning to build intelligent solutions, and we believe that 2022 is the year when AI and ML become part of every developer\u2019s toolbox. </p><p>At the I/O keynotes we showed our fully open source ecosystem that takes you from end to end with Machine Learning. There are developer tools for managing data, training your models, and deploying them to a variety of surfaces from global scale cloud all the way down to tiny microcontrollers\u2026and of course ways to monitor and maintain your systems with MLOps. All of this comes with a common set of accelerated hardware for training and inference, along with open source tooling for responsible AI end-to-end. </p><p>You can get a tour through this ecosystem in the Keynote \u201cAI and Machine Learning updates for Developers\u201d </p> <center></center>   <p><h2><strong>Responsible AI review processes: From a developer\u2019s point of view<br /></strong></h2></p><p>We can all agree that responsible and ethical AI is important, but when you want to <em>build</em> responsibly, you need tooling. We could, and will, create a whole video series about these tools, but the great content to watch right now is the talk on the Responsible AI review process. Googlers who worked on projects like the Covid-19 public forecasts or the Celebrity Recognition APIs will take you step-by-step through their thought process and how the tools lined up to help them build more responsibly and thoughtfully. You\u2019ll also learn about some of the new releases in Responsible AI tools, such as the <a href=\"https://tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_overview\">Counterfactual Logit Pairing library</a>.   </p>   <p>   <center></center></p> <p><h2><strong>Adding machine learning to your developer toolbox<br /></strong></h2></p><p>If you\u2019re just getting started on your journey and you want ML to be a part of your toolbox, you probably have a million questions. Follow a developer's journey through the best offerings, from a turnkey API that can solve basic problems fast, to custom models that can be tuned and deployed.    </p> <p>  <center></center></p> <p><h2><strong>TensorFlow.js: From prototype to production, what's new in 2022?</strong></h2></p><p>If you\u2019re a web developer there\u2019s a whole bunch of new updates, from the announcement of a new set of courses that will take you from first principles through a deep dive of what\u2019s possible to lots of new models available to web devs. These include a selfie depth estimation model that can be used for cool things like a 3D effect in your pictures without needing any kind of extra sensor. You\u2019ll also see 3D pose estimation that allows you to run at a high FPS to get real time results, allowing you to do things like having a full animated character following your body motion. All in the browser!  </p> <p>  <center></center></p> <p><h2><strong>Deploy a custom ML model to mobile</strong></h2></p><p>If you want to build better mobile apps with AI and Machine Learning, you probably need to understand the ins and outs of getting models to execute on Android or iOS devices, including shrinking them and optimizing them to be power friendly.  Supercharge your model with new releases from the TensorFlow Lite team that let you quantize, debug, and accelerate your model on CPU or delegated GPUs, and a whole lot more.  </p> <p>  <center></center></p> <p><h2><strong>Further on the edge with Coral Dev Board Micro</strong></h2></p><p>Speaking of acceleration, this year at I/O we introduced the Coral Dev Board Micro. This is a new microcontroller class device with an on-board Edge TPU that\u2019s powerful enough to run multiple models in tandem. The Coral team has also updated their catalog of pre-trained models, now including over 40 models now available for you to use on embedded systems out of the box! </p> <p><center></center></p> <p><h2><strong>Tips and tricks for distributed large model training</strong></h2></p><p>On the other side of the spectrum, if you want to train large models, you\u2019ll need to understand how to shard training and data across multiple processors or cores. We\u2019ve released lots of new guidance and updates for model and data parallelism. You can learn all about them in this talk, including lessons learned from Google researchers in building language models. </p> <p>  <center></center></p> <p><h2><strong>Easier data preprocessing with Keras</strong></h2></p><p>Of course, not all data is big data, and if you\u2019re not building giant models, you still need to be able to manage your data. Often this is where devs will write the most code for ML, so we want to highlight some ways of making this easier, in particular with Keras. Keras's new preprocessing layers that not only make vectorization and augmentation much easier, but also allow for precomputation to make your training more efficient by reducing idle time. Learn about data preprocessing from the creator of Keras! </p> <p>  <center></center></p> <p><h2><strong>An introduction to MLOps with TFX</strong></h2></p><p>Finally, let\u2019s not forget MLOps and TFX, the open source, end-to-end pipeline management tool. Check out the talk from Robert Crowe who will help you understand everything, from why you need MLOps to managing your process through managing change. You\u2019ll see the component model in TFX, and get an introduction to the new TFX-Addons community that\u2019s focussed on building new ones. Check it all out in this talk!! </p> <p>  <center></center></p> <p>I/O wasn\u2019t just about new releases and talks! If you are inspired by any of what you saw, we also have workshops and learning paths you can dig into to learn in more detail.  </p> <p>  <center></center></p> <p>  <center></center></p> <p>  <center></center></p> <p>  <center></center></p> <p><a href=\"https://www.youtube.com/playlist?list=PLQY2H8rRoyvyY0AsvPIkb7rg7ogc5dgXW\">Full playlist</a> to all AI/ML talks and workshops. </p><p>That\u2019s it for this roundup of AI and ML at Google I/O 2022. We hope you\u2019ve enjoyed it, and we\u2019d love to hear your feedback when you explore the content.  Please drop by the <a href=\"https://discuss.tensorflow.org/\">TensorFlow Forum</a> and let us know what you think! </p>",
            "pubdate": "Thu, 12 May 2022 16:16:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "TensorFlow Lite for education and makers": {
            "url": "https://blog.tensorflow.org/2022/05/tensorflow-lite-for-education-and-makers.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqEDXa1_eLG5p2UEpcB9lncW024b695qrPeQyjNx15n_rnSQ68lV4VOPTWZKDm6S6l1OcXIcxkAp_-MQc906pOCYd8kvAaHLQF_0TrRIOGKG-MHRMKalvZWqmWFiWASghlbFQNOgjYQ1DbBwkw8echZ7u4Pre844E-ASgSIbr1UV5oQg4kM2DPsFvb/s1600/image1.jpg\" style=\"display: none;\" /> <p><em>Posted by Scott Main, AIY Projects and Coral</em></p><p> <a name=\"more\"></a><p></p> Back in 2017, we began AIY Projects to make do-it-yourself artificial intelligence projects accessible to anybody. Our first project was the <a href=\"https://aiyprojects.withgoogle.com/voice/\">AIY Voice Kit</a>, which allows you to build your own intelligent device that responds to voice commands. Then we released the <a href=\"https://aiyprojects.withgoogle.com/vision\">AIY Vision Kit</a>, which can recognize objects seen by its camera using on-device TensorFlow models. We were amazed by the projects people built with these kits and thrilled to see educational programs use them to introduce young engineers to the possibilities of computer science and machine learning (ML). So I'm excited to continue our mission to bring machine learning to everyone with the more powerful and more customizable AIY Maker Kit. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqEDXa1_eLG5p2UEpcB9lncW024b695qrPeQyjNx15n_rnSQ68lV4VOPTWZKDm6S6l1OcXIcxkAp_-MQc906pOCYd8kvAaHLQF_0TrRIOGKG-MHRMKalvZWqmWFiWASghlbFQNOgjYQ1DbBwkw8echZ7u4Pre844E-ASgSIbr1UV5oQg4kM2DPsFvb/s1600/image1.jpg\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqEDXa1_eLG5p2UEpcB9lncW024b695qrPeQyjNx15n_rnSQ68lV4VOPTWZKDm6S6l1OcXIcxkAp_-MQc906pOCYd8kvAaHLQF_0TrRIOGKG-MHRMKalvZWqmWFiWASghlbFQNOgjYQ1DbBwkw8echZ7u4Pre844E-ASgSIbr1UV5oQg4kM2DPsFvb/s1600/image1.jpg\" /></a> <p><strong>Making ML accessible to all</strong></p><p>The Voice Kit and Vision Kit are a lot of fun to put together and they include great programs that demonstrate the possibilities of ML on a small device. However, they don't provide the tools or procedures to help beginners achieve their own ML project ideas. When we released those kits in 2017, it was actually quite difficult to train an ML model, and getting a model to run on a device like a Raspberry Pi was even more challenging. Nowadays, if you have some experience with ML and know where to look for help, it's not so surprising that you can train an object detection model in your web browser in less than an hour, or that you can run a pose detection model on a battery-powered device. But if you don't have any experience, it can be difficult to discover the latest ML tools, let alone get started with them. </p><p>We intend to solve that with the Maker Kit. With this kit, we're not offering any new hardware or ML tools; we're offering a simplified workflow and a series of tutorials that use the latest tools to train TensorFlow Lite models and execute them on small devices. So it's all existing technology, but better packaged so beginners can stop searching and start building incredible things right away. </p><p><strong>Simplified tools for success</strong></p><p>The material we've collected and created for the Maker Kit offers an end-to-end experience that's ideal for educational programs and users who just want to make something with ML as fast as possible. </p><p>The hardware setup requires a Raspberry Pi, a Pi Camera, a USB microphone, and a <a href=\"https://coral.ai/products/accelerator\">Coral USB Accelerator</a> so you can execute advanced vision models at high speed on the Coral Edge TPU. If you want your hardware in a case, we offer two DIY options: a 3D-printed case design or a cardboard case you can build using materials at home. </p><p>Once it's booted up with our Maker Kit system image, just run some of our code examples and follow our coding tutorials. You'll quickly discover how easy it is to accomplish amazing things with ML that were recently considered accessible only to experts, including object detection, pose classification, and speech recognition. </p> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5nc5Cgt8SUJFEeLKzRprsr-7lKqY1zXrce3T2NxIRlw0PDSxYey65ucFGWeufPKc4K5qAMsM3KsBnRIRfyrdmnX2xgxEvyRkT0sHNJmnT-UFY5NBlae6def8ej_Tg8oR_gJfZEu2Xg46RykNQlb23Q8hfllvtf62Jiw4nZE9dcm3cAu1yt4f-jFmi/s1600/image2.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5nc5Cgt8SUJFEeLKzRprsr-7lKqY1zXrce3T2NxIRlw0PDSxYey65ucFGWeufPKc4K5qAMsM3KsBnRIRfyrdmnX2xgxEvyRkT0sHNJmnT-UFY5NBlae6def8ej_Tg8oR_gJfZEu2Xg46RykNQlb23Q8hfllvtf62Jiw4nZE9dcm3cAu1yt4f-jFmi/s1600/image2.png\" /></a> <p>Our code examples use some pre-trained models and you can get more models that are accelerated on the Edge TPU from the <a href=\"https://coral.ai/models\">Coral models library</a>. However, training your own models allows you to explore all new project ideas. So the Maker Kit also offers step-by-step tutorials that show you how to collect your own datasets and train your own vision and audio models. </p><p>Last but not least, we want you to spend nearly all your time writing the code that's unique to your project. So we created a Python library that reduces the amount of code needed to perform an inference down to a tiny part of your project. For example, this is how you can run an object detection model and draw labeled bounding boxes on a live camera feed: </p> <pre><code class=\"\">from aiymakerkit import vision<br />from aiymakerkit import utils<br />import models<br /><br />detector = vision.Detector(models.OBJECT_DETECTION_MODEL)<br />labels = utils.read_labels_from_metadata(models.OBJECT_DETECTION_MODEL)<br /><br />for frame in vision.get_frames():<br />    objects = detector.get_objects(frame, threshold=0.4)<br />    vision.draw_objects(frame, objects, labels)<br /></code></pre> <p>Our intent is to hide the code you don't absolutely need. You still have access to structured inference results and program flow, but without any boilerplate code to handle the model. </p><p>This aiymakerkit library is built upon TensorFlow Lite and it's <a href=\"https://github.com/google-coral/aiy-maker-kit\">available on GitHub</a>, so we invite you to explore the innards and extend the Maker Kit API for your projects. </p><p><strong>Getting started</strong></p><p>We created the Maker Kit to be fully customizable for your projects. So rather than provide all the materials in a box with a predetermined design, we designed it with hardware that's already available in stores (listed on our website) and with optional instructions to build your own case. </p><p>To get started, visit our website at <a href=\"https://g.co/aiy/maker\">g.co/aiy/maker</a>, gather the required materials, flash our system image, and follow our programming tutorials to start exploring the possibilities. With this head start toward building smart applications that run entirely on an embedded system, we can't wait to see what you will create. </p>",
            "pubdate": "Tue, 17 May 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                17
            ],
            "email_sent": true
        },
        "What's new in TensorFlow 2.9?": {
            "url": "https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png\" style=\"display: none;\" /> <p><em>Posted by Goldie Gadde and Douglas Yarrington for the TensorFlow team</em><p> <a name=\"more\"></a><p></p> <p>TensorFlow 2.9 has been released! Highlights include performance improvements with oneDNN, and the release of DTensor, a new API for model distribution that can be used to seamlessly move from data parallelism to model parallelism  </p><p>We\u2019ve also made improvements to the core library, including Eigen and <code>tf.function</code> unification, deterministic behavior, and new support for Windows' <a href=\"https://docs.microsoft.com/en-us/windows/wsl/install\">WSL2</a>. Finally, we\u2019re releasing new experimental APIs for tf.function retracing and Keras Optimizers. Let's take a look at these new and improved features. </p><h3>Improved CPU performance: oneDNN by default</h3>  <p>We have worked with Intel to integrate the <a href=\"https://github.com/oneapi-src/oneDNN\">oneDNN</a> performance library with TensorFlow to achieve top performance on Intel CPUs. Since TensorFlow 2.5, TensorFlow has had <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20210930-enable-onednn-ops.md\">experimental support</a> for oneDNN, which could provide up to a 4x <a href=\"https://medium.com/intel-analytics-software/leverage-intel-deep-learning-optimizations-in-tensorflow-129faa80ee07\">performance improvement</a>. In TensorFlow 2.9, we are turning on oneDNN optimizations by default on Linux x86 packages and for CPUs with neural-network-focused hardware features such as AVX512_VNNI, AVX512_BF16, AMX, and others, which are found on <a href=\"https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html\">Intel Cascade Lake</a> and newer CPUs.  </p><p>Users running TensorFlow with oneDNN optimizations enabled might observe slightly different numerical results from when the optimizations are off. This is because floating-point round-off approaches and order differ, and can create slight errors. If this causes issues for you, turn the optimizations off by setting <code>TF_ENABLE_ONEDNN_OPTS=0</code> before running your TensorFlow programs. To enable or re-enable them, set  <code>TF_ENABLE_ONEDNN_OPTS=1</code> before running your TensorFlow program. To verify that the optimizations are on, look for a message beginning with <code>\"oneDNN custom operations are on\"</code> in your program log. We welcome feedback on <a href=\"https://github.com/tensorflow/tensorflow\">GitHub</a> and the <a href=\"https://discuss.tensorflow.org/\">TensorFlow Forum</a>. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png\" /></a>   <h3>Model parallelism with DTensor</h3>  <p>DTensor is a new TensorFlow API for distributed model processing that allows models to seamlessly move from data parallelism to single program multiple data (<a href=\"https://www.tensorflow.org/guide/dtensor_overview\">SPMD</a>) based model parallelism, including <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/train-ml-models-on-large-images-and-3d-volumes-with-spatial-partitioning-on-cloud-tpus\">spatial partitioning</a>. This means you have tools to easily train models where the model weights or inputs are so large they don\u2019t fit on a single device. (If you are familiar with <a href=\"https://github.com/tensorflow/mesh\">Mesh TensorFlow</a> in TF1, DTensor serves a similar purpose.) </p><p>DTensor is designed with the following principles at its core: </p><ul> <li><strong>A device-agnostic API</strong>:  This allows the same model code to be used on CPU, GPU, or TPU, including models partitioned across device types.  <li><strong>Multi-client execution</strong>: Removes the coordinator and leaves each task to drive its locally attached devices, allowing scaling a model with no impact to startup time.  <li><strong>A global perspective vs. per-replica: </strong>Traditionally with TensorFlow, distributed model code is written around replicas, but with DTensor, model code is written from the global perspective and per replica code is generated and run by the DTensor runtime. Among other things, this means no uncertainty about whether  batch normalization is happening at the global level or the per replica level. </li></ul><p>We have developed several introductory tutorials on DTensor, from DTensor concepts to training DTensor ML models with Keras: </p><ul> <li><a href=\"https://www.tensorflow.org/guide/dtensor_overview\">DTensor Concepts</a> <li><a href=\"https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial\">Distributed ML with DTensors</a> <li><a href=\"https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial\">Using DTensors with Keras</a></li></ul><h3> TraceType for tf.function</h3>  <p>We have revamped the way tf.function retraces to make it simpler, predictable, and configurable. </p><p>All arguments of <code>tf.function</code> are assigned a <code>tf.types.experimental.TraceType. </code>Custom user classes can declare a <code>TraceType</code>  using the Tracing Protocol (<code>tf.types.experimental.SupportsTracingProtocol</code>). </p><p>The <code>TraceType</code> system makes it easy to understand retracing rules. For example, <a href=\"https://en.wikipedia.org/wiki/Subtyping\">subtyping</a> rules indicate what type of arguments can be used with particular function traces. Subtyping also explains how different specific shapes are joined into a generic shape that is their supertype, to reduce the number of traces for a function.  </p><p>To learn more, see the new APIs for <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType\">tf.types.experimental.TraceType</a></code>, <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/types/experimental/SupportsTracingProtocol\">tf.types.experimental.SupportsTracingProtocol</a></code>, and the <code>reduce_retracing</code> parameter of <code>tf.function</code>.  <h3>Support for WSL2</h3>  <p>The <a href=\"https://docs.microsoft.com/en-us/windows/wsl/about\">Windows Subsystem for Linux</a> lets developers run a Linux environment directly on Windows, without the overhead of a traditional virtual machine or dual boot setup. TensorFlow now supports WSL2 out of the box, including GPU acceleration. Please see the documentation for more details about the requirements and how to install WSL2 on Windows. </p><h3>Deterministic behavior</h3>  <p>The API <code>tf.config.experimental.enable_op_determinism</code> makes TensorFlow ops deterministic. </p><p>Determinism means that if you run an op multiple times with the same inputs, the op returns the exact same outputs every time. This is useful for debugging models, and if you train your model from scratch several times with determinism, your model weights will be the same every time. Normally, many ops are non-deterministic due to the use of threads within ops which can add floating-point numbers in a nondeterministic order. </p><p>TensorFlow 2.8 introduced <a href=\"https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism\">an API</a> to make ops deterministic, and TensorFlow 2.9 improved determinism performance in <code>tf.data</code> in some cases. If you want your TensorFlow models to run deterministically, just add the following to the start of your program: </p><p>``` </p><p>tf.keras.utils.set_random_seed(1) </p><p>tf.config.experimental.enable_op_determinism() </p><p>``` </p><p>The first line sets the random seed for Python, NumPy, and TensorFlow, which is necessary for determinism. The second line makes each TensorFlow op deterministic. Note that determinism in general comes at the expense of lower performance and so your model may run slower when op determinism is enabled. </p><h3>Optimized Training with Keras</h3>  <p>In TensorFlow 2.9, we are releasing a new experimental version of the Keras Optimizer API, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental\">tf.keras.optimizers.experimental</a>. The API provides a more unified and expanded catalog of built-in optimizers which can be more easily customized and extended.  </p><p>In a future release, <code>tf.keras.optimizers.experimental.Optimizer</code> (and subclasses) will replace <code>tf.keras.optimizers.Optimizer</code> (and subclasses), which means that workflows using the legacy Keras optimizer will automatically switch to the new optimizer. The current (legacy) tf.keras.optimizers.* API will still be accessible via tf.keras.optimizers.legacy.*, such as tf.keras.optimizers.legacy.Adam. </p><p>Here are some highlights of the new optimizer class: </p><ul> <li>Incrementally faster training for some models.  <li>Easier to write customized optimizers.  <li>Built-in support for moving average of model weights (\"Polyak averaging\"). </li></ul><p>For most users, you will need to take no action. But, if you have an advanced workflow falling into the following cases, please make corresponding changes: </p><p><strong>Use Case 1: You implement a customized optimizer based on the Keras optimizer</strong></p><p>For these works, please first check if it is possible to change your dependency to <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental\">tf.keras.optimizers.experimental.Optimizer. </a></code>If for any reason you decide to stay with the old optimizer (we discourage it), then you can change your optimizer to <code>tf.keras.optimizers.legacy.Optimizer</code> to avoid being automatically switched to the new optimizer in a later TensorFlow version.  <p><strong>Use Case 2: Your work depends on third-party Keras-based optimizers (such as tensorflow_addons)</strong></p><p>Your work should run successfully as long as the library continues to support the specific optimizer. However, if the library maintainers fail to take actions to accommodate the Keras optimizer change, your work would error out. So please stay tuned with the third-party library\u2019s announcement, and<a href=\"https://github.com/keras-team/keras/issues\"> file a bug to Keras team</a> if your work is broken due to optimizer malfunction.  </p><p><strong>Use Case 3: Your work is based on TF1</strong></p><p>First of all, please try <a href=\"https://www.tensorflow.org/guide/migrate\">migrating to TF2</a>. It is worth it, and may be easier than you think! If for any reason migration is not going to happen soon, then please replace your <code>tf.keras.optimizers.XXX</code> to <code>tf.keras.optimizers.legacy.XXX</code> to avoid being automatically switched to the new optimizer. </p><p><strong>Use Case 4: Your work has customized gradient aggregation logic</strong></p><p>Usually this means you are doing gradients aggregation outside the optimizer, and calling <code>apply_gradients()</code> with <code>experimental_aggregate_gradients=False</code>. We changed the argument name, so please change your optimizer to <code>tf.keras.optimizers.experimental.Optimizer</code> and set <code>skip_gradients_aggregation=True</code>. If it errors out after making this change, please <a href=\"https://github.com/keras-team/keras/issues\">file a bug</a> to Keras team. </p><p><strong>Use Case 5: Your work has direct calls to deprecated optimizer public APIs</strong></p><p>Please check if your method call has a match <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental\">here</a>. change your optimizer to <code>tf.keras.optimizers.experimental.Optimizer</code>.  If for any reason you want to keep using the old optimizer, change your optimizer to <code>tf.keras.optimizers.legacy.Optimizer</code>. </p><p><strong>Next steps</strong></p><p>Check out the <a href=\"https://github.com/tensorflow/tensorflow/releases\">release notes</a> for more information. To stay up to date, you can read the TensorFlow <a href=\"https://blog.tensorflow.org/\">blog</a>, follow <a href=\"http://twitter.com/tensorflow\">twitter.com/tensorflow</a>, or subscribe to <a href=\"https://youtube.com/tensorflow\">youtube.com/tensorflow</a>. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at <a href=\"http://goo.gle/TFCS\">goo.gle/TFCS</a>. For feedback, please file an issue on <a href=\"https://github.com/tensorflow/tensorflow/issues\">GitHub</a> or post to the <a href=\"https://discuss.tensorflow.org/\">TensorFlow Forum</a>. Thank you! </p>",
            "pubdate": "Wed, 18 May 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "Real-time SKU detection in the browser using TensorFlow.js": {
            "url": "https://blog.tensorflow.org/2022/05/real-time-sku-detection-in-browser.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png\" style=\"display: none;\" />  <p><em>Posted by <a href=\"https://www.linkedin.com/in/hugozanini/?locale=en_US\">Hugo Zanini</a>, Data Product Manager</em><p> <a name=\"more\"></a><p></p> <p>Last year, I published an article on<a href=\"https://blog.tensorflow.org/2021/01/custom-object-detection-in-browser.html\"> how to train custom object detection in the browser using TensorFlow.js</a>. This received lots of  interest from developers from all over the world who tried to apply the solution to their personal or business projects.While answering reader\u2019s questions on my first article, I noticed a few difficulties in adapting our solution to large datasets, and deploying the resulting model in production using the new version of TensorFlow.js. </p><p>Therefore, the goal of this article is to share a solution for a well-known problem in the consumer packaged goods (CPG) industry: real-time and offline <a href=\"https://en.wikipedia.org/wiki/Stock_keeping_unit\">SKU</a> detection using TensorFlow.js. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-RgZkEm4Rb9QERMU7H4wVyk5Z6KdeiCKoyGOOlOnyEiefkPXnDbFlVAW-ORzatYYq2bgkEc9XXXux_WzoPzlT3u-13WyNL8p8-dNo73C1k-ywKzuAAe207cSvaMx84_MrcqrT-qaVE-yL39_BbFX6ba3G9PepBjusD3M_wS0gsSVcIBALq6XjHpiy/s1600/ezgif.com-gif-maker.gif\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-RgZkEm4Rb9QERMU7H4wVyk5Z6KdeiCKoyGOOlOnyEiefkPXnDbFlVAW-ORzatYYq2bgkEc9XXXux_WzoPzlT3u-13WyNL8p8-dNo73C1k-ywKzuAAe207cSvaMx84_MrcqrT-qaVE-yL39_BbFX6ba3G9PepBjusD3M_wS0gsSVcIBALq6XjHpiy/s1600/ezgif.com-gif-maker.gif\" /></a> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-RgZkEm4Rb9QERMU7H4wVyk5Z6KdeiCKoyGOOlOnyEiefkPXnDbFlVAW-ORzatYYq2bgkEc9XXXux_WzoPzlT3u-13WyNL8p8-dNo73C1k-ywKzuAAe207cSvaMx84_MrcqrT-qaVE-yL39_BbFX6ba3G9PepBjusD3M_wS0gsSVcIBALq6XjHpiy/s1600/ezgif.com-gif-maker.gif\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Offline SKU detection running in real time on a smartphone using TensorFlow.js  </td></tr></tbody></table>  <p><strong>The problem</strong></p><p>Items consumed frequently by consumers (foods, beverages, household products, etc) require an extensive routine of replenishment and placement of those products at their point of sale (supermarkets, convenience stores, etc).  </p><p>Over the past few years, researchers have shown repeatedly that about two-thirds of purchase decisions are made after customers enter the store. One of the biggest challenges for consumer goods companies is to guarantee the availability and correct placement of their product in-stores.   </p><p>At stores, teams organize the shelves based on marketing strategies, and manage the level of products in the stores. The people working on these activities may count the number of SKUs of each brand in a store to estimate product stocks and market share, and help to shape marketing strategies. </p><p>These estimations though are very time-consuming. Taking a photo and using an algorithm to count the SKUs on the shelves to calculate a brand\u2019s market share could be a good solution. </p><p>To use an approach like that, the detection should run in real-time such that as soon as you point a phone camera to the shelf, the algorithm recognizes the brands and calculates the market shares. And, as the internet inside the stores is generally limited, the detection should work offline. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png\" /></a> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXJKi_ujdUUveIPpH2BeriYxl3K7TzTUfsocSRD1xKulteBDQZkZYmQBB1z_yW4C6orHg825mRAhFCJTEuOrtHjpMFpVUVcBUCQ33OYwkmPRDKdTs9oH4RBHrb2Ay9p_j3bDhsIfjwzukA1Sr-uT0uSXA_kpXnJSi62XSi0bHPDsaPMlQZZyGReTiB/s1600/image7.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"> Example workflow</td></tr></tbody></table>   <p>This post is going to show how to implement the real-time and offline image recognition solution to identify generic SKUs using the<a href=\"https://github.com/eg4000/SKU110K_CVPR19\"> SKU110K dataset</a> and the<a href=\"https://arxiv.org/abs/1801.04381\"> MobileNetV2</a> network. </p><p>Due to the lack of a public dataset with labeled SKUs of different brands, we\u2019re going to create a generic algorithm, but all the instructions can be applied in a multiclass problem. </p><p>As with every machine learning flow, the project will be divided into four steps, as follows: </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsTYleef2QUeRdcScNczV1XV50dtvAvS3M9jh47bAwfMEG0zJIOt0VJvlteFAaoPvA7D2U21L1zxmo_41WG6QVf46l4hnIsbCPn63EWCw4Dp_qSTEL2O9FIUOzPMElANPmlkMFd64ZFzD7eOrtkDqpu8_-CM4R5OM-v8VPtd9606_i1dzTjjD8NvSC/s1600/image3.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsTYleef2QUeRdcScNczV1XV50dtvAvS3M9jh47bAwfMEG0zJIOt0VJvlteFAaoPvA7D2U21L1zxmo_41WG6QVf46l4hnIsbCPn63EWCw4Dp_qSTEL2O9FIUOzPMElANPmlkMFd64ZFzD7eOrtkDqpu8_-CM4R5OM-v8VPtd9606_i1dzTjjD8NvSC/s1600/image3.png\" /></a>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsTYleef2QUeRdcScNczV1XV50dtvAvS3M9jh47bAwfMEG0zJIOt0VJvlteFAaoPvA7D2U21L1zxmo_41WG6QVf46l4hnIsbCPn63EWCw4Dp_qSTEL2O9FIUOzPMElANPmlkMFd64ZFzD7eOrtkDqpu8_-CM4R5OM-v8VPtd9606_i1dzTjjD8NvSC/s1600/image3.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Object Detection Model Production Pipeline</td></tr></tbody></table>    <p><strong>Preparing the data</strong></p><p>The first step to training a good model is to gather good data. As mentioned before, this solution is going to use a dataset of SKUs in different scenarios. The purpose of SKU110K was to create a benchmark for models capable of recognizing objects in densely packed scenes. </p><p>The dataset is provided in the <a href=\"https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5\">Pascal VOC</a> format and has to be converted to <em><a href=\"https://keras.io/examples/keras_recipes/creating_tfrecords/\">tf.record</a></em>. The script to do the conversion is<a href=\"https://github.com/hugozanini/realtime-sku-detection/blob/main/pascal-to-tfrecord.py\"> available here</a> and the<em> tf.record</em> version of the dataset is also available in my <a href=\"https://github.com/hugozanini/realtime-sku-detection\">project repository</a>. As mentioned before, SKU110K is a large and very challenging dataset to work with. It contains many objects, often looking similar or even identical, positioned in close proximity. </p>  <center>  </center>  <p>To work with this dataset, the neural network chosen has to be very effective in recognizing patterns and be small enough to run in real-time in TensorFlow.js. </p><p><strong>Choosing the model</strong></p><p>There are a variety of neural networks capable of solving the SKU detection problem. But, the architectures that easily achieve a high level of precision are very dense and don't have reasonable inference times when converted to TensorFlow.js to run in real-time. </p><p>Because of that, the approach here is going to be to focus on optimizing a mid-level neural network to achieve reasonable precision working on densely packed scenes and run the inferences in real-time. Analyzing the<a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\"> TensorFlow 2.0 Detection Model Zoo</a>, the challenge will be to try to solve the problem using the lighter single-shot model available: SSD MobileNet v2 320x320 which seems to fit the criteria required. The architecture is proven to be able to recognize up to 90 classes and can be trained to identify different SKUs. </p><p><strong>Training the model</strong></p><p>With a good dataset and the model selected, it\u2019s time to think about the training process. TensorFlow 2.0 provides an<a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\"> Object Detection API</a> that makes it easy to construct, train, and deploy object detection models. In this project, we\u2019re going to use this API and train the model using a<a href=\"https://colab.research.google.com/drive/1dyuGpeLh1K4rqOkxELLTTlSqhfbTnFac?usp=sharing\"> Google Colaboratory Notebook</a>. The remainder of this section explains how to set up the environment, the model selection, and training. If you want to jump straight to the Colab Notebook,<a href=\"https://colab.research.google.com/drive/1WDXd1IzyQRw1oDmcapAM1PyZxgw2UVJM?usp=sharing\"> click here</a>. </p><p><strong>Setting up the environment</strong></p><p><a href=\"https://colab.research.google.com/\">Create</a> a new Google Colab notebook and select a GPU as the hardware accelerator: </p>   <pre class=\"prettyprint\">Runtime &gt; Change runtime type &gt; Hardware accelerator: GPU<br /></pre>  <p>Clone, install, and test the TensorFlow Object Detection API: </p> <center>  </center> <p>Next, download and extract the dataset using the following commands: </p>  <center>  </center>  <p><strong>Setting up the training pipeline</strong></p><p>We\u2019re ready to configure the training pipeline. TensorFlow 2.0 provides pre-trained weights for the SSD Mobilenet v2 320x320 on the<a href=\"https://cocodataset.org/#home\"> COCO 2017 Dataset</a>, and they are going to be downloaded using the following commands: </p>  <center>  </center> <p>The downloaded weights were pre-trained on the<a href=\"https://cocodataset.org/#home\"> COCO 2017 Dataset</a>, but the focus here is to train the model to recognize one class so these weights are going to be used only to initialize the network\u200a\u2014\u200athis technique is known as<a href=\"https://www.tensorflow.org/tutorials/images/transfer_learning\"> transfer learning</a>, and it\u2019s commonly used to speed up the learning process. </p><p>The last step is to set up the hyperparameters on the configuration file that is going to be used during the training. Choosing the best hyperparameters is a task that requires some experimentation and, consequently, computational resources. </p><p>I took a standard configuration of MobileNetV2 parameters from the TensorFlow <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2\">Models Config Repository</a> and performed a sequence of experiments (thanks Google Developers for the free resources) to optimize the model to work with densely packed scenes on the SKU110K dataset. Download the configuration and check the parameters using the code below. </p>  <center>  </center>  <center>  </center> <p>With the parameters set, start the training by executing the following command: </p>  <center>  </center><p>To identify how well the training is going, we use the loss value. Loss is a number indicating how bad the model\u2019s prediction was on the training samples. If the model\u2019s prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples (<a href=\"https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss\">Descending into ML: Training and Loss | Machine Learning Crash Course</a>). </p><p>The training process was monitored through Tensorboard and took around 22h to finish on a 60GB machine using an NVIDIA Tesla P4. The final losses can be checked below </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4k_zpucjpl8y6HA_I4lB-wH41VY_Eg9awRHYD9kZCE7sPDJpAareh0sOhzi0Mbaeae1cxSK2vuR_zn3ZAHrat0_r5Dk3ttFUXIPoBopVPJCefHsj_3DyhVqsBM4F9aCvAauEjwfQ9s_aG0asu8k9JGCZf4D6W6DlgKAIbLAjCgZgVLnLRZutj8etf/s1600/image9.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4k_zpucjpl8y6HA_I4lB-wH41VY_Eg9awRHYD9kZCE7sPDJpAareh0sOhzi0Mbaeae1cxSK2vuR_zn3ZAHrat0_r5Dk3ttFUXIPoBopVPJCefHsj_3DyhVqsBM4F9aCvAauEjwfQ9s_aG0asu8k9JGCZf4D6W6DlgKAIbLAjCgZgVLnLRZutj8etf/s1600/image9.png\" style=\"display: block; padding: 1em 0px; text-align: center;\" /></a> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4k_zpucjpl8y6HA_I4lB-wH41VY_Eg9awRHYD9kZCE7sPDJpAareh0sOhzi0Mbaeae1cxSK2vuR_zn3ZAHrat0_r5Dk3ttFUXIPoBopVPJCefHsj_3DyhVqsBM4F9aCvAauEjwfQ9s_aG0asu8k9JGCZf4D6W6DlgKAIbLAjCgZgVLnLRZutj8etf/s1600/image9.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Total training loss </td></tr></tbody></table>   <p><strong>Validate the model</strong></p><p>Now let\u2019s evaluate the trained model using the test data: </p> <center>  </center> <p>The evaluation was done across 2740 images and provides three metrics based on the<a href=\"https://cocodataset.org/#detection-eval\"> COCO detection evaluation metrics</a>: precision, recall, and loss (<a href=\"https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\">Classification: Precision and Recall | Machine Learning Crash Course</a>). The same metrics are available via Tensorboard and can be analyzed in an easier way </p>   <pre class=\"prettyprint\">%load_ext tensorboard<br />%tensorboard --logdir '/content/training/'<br /></pre>  <p>You can then explore all training and evaluation metrics. </p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIuzWSK8B52q9FIJJ0ARUu4Y--esT101VSCIkUEg9JMTm5H0zy1r9PmkOkLTHl9ve-3DWWKa09876MB0TGYkzJXgRGMpSgriZZmOivA7fK4FJ7z_hXkEsiMkiGJBSgpfrD-MYBUlowijEKqECWHUEKxGxon9JDwqIllWlFuOD0qbEwBrblrnWDuToP/s1600/image8.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIuzWSK8B52q9FIJJ0ARUu4Y--esT101VSCIkUEg9JMTm5H0zy1r9PmkOkLTHl9ve-3DWWKa09876MB0TGYkzJXgRGMpSgriZZmOivA7fK4FJ7z_hXkEsiMkiGJBSgpfrD-MYBUlowijEKqECWHUEKxGxon9JDwqIllWlFuOD0qbEwBrblrnWDuToP/s1600/image8.png\" /></a> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhIuzWSK8B52q9FIJJ0ARUu4Y--esT101VSCIkUEg9JMTm5H0zy1r9PmkOkLTHl9ve-3DWWKa09876MB0TGYkzJXgRGMpSgriZZmOivA7fK4FJ7z_hXkEsiMkiGJBSgpfrD-MYBUlowijEKqECWHUEKxGxon9JDwqIllWlFuOD0qbEwBrblrnWDuToP/s1600/image8.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Main evaluation metrics </td></tr></tbody></table>   <p><strong>Exporting the model</strong></p><p>Now that the training is validated, it\u2019s time to export the model. We\u2019re going to convert the training checkpoints to a <em><a href=\"https://developers.google.com/protocol-buffers/docs/proto3\">protobuf </a></em>(pb) file. This file is going to have the graph definition and the weights of the model. </p><center>  </center>  <p>As we\u2019re going to deploy the model using TensorFlow.js and Google Colab has a maximum lifetime limit of 12 hours, let\u2019s download the trained weights and save them locally. When running the command <code>files.download(\"/content/saved_model.zip\")</code>, the Colab will prompt the file download automatically. </p> <center> </center>   <p><strong>Deploying the model</strong></p><p>The model is going to be deployed in a way that anyone can open a PC or mobile camera and perform inference in real-time through a web browser. To do that, we\u2019re going to convert the saved model to the TensorFlow.js layers format, load the model in a JavaScript application and make everything available on<a href=\"https://codesandbox.io/\"> CodeSandbox</a>. </p><p><strong>Converting the model</strong></p><p>At this point, you should have something similar to this structure saved locally: </p><p>%MD </p><p>\u251c\u2500\u2500 inference-graph </p><p>\u2502 \u251c\u2500\u2500 saved_model </p><p>\u2502 \u2502 \u251c\u2500\u2500 assets </p><p>\u2502 \u2502 \u251c\u2500\u2500 saved_model.pb </p><p>\u2502 \u2502 \u251c\u2500\u2500 variables </p><p>\u2502 \u2502 \u251c\u2500\u2500 variables.data-00000-of-00001 </p><p>\u2502 \u2502 \u2514\u2500\u2500 variables.index </p><p>Before we start, let\u2019s create an isolated Python environment to work in an empty workspace and avoid any library conflict.<a href=\"https://virtualenv.pypa.io/en/latest/installation.html\"> Install virtualenv</a> and then open a terminal in the inference-graph folder and create and activate a new virtual environment: </p>   <pre class=\"prettyprint\">virtualenv -p python3 venv<br />source venv/bin/activate<br /></pre>  <p>Install the<a href=\"https://github.com/tensorflow/tfjs/tree/master/tfjs-converter\"> TensorFlow.js converter</a>: </p>   <pre class=\"prettyprint\">pip install tensorflowjs[wizard]<br /></pre>  <p>Start the conversion wizard: </p>   <pre class=\"prettyprint\">tensorflowjs_wizard<br /></pre>  <p>Now, the tool will guide you through the conversion, providing explanations for each choice you need to make. The image below shows all the choices that were made to convert the model. Most of them are the standard ones, but options like the shard sizes and compression can be changed according to your needs. </p><p>To enable the browser to cache the weights automatically, it\u2019s recommended to split them into shard files of around 4MB. To guarantee that the conversion is going to work, don\u2019t skip the op validation as well, not all TensorFlow operations are supported so some models can be incompatible with TensorFlow.js\u200a\u2014\u200aSee<a href=\"https://github.com/tensorflow/tfjs/blob/master/tfjs-converter/docs/supported_ops.md\"> this list for</a> which ops are currently supported on the various backends that TensorFlow.js executes on such as WebGL, WebAssembly, or plain JavaScript. </p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMqCki32tUqEILujT0zrG-28wt9CxxQP1IQBwpWYi0hAQfix9TLFHcz9otuK_-kFn1KZbx068Z44cNtzfZ0zr-N6gev0SHhpQbDLt-PE5LeIInTphf9KN1FPPMUaTa14uRREpwhFJQuwy6mZd9oS7ZC5YT2yjrHGTTotKVt5vx6vJ0uGuFQtM9zdt9/s1600/image10.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMqCki32tUqEILujT0zrG-28wt9CxxQP1IQBwpWYi0hAQfix9TLFHcz9otuK_-kFn1KZbx068Z44cNtzfZ0zr-N6gev0SHhpQbDLt-PE5LeIInTphf9KN1FPPMUaTa14uRREpwhFJQuwy6mZd9oS7ZC5YT2yjrHGTTotKVt5vx6vJ0uGuFQtM9zdt9/s1600/image10.png\" style=\"display: block; padding: 1em 0px; text-align: center;\" /></a> <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMqCki32tUqEILujT0zrG-28wt9CxxQP1IQBwpWYi0hAQfix9TLFHcz9otuK_-kFn1KZbx068Z44cNtzfZ0zr-N6gev0SHhpQbDLt-PE5LeIInTphf9KN1FPPMUaTa14uRREpwhFJQuwy6mZd9oS7ZC5YT2yjrHGTTotKVt5vx6vJ0uGuFQtM9zdt9/s1600/image10.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><p><em>Model conversion using TensorFlow.js Converter (Full resolution image <a href=\"https://drive.google.com/file/d/1G-8uRdLeP6FpCtMeDNZ9yxHyygoKUSc2/view?usp=sharing\">here</a>)</em></p></td></tr></tbody></table>  <p>If everything works well, you\u2019re going to have the model converted to the TensorFlow.js layers format in the web_model directory. The folder contains a model.json file and a set of sharded weights files in a binary format. The model.json has both the model topology (aka \u201carchitecture\u201d or \u201cgraph\u201d: a description of the layers and how they are connected) and a manifest of the weight files (<a href=\"https://cocodataset.org/#detection-eval\">Lin, Tsung-Yi, et al</a>). The contents of the web_model folder currently contains the files shown below: </p>   <pre class=\"prettyprint\">\u2514 web_model<br />  \u251c\u2500\u2500 group1-shard1of5.bin<br />  \u251c\u2500\u2500 group1-shard2of5.bin<br />  \u251c\u2500\u2500 group1-shard3of5.bin<br />  \u251c\u2500\u2500 group1-shard4of5.bin<br />  \u251c\u2500\u2500 group1-shard5of5.bin<br />  \u2514\u2500\u2500 model.json<br /></pre>  <p><strong>Configuring the application</strong></p><p>The model is ready to be loaded in JavaScript. I\u2019ve created an application to perform inference directly from the browser. Let\u2019s<a href=\"https://github.com/hugozanini/realtime-sku-detection/tree/web\"> clone the repository</a> to figure out how to use the converted model in real-time. This is the project structure: </p>   <pre class=\"prettyprint\">\u251c\u2500\u2500 models<br />\u2502\t\u251c\u2500\u2500 group1-shard1of5.bin<br />\u2502\t\u251c\u2500\u2500 group1-shard2of5.bin<br />\u2502\t\u251c\u2500\u2500 group1-shard3of5.bin<br />\u2502\t\u251c\u2500\u2500 group1-shard4of5.bin<br />\u2502\t\u251c\u2500\u2500 group1-shard5of5.bin<br />\u2502\t\u2514\u2500\u2500 model.json<br />\u251c\u2500\u2500 package.json<br />\u251c\u2500\u2500 package-lock.json<br />\u251c\u2500\u2500 public<br />\u2502   \u2514\u2500\u2500 index.html<br />\u251c\u2500\u2500 README.MD<br />\u2514\u2500\u2500 src<br />\t\u251c\u2500\u2500 index.js<br />\t\u2514\u2500\u2500 styles.css<br /></pre>  <p>For the sake of simplicity, I have already provided a converted SKU-detector model in the model's folder. However, let\u2019s put the web_model generated in the previous section in the models folder and test it. </p><p>Next, install the<a href=\"https://www.npmjs.com/package/http-server\"> http-server</a>: </p>   <pre class=\"prettyprint\">npm install http-server -g<br /></pre>  <p>Go to the <em>models</em> folder and run the command below to make the model available at http://127.0.0.1:8080 . This is a good choice when you want to keep the model weights in a safe place and control who can request inferences to it. The -c1 parameter is added to disable caching, and the --cors flag enables<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\"> cross-origin resource sharing</a> allowing the hosted files to be used by the client-side JavaScript for a given domain. </p>   <pre class=\"prettyprint\">http-server -c1 --cors .<br /></pre>  <p>Alternatively, you can upload the model files somewhere else - even on a different domain if needed. In my case, I chose my own Github repo and referenced the model.json folder URL in the load_model function as shown below: </p>   <pre class=\"prettyprint\">async function load_model() {<br />\t// It's possible to load the model locally or from a repo.<br />\t// Load from localhost locally:<br />      const model = await loadGraphModel(\"http://127.0.0.1:8080/model.json\");<br />\t// Or Load from another domain using a folder that contains model.json.<br />      // const model = await loadGraphModel(\"https://github.com/hugozanini/realtime-sku-detection/tree/web\");<br />\treturn model;<br />}<br /></pre>  <p>This is a good option because it gives more flexibility to the application and makes it easier to run on public web servers. </p><p>Pick one of the methods to load the model files in the function load_model <a href=\"https://github.com/hugozanini/realtime-sku-detection/blob/79f7414de797abcc6681ecef582790c33e96369d/src/index.js#L10\">(lines 10\u201315 in the file src&gt;index.js</a>).  </p><p>When loading the model, TensorFlow.js will perform the following requests: </p>   <pre class=\"prettyprint\">GET /model.json<br />GET /group1-shard1of5.bin<br />GET /group1-shard2of5.bin<br />GET /group1-shard3of5.bin<br />GET /group1-shardo4f5.bin<br />GET /group1-shardo5f5.bin<br /></pre>  <p><strong>Publishing in CodeSandbox</strong></p><p><a href=\"https://codesandbox.io/\">CodeSandbox</a> is a simple tool for creating web apps where we can upload the code and make the application available for everyone on the web. By uploading the model files in a GitHub repo and referencing them in the load_model function, we can simply log into <em>CodeSandbox</em>, click on New project &gt; Import from Github, and select the app repository. </p><p>Wait some minutes to install the packages and your app will be available at a public URL that you can share with others. Click on Show &gt; In a new window and a tab will open with a live preview. Copy this URL and paste it in any web browser (PC or Mobile) and your object detection will be ready to run. A ready to use project can be found <a href=\"https://codesandbox.io/s/sku-detection-mobilenet-wtvbj?file=/src/index.js\">here</a> as well if you prefer.  </p><strong>Conclusion</strong><p>Besides the precision, an interesting part of these experiments is the inference time\u200a\u2014\u200aeverything runs in real-time in the browser via JavaScript. SKU detection models running in the browser, <a href=\"https://www.tensorflow.org/js/guide/save_load#local_storage_browser_only\">even offline</a>, and using few computational resources is a must in many consumer packaged goods company applications, along with other industries too. </p><p>Enabling a Machine Learning solution to run on the client-side is a key step to guarantee that the models are being used effectively at the point of interaction with minimal latency and solve the problems when they happen: right in the user's hand. </p><p>Deep learning should not be costly and should be used beyond just research, for real world use cases, which JavaScript is great for production deployments. I hope this article will serve as a basis for new projects involving Computer Vision, TensorFlow, and create an easier flow between Python and Javascript. </p><p>If you have any questions or suggestions you can reach me on<a href=\"https://twitter.com/hugoznn\"> Twitter</a>. </p><p>Thanks for reading! </p><p><strong>Acknowledgments</strong></p><p>I\u2019d like to thank the<a href=\"https://developers.google.com/community/gdg\"> Google Developers Group</a>, for providing all the computational resources for training the models, and the authors of the<a href=\"https://arxiv.org/abs/1904.00853\"> SKU 110K Dataset</a>, for creating and open-sourcing the dataset used in this project. </p>",
            "pubdate": "Mon, 23 May 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                23
            ],
            "email_sent": true
        },
        "5 steps to go from a notebook to a deployed model": {
            "url": "https://blog.tensorflow.org/2022/05/5-steps-to-go-from-notebook-to-deployed.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaEGCozhmfcLBv-hG8Uath2-SdRoNrWtVDfNvE_7RrmqB-fGoHWgkwQTU-Rxep9Z32WX3edWj0SgYf8fWsRrky04ezcAaqFfuJaW64E1uWcKMDiETB5HzGBuC9XZEkr4eaabS863ez4v1C_TdK37TJH-pF31Bjd4ofwLsUhcd1aB2267iJ265hQGoS/s1600/image9.png\" style=\"display: none;\" /> <p><em>Posted by Nikita Namjoshi, Google Cloud Developer Advocate </em><p> <a name=\"more\"></a><p></p>  <p>When you start working on a new machine learning problem, I\u2019m guessing the first environment you use is a notebook. Maybe you like running <a href=\"https://jupyter.org/\">Jupyter</a> in a local environment, using a <a href=\"https://www.kaggle.com/code\">Kaggle Kernel</a>, or my personal favorite, <a href=\"https://colab.research.google.com/\">Colab</a>. With tools like these, creating and experimenting with machine learning is becoming increasingly accessible. But while experimentation in notebooks is great, it\u2019s easy to hit a wall when it comes time to elevate your experiments up to production scale. Suddenly, your concerns are more than just getting the highest accuracy score. </p><p>  </p><p>What if you have a long running job, want to do distributed training, or host a model for online predictions? Or maybe your use case requires more granular permissions around security and data privacy. What is your data going to look like at serving time, how will you handle code changes, or monitor the performance of your model overtime? </p><p>  </p><p>Making production applications or training large models requires additional tooling to help you scale beyond just code in a notebook, and using a cloud service provider can help. But that process can feel a bit daunting. Take a look at the full list of <a href=\"https://googlecloudcheatsheet.withgoogle.com/\">Google Cloud products</a>, and you might be completely unsure where to start. </p><p>So to make your journey a little easier, I\u2019ll show you a fast path from experimental notebook code to a deployed model in the cloud. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaEGCozhmfcLBv-hG8Uath2-SdRoNrWtVDfNvE_7RrmqB-fGoHWgkwQTU-Rxep9Z32WX3edWj0SgYf8fWsRrky04ezcAaqFfuJaW64E1uWcKMDiETB5HzGBuC9XZEkr4eaabS863ez4v1C_TdK37TJH-pF31Bjd4ofwLsUhcd1aB2267iJ265hQGoS/s1600/image9.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaEGCozhmfcLBv-hG8Uath2-SdRoNrWtVDfNvE_7RrmqB-fGoHWgkwQTU-Rxep9Z32WX3edWj0SgYf8fWsRrky04ezcAaqFfuJaW64E1uWcKMDiETB5HzGBuC9XZEkr4eaabS863ez4v1C_TdK37TJH-pF31Bjd4ofwLsUhcd1aB2267iJ265hQGoS/s1600/image9.png\" /></a>  <p>The code used in this sample can be <a href=\"https://github.com/nikitamaia/tensorflow-examples/blob/main/prod_in_5_steps.ipynb\">found here</a>. This notebook trains an image classification model on the <a href=\"https://www.tensorflow.org/datasets/catalog/tf_flowers\">TF Flowers dataset.</a> You\u2019ll see how to deploy this model in the cloud and get predictions on a new flower image via a REST endpoint. </p><p>Note that you\u2019ll need a Google Cloud project with billing enabled to follow this tutorial. If you\u2019ve never used Google Cloud before, you can <a href=\"https://cloud.google.com/free/docs/gcp-free-tier\">follow these instructions</a> to set up a project and get $300 in free credits to experiment with. </p><p>Here are the five steps you\u2019ll take: </p><ol> <li>Create a Vertex AI Workbench managed notebook  <li>Upload .ipynb file  <li>Launch notebook execution  <li>Deploy model  <li>Get predictions </li></ol><h3>Create a Vertex AI Workbench managed notebook</h3>  <p>To train and deploy the model, you\u2019ll use <a href=\"https://cloud.google.com/vertex-ai\">Vertex AI</a>, which is Google Cloud\u2019s managed machine learning platform. Vertex AI contains lots of different products that help you across the entire lifecycle of an ML workflow. You\u2019ll use a few of these products today, starting with <a href=\"https://cloud.google.com/vertex-ai-workbench\">Workbench</a>, which is the managed notebook offering. </p><p>Under the Vertex AI section of the <a href=\"https://console.cloud.google.com/\">cloud console</a>, select \u201cWorkbench\u201d.  Note that if this is the first time you\u2019re using Vertex AI in a project, you\u2019ll be prompted to enable the Vertex API and the Notebooks API. So be sure to click the button in the UI to do so. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghh2FVADiuk_W08IMOyrpwm4TvUq8zCnUjGuw_-pd0U02NLUhHUk6E7BXstgdsVNPJNuW5I_QCzdx7PDDEUUMc2um3N76-1qSgBgvD1Q7vZE1iEb0Hov64MtlL1ZD4wKZ6BVZ7lyPSW-GwmovnlXxYZd8_1hBltrx5LQE-GeZn5LCZ1QIG3_vhZvas/s1600/image13.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghh2FVADiuk_W08IMOyrpwm4TvUq8zCnUjGuw_-pd0U02NLUhHUk6E7BXstgdsVNPJNuW5I_QCzdx7PDDEUUMc2um3N76-1qSgBgvD1Q7vZE1iEb0Hov64MtlL1ZD4wKZ6BVZ7lyPSW-GwmovnlXxYZd8_1hBltrx5LQE-GeZn5LCZ1QIG3_vhZvas/s1600/image13.png\" /></a>  <p>Next, select <strong>MANAGED NOTEBOOKS</strong>, and then <strong>NEW NOTEBOOK</strong>. </p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihGUgeiCGozEfcrQ6fyHxY1To62hgwD6j0q_2EYQ9NJFz8sblOLVOLzqE0pjNt0kGIW1nxpAi5F00pVNh-w-CD-gYstBUHc1_pQh7IahDOJfs7Lm5FedjrmXvcQg_R2fneHIykWx3rUXRaKUGHa1yQI6fMan1HDaQuHTzq3aDF2oMq1_1JaYHhdlIg/s1600/image16.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihGUgeiCGozEfcrQ6fyHxY1To62hgwD6j0q_2EYQ9NJFz8sblOLVOLzqE0pjNt0kGIW1nxpAi5F00pVNh-w-CD-gYstBUHc1_pQh7IahDOJfs7Lm5FedjrmXvcQg_R2fneHIykWx3rUXRaKUGHa1yQI6fMan1HDaQuHTzq3aDF2oMq1_1JaYHhdlIg/s1600/image16.png\" /></a>  <p>Under <strong>Advanced Settings </strong>you can customize your notebook by specifying the machine type and location, adding GPUs, providing custom containers, and enabling terminal access. For now, keep the default settings and just provide a name for your notebook. Then click CREATE. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKANmUL-SL7BdnReszium05E8nZx_-16hHGaosCxKW1xs_H8cIW7arNAz03Ic9BOOfhzVFrp4PUIjG7Kg8K5C7CwBf47OA98xs2jSydxzceqNRZwrrKwbpjm4QPvNT63b01nHuDtfDeq8aYboOCYYulgl4r61Rp1THibDhkffUYp-3-oXUu8Y1ZXrx/s1600/image3.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKANmUL-SL7BdnReszium05E8nZx_-16hHGaosCxKW1xs_H8cIW7arNAz03Ic9BOOfhzVFrp4PUIjG7Kg8K5C7CwBf47OA98xs2jSydxzceqNRZwrrKwbpjm4QPvNT63b01nHuDtfDeq8aYboOCYYulgl4r61Rp1THibDhkffUYp-3-oXUu8Y1ZXrx/s1600/image3.png\" /></a>  <p>You\u2019ll know your notebook is ready when you see the <strong>OPEN JUPYTERLAB</strong> text turn blue. The first time you open the notebook, you\u2019ll be prompted to authenticate and you can follow the steps in the UI to do so. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHGQfxOqMSyvo29b2cBzZnuxUdID42F1HzMruez6tjId58A48C4XHSxOY0gQkFJLrTCaOOYzH3t_5D2XNFp9Ltxv5q4LYW1uKg1ENbTH4lQBZuF69y_HfTWqDmUeAwVmQWdKtQrN3qn0ZK20KcOHifAqQASzKbSuCtJD7cYXPskB9F5Jxdks_X6YmQ/s1600/image8.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjHGQfxOqMSyvo29b2cBzZnuxUdID42F1HzMruez6tjId58A48C4XHSxOY0gQkFJLrTCaOOYzH3t_5D2XNFp9Ltxv5q4LYW1uKg1ENbTH4lQBZuF69y_HfTWqDmUeAwVmQWdKtQrN3qn0ZK20KcOHifAqQASzKbSuCtJD7cYXPskB9F5Jxdks_X6YmQ/s1600/image8.png\" /></a>  <p>When you open the JupyterLab instance, you\u2019ll see a few different notebook options. Vertex AI Workbench provides different kernels (TensorFlow, R, XGBoost, etc), which are managed environments preinstalled with common libraries for data science. If you need to add additional libraries to a kernel, you can use pip install from a notebook cell, just like you would in Colab. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqRGpn0jG-3ZAawJVolo4v0AnRlvfUn9UP9e4iHI6vJtaAUubWdT3Wpha-NuyRB2Z89YFUccZnb1XIA7ZFcQDRf-XWfe4uKuOJMCOWwwfAyjFv9lPROyJqUHBRIFsFiGN6L4teuOGiDxAXFcs30haIuIO1ax56ZU3CKJQpMfoPBt82aKuSwPx4Ug9X/s1600/image17.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqRGpn0jG-3ZAawJVolo4v0AnRlvfUn9UP9e4iHI6vJtaAUubWdT3Wpha-NuyRB2Z89YFUccZnb1XIA7ZFcQDRf-XWfe4uKuOJMCOWwwfAyjFv9lPROyJqUHBRIFsFiGN6L4teuOGiDxAXFcs30haIuIO1ax56ZU3CKJQpMfoPBt82aKuSwPx4Ug9X/s1600/image17.png\" /></a>     <p>Step one is complete! You\u2019ve created your managed JupyterLab environment. </p><h3>Upload .ipynb file</h3>  <p>Now it\u2019s time to get our TensorFlow code into Google Cloud. If you\u2019ve been working in a different environment (Colab, local, etc), you can upload any code artifacts you need to your Vertex AI Workbench managed notebook, and you can even integrate with GitHub. In the future, you can do all of your development right in Workbench, but for now let\u2019s assume you\u2019ve been using Colab. </p><p>Colab notebooks can be exported as .ipynb files. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5bW86zwVwwlMwJ0vYV_ECnnUj-TWgo70WdxTlob_ozUfYHOind4gZE6kaE6Etz3Phn3Pdk6yGrspi3etzdIXSQLUVK8suEZfqU3BFJCV73peLUSqWKNGDoIP0jaMGW6eTUssRQ12k8KGEhb1cOb3jIv_IJdLoJDLr-eit-A_c_0cSm4hGAK9bk6aa/s1600/image10.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5bW86zwVwwlMwJ0vYV_ECnnUj-TWgo70WdxTlob_ozUfYHOind4gZE6kaE6Etz3Phn3Pdk6yGrspi3etzdIXSQLUVK8suEZfqU3BFJCV73peLUSqWKNGDoIP0jaMGW6eTUssRQ12k8KGEhb1cOb3jIv_IJdLoJDLr-eit-A_c_0cSm4hGAK9bk6aa/s1600/image10.png\" /></a>  <p>You can upload the file to Workbench by clicking the \u201cupload files\u201d icon. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheg17QjgbIMjvXL62woZRNxGDqQ0IB_yPxiTYL0sOxM73No7rOGspTqg8K5k4SbRAp1Vx1pg7zExC-g1D3sU1dZ9yvvoaBA2XDsJqj2CdQkcv3LfCMZWHbKhYY6niKRdw6q2-e0P7DJun6WTEylNFVRqdXe0qoEtyPcKu1hSCIDQ5BrZYYXRYU3kek/s1600/image6.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEheg17QjgbIMjvXL62woZRNxGDqQ0IB_yPxiTYL0sOxM73No7rOGspTqg8K5k4SbRAp1Vx1pg7zExC-g1D3sU1dZ9yvvoaBA2XDsJqj2CdQkcv3LfCMZWHbKhYY6niKRdw6q2-e0P7DJun6WTEylNFVRqdXe0qoEtyPcKu1hSCIDQ5BrZYYXRYU3kek/s1600/image6.png\" /></a>  <p>When you open the notebook in Workbench, you\u2019ll be prompted to select the kernel, which is the environment where your notebook is run. There are a few different kernels you can choose from, but since this code sample uses TensorFlow, you\u2019ll want to select the TensorFlow 2 kernel.  </p> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbrVCgkT_e1X-tBsCB9MwegaFuxqZx8E91MYC8GRU596yMNkDSG48FjmpPp1W3sZMeeX2NBvUw-0qMO9wfSlnV9gGDkbaqOW1F8tgEzhHLgnxNbvug_0ii1_CC4RnlmgctX9dbpuB_9Y9Do5l9TNsdYnwaBdnnw-ddkRdVc_tdcBk6X5Pdc6BFwLyc/s1600/image14.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbrVCgkT_e1X-tBsCB9MwegaFuxqZx8E91MYC8GRU596yMNkDSG48FjmpPp1W3sZMeeX2NBvUw-0qMO9wfSlnV9gGDkbaqOW1F8tgEzhHLgnxNbvug_0ii1_CC4RnlmgctX9dbpuB_9Y9Do5l9TNsdYnwaBdnnw-ddkRdVc_tdcBk6X5Pdc6BFwLyc/s1600/image14.png\" /></a>   <p>After you select the kernel, any cells you execute in your notebook will run in this managed TensorFlow environment. For example, if you execute the import cell, you\u2019ll see that you can import TensorFlow, TensorFlow Datasets, and NumPy. This is because all of these libraries are included in the Vertex AI Workbench TensorFlow 2 kernel. Unsurprisingly, if you try to execute that same notebook cell in the XGBoost kernel, you\u2019ll see an error message since TensorFlow is not installed there. </p><h3>Launch a notebook execution</h3>  <p>While we could run the rest of the notebook cells manually, for models that take a long time to train, a notebook isn\u2019t always the most convenient option. And if you\u2019re building an application with ML, it\u2019s unlikely that you\u2019ll only need to train your model once. Over time, you\u2019ll want to retrain your model to make sure it stays fresh and keeps producing valuable results.  </p><p>Manually executing the cells of your notebook might be the right option when you\u2019re getting started with a new machine learning problem. But when you want to automate experimentation at a large scale, or retrain models for a production application, a managed ML training option will make things much easier.  </p><p>The quickest way to launch a training job is through the <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/schedule-and-execute-notebooks-with-vertex-ai-workbench\">notebook execution feature</a>, which will run the notebook cell by cell on the Vertex AI managed training service.  </p><p>When you launch the training job, it\u2019s going to run on a machine you won\u2019t have access to after the job completes. So you don\u2019t want to save the TensorFlow model artifacts to a local path. Instead, you\u2019ll want to save to <a href=\"https://cloud.google.com/storage\">Cloud Storage</a>, which is Google Cloud\u2019s object storage, meaning you can store images, csv files, txt files, saved model artifacts. Just about anything.  </p><p>Cloud storage has the concept of a \u201cbucket\u201d which is what holds your data. You can <a href=\"https://console.cloud.google.com/storage/create-bucket\">create them via the UI</a>. Everything you store in Cloud Storage must be contained in a bucket. And within a bucket, you can create folders to organize your data.  </p><p>Each file in Cloud Storage has a path, just like a file on your local filesystem. Except that Cloud Storage paths always start with <strong><code>gs://</code></strong> <p>You\u2019ll want to update your training code so that you\u2019re saving to a Cloud Storage bucket instead of a local path.  </p><p>For example, here I\u2019ve updated the last cell of the notebook from <code>model.save('model_ouput\").</code>Instead of saving locally, I\u2019m now saving the artifacts to a bucket called <code>nikita-flower-demo-bucket</code> that I\u2019ve created in my project. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj016o6cNcn4wgdZCSRHNk-NXMco53VVl1znMI_KLd8vk7cx9dKB_qXVdKNWn1Vl0xFvXkKaRFkr7r1epgWKshfr_distsHx3kpDXEmhaORIWp7kIhtbek8JbZCcr0ifq9R1xLtoTmU58q8SjJMgCsg5BdRktraQz-lMSAiEB5NUSGAjVD0oCbslQkJ/s1600/image5.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj016o6cNcn4wgdZCSRHNk-NXMco53VVl1znMI_KLd8vk7cx9dKB_qXVdKNWn1Vl0xFvXkKaRFkr7r1epgWKshfr_distsHx3kpDXEmhaORIWp7kIhtbek8JbZCcr0ifq9R1xLtoTmU58q8SjJMgCsg5BdRktraQz-lMSAiEB5NUSGAjVD0oCbslQkJ/s1600/image5.png\" /></a>  <p>Now we\u2019re ready to launch the execution.</p>   <p>Select the Execute button, give your execution a name, then add a GPU. Under Environment, select the TensorFlow 2.7 GPU image. This container comes preinstalled with TensorFlow and many other data science libraries.</p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5I4758rM0cTwHQaF3rvHyvLxTNrmFFmiDBEho2mnHINBanIyVGSkle1ByQaPH6oil_sI-bdAbMNRr_BtaQK3AUXOLFJK9D1Lj5b7GgL9qRpEZSKFybARXLhBzYT6UQZaT56_3r46ocYuobhbNwmXiQ45V2capSE2U3TVaeMwO5H4PtPanBaBSBG_L/s1600/image18.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5I4758rM0cTwHQaF3rvHyvLxTNrmFFmiDBEho2mnHINBanIyVGSkle1ByQaPH6oil_sI-bdAbMNRr_BtaQK3AUXOLFJK9D1Lj5b7GgL9qRpEZSKFybARXLhBzYT6UQZaT56_3r46ocYuobhbNwmXiQ45V2capSE2U3TVaeMwO5H4PtPanBaBSBG_L/s1600/image18.png\" /></a>  <p>Then click <strong>SUBMIT</strong>. </p><p>You can track the status of your training job in the <strong>EXECUTIONS</strong> tab. The notebook and the output of each cell will be visible under <strong>VIEW RESULT</strong> when the job finishes and is stored in a GCS bucket. This means you can always tie a model run back to the code that was executed. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF8l2FGcd8AORAsZgsVBh4OYTAbLWNK_69Pv2xGrpkUEbvo5no0-uN0X2bVCauEKL2mQNzWw3PyLbf-gN_k--DkGp29cCzRJBEdvuVKK7zdQH2xpBNBN8pm4z6Quo6kh96EztPtUX0BFdHPEvAgFx_jY13PbBBovtU6_Lv9YNi4kgHkx1Rl2Zk0-xw/s1600/image7.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgF8l2FGcd8AORAsZgsVBh4OYTAbLWNK_69Pv2xGrpkUEbvo5no0-uN0X2bVCauEKL2mQNzWw3PyLbf-gN_k--DkGp29cCzRJBEdvuVKK7zdQH2xpBNBN8pm4z6Quo6kh96EztPtUX0BFdHPEvAgFx_jY13PbBBovtU6_Lv9YNi4kgHkx1Rl2Zk0-xw/s1600/image7.png\" /></a>    <p>When the training completes you\u2019ll be able to see the TensorFlow saved model artifacts in your bucket. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBaxFQRaan-EMhe42E7f5vpmhOjW5OesLHf1DfEs9MWf4UMhW_xA7voHpwgTLxditmg8OWty183UzuS7T1cmVRKEiWsVxy_-n8iXRbmRrvzHX2YZAQbSURSuF5-Tg0ytFNBJgs19ltLr5HLEya_eHS9nSU6RxGua6U_6mdmrF5BZ0RMToR4oIKLJ77/s1600/image2.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBaxFQRaan-EMhe42E7f5vpmhOjW5OesLHf1DfEs9MWf4UMhW_xA7voHpwgTLxditmg8OWty183UzuS7T1cmVRKEiWsVxy_-n8iXRbmRrvzHX2YZAQbSURSuF5-Tg0ytFNBJgs19ltLr5HLEya_eHS9nSU6RxGua6U_6mdmrF5BZ0RMToR4oIKLJ77/s1600/image2.png\" /></a>  <h3>Deploy to endpoint</h3>  <p>Now you know how to quickly launch serverless training jobs on Google Cloud. But ML is not just about training. What\u2019s the point of all this effort if we don\u2019t actually use the model to do something, right? </p><p>Just like with training, we could execute predictions directly from our notebook by calling <code>model.predict</code>. But when we want to get predictions for lots of data, or get low latency predictions on the fly, we\u2019re going to need something more powerful than a notebook. </p><p>Back in your Vertex AI Workbench managed notebook, you can paste the code below in a cell, which will use the Vertex AI Python SDK to deploy the model you just trained to the Vertex AI Prediction service. Deploying the model to an endpoint associates the saved model artifacts with physical resources for low latency predictions. </p><p>First, import the <a href=\"https://googleapis.dev/python/aiplatform/latest/index.html\">Vertex AI Python SDK</a>. </p>   <pre class=\"prettyprint\">from google.cloud import aiplatform<br /></pre>  <p>Then, upload your model to the <a href=\"https://cloud.google.com/vertex-ai/docs/model-registry/introduction\">Vertex AI Model Registry</a>. You\u2019ll need to give your model a name, and provide a serving container image, which is the environment where your predictions will run. Vertex AI provides <a href=\"https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\">pre-built containers</a> for serving, and in this example we\u2019re using the TensorFlow 2.8 image.  </p><p> <p>You\u2019ll also need to replace <code>artifact_uri</code> with the path to the bucket where you stored your saved model artifacts. For me, that was \u201cnikita-flower-demo-bucket\u201d. You\u2019ll also need to replace <code>project </code>with your project ID. </p>   <pre class=\"prettyprint\">my_model = aiplatform.Model.upload(display_name='flower-model',<br />                                  artifact_uri='gs://{YOUR_BUCKET}',<br />                                  serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest',<br />                                  project={YOUR_PROJECT})</pre> <p>Then deploy the model to an endpoint. I\u2019m using default values for now, but if you\u2019d like to learn more about <a href=\"https://cloud.google.com/vertex-ai/docs/general/deployment\">traffic splitting</a>, and <a href=\"https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\">autoscaling</a>, be sure to check out the docs. Note that if your use case does not require low latency predictions, you don\u2019t need to deploy the model to an endpoint and can use the <a href=\"https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions\">batch prediction feature instead</a>. </p>    <pre class=\"prettyprint\">endpoint = my_model.deploy(<br />     deployed_model_display_name='my-endpoint',<br />     traffic_split={\"0\": 100},<br />     machine_type=\"n1-standard-4\",<br />     accelerator_count=0,<br />     min_replica_count=1,<br />     max_replica_count=1,<br />   )<br /></pre>  <p>Once the deployment has completed, you can see your model and endpoint in the console </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_Ui0U3ZY1f9JrgGtsLgpdyJUZF9hk-ZEPsoZcHI5tkEyNEzx5dAA-Kupme8hMu5cWClhY4C3IJE3lrrWQNPtdySFoUXt2_tJUHC6dl6prVDfTFRnIHm1D3YR_l3WQv6iut8N17Pf8tgPI3YMVy1R7IMhD88_yKGjXu-JKCjfDaHl1j8TmLqrG9x0R/s1600/image4.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi_Ui0U3ZY1f9JrgGtsLgpdyJUZF9hk-ZEPsoZcHI5tkEyNEzx5dAA-Kupme8hMu5cWClhY4C3IJE3lrrWQNPtdySFoUXt2_tJUHC6dl6prVDfTFRnIHm1D3YR_l3WQv6iut8N17Pf8tgPI3YMVy1R7IMhD88_yKGjXu-JKCjfDaHl1j8TmLqrG9x0R/s1600/image4.png\" /></a>   <h3>Get predictions</h3>  <p>Now that this model is deployed to an endpoint, you can hit it like any other REST endpoint. This means you can integrate your model and get predictions into a downstream application. </p><p>For now, let\u2019s just test it out directly within Workbench. </p><p>First, open a new TensorFlow notebook. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqGeHRbU8l4f8ebG6ditY8RYgIJz7CmVb5JFySpyps8ErdwDwze3SXwy7XL5FaINc6cvpe3Qr4EnlpxjcWaCfD1Y64n-vN7khhGnNd2zd3_j3Eqa_qb9csHEHElRjaq-5JN6_7sfvHF4tQzHcC-RwF7TrSNTxMvK4xrikJaP9sN7sbDK8dn3Y5kv-b/s1600/image15.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhqGeHRbU8l4f8ebG6ditY8RYgIJz7CmVb5JFySpyps8ErdwDwze3SXwy7XL5FaINc6cvpe3Qr4EnlpxjcWaCfD1Y64n-vN7khhGnNd2zd3_j3Eqa_qb9csHEHElRjaq-5JN6_7sfvHF4tQzHcC-RwF7TrSNTxMvK4xrikJaP9sN7sbDK8dn3Y5kv-b/s1600/image15.png\" /></a>   <p>In the notebook, import the Vertex AI Python SDK. </p>   <pre class=\"prettyprint\">from google.cloud import aiplatform<br /></pre>  <p>Then, create your endpoint, replacing <code>project_number</code> and <code>endpoint_id</code>. </p>   <pre class=\"prettyprint\">endpoint = aiplatform.Endpoint(<br />    endpoint_name=\"projects/{project_number}/locations/us-central1/endpoints/{endpoint_id}\")<br /></pre>  <p>You can find your endpoint_id in the Endpoints section of the cloud Console. </p>  <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwmebEfdn-7ylHSL4m3u5L6uf2jG3J8cBxGkv1bvKVXkkJIKOovR17otfuXD40mKbYnWoo2pDuZRyvJVnONhsWNUPYdA_a1WmTEjT_mmP3R5g7eVFNFLPJOrVBJaWtDlma7l-nlVhOl8g3Ikhp-CpzWHHVaJ0o5vm_VPXCfXeeOy5ztXrNMHVs9_u4/s1600/image1.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwmebEfdn-7ylHSL4m3u5L6uf2jG3J8cBxGkv1bvKVXkkJIKOovR17otfuXD40mKbYnWoo2pDuZRyvJVnONhsWNUPYdA_a1WmTEjT_mmP3R5g7eVFNFLPJOrVBJaWtDlma7l-nlVhOl8g3Ikhp-CpzWHHVaJ0o5vm_VPXCfXeeOy5ztXrNMHVs9_u4/s1600/image1.png\" /></a>    <p>You can find your Project Number on the home page of the console. Note that this is different from the Project ID. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCzHklYkMfdNGMXUqAdyd4S4vA3MVj6fnDO5TqTtDboT54__tHTUXQ0mPZoby54ml34CMDDH6qD5FJLLAczlZuKm9DP17L4EB-lDgUOBRw1al0yf5p0JbV4jmtZ9pSDNmy5_i0J7nR2N-Sp3lUJ8ZeCGo4TY73ZrN8R5agNLQkGLZAgzjsnusiMxls/s1600/image19.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCzHklYkMfdNGMXUqAdyd4S4vA3MVj6fnDO5TqTtDboT54__tHTUXQ0mPZoby54ml34CMDDH6qD5FJLLAczlZuKm9DP17L4EB-lDgUOBRw1al0yf5p0JbV4jmtZ9pSDNmy5_i0J7nR2N-Sp3lUJ8ZeCGo4TY73ZrN8R5agNLQkGLZAgzjsnusiMxls/s1600/image19.png\" /></a>  <p>When you send a request to an online prediction server, the request is received by an HTTP server. The HTTP server extracts the prediction request from the HTTP request content body. The extracted prediction request is forwarded to the serving function. The basic format for online prediction is a list of data instances. These can be either plain lists of values or members of a JSON object, depending on how you configured your inputs in your training application. </p><p>To test the endpoint, I first uploaded an image of a flower to my workbench instance. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgO_nfBF4Wa-7o472Juo5vpaJn9bA-kEVXSndD4toP4xDdCOudnsOlQ9_VCYpO0PA3zcaHXDvz_zHa4A2H-ObvKboyuCB6OxcMp-HAyD9S5eiovzzAWwv3xfP_f6nkCEUOV1Ilv-tfJdjnCcm1N9SmgiGaFk_bUXkF7DmSOAljgXc4WwrUBL1-3O0a4/s1600/image11.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgO_nfBF4Wa-7o472Juo5vpaJn9bA-kEVXSndD4toP4xDdCOudnsOlQ9_VCYpO0PA3zcaHXDvz_zHa4A2H-ObvKboyuCB6OxcMp-HAyD9S5eiovzzAWwv3xfP_f6nkCEUOV1Ilv-tfJdjnCcm1N9SmgiGaFk_bUXkF7DmSOAljgXc4WwrUBL1-3O0a4/s1600/image11.png\" /></a>  <p>The code below opens and resizes the image with PIL, and converts it into a numpy array.  </p>   <pre class=\"prettyprint\">import numpy as np<br />from PIL import Image<br /><br />IMAGE_PATH = 'test_image.jpg'<br /><br />im = Image.open(IMAGE_PATH)<br />im = im.resize((150, 150))<br /></pre>  <p>Then, we convert our numpy data to type float32 and to a list. We convert to a list because numpy data is not JSON serializable so we can\u2019t send it in the body of our request. Note that we <em>don\u2019t</em> need to scale the data by 255 because that step was included as part of our model architecture using <code>tf.keras.layers.Rescaling(1./255). </code>To avoid having to resizing our image, we could have added<code> tf.keras.layers.Resizing</code> to our model, instead of making it part of the <code>tf.data </code>pipeline. </p>   <pre class=\"prettyprint\"># convert to float32 list<br />x_test = [np.asarray(im).astype(np.float32).tolist()]<br /></pre>  <p>Then, we call call predict </p>   <pre class=\"prettyprint\">endpoint.predict(instances=x_test).predictions<br /></pre>  <p>The result you get is the output of the model, which is a softmax layer with 5 units.  Looks like class at index 2 (tulips) scored the highest. </p>   <pre class=\"prettyprint\">[[0.0, 0.0, 1.0, 0.0, 0.0]]<br /></pre>  <p>Tip: to save costs, be sure to undeploy your endpoint if you\u2019re not planning to use it!  You can undeploy by going to the Endpoints section of the console, selecting the endpoint and then the <strong>Undeploy model form endpoint</strong> option. You can always redeploy in the future if needed. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6NzTUpvH0NXX8GnSuN0fmgnOSKkBUhCVDhZ02_JHwvBbzVz0gKdwVjHS_MT6-UlMpt8tpj0DwSmUOt2IUxbAEgIuCNa24_DISvI274gMiDM6oCIvBI9t1joLh7whUyqHzTFO3Mbzf8nrUMrHcKTx2M7i5D6lfys2HuBfJNTWyqtCScPiHO6qEt9mo/s1600/image12.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6NzTUpvH0NXX8GnSuN0fmgnOSKkBUhCVDhZ02_JHwvBbzVz0gKdwVjHS_MT6-UlMpt8tpj0DwSmUOt2IUxbAEgIuCNa24_DISvI274gMiDM6oCIvBI9t1joLh7whUyqHzTFO3Mbzf8nrUMrHcKTx2M7i5D6lfys2HuBfJNTWyqtCScPiHO6qEt9mo/s1600/image12.png\" /></a>  <p>For more realistic examples, you\u2019ll probably want to directly send the image itself to the endpoint, instead of loading it in NumPy first. If you\u2019d like to see an example, <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_tf_serving_function.ipynb\">check out this notebook.</a></p><h3>What\u2019s Next</h3>  <p>You now know how to get from notebook experimentation to deployment in the cloud. With this framework in mind, I hope you start thinking about how you can build new ML applications with notebooks and Vertex AI. <br /><br />If you\u2019re interested in learning even more about how to use Google Cloud to get your TensorFlow models into production, be sure to register for the upcoming Google Cloud <a href=\"https://cloudonair.withgoogle.com/events/summit-applied-ml-2022?utm_source=cgc-blog&amp;utm_medium=blog&amp;utm_campaign=FY22-Q2-global-EXPMKT14-onlineevent-er-applied-ml-summit-2022-main&amp;utm_content=tensorflow_blog&amp;utm_term=-\">Applied ML Summit. </a> This virtual event is scheduled for 9th June and brings together the world\u2019s leading professional machine learning engineers and data scientists. Connect with other ML engineers and data scientists and discover new ways to speed up experimentation, quickly get into production, scale and manage models, and automate pipelines to deliver impact. <a href=\"https://cloudonair.withgoogle.com/events/summit-applied-ml-2022\">Reserve your seat today!</a></p>",
            "pubdate": "Wed, 25 May 2022 16:04:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                25
            ],
            "email_sent": true
        },
        "New documentation on tensorflow.org": {
            "url": "https://blog.tensorflow.org/2022/06/new-documentation-on-tensorfloworg.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAojM5Hs8D3sh_arXYrTDG6wOCUAJeF3atUeHVLyoHRh5GTBxaSimWiMmRc2-ot6rfY5R7aE-_axl51X9CyCqQDst4FyW55HlZ7MR6SG3qjpET9KaZdOz0hhii5li4l1CtgeoSQ7uGrzXVAJ98ZZ7uBt-PZ71fzCKI3FNKgUuE7iR1-fk8ohnZnzbI/s1600/output_1iFcAD0WF78p_3.png\" style=\"display: none;\" />  <p><em>Posted by the TensorFlow team</em></p><p>     <a name=\"more\"></a><p></p>   <p>As <a href=\"https://blog.tensorflow.org/2022/05/ai-and-machine-learning-io-recap.html\">Google I/O</a> took place, we published a lot of exciting new docs on <a href=\"https://www.tensorflow.org/\">tensorflow.org</a>, including updates to model parallelism and model remediation, <a href=\"https://www.tensorflow.org/lite\">TensorFlow Lite</a>, and the <a href=\"https://github.com/tensorflow/models\">TensorFlow Model Garden</a>. Let's take a look at what new things you can learn about! </p><h4>Counterfactual Logit Pairing</h4>  <p>The Responsible AI team added a new model remediation technique as part of their <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation\">Model Remediation</a> library. The TensorFlow Model Remediation library provides training-time techniques to intervene on the model such as  changing the model itself by introducing or altering model objectives.  Originally, model remediation launched with its first technique, <a href=\"https://blog.tensorflow.org/2020/11/applying-mindiff-to-improve-model.html\">MinDiff</a>, which minimizes the difference in performance between two slices of data. </p><p>New at I/O is <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_overview\">Counterfactual Logit Pairing (CLP)</a>.  This is a technique that seeks to ensure that a model\u2019s prediction doesn\u2019t change when a sensitive attribute referenced in an example is either removed or replaced. For example, in a toxicity classifier, examples such as \"I am a man\" and \"I am a lesbian\" should be equal and not classified as toxic. </p><p>Check out the <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_usage_steps\">basic tutorial</a>, the <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras\">Keras tutorial</a>, and the <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/api_docs/python/model_remediation/counterfactual\">API reference</a>. </p><h4>Model parallelism: DTensor</h4>  <p>DTensor provides a global programming model that allows developers to operate on tensors globally while managing distribution across devices. DTensor distributes the program and tensors according to the sharding directives through a procedure called <em><a href=\"https://en.wikipedia.org/wiki/SPMD\">Single program, multiple data (SPMD)</a> expansion</em>.   </p><p>By decoupling the overall application from sharding directives, DTensor enables running the same application on a single device, multiple devices, or even multiple clients, while preserving its global semantics.  If you remember Mesh TensorFlow from TF1, DTensor can address the same issue that Mesh addressed:  training models that may be larger than a single core. </p><p>With TensorFlow 2.9, we made DTensor, that had been in nightly builds, visible on <a href=\"https://www.tensorflow.org/\">tensorflow.org</a>.  Although DTensor is experimental, you're welcome to try it out. Check out the <a href=\"https://www.tensorflow.org/guide/dtensor_overview\">DTensor Guide</a>, the <a href=\"https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial\">DTensor Keras Tutorial</a>, and the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/dtensor\">API reference</a>. </p><h4>New in TensorFlow Lite</h4>  <p>We made some big changes to the TensorFlow Lite site, including to the getting started docs. </p><h5>Developer Journeys</h5>  <p>First off, we now organize the developer journeys by platform (Android, iOS, and other edge devices) to make it easier to get started with your platform.  Android gained a new <a href=\"https://www.tensorflow.org/lite/android\">learning roadmap</a> and <a href=\"https://www.tensorflow.org/lite/android/quickstart\">quickstart</a>.  We also earlier added a guide to the new beta for <a href=\"https://www.tensorflow.org/lite/android/play_services\">TensorFlow Lite in Google Play services</a>.  These quickstarts include examples in both Kotlin and Java, and upgrade our example code to <a href=\"https://developer.android.com/training/camerax\">CameraX</a>, as recommended by our colleagues in Android developer relations! </p><p>If you want to immediately run an Android sample, one can now be imported directly from Android studio.  When starting a new project, choose: <strong>New Project > Import Sample...</strong> and look for <strong>Artificial Intelligence > TensorFlow Lite in Play Services</strong> image classification example application.  This is the sample that can help you find your mug...or other objects: </p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitL9xenSIzgEVUPOUhPMlczODfKKR08sF-icqlPfB4QmCD5dMaP4aQifb18ZDU5E2PtkuU6yonEbTXVIurMmGCWIJjyw0Upz_UD6rKeBB0tygva8DoOVCKuOSUAnImPbpoz8r114ew1Ejhuk-mFfDZNSZXkbfMWuMF3rgr-qiNIjjEdM2kcUy0Cho6/s1600/app_gif.gif\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitL9xenSIzgEVUPOUhPMlczODfKKR08sF-icqlPfB4QmCD5dMaP4aQifb18ZDU5E2PtkuU6yonEbTXVIurMmGCWIJjyw0Upz_UD6rKeBB0tygva8DoOVCKuOSUAnImPbpoz8r114ew1Ejhuk-mFfDZNSZXkbfMWuMF3rgr-qiNIjjEdM2kcUy0Cho6/s1600/app_gif.gif\" /></a>     <h4>Model Maker</h4>  <p>The <a href=\"https://www.tensorflow.org/lite/guide/model_maker\">TensorFlow Lite Model Maker</a> library simplifies the process of training a TensorFlow Lite model using custom datasets. It uses transfer learning to reduce the amount of training data required and reduce training time, and comes pre-built with seven common tasks including image classification, object detection, and text search. </p><p>We added a new tutorial for <a href=\"https://www.tensorflow.org/lite/tutorials/model_maker_text_searcher\">text search</a>.  This type of model lets you take a text query and search for the most related entries in a text dataset, such as a database of web pages.  On mobile, you might use this for auto reply or semantic document search. </p><p>We also published the full <a href=\"https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker\">Python library reference</a>.   </p><h4>TF Lite model page</h4>  <p>Finding the right model for your use case can sometimes be confusing. We've written <a href=\"https://www.tensorflow.org/lite/models\">more guidance</a> on how to choose the right model for your task, and what to consider to make that decision.You can also find links to models for common use cases. </p><h3>Model Garden: State of the art models ready to go</h3>  <p>The <a href=\"https://www.tensorflow.org/guide/model_garden\">TensorFlow Model Garden</a> provides implementations of many state-of-the-art machine learning (ML) models for vision and natural language processing (NLP), as well as workflow tools to let you quickly configure and run those models on standard datasets.  The Model Garden covers both vision and text tasks, and a flexible training loop library called Orbit. Models come with pre-built configs to train to state-of-the-art, as well as many useful specialized ops.  </p><p>We're just getting started documenting all the great things you can do with the Model Garden.  Your first stops should be the <a href=\"https://www.tensorflow.org/guide/model_garden\">overview</a>,  <a href=\"https://github.com/tensorflow/models/tree/master/official\">lists of available models</a>, and <a href=\"https://www.tensorflow.org/tutorials/images/classification_with_model_garden\">the image classification tutorial</a>.  </p><h3>Other exciting things!</h3>  <p>Don't miss the crown-of-thorns starfish detector!  Find your own COTS on real images from the Great Barrier reef.  See the <a href=\"https://www.youtube.com/watch?v=5OuHe_skk0M\">video</a>, read the <a href=\"https://blog.tensorflow.org/2022/05/Kaggle-Great-Barrier-Reef-ML.html\">blog post</a>, and <a href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/official/projects/cots_detector/crown_of_thorns_starfish_detection_pipeline.ipynb\">try out the model</a> in Colab yourself. </p><p>Also, there is a new tutorial on <a href=\"https://www.tensorflow.org/tutorials/generative/data_compression\">TensorFlow compression</a>, which does lossy compression using neural networks.  This example uses something like an autoencoder to compress and decompress MNIST. </p><p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAojM5Hs8D3sh_arXYrTDG6wOCUAJeF3atUeHVLyoHRh5GTBxaSimWiMmRc2-ot6rfY5R7aE-_axl51X9CyCqQDst4FyW55HlZ7MR6SG3qjpET9KaZdOz0hhii5li4l1CtgeoSQ7uGrzXVAJ98ZZ7uBt-PZ71fzCKI3FNKgUuE7iR1-fk8ohnZnzbI/s1600/output_1iFcAD0WF78p_3.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAojM5Hs8D3sh_arXYrTDG6wOCUAJeF3atUeHVLyoHRh5GTBxaSimWiMmRc2-ot6rfY5R7aE-_axl51X9CyCqQDst4FyW55HlZ7MR6SG3qjpET9KaZdOz0hhii5li4l1CtgeoSQ7uGrzXVAJ98ZZ7uBt-PZ71fzCKI3FNKgUuE7iR1-fk8ohnZnzbI/s1600/output_1iFcAD0WF78p_3.png\" /></a>               </p><p>And, of course, don't miss all the <a href=\"https://blog.tensorflow.org/2022/05/ai-and-machine-learning-io-recap.html\">great I/O</a> talks you can <a href=\"https://www.youtube.com/watch?v=OMFdgzeZGqU&amp;list=PLQY2H8rRoyvyY0AsvPIkb7rg7ogc5dgXW\">watch on YouTube</a>.  Thank you! </p>",
            "pubdate": "Wed, 01 Jun 2022 20:23:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                1
            ],
            "email_sent": true
        },
        "Memory-efficient inference with XNNPack weights cache": {
            "url": "https://blog.tensorflow.org/2022/06/memory-efficient-inference-with-xnnpack.html",
            "description": "<img src=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\" style=\"display: none;\" /> <p><em>Posted by Zhi An Ng and Marat Dukhan, Google</em><p> <a name=\"more\"></a><p></p> <p><a href=\"https://github.com/google/XNNPACK\">XNNPack</a> is the default TensorFlow Lite CPU inference engine for floating-point models, and <a href=\"https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html\">delivers meaningful speedups across mobile, desktop, and Web platforms</a>. One of the optimizations employed in XNNPack is repacking the static weights of the Convolution, Depthwise Convolution, Transposed Convolution, and Fully Connected operators into an internal layout optimized for inference computations. During inference, the repacked weights are accessed in a sequential pattern that is friendly to the processors\u2019 pipelines. </p><p>The inference latency reduction comes at a cost: repacking essentially creates an extra copy of the weights inside XNNPack. When the TensorFlow Lite model is memory-mapped, the operating system eventually releases the original copy of the weights and makes the overhead disappear. However, some use-cases require creating multiple copies of a TensorFlow Lite interpreter, each with its own XNNPack delegate, for the same model. As the XNNPack delegates belonging to different TensorFlow Lite interpreters are unaware of each other, every one of them creates its own copy of repacked weights, and the memory overhead grows linearly with the number of delegate instances. Furthermore, since the original weights in the model are static, the repacked weights in XNNPack are also the same across all instances, making these copies wasteful and unnecessary. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0_1yuRglS2gJE3vbjfqhRa7iFDORvqKjNBDG82MoqnFqMIw6CIF7j39LNNVYInCKRWrWJUmbHajUwncM2ZSfN6X4OJSbRFfdySrGSyte2Z9K-cUzMfvRbqXN20J_ZmZQwMRnHdcFL474AJB_-WGjTYumA2K87uIPQeRN30kgKADAT9gqSAWLxNMEm/s1600/image2.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0_1yuRglS2gJE3vbjfqhRa7iFDORvqKjNBDG82MoqnFqMIw6CIF7j39LNNVYInCKRWrWJUmbHajUwncM2ZSfN6X4OJSbRFfdySrGSyte2Z9K-cUzMfvRbqXN20J_ZmZQwMRnHdcFL474AJB_-WGjTYumA2K87uIPQeRN30kgKADAT9gqSAWLxNMEm/s1600/image2.png\" /></a>   <p>Weights cache is a mechanism that allows multiple instances of the XNNPack delegate accelerating the same model to optimize their memory usage for repacked weights. With a weights cache, all instances use the same underlying repacked weights, resulting in a constant memory usage, no matter how many interpreter instances are created. Moreover, elimination of duplicates due to weights cache may improve performance through increased efficiency of a processor\u2019s cache hierarchy. Note: the weights cache is an opt-in feature available only via the C++ API. </p>    <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNOdfLIzlJZY9Oo8ivGTWLpVNTPiKgDrRkPR3pBo_0pqmpeuyd1Pm-K2F0wRvgzELx-vKyyI-sgYTddVL14L1YyhcJ59hm9dB4DgrlTm0qrOcUsW1fiH5zCXdbFB_NEnX7tVD59LxfibNH702Z59EHY6jTEn-blXwYqlEOqqb6xQwkc3O_lXaCHcJ6/s1600/image3.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNOdfLIzlJZY9Oo8ivGTWLpVNTPiKgDrRkPR3pBo_0pqmpeuyd1Pm-K2F0wRvgzELx-vKyyI-sgYTddVL14L1YyhcJ59hm9dB4DgrlTm0qrOcUsW1fiH5zCXdbFB_NEnX7tVD59LxfibNH702Z59EHY6jTEn-blXwYqlEOqqb6xQwkc3O_lXaCHcJ6/s1600/image3.png\" /></a>   <p>The chart below shows the high water mark memory usage (vertical axis) of creating multiple instances (horizontal axis). It compares the baseline, which does not use weights cache, with using weights cache with soft finalization. The peak memory usage when using weights cache grows much slower with respect to the number of instances created. For this example, using weights cache allows you to double the number of instances created with the same peak memory budget. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXNXKTze0EiLb2O_RS1jkeDy8k5u5ksYVdJYOndYstPacET-lfeNU_9wyxS9UcMqpHUoiFqxePad3MPXXDRHcXDOYbmA5vAya-aGK5OUS_kS5LTQmqvoPF-eeQ4MInpHHLg0Y4EXJbxPTDAwtnJuN_ioOohKdyp8Ngu8dtQVYgjyM0N8sFrn_fTrfE/s1600/image1.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXNXKTze0EiLb2O_RS1jkeDy8k5u5ksYVdJYOndYstPacET-lfeNU_9wyxS9UcMqpHUoiFqxePad3MPXXDRHcXDOYbmA5vAya-aGK5OUS_kS5LTQmqvoPF-eeQ4MInpHHLg0Y4EXJbxPTDAwtnJuN_ioOohKdyp8Ngu8dtQVYgjyM0N8sFrn_fTrfE/s1600/image1.png\" /></a>  <p>The weights cache object is created by the <code>TfLiteXNNPackDelegateWeightsCacheCreate</code> function, and passed to the XNNPack delegate via the delegate options. XNNPack delegate will then use the weights cache to store repacked weights. Importantly, the weights cache must be finalized before any inference invocation. </p>   <pre class=\"prettyprint\">// Example demonstrating how to create and finalize a weights cache.<br />std::unique_ptr&lt;tflite::Interpreter> interpreter;<br />TfLiteXNNPackDelegateWeightsCache* weights_cache =<br />    TfLiteXNNPackDelegateWeightsCacheCreate();<br />TfLiteXNNPackDelegateOptions xnnpack_options =<br />    TfLiteXNNPackDelegateOptionsDefault();<br />xnnpack_options.weights_cache = weights_cache;<br />TfLiteDelegate* delegate =<br />    TfLiteXNNPackDelegateCreate(&amp;xnnpack_options);<br />if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {<br />    // Static weights will be packed and written into weights_cache.<br />}<br />TfLiteXNNPackDelegateWeightsCacheFinalizeHard(weights_cache);<br /><br />// Calls to interpreter->Invoke and interpreter->AllocateTensors must<br />// be made here, between finalization and deletion of the cache.<br />// After the hard finalization any attempts to create a new XNNPack<br />// delegate instance using the same weights cache object will fail.<br /><br />TfLiteXNNPackWeightsCacheDelete(weights_cache);<br /></pre>  <p>There are two ways to finalize a weights cache, and in the example above we use <code>TfLiteXNNPackDelegateWeightsCacheFinalizeHard</code> which performs <em>hard</em> finalization. The <em>hard</em> finalization has the least memory overhead, as it will trim the memory used by the weights cache to the absolute minimum. However, no new delegates can be created with this weights cache object after the hard finalization - the number of XNNPack delegate instances using this cache is fixed in advance. The other kind of finalization is a <em>soft</em> finalization. Soft finalization has higher memory overhead, as it leaves sufficient space in the weights cache for some internal bookkeeping. The advantage of the soft finalization is that the same weights cache can be used to create new XNNPack delegate instances, provided that the delegate instances use exactly the same model. This is useful if the number of delegate instances is not fixed or known beforehand. </p>   <pre class=\"prettyprint\">// Example demonstrating soft finalization and creating multiple<br />// XNNPack delegate instances using the same weights cache.<br />std::unique_ptr&lt;tflite::Interpreter> interpreter;<br />TfLiteXNNPackDelegateWeightsCache* weights_cache =<br />    TfLiteXNNPackDelegateWeightsCacheCreate();<br />TfLiteXNNPackDelegateOptions xnnpack_options =<br />    TfLiteXNNPackDelegateOptionsDefault();<br />xnnpack_options.weights_cache = weights_cache;<br />TfLiteDelegate* delegate =<br />    TfLiteXNNPackDelegateCreate(&amp;xnnpack_options);<br />if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {<br />    // Static weights will be packed and written into weights_cache.<br />}<br />TfLiteXNNPackDelegateWeightsCacheFinalizeSoft(weights_cache);<br /><br />// Calls to interpreter->Invoke and interpreter->AllocateTensors can<br />// be made here, between finalization and deletion of the cache.<br />// Notably, new XNNPack delegate instances using the same cache can<br />// still be created, so long as they are used for the same model.<br /><br />std::unique_ptr&lt;tflite::Interpreter> new_interpreter;<br />TfLiteDelegate* new_delegate =<br />    TfLiteXNNPackDelegateCreate(&amp;xnnpack_options);<br />if (new_interpreter->ModifyGraphWithDelegate(new_delegate) !=<br />    kTfLiteOk)<br />{<br />    // Repacked weights inside of the weights cache will be reused,<br />    // no growth in memory usage<br />}<br /><br />// Calls to new_interpreter->Invoke and<br />// new_interpreter->AllocateTensors can be made here.<br />// More interpreters with XNNPack delegates can be created as needed.<br /><br />TfLiteXNNPackWeightsCacheDelete(weights_cache);<br /></pre>  <h3>Next steps</h3>  <p>With the weights cache, using XNNPack for batch inference will reduce memory usage, leading to better performance. Read more about how to use weights cache with XNNPack at the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md\">README</a> and report any issues at <a href=\"https://github.com/google/XNNPACK/issues\">XNNPack's GitHub page</a>. </p><p>To stay up to date, you can read the TensorFlow blog, follow twitter.com/tensorflow, or subscribe to youtube.com/tensorflow. If you\u2019ve built something you\u2019d like to share, please submit it for our Community Spotlight at goo.gle/TFCS. For feedback, please file an issue on GitHub or post to the TensorFlow Forum. Thank you! </p>",
            "pubdate": "Mon, 06 Jun 2022 16:13:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                6
            ],
            "email_sent": true
        },
        "OCR in the browser using TensorFlow.js": {
            "url": "https://blog.tensorflow.org/2022/06/ocr-in-browser-using-tensorflowjs.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUZjFVuG9BSKB8i2rF2n83tL9sVWXegW2Lu7ia6nSt-L8FWcpzEkP2rBFKMcRCkz5vXW7Ma4dvqgB6xL-AWhFP6HCPZ11Kj_rXyMj7KvARsj2T4POi5RlNuMn7XrhleBAww-TeLdoF3uvv0KSt32TqKaJfP8-gkl3HWLioEYI9jZrjsqlfjjWwrecq/s1600/image3.png\" style=\"display: none;\" /> <p><em>A guest post by <a href=\"https://www.linkedin.com/in/charles-gaillard-1738a614a/\">Charles Gaillard</a>, <a href=\"https://mindee.com/\">Mindee</a></em></p> <a name=\"more\"></a><p></p>  <h3>Introduction</h3>  <p>Optical Character Recognition (OCR) refers to technologies capable of capturing text elements from images or documents and converting them into a machine-readable text format. If you want to learn more on that topic, <a href=\"https://blog.mindee.com/ocr-explained/\">this article</a> is a good introduction. </p><p>At <a href=\"https://mindee.com/\">Mindee</a>, we have developed an open-source Python-based OCR called <a href=\"https://github.com/mindee/doctr\">DocTR</a>, however we also wanted to deploy it in the browser to ensure that it was accessible to all developers - especially as <a href=\"https://insights.stackoverflow.com/survey/2021#most-popular-technologies-language-prof\">~70% developers choose to use JavaScript</a>. <br /><br />We managed to achieve this using the <a href=\"https://js.tensorflow.org/api/latest/\">TensorFlow.js API</a>, which resulted in <a href=\"https://demo-doctr-tensorflowjs.mindee.com/\">a web demo</a> that you can now try for yourself using images of your own. </p>   <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMI_POfn1KTijChup8uEzoTHu_f-Iej0qEkVEt-HXSo68zYZbDwK3HpO-y1uVk5Oqd0xzzX6fvR4xsYQ3zRsLAo4JdgbywdDqLCfcJyL_GtNmDT4Vx0jceb34a9BrkCmBa9y6_uKhUwPZBaLLyRYqIaalYRdoSPI4GhK3sHV13-Mp2MDm7g15_gnkw/s1600/image3.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The demo interface with a picture of 2 receipts being parsed by the OCR: 89 words were found here </td></tr></tbody></table> <p>This demo is designed to be very simple to use and run quickly on most computers, therefore we provided a single pretrained model that we trained with a small (512 x 512) input size to save memory. Images are resized to be squares, so it generalizes well to most of the documents which have an aspect ratio close to 1: cards, smaller receipts, tickets, A4, etc. For rectangles with a very high aspect ratio, segmentation results might not be as good because we don\u2019t preserve the aspect ratio (with padding) at the text detection step. It is optimized to work on documents with a significant word size (for example receipts, cards, etc). Keep in mind that these models have been designed to offer performance while running in the browser. Hence, performance might not be optimal on documents that have a very small writing size vs the size of the document or images with a very high aspect ratio. </p><h3>Dive into the architecture</h3>  <p>OCR models can be divided into 2 parts: A detection model and a text recognition model. In DocTR, the detection model is a CNN (convolutional neural network) which segments the input image to find text areas, then text boxes are cropped around each detected word and sent to a recognition model. The second model is a convolutional recurrent neural network (CRNN), which extracts features from word-images and then decodes the sequence of letters on the image with recurrent layers (LSTM). </p>    <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3BTkF3r714C-R0T3mok-qCUgGYjbVTe896k2TGPc11BGFmi0IX2NSvYnXR8YIpo2ipJrmgWQCf_lB1vpjHqnmsrqtnZKY5OmK-rBq0mUEdv5mnn01yqF8WbueGwybfVpCkcTQH5DbW2iPJqyWVPVZGxTNpkEdSPaRpDZ4kSGdqedJCzpe6-ArkCuu/s1600/image2.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Global architecture of the OCR model used in this Demo </td></tr></tbody></table>  <h4>Detection model</h4>  <p>We have different architectures implemented in DocTR, but we chose a very light one for use on the client side as device hardware can change from person to person. Here we used a mobilenetV2 backbone with a <a href=\"https://arxiv.org/pdf/1911.08947.pdf\">DB (Differentiable Binarization) head</a>. The implementation details can be found in the <a href=\"https://github.com/mindee/doctr\">DocTR</a> Github. We trained this model with an input size of (512, 512, 3) to decrease latency and memory usage. We have a private dataset composed of 130,000 annotated documents that was used to train this model. </p><h4>Recognition model</h4>  <p>The recognition model we used is also our lighter architecture: a CRNN (convolutional recurrent neural network) with a mobilenetV2 backbone. More information on this architecture can be found <a href=\"https://arxiv.org/pdf/1507.05717.pdf\">here</a>. It is basically composed of the first half of the mobilenetV2 layers to extract features and it is followed by 2 <a href=\"https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0\">bi-LSTMs</a> to decode visual features as character sequences (words). It uses the <a href=\"https://www.cs.toronto.edu/~graves/icml_2006.pdf\">CTC loss</a>, introduced by Alex Graves, to decode a sequence efficiently. We have an input size of (32, 128, 3) for word images in this model, and we use padding to preserve the aspect ratio of crops. It is trained on our private dataset, composed of 11 millions text boxes extracted from different documents. This dataset has a wide variety of fonts, since it is composed of documents which come from many different data sources. We used data augmentation so that it generalizes well on different fonts, backgrounds, and renderings. It should also give decent results on handwritten text as long as it is human-readable. </p><h3>Model conversion & code implementation</h3>  <p>As our model was originally implemented using TensorFlow, Python conversion was required to run the resulting models in the web browser at scale. To do this we exported a tensorflow <code>SavedModel</code> for each Python model trained and used the <code><a href=\"https://www.tensorflow.org/js/guide/conversion\">tensorflowjs_converter command line tool</a></code> to quickly convert our saved models to the TensorFlow.js JSON format required for execution in the browser.  <p>The resulting converted models were then integrated into our <a href=\"https://reactjs.org\">React.js</a> front end application that powered the user interface of the <a href=\"https://demo-doctr-tensorflowjs.mindee.com/\">demo</a>. More precisely, we used <a href=\"https://mui.com/\">MUI</a> to design the components of the interface for our in-house front-end SDK <a href=\"https://react-mindee-js.netlify.app/\">react-mindee-js</a> (which provides computer vision tools) and <a href=\"https://opencv.org/\">OpenCV.js</a> for the detection model post processing. This post processing step took the raw binarized segmentation map and converted it to a list of polygons with OpenCV.js functions.  We could then crop those boxes from the source image to finally obtain word images ready to be sent to the recognition model. </p><h3>Speed & performance</h3>  <p>We had to manage the tradeoff between speed and performance efficiently. OCR models are quite slow because you have 2 tasks (text areas segmentation + words recognition) that can't be parallelized, so we had to use lightweight models to ensure speedy execution on most devices. <br /><br />On an modern computer with an RTX 2060 and an i7 9th Gen, the detection task takes around 750 milliseconds per image, and the recognition model around 170 milliseconds per batch of 32 crops (words) with the WebGL backend, benchmarked with the <a href=\"https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/local-benchmark/README.md\">TensorFlow.js benchmarking tool</a>. </p><p>Wrapping up the 2 models and the vision operations (detection post processing), the end-to-end OCR runs in less than 2 seconds on small documents (less than 100 words) and the prediction time can only take a few seconds more to run on very dense documents with a lot of words. </p>   <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYe67hhAS-6v1JTtluOekvu72YGChcu0wc3OQlAIFlhSIOZrSWS_vqYMppePEWeuEi5q4xOBLoO3BqSar4PA0dvY9NWCDTY90S0jDhM5CbECshmTlUBjOT8SA_1m4CFxXTM048bbh1FGEa_2eSffkbBEo3GW2pAUdaN0qHcEWeOvH6NRi39_ZDhzZ-/s1600/image1.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYe67hhAS-6v1JTtluOekvu72YGChcu0wc3OQlAIFlhSIOZrSWS_vqYMppePEWeuEi5q4xOBLoO3BqSar4PA0dvY9NWCDTY90S0jDhM5CbECshmTlUBjOT8SA_1m4CFxXTM048bbh1FGEa_2eSffkbBEo3GW2pAUdaN0qHcEWeOvH6NRi39_ZDhzZ-/s1600/image1.png\" /></a>  <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjYe67hhAS-6v1JTtluOekvu72YGChcu0wc3OQlAIFlhSIOZrSWS_vqYMppePEWeuEi5q4xOBLoO3BqSar4PA0dvY9NWCDTY90S0jDhM5CbECshmTlUBjOT8SA_1m4CFxXTM048bbh1FGEa_2eSffkbBEo3GW2pAUdaN0qHcEWeOvH6NRi39_ZDhzZ-/s1600/image1.png\" /></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A screenshot of the demo interface with a very dense old A4 document being parsed by the OCR: 738 words are identified. </td></tr></tbody></table> <h3>Conclusion</h3>  <p><a href=\"https://demo-doctr-tensorflowjs.mindee.com/\">This demo</a> powered by <a href=\"https://www.tensorflow.org/js\">TensorFlow.js</a> is a way to give access to an online, relatively quick and robust document OCR to almost everyone, which is one of the first of its kind powered by TensorFlow.js entirely in the browser.<br /><br />As we are executing the model on the client side, exact performance will vary depending on the hardware of the device it is run on. However the goal here is more to demonstrate that even complex and state-of-the-art deep learning models can be deployed in the browser and run on almost every machine in an efficient manner that can be very useful, especially for potentially sensitive document information, where you do not want to send the document to the cloud for analysis. </p><p>We are excited to offer this solution for all to use, and keen to follow the future of the Web ML industry, where things will no doubt get faster with time as new web standards like <a href=\"https://www.w3.org/TR/webgpu/\">WebGPU</a> become mainstream and enabled by default on modern web browsers. </p>",
            "pubdate": "Tue, 07 Jun 2022 15:44:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                7
            ],
            "email_sent": true
        },
        "Adding Quantization-aware Training and Pruning to the TensorFlow Model Garden": {
            "url": "https://blog.tensorflow.org/2022/06/Adding-Quantization-aware-Training-and-Pruning-to-the-TensorFlow-Model-Garden.html",
            "description": "<p><em>Posted by Jaehong Kim, Rino Lee, and Fan Yang, Software Engineers</em></p><p>   </p><a name=\"more\"></a><p></p> <p>The <a href=\"https://www.tensorflow.org/model_optimization\" target=\"_blank\">TensorFlow model optimization toolkit</a> (TFMOT) provides modern optimization techniques such as quantization aware training (QAT) and pruning. Since the <a href=\"https://blog.tensorflow.org/2018/09/introducing-model-optimization-toolkit.html\" target=\"_blank\">introduction</a> of TFMOT, we have been continuously improving its usability and coverage. Today, we are excited to announce that we are extending the TFMOT model coverage to popular computer vision models in the TensorFlow <a href=\"https://github.com/tensorflow/models\" target=\"_blank\">Model Garden</a>. </p><p>To do so, we added <a href=\"https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/default_8bit\" target=\"_blank\">8-bit QAT API support</a> for subclassed models and custom layers, and <a href=\"https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PrunableLayer#get_prunable_weights\" target=\"_blank\">Pruning API support</a>. You can use these new features in the model garden, and when developing your own models as well. With this, we have showcased applying QAT and pruning to several canonical computer vision models, while accelerating the model development cycle significantly. </p><p>In this article, we will describe the technical challenges we encountered to apply QAT and pruning to the subclass models and custom layers. And show the optimized results to show the benefits from optimization techniques. </p><h3>New support for Model Garden models</h3>  <h4>Quantization</h4>  <p>We have resolved a few technical challenges to support subclassed models and simplified the process of applying QAT API. All the new changes have already been taken care of by TFMOT and Model Garden to save users from knowing all technical details. The final user-facing API to apply QAT on a computer vision model in Model Garden is quite straightforward. By applying <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/configs/experiments/image_classification/imagenet_mobilenetv2_qat_gpu.yaml#L30-L31\" target=\"_blank\">a few configuration changes</a>, you can enable QAT to finetune a pre-trained model and obtain a deployable on-device model in just a few hours. There is minimal to no code change at all. Here we will talk about those challenges and how we addressed them. </p><p>The previous QAT API assumed that the model only contained built-in layers. To support nested functional models, we apply the QAT method to different parts of the model individually. For example, to apply QAT to an image classification model (M) in the Model Garden that consists of two sub modules: the backbone network (B) and the classification head (C). Here B is a nested model within M, and C is a layer. Both B and C only contain built-in layers. Instead of directly quantizing the entire classification model M, we quantize the backbone B and classification head C individually. First, we apply QAT to backbone B only. Then we connect the quantized backbone B to its corresponding classification head C to form a new classification model, and annotate C to be quantized. Finally, we quantize the entire new model, which effectively applies QAT to the annotated classification head C.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiUG8GI1S4QmJU2L4Ei5QG_ByJZD2eZsU86h7yfmsjoRqpXrekX6lDsUHsx5_JiilOLk8RgOBRSDuabwUgFmzx5VXRelmk1HWurMgjTu_hSorImB71bdN-wbMIebYjdC_YSDl7CAKeKoWQIq5E9Km-a7dbbjZ-OgFCFOs6_gF_QA_RFQ-l8w_53JPu/s1600/image1.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiUG8GI1S4QmJU2L4Ei5QG_ByJZD2eZsU86h7yfmsjoRqpXrekX6lDsUHsx5_JiilOLk8RgOBRSDuabwUgFmzx5VXRelmk1HWurMgjTu_hSorImB71bdN-wbMIebYjdC_YSDl7CAKeKoWQIq5E9Km-a7dbbjZ-OgFCFOs6_gF_QA_RFQ-l8w_53JPu/s1600/image1.png\" /></a></div></td></tr></tbody></table> <p>When the backbone network also contains custom layers rather than built-in layers, we add quantized versions of those custom layers first. For example, if the backbone network (B) or the classification head (C) of the classification model (M) also contain a custom layer called <span style=\"color: #38761d; font-family: courier;\">MyLayer</span>, we create its QAT counterpart called <span style=\"color: #38761d; font-family: courier;\">MyLayerQuantized</span> and wrap any built-in layers within it by a quantize wrapper API. We do this recursively if there are any nested custom layers, until all built-in layers are properly wrapped. </p><p>The remaining part after applying quantize is loading the weights from the original model because the QAT-applied model contains more parameters due to additional quantization parameters. Our current solution is variable name filtering. We have added a logic to load the weights from the original model to filtered weight from the QAT-applied model to support fine-tuning from pre-trained models. </p><h4>Pruning</h4>  <p>Along with QAT, we provide two Model garden models with pruning, which is another in-training model optimization technique of MOT. Pruning sparsifies (forces a fixed portion of elements to zero) the given model\u2019s weights during training for computation and storage efficiency.  </p><p>Users can easily <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/pruning/configs/experiments/image_classification/imagenet_mobilenetv2_pruning_gpu.yaml#L31-L38\" target=\"_blank\">set pruning parameters in Model Garden configs</a>. For better pruned model quality, starting pruning from a pre-trained dense model and careful tuning pruning schedule over training steps are well-known techniques. Both are available in Model Garden Pruning configs.  </p><p>This work also provides an example of nested functional layer support in pruning. The way we used here using <span style=\"color: #38761d; font-family: courier;\">get_prunable_weight</span><span style=\"color: #38761d; font-family: courier;\">()</span> is also applicable to any other Keras models with custom layers.  </p><p>With the provided two Model Garden Pruning configs, users can quickly demonstrate pruning to ResNet50 and MobileNetV2 models for image classification. Understanding the practical usage of Pruning API and the pruning process by monitoring tensorboard are also another takeaways of this work. </p><h3>Examples and Results</h3>  <p>We support two tasks, image classification and semantic segmentation. Specifically, for QAT in image classification, we support the common MobileNet family, including <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/configs/image_classification.py#L42\" target=\"_blank\">MobileNetV2</a>, <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/configs/image_classification.py#L42\" target=\"_blank\">MobileNetV3</a> (large), <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/configs/image_classification.py#L42\" target=\"_blank\">Multi-Hardware MobileNet</a> (AVG), and <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/configs/image_classification.py#L31\" target=\"_blank\">ResNet</a> (through quantization on common building blocks such as <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/modeling/layers/nn_blocks.py#L425\" target=\"_blank\">InvertedBottleneckBlockQuantized</a> and <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/modeling/layers/nn_blocks.py#L34\" target=\"_blank\">BottleneckBlockQuantized</a>). For QAT in semantic segmentation, we support MobileNetV2 backbone with <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/modeling/layers/nn_layers.py#L676\" target=\"_blank\">DeepLab V3/V3+</a>. For Pruning in image classification we support <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/pruning/configs/image_classification.py#L72\" target=\"_blank\">MobileNetV2</a> and <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/pruning/configs/image_classification.py#L59\" target=\"_blank\">ResNet</a>. Please refer to the documentations of <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/qat/vision/README.md\" target=\"_blank\">QAT</a> and <a href=\"https://github.com/tensorflow/models/blob/master/official/projects/pruning/README.md\" target=\"_blank\">pruning</a> for more details. </p><h4>Create QAT Models using Model Garden</h4>  <p>Using QAT with Model Garden is simple and straightforward. First, we train a floating point model following the standard process of training models using Model Garden. After training converges, we take the best checkpoint as our starting point to apply QAT, analogous to a finetuning stage. Soon, we will obtain a model that is more quantization friendly. Such model then can be converted to a TFLite model for on-device deployment.   </p><p>For image classification, we evaluate the top-1 accuracy on the ImageNet validation set. As shown in Table 1, QAT model consistently outperforms PTQ model by a large margin, which achieves comparable latency. Notably, on models where PTQ fails to produce reasonable results (MobileNetV3), QAT is still capable of generating a strong quantized model with negligible accuracy drop.  </p><p><em>Table 1. Accuracy and latency comparison of supported models for ImageNet classification. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite  model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.</em></p><div align=\"center\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border-collapse: collapse; border: none;\">        <tbody>            <tr style=\"height: 49.5pt;\">                <td rowspan=\"2\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">model</span></p>                </td>                <td rowspan=\"2\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: xx-small; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">reso-</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: xx-small; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">lution</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\"><br /></td>                <td colspan=\"6\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">TFLite Model</span></p>                </td>            </tr>            <tr style=\"height: 49.5pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 7.5pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 accuracy</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 accuracy (FP32)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 accuracy (PTQ INT8)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 accuracy (QAT INT8)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (FP32, ms/img)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (PTQ</span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">INT8, ms/img)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: black; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (QAT INT8, ms/img)</span></p>                </td>            </tr>            <tr style=\"height: 49.5pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">ResNet50</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">76.7</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">76.7</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">76.4</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">77.2</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">184.01</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">48.73</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">64.49</span></p>                </td>            </tr>            <tr style=\"height: 48.75pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MobileNet V2</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">72.8</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">72.8</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">72.4</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">72.8</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">16.74</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">6.85</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">6.84</span></p>                </td>            </tr>            <tr style=\"height: 48.75pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: white; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MobileNet V3 Large</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.1</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.1</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">34.5*</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">74.4</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">13.32</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">6.43</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">6.85</span></p>                </td>            </tr>            <tr style=\"height: 49.5pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MobileNet Multi-HW AVG</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.3</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.2</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">73.5</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.1</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">20.97</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">7.73</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">7.73</span></p></td></tr></tbody></table></div><p><em>* PTQ fails to quantize MobileNet V3 properly due to hard-swish activation, thus leading to low accuracy.</em></p><p>We have a similar observation on semantic segmentation: PTQ introduces 1.3 mIoU drop, compared to FP32 model, while QAT model minimizes the drop to just 0.7 and maintains comparable latency. On average, we expect QAT will only introduce 0.5 top-1 accuracy drop for image classification and less than 1 mIoU drop for semantic segmentation. </p><p><em>Table 2. Accuracy and latency comparison of a MobileNet v2 + DeepLab v3 on Pascal VOC segmentation. Latency is measured on a Samsung Galaxy S21 using 1-thread CPU. FP32 refers to the unquantized floating point TFLite  model. PTQ INT8 refers to full integer post-training quantization. QAT INT8 refers to the quantized QAT model.</em></p><div align=\"center\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border-collapse: collapse; border: none;\">        <tbody>            <tr style=\"height: 49.5pt;\">                <td rowspan=\"2\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">model</span></p>                </td>                <td rowspan=\"2\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 8pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">reso-</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 8pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">lution</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\"><br /></td>                <td colspan=\"6\" style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">TFLite Model</span></p>                </td>            </tr>            <tr style=\"height: 49.5pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 7pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mIoU</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mIoU (FP32)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mIoU (PTQ</span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">INT8)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">mIoU (QAT INT8)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (FP32, ms/img)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (PTQ</span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">INT8, ms/img)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Latency (QAT INT8, ms/img)</span></p>                </td>            </tr>            <tr style=\"height: 49.5pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MobileNet v2 + DeepLab v3</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">512x512</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.27</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.30</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">73.95</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">74.68</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">136.60</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">60.94</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: middle;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">55.53</span></p></td></tr></tbody></table></div><h4>Pruning Models in Model Garden</h4>  <p>We support ResNet50 and MobileNet V2 for image classification. Pretrained dense models for each task are generated using the Model Garden training configs. The pruned model can be converted to the TFLite model. By simply setting a flag for sparsity in TFLite conversion, one can get a benefit of model size reduction through sparse data format.  </p><p>For image classification, we again evaluate the top-1 accuracy on the ImageNet validation set, as well as the size of converted TFLite models. As sparsity level increases, the model size becomes more compact but accuracy degrades. The accuracy impact in high sparsity is more severe in parameter-efficient models like MobileNetV2.  </p><p><em>Table 3. Accuracy and model size comparison of ResNet-50 and MobileNet v2 for ImageNet classification. Model size is measured by disk usage of saved TFLite models. Dense refers to the unpruned TFLite model, and 50% sparsity refers to the TFLite model with all prunable layers\u2019 weights randomly pruned 50% of their elements. </em></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border-collapse: collapse; border: none;\">        <tbody>            <tr style=\"height: 41.25pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Model</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto, sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10px; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Reso</span><span face=\"Roboto, sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 10px; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">lution</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 Accuracy (Dense)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 Accuracy (50% sparsity)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Top-1 Accuracy (80% sparsity)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">TFLite Model size (Dense)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">TFLite Model size (Mb, 50% sparsity)</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">TFLite Model size (Mb, 80% sparsity)</span></p>                </td>            </tr>            <tr style=\"height: 30pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">MobileNet V2</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 8pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">72.768%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">71.334%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">61.378%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">13.36 Mb</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">9.74 Mb</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">4.00 Mb</span></p>                </td>            </tr>            <tr style=\"height: 30pt;\">                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">ResNet50</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 8pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">224x224</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">76.704%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">76.61%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">75.508%</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">97.44 Mb</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">70.34 Mb</span></p>                </td>                <td style=\"background-color: white; border-color: rgb(32, 33, 36); border-style: solid; overflow: hidden; padding: 3pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Roboto,sans-serif\" style=\"background-color: transparent; color: #202124; font-size: 9pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">28.35 Mb</span></p>                </td>            </tr>        </tbody>    </table></div><h3 style=\"text-align: left;\">Conclusions</h3><p>We have presented an extension to TFMOT that offers QAT and pruning support for computer vision models in Model Garden. We highlight the ease of use and outstanding trade-offs about maintaining accuracy while keeping low latency or small model size. </p><p>While we believe this is a simple and user-friendly solution to enable QAT and pruning, we know this is just the beginning of streamlined works to provide even better usability.  </p><p>Currently, supported tasks are limited to image classification and semantic segmentation. We will continue to add more support to other tasks, such as object detection and instance segmentation. We will also add more models, such as transformer based models, and improve the usability of TFMOT and Model Garden\u2019s API. Thanks for your interest in this work.  </p><h3>Acknowledgements </h3>  <p>We would like to thank everyone who contributed to this work, including Model Garden, Model Optimization, and our collaborators from Research. Special thanks to David Rim (emeritus), Ethan Kim (emeritus) from the Model Optimization team; Abdullah Rashwan, Xianzhi Du, Yeqing Li, Jaeyoun Kim, Jing Li from the Model Garden team; Yuqi Li from the on-device ML team. </p><p></p><p></p><p></p>",
            "pubdate": "Thu, 09 Jun 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "Profiling XNNPACK with TFLite": {
            "url": "https://blog.tensorflow.org/2022/06/Profiling-XNNPACK-with-TFLite.html",
            "description": "<p><em>Posted by Alan Kelly, Software Engineer</em></p><p> </p><a name=\"more\"></a><p></p> <p>We are happy to share that detailed <a href=\"https://www.tensorflow.org/lite/performance/measurement\" target=\"_blank\">profiling information</a> for XNNPACK is now available in TensorFlow 2.9.1 and later. <a href=\"https://github.com/google/XNNPACK\" target=\"_blank\">XNNPACK</a> is a highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 platforms, and it is the default TensorFlow Lite CPU inference engine for floating-point models. </p><p>The most common and expensive neural network operators, such as fully connected layers and convolutions, are executed by XNNPACK so that you get the best performance possible from your model. Historically the profiler would measure the runtime for the entire section of delegated graph, meaning that the runtime of all delegated operators was accumulated in one result, making it difficult to identify the individual operations that were slow.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGaMOZjmmcdeCHArC2JZGpTO3nsFS6Pdv4_mR7Krfqjyw5hbMWoy1TBJkDg5h9P62LPDIXoaPvj8NdwcszzXK_IhsS3Z39jx-q25Ud-Os7ShQkm2YjIhNX0Bn8R3Cfa-hcz_jZXvF_a8W9tpE2PDiX9A5d32qkAgfNpled0X_1DJuxHfoFOOtMdC4b/s1600/image6.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGaMOZjmmcdeCHArC2JZGpTO3nsFS6Pdv4_mR7Krfqjyw5hbMWoy1TBJkDg5h9P62LPDIXoaPvj8NdwcszzXK_IhsS3Z39jx-q25Ud-Os7ShQkm2YjIhNX0Bn8R3Cfa-hcz_jZXvF_a8W9tpE2PDiX9A5d32qkAgfNpled0X_1DJuxHfoFOOtMdC4b/s1600/image6.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\"><i>Previous TFLite profiling results when XNNPACK was used. The runtime of all delegated operators was accumulated in one row.</i></span></td></tr></tbody></table><p></p><p></p><p>If you are using TensorFlow Lite 2.9.1 or later, it gives the per operator profile even for the section that is delegated to XNNPACK so that you no longer need to decide between fast inference and detailed performance information. The operator name, data layout (NHWC for example), datatype (FP32) and microkernel type (if applicable) are shown.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC4hVKqTuz3ZI3BDW-N9XFOthfw0GAmJrCuh0QCjgXglrO69vef8Nzj8-o9NZjF5sW9_mvqg3fQa8PuQO1b14ITkAcMx2cjqfcXKAnU3CpF7L_JE7qyjt8F-SmVXS-Foug7IKB7bYpknoYu1GWhyJqey-ZLL44YtitJyBrGYbYLCLU8p1VyqKjecm_/s1563/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjC4hVKqTuz3ZI3BDW-N9XFOthfw0GAmJrCuh0QCjgXglrO69vef8Nzj8-o9NZjF5sW9_mvqg3fQa8PuQO1b14ITkAcMx2cjqfcXKAnU3CpF7L_JE7qyjt8F-SmVXS-Foug7IKB7bYpknoYu1GWhyJqey-ZLL44YtitJyBrGYbYLCLU8p1VyqKjecm_/s16000/image2.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: start;\"><i>New detailed per-operator profiling information is now shown. The operator name, data layout, data type and microkernel type are visible.</i></span></td></tr></tbody></table><div style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both; text-align: left;\">Now, you get lots of helpful information, such as the runtime per operator and the percentage of the total runtime that it accounts for. The runtime of each node is given in the order in which they were executed. The most expensive operators are also listed.</div></div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjszotPDs2GEy4drnKKcO1gPTCmmEYpf60Yk3iTA2MCogHpXF-pBWUYsH__DIoFkWhJisBBTP6uebX3MAyC0XFthmV5vcGFBndJF0L1EodeESG4tMJ9uY9z0IjotVNySAjcghi40WGRLZOFyneNB2J96pXlMEMXijMxRikoT68yzL1j1jgBMygupjWV/s1600/image3.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjszotPDs2GEy4drnKKcO1gPTCmmEYpf60Yk3iTA2MCogHpXF-pBWUYsH__DIoFkWhJisBBTP6uebX3MAyC0XFthmV5vcGFBndJF0L1EodeESG4tMJ9uY9z0IjotVNySAjcghi40WGRLZOFyneNB2J96pXlMEMXijMxRikoT68yzL1j1jgBMygupjWV/s1600/image3.png\" /></a></div></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\"><i>The most expensive operators are listed. In this example, you can see that a deconvolution accounted for 33.91% of the total runtime.</i></span></td></tr></tbody></table><p></p><p></p><p> XNNPACK can also perform inference in half-precision (16 bit) floating point format if the hardware supports these operations natively, and IEEE16 inference is supported for every floating-point operator in the model, and the model\u2019s `reduced_precision_support` metadata indicates that it is compatible with FP16 inference. FP16 inference can also be forced. More information is available <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#floating-point-ieee-fp16-operators-experimental\" target=\"_blank\">here</a>. If half precision has been used, then F16 will be present in the Name column:</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxQNnx-7gGV730igjxQLFBYhj8EH9B7FXaiRmJ61E2bD5lDfpwJ_6UFC0ViXc_EdjX4bpuxJkSDfhrRuHvu9UB0-GRYsyF9co3aqIpYBDyh2wQVq0_7yKDaFGwSN2om7c18piUo_6SYD5uU6N4J1yzzjiBbeM4u1krWhzTOTkiuHlNCi1NUUSxp3a6/s1600/image5.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxQNnx-7gGV730igjxQLFBYhj8EH9B7FXaiRmJ61E2bD5lDfpwJ_6UFC0ViXc_EdjX4bpuxJkSDfhrRuHvu9UB0-GRYsyF9co3aqIpYBDyh2wQVq0_7yKDaFGwSN2om7c18piUo_6SYD5uU6N4J1yzzjiBbeM4u1krWhzTOTkiuHlNCi1NUUSxp3a6/s1600/image5.png\" /></a></div></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\"><i>FP16 inference has been used.</i></span></td></tr></tbody></table><p></p><p></p><p> Here, unsigned quantized inference has been used (QU8).</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5mSDtQ3FzM_t0tuazFhgIEOdjRhbZG_ReGkegnKKmqfIeEXAzVwS3Xcl-SPFz6EoBFsgx2SlZWWOJ5wMOg-Jra8_hvZjAE_9eAB-3XVgDcv2qOHYDPqMzk7XIynvAA2qMWAEwPWgjwh5uaWpVePcHG4UyxXU7kjPI7VqaLBf0zh5WOlugtT7boDdN/s1600/TF%20Blog.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5mSDtQ3FzM_t0tuazFhgIEOdjRhbZG_ReGkegnKKmqfIeEXAzVwS3Xcl-SPFz6EoBFsgx2SlZWWOJ5wMOg-Jra8_hvZjAE_9eAB-3XVgDcv2qOHYDPqMzk7XIynvAA2qMWAEwPWgjwh5uaWpVePcHG4UyxXU7kjPI7VqaLBf0zh5WOlugtT7boDdN/s1600/TF%20Blog.png\" /></a></div></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\"><i>QU8 indicates that unsigned quantized inference has been used</i></span></td></tr></tbody></table><p></p><p></p><p>  And finally, sparse inference has been used. Sparse operators require that the data layout change from NHWC to NCHW as this is more efficient. This can be seen in the operator name.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJW-1poK1ASuATN0IjayI53Hj6OYeyWvsuQdPOitXVjNbtdJiMpOK6jQVAK5QBTyeoCvsiZqMG93SrA9NZNyk2hVHEWWgF_7PILtAIdgBIAni-UWivl07GhDxYqYzMt8hDsO1PbjVMsaBgGIet5oTcbe0kijN9t-QIqtdIYVStmIExc_RaSZoOCng1/s1600/image1.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhJW-1poK1ASuATN0IjayI53Hj6OYeyWvsuQdPOitXVjNbtdJiMpOK6jQVAK5QBTyeoCvsiZqMG93SrA9NZNyk2hVHEWWgF_7PILtAIdgBIAni-UWivl07GhDxYqYzMt8hDsO1PbjVMsaBgGIet5oTcbe0kijN9t-QIqtdIYVStmIExc_RaSZoOCng1/s1600/image1.png\" /></a></div></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\"><i>SPMM microkernel indicates that the operator is evaluated via SParse matrix-dense Matrix Multiplication. Note that sparse inference use NCHW layout (vs the typical NHWC) for the operators.</i></span></td></tr></tbody></table><p></p><p></p><p>Note that when some operators are delegated to XNNPACK, and others aren\u2019t, two sets of profile information are shown. This happens when not all operators in the model are supported by XNNPACK. The next step in this project is to merge profile information from XNNPACK operators and TensorFlow Lite into one profile.</p><h2><strong>Next Steps</strong></h2>  <p>You can learn more about performance measurement and profiling in TensorFlow Lite by visiting this <a href=\"https://www.tensorflow.org/lite/performance/measurement\" target=\"_blank\">guide</a>. Thanks for reading!</p><br />",
            "pubdate": "Wed, 15 Jun 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                15
            ],
            "email_sent": true
        },
        "Bringing Machine Learning to every developers toolbox": {
            "url": "https://blog.tensorflow.org/2022/06/ bringing-machine-learning-to-every-developers-toolbox.html",
            "description": "<p><em>Posted by Laurence Moroney and <a href=\"https://twitter.com/random_forests\">Josh Gordon</a> for the TensorFlow team</em></p><p> </p><a name=\"more\"></a><p></p> <p style=\"text-align: left;\">With the release of the recent <a href=\"https://survey.stackoverflow.co/2022/#most-loved-dreaded-and-wanted-misc-tech-want\" target=\"_blank\">Stack Overflow Developer Survey</a>, we\u2019re delighted to see the growth of TensorFlow as <a href=\"https://survey.stackoverflow.co/2022/#most-popular-technologies-misc-tech\" target=\"_blank\">the most-used ML tool</a>, being adopted by 3 million software developers to enhance their products and solutions using Machine Learning. And we\u2019re only getting started \u2013 the survey showed that TensorFlow was the <a href=\"https://survey.stackoverflow.co/2022/#most-loved-dreaded-and-wanted-misc-tech-want\" target=\"_blank\">most wanted</a> framework amongst developers, with an estimated 4 million developers wanting to adopt it in the near future.</p><p style=\"text-align: left;\"></p>TensorFlow is now being downloaded over 18M times per month and has amassed 166k stars on GitHub \u2013 more than any other ML framework. Within Google, it powers virtually all AI production workflows, including Search, Ads, YouTube, GMail, Maps, Play, Photos, and many more. It also powers production systems at many of the largest companies in the world \u2013 Apple, Netflix, Stripe, Tencent, Uber, Roche, LinkedIn, Twitter, Baidu, Orange, LVMH, and countless others. And every month, over 3,000 new scientific publications that mention TensorFlow or Keras are being indexed by Google Scholar, including important applied science like the <a href=\"https://candle.cels.anl.gov/\">CANDLE</a> research into understanding cancer.<div><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglKMff39bQMNWFnXhr8C3F8ICwnNyKup_rQBU6X5v9fAtlyfFQ8GdltFJ1yvkx-alW7fRBwUrQIBPWNYQwRnQt6DmA-GHJAIDIPmLrqUZuMtZHGqiTklYbKPCzFkYGKUi0vqJDSsZH4xYqL2g4uPYsCZhk4JQuepVbMV60Q93VeHiP38ELbOEKgAfh/s1080/TF%20Logo%20(1080%20%C3%97%20400%20px).png\" style=\"margin-left: 1em; margin-right: 1em; text-align: center;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglKMff39bQMNWFnXhr8C3F8ICwnNyKup_rQBU6X5v9fAtlyfFQ8GdltFJ1yvkx-alW7fRBwUrQIBPWNYQwRnQt6DmA-GHJAIDIPmLrqUZuMtZHGqiTklYbKPCzFkYGKUi0vqJDSsZH4xYqL2g4uPYsCZhk4JQuepVbMV60Q93VeHiP38ELbOEKgAfh/s16000/TF%20Logo%20(1080%20%C3%97%20400%20px).png\" /></a>We continue to grow the family of products and open source services that make up the Google AI/ML ecosystem. In recent years, we learned that a single universal framework could not work for all scenarios \u2013 in particular, the needs of production and cutting edge research are often in conflict. So we created JAX, a minimalistic API for distributed numerical computing to power the next era of scientific computing research. JAX is excellent for pushing new frontiers: reaching new scales of parallelism, advancing new algorithms and architectures, and developing new compilers and systems. The adoption of JAX by researchers has been exciting, and advances such as <a href=\"https://www.deepmind.com/research/highlighted-research/alphafold\" target=\"_blank\">AlphaFold</a> and <a href=\"https://imagen.research.google/\" target=\"_blank\">Imagen</a> underscore this. <p>In this new multi-framework world, TensorFlow is our answer to the needs of applied ML developers \u2013 engineers who need to build and deploy reliable, stable, performant ML systems, at any scale, and for any platform.&nbsp;Our vision is to create a cohesive ecosystem where researchers and engineers can leverage components that work together regardless of the framework where they originated. We've already made strides towards JAX and TensorFlow interoperability, in particular via <a href=\"https://github.com/google/jax/tree/main/jax/experimental/jax2tf\">jax2tf</a>. Researchers who develop JAX models will be able to bring them to production via the tools of the TensorFlow platform.</p><p>Going forward, we intend to continue to develop TensorFlow as the best-in-class platform for applied ML, side-by-side with JAX to push the boundaries of ML research. We will continue to invest in both ML frameworks to drive forward research and applications for our millions of users. </p><p>There\u2019s lots of great stuff baking that we can\u2019t wait to share with you, so watch this blog for more details! </p><p>PS: Interested in working on any of our AI and ML frameworks? We're <a href=\"https://careers.google.com/\" target=\"_blank\">hiring</a>.  </p></div>",
            "pubdate": "Mon, 27 Jun 2022 19:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                27
            ],
            "email_sent": true
        },
        "How Roboflow enables thousands of developers to use computer vision with TensorFlow.js": {
            "url": "https://blog.tensorflow.org/2022/07/how-roboflow-enables-thousands-of-developers-to-use-computer-vision-with-TensorFlow.js.html",
            "description": "<p><em>A guest post by <a href=\"https://twitter.com/braddwyer\" target=\"_blank\">Brad Dwyer</a>, co-founder and CTO, Roboflow</em></p><p> </p><a name=\"more\"></a><p></p> <p><a href=\"https://roboflow.com\" target=\"_blank\">Roboflow</a> lets developers build their own computer vision applications, from data preparation and model training to deployment and active learning. Through building <a href=\"https://twitter.com/braddwyer/status/910030265006923776\" target=\"_blank\">our own applications</a>, we learned firsthand how tedious it can be to train and deploy a computer vision model. That\u2019s why we launched Roboflow in January 2020 \u2013 we believe every developer should have computer vision available in their toolkit. Our mission is to remove any barriers that might prevent them from succeeding. </p><p>Our end-to-end computer vision platform simplifies the process of collecting images, creating datasets, training models, and deploying them to production. Over 100,000 developers build with Roboflow\u2019s tools. TensorFlow.js makes up a core part of<a href=\"https://docs.roboflow.com/inference\" target=\"_blank\"> Roboflow's deployment stack</a> that has<a href=\"https://blog.roboflow.com/computer-vision-datasets-and-apis/\" target=\"_blank\"> now powered over 10,000 projects</a> created by developers around the world. </p><p>As an early design decision, we decided that, in order to provide the best user experience, we needed to be able to run users' models directly in their web browser (along with our API, edge devices, and on-prem) instead of requiring a round-trip to our servers. The three primary concerns that motivated this decision were latency, bandwidth, and cost. </p><p>For example, Roboflow powers<a href=\"https://spelltable.wizards.com/\" target=\"_blank\"> SpellTable</a>'s<a href=\"https://twitter.com/SpellTable/status/1491877698293092352\" target=\"_blank\"> Codex</a> feature which uses a computer vision model to identify <em>Magic: The Gathering</em> cards. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhRIq1GZCW_M4yiPxRQqUfY9TAGkxrRZ_s3peedpES5VyFHkMOK-eVumBaW-pwoE_A3Q9IX_ar4Zp9HzDoQdxX4W4SgCXjgcKYt3BHjmbWyteaS-GegM9ya8OyzUKIouFq9mqxJcXclzwfsNrQMfcnEBN5ScGfIQuSgka_7kJk9IlEeb4cjqCzrPv0d/s1600/Roboflow%20blog%202.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>From <a href=\"https://twitter.com/SpellTable/status/1491877698293092352\" target=\"_blank\">Twitter</a></i></td></tr></tbody></table> <h3><strong>How Roboflow Uses TensorFlow.js</strong></h3>  <p>Whenever a user's model finishes<a href=\"https://docs.roboflow.com/train\" target=\"_blank\"> training on Roboflow's backend</a>, the model is converted and automatically converted to support sevel various deployment targets; one of those targets is TensorFlow.js. While TensorFlow.js is not the only way to <a href=\"https://roboflow.com/deploy\" target=\"_blank\">deploy a computer vision model</a> with Roboflow, some ways TensorFlow.js powers features within Roboflow include: </p><h4><strong>roboflow.js</strong></h4>  <p><a href=\"https://docs.roboflow.com/inference/web-browser\" target=\"_blank\">roboflow.js</a> is a JavaScript SDK developers can use to integrate their trained model into a web app or Node.js app. Check this video for a quick introduction: </p> <div class=\"separator\" style=\"clear: both; text-align: left;\"></div><h4><strong>Inference Server</strong></h4><p><a href=\"https://github.com/roboflow-ai/inference-server\" target=\"_blank\">The Roboflow Inference Server</a> is a cross-platform microservice that enables developers to self-host and serve their model on-prem. (Note: while not all of Roboflow\u2019s inference servers are TFjs-based, it is one supported means of model deployment.) </p><p>The <a href=\"https://github.com/tensorflow/tfjs/tree/master/tfjs-node\" target=\"_blank\">tfjs-node</a> container runs via Docker and is GPU-accelerated on any machine with CUDA and a compatible NVIDIA graphics card, or using a CPU on any Linux, Mac, or Windows device. </p><h4><strong>Preview</strong></h4>  <p>Preview is an <a href=\"https://blog.roboflow.com/introducing-the-roboflow-inference-widget/\" target=\"_blank\">in-browser widget</a> that lets developers seamlessly test their models on images, video, and webcam streams. </p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3fH2YX7U79s6s7RHZJuPlYJcqkc8PB1mGwD5wPlmom2yh3q-3y3vAoaopZ0U-a7RfCAZNCk7KBKt1RDq2refpSuF50oj6vcg6mtEuQP7UwwgodufA2HKB0czwHW1SMSt1uVcsIOH2dfa2Rc6cOqzSR6pmC4YXJW_tHD5LND9j2UszTUYCkn-6kjlH/s1600/Roboflow%20blog%203.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3fH2YX7U79s6s7RHZJuPlYJcqkc8PB1mGwD5wPlmom2yh3q-3y3vAoaopZ0U-a7RfCAZNCk7KBKt1RDq2refpSuF50oj6vcg6mtEuQP7UwwgodufA2HKB0czwHW1SMSt1uVcsIOH2dfa2Rc6cOqzSR6pmC4YXJW_tHD5LND9j2UszTUYCkn-6kjlH/s1600/Roboflow%20blog%203.png\" /></a></div><h4><strong>Label Assist</strong></h4>  <p>Label Assist is a<a href=\"https://roboflow.com/annotate\" target=\"_blank\"> model-assisted image labeling tool</a> that lets developers use their previous model's predictions as the starting point for annotating additional images. </p><p>One way users leverage Label Assist is in-browser predictions: </p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjHPQMr2knMeHlXX-bOFrx6XxXKKr-PzTZKVWqJ6ffj_VcuKY-CKta-3GjuXRQZx9MXAc-SHODMqnaNVRG4FcRRkBLgp2h6ZQ18AdE7x56poCzZaUfiJPN0hTuinPuC8nt8qcYMgNej1gjLwKkTm5oQJ52LOkBY0yr-cVXrN0CM4iXFRRXOVCIrfG/s1600/Roboflow%20blog%201.gif\" style=\"display: block; padding: 1em 0; text-align: center; clear: left; float: left;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjwjHPQMr2knMeHlXX-bOFrx6XxXKKr-PzTZKVWqJ6ffj_VcuKY-CKta-3GjuXRQZx9MXAc-SHODMqnaNVRG4FcRRkBLgp2h6ZQ18AdE7x56poCzZaUfiJPN0hTuinPuC8nt8qcYMgNej1gjLwKkTm5oQJ52LOkBY0yr-cVXrN0CM4iXFRRXOVCIrfG/s1600/Roboflow%20blog%201.gif\" /></a></div><h3><strong>Why We Chose TensorFlow.js</strong></h3>  <p>Once we had decided we needed to run in the browser, TensorFlow.js was a clear choice. </p><p>Because TFJS runs in our users' browsers and on their own compute, we are able to provide ML-powered features to our full user base of over 100,000 developers, including those on<a href=\"https://roboflow.com/pricing\" target=\"_blank\"> our free Public plan</a>. That simply wouldn't be feasible if we had to spin up a fleet of cloud-hosted GPUs. </p><h3><strong>Behind the Scenes</strong></h3>  <p>To implement<a href=\"https://docs.roboflow.com/inference/web-browser\" target=\"_blank\"> roboflow.js</a> with TensorFlow.js was relatively straightforward. </p><p>We had to change a couple of layers in our neural network to<a href=\"https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0\" target=\"_blank\"> ensure all of our ops were supported</a> on the runtimes we wanted to use, integrate the<a href=\"https://www.npmjs.com/package/@tensorflow/tfjs-converter\" target=\"_blank\"> tfjs-converter</a> into our training pipeline, and port our pre-processing and post-processing code to JavaScript from Python. From there, it was smooth sailing. </p><p>Once we'd built roboflow.js for our customers, we utilized it internally to power features like Preview, <a href=\"https://blog.roboflow.com/announcing-label-assist/\" target=\"_blank\">Label Assist</a>, and one implementation of the Inference Server. </p><h3><strong>Try it Out</strong></h3>  <p>The easiest way to try roboflow.js is by using Preview on<a href=\"https://universe.roboflow.com\" target=\"_blank\"> Roboflow Universe</a>, where we host over 7,000 pre-trained models that our users have shared. Any of these models can be readily built into your applications for things like <a href=\"https://universe.roboflow.com/augmented-startups/playing-cards-ow27d/model/2\" target=\"_blank\">seeing playing cards</a>, <a href=\"https://universe.roboflow.com/surfline/surfer-spotting\" target=\"_blank\">counting surfers</a>, <a href=\"https://universe.roboflow.com/augmented-startups/vehicle-registration-plates-trudk\" target=\"_blank\">reading license plates</a>, and <a href=\"https://universe.roboflow.com/explo1-w7h8c/see-sci\" target=\"_blank\">spotting bacteria under microscope</a>, and more. </p><p>On the Deployment tab of<a href=\"https://universe.roboflow.com/search?q=trained%2520model\" target=\"_blank\"> any project with a trained model</a>, you can drop a video or use your webcam to run inference right in your browser. To see a live in-browser example, give this community created <a href=\"https://universe.roboflow.com/joseph-nelson/mask-wearing/model/11\" target=\"_blank\">mask detector</a> a try by clicking the \u201cWebcam\u201d icon: </p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSymprLcTpg7tM481QkcQ_gF3rXdwPyNmXehz3q_jq8dxR1_TE4dTi43ayquf9ngWPVEq7YLHJj61y_6BBN-DNreNMJpXAP9_J61FdZTa6haeL6cE-poQ2EwKBv7prPQByMJLZCxXVu4VMuo3bpkwE3F1qstHVKwVFf0CQjmL8VYixS_4a7nUSqLeE/s1600/Roboflow%20blog%205.gif\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSymprLcTpg7tM481QkcQ_gF3rXdwPyNmXehz3q_jq8dxR1_TE4dTi43ayquf9ngWPVEq7YLHJj61y_6BBN-DNreNMJpXAP9_J61FdZTa6haeL6cE-poQ2EwKBv7prPQByMJLZCxXVu4VMuo3bpkwE3F1qstHVKwVFf0CQjmL8VYixS_4a7nUSqLeE/s1600/Roboflow%20blog%205.gif\" /></a></div><p>To train your own model for a custom use case, you can<a href=\"https://app.roboflow.com\" target=\"_blank\"> create a free Roboflow account</a> to collect and label a dataset, then train and deploy it for use with roboflow.js in a single click. This enables you to use your model wherever you may need.  </p><h4 style=\"text-align: left;\"><strong>About Roboflow</strong></h4><p>Roboflow makes it easy for developers to use computer vision in their applications. Over 100,000 users have built with the company's end-to-end platform for image and video collection, organization, annotation, preprocessing, model training, and model deployment. Roboflow provides the tools for companies to improve their datasets and build more accurate computer vision models faster so their teams can focus on their domain problems without reinventing the wheel on vision infrastructure.  </p><p><a href=\"https://universe.roboflow.com/\" target=\"_blank\">Browse datasets on Roboflow Universe</a></p><p><a href=\"https://docs.roboflow.com/inference/web-browser\" target=\"_blank\">Get started in the Roboflow documentation</a></p><p><a href=\"https://roboflow.com/features\" target=\"_blank\">View all available Roboflow features</a></p><p></p><p></p>",
            "pubdate": "Wed, 27 Jul 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                7,
                27
            ],
            "email_sent": true
        },
        "Load-testing TensorFlow Servings REST Interface": {
            "url": "https://blog.tensorflow.org/2022/07/load-testing-TensorFlow-Servings-REST-interface.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3AfD6AO48MESlPbfK1z8bQQhkLoaYOP03SeyJwPSrcJb575FYz822YrwZd4x7fMA8YDTUuWZ1ESnQYLNWNR9dVW2F6Mp9p2m_5uIHbvoNiyPFQjGj81nWdb4SvWva0XVCMPG-aVvji5GHJnS61c_SBCRzMg1bZ6TCS8y4TOu2Rv3veCubUUj1HUsc/s1600/TF%20Blog%202.png\" style=\"display: none;\" /> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3AfD6AO48MESlPbfK1z8bQQhkLoaYOP03SeyJwPSrcJb575FYz822YrwZd4x7fMA8YDTUuWZ1ESnQYLNWNR9dVW2F6Mp9p2m_5uIHbvoNiyPFQjGj81nWdb4SvWva0XVCMPG-aVvji5GHJnS61c_SBCRzMg1bZ6TCS8y4TOu2Rv3veCubUUj1HUsc/s1600/TF%20Blog%202.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3AfD6AO48MESlPbfK1z8bQQhkLoaYOP03SeyJwPSrcJb575FYz822YrwZd4x7fMA8YDTUuWZ1ESnQYLNWNR9dVW2F6Mp9p2m_5uIHbvoNiyPFQjGj81nWdb4SvWva0XVCMPG-aVvji5GHJnS61c_SBCRzMg1bZ6TCS8y4TOu2Rv3veCubUUj1HUsc/s1600/TF%20Blog%202.png\" /></a> <p><em>Posted by <a href=\"https://github.com/deep-diver\" target=\"_blank\">Chansung Park</a> and <a href=\"https://github.com/sayakpaul\" target=\"_blank\">Sayak Paul</a> (ML-GDEs)</em></p> <a name=\"more\"></a><p></p> <p>In this post, we\u2019ll share the lessons and findings learned from conducting load tests for an image classification model across numerous deployment configurations. These configurations involve REST-based deployments with TensorFlow Serving. In this way, we aim to equip the readers with a holistic understanding of the differences between the configurations. </p><p>This post is less about code and more about the architectural decisions we had to make for performing the deployments. We\u2019ll first provide an overview of our setup including the technical specifications. We\u2019ll also share our commentaries on the design choices we made and their impact.  </p><h2>Technical Setup</h2>  <p><a href=\"https://www.tensorflow.org/tfx/guide/serving\" target=\"_blank\">TensorFlow Serving</a> is feature-rich and has targeted specifications embedded in its designs (more on this later). For <a href=\"https://cloud.google.com/ai-platform/prediction/docs/online-vs-batch-prediction\" target=\"_blank\">online prediction scenarios</a>, the model is usually exposed as some kind of service.  </p><p>To perform our testing we use a <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\">pre-trained ResNet50 model</a> which can classify a variety of images into different categories. We then serve this model in the following way: </p><ul><blockquote> <li><a href=\"https://www.docker.com/\" target=\"_blank\">Docker</a> to containerize the environment.  </li><li><a href=\"https://kubernetes.io/\" target=\"_blank\">Kubernetes</a> to orchestrate a cluster of container nodes for scalability. We use <a href=\"https://cloud.google.com/kubernetes-engine\" target=\"_blank\">Kubernetes Engine</a> (GKE) to manage this.   </li><li><a href=\"https://github.com/features/actions\" target=\"_blank\">GitHub Actions</a> to automatically roll out deployments on GKE.  </li>  </blockquote></ul><p>Our deployment platform (nodes on the Kubernetes Cluster) is CPU-based. We don\u2019t employ GPUs at any stage of our processes. For this purpose, we can build a CPU-optimized TensorFlow Serving image and take advantage of a few other options which can reduce the latency and boost the overall throughput of the system. We will discuss these later in the post.  </p><p>You can find all the code and learn how the deployments were performed in <a href=\"https://github.com/deep-diver/ml-deployment-k8s-tfserving\" target=\"_blank\">this repository</a>. Here, you\u2019ll find example notebooks and detailed setup instructions for playing around with the code. As such, we won\u2019t be discussing the code line by line but rather shed light on the most important parts when necessary. </p><p>Throughout the rest of this post, we\u2019ll discuss the key considerations for the deployment experiments respective to TensorFlow Serving including its motivation, limitations, and our experimental results.  </p><p><em>With the emergence of serverless offerings like <a href=\"https://cloud.google.com/vertex-ai\" target=\"_blank\">Vertex AI</a>, it has never been easier to deploy models and scale them securely and reliably. These services help reduce the time-to-market tremendously and increase overall developer productivity. That said,  there might still be instances where you\u2019d like more granular control over things. This is one of the reasons why we wanted to do these experiments in the first place. </em></p><h2>Considerations</h2>  <p>TensorFlow Serving has its own sets of constraints and design choices that can impact a deployment. In this section, we provide a concise overview of these considerations.  </p><p><strong>Deployment infrastructure:</strong> We chose GKE because Kubernetes is a standard deployment platform when using GCP, and GKE lets us focus on the ML parts without worrying about the infrastructure since it is a fully managed Google Cloud Platform service. Our main interest is in how to deploy models for CPU-based environments, so we have prepared a CPU-optimized TensorFlow Serving image.  </p><p><strong>Trade-off between more or fewer servers:</strong> We started experiments for TensorFlow Serving setups with the simplest possible VMs equipped with 2vCPU and 4GB RAM, then we gradually upgraded the specification up to 8vCPU and 64GB RAM. On the other hand, we decreased the number of nodes in the Kubernetes cluster from 8 to 2 because it is a trade-off between costs to deploy cheaper servers versus fewer expensive servers.  </p><p><strong>Options to benefit multi-core environments:</strong> We wanted to see if high-end VMs can outperform simple VMs with options to take advantage of the multi-core environment even though there are fewer nodes. To this end, we experimented with a different number <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/config/threading/set_inter_op_parallelism_threads\" target=\"_blank\">inter_op_parallelism</a></code> and <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/config/threading/set_intra_op_parallelism_threads\" target=\"_blank\">intra_op_parallelism</a></code> threads for TensorFlow Serving deployment set according to the number of CPU cores.   </p><p><strong>Dynamic batching and other considerations:</strong> Modern ML frameworks such as TensorFlow Serving usually support dynamic batching, initial model warm-up, multiple deployments of multiple versions of different models, and more out of the box. For our purpose of online prediction, we have not tested these features carefully. However, dynamic batching capability is also worth exploring to enhance the performance according to the <a href=\"https://github.com/tensorflow/serving/blob/r2.0/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning\" target=\"_blank\">official document</a>. We have seen that the default batching configuration could reduce the latency a little even though the results of that are not included in this blog post. </p><h2>Experiments</h2>  <p>We have prepared the following environments. In TensorFlow Serving, the number of <code>intra_op_parallelism</code>_<code>threads</code> is set equal to the number of CPU cores while the number of <code>inter_op_parallelism_threads</code> is set from 2 to 8 for experimental purposes as it controls the number of threads to parallelize the execution of independent operations. Below we provide the details on the adjustments we performed on the number of vCPUs, RAM size, and the number of nodes for each Kubernetes cluster. Note that the number of vCPUs and the RAM size are applicable for the cluster nodes individually.  </p><p>The load tests are conducted using <a href=\"https://locust.io/\" target=\"_blank\">Locust</a>. We have run each load test for 5 minutes. The number of requests are controlled by the number of users, and it depends on the circumstances on the client side. We increased the number of users by one every second up to 150 which we found the handled number of requests reaches the plateau, and the requests are spawned every second to understand how TensorFlow Serving behaves. So you can assume that requests/second doesn't reflect the real-world situation where clients try to send requests at any time. </p><p>We experimented with the following node configurations on a Kubernetes cluster. The configurations are read like so: {num_vcpus_per_node}-{ram}_{num_nodes}: </p><ul style=\"text-align: left;\"> <li><strong>2vCPUs, 4GB RAM, 8 Nodes</strong></li><li><strong>4vCPUs, 8GB RAM, 4 Nodes</strong></li><li><strong>8vCPUs, 16GB RAM, 2 Nodes</strong></li><li><strong>8vCPUs, 64GB RAM, 2 Nodes</strong></li><ul></ul> </ul><p>You can find code for experimenting with these different configurations in the above-mentioned repositories. The deployment for each experiment is provisioned through <a href=\"https://kustomize.io/\" target=\"_blank\">Kustomize</a> to overlay the base configurations, and file-based configurations are injected through <a href=\"https://kubernetes.io/docs/concepts/configuration/configmap/\" target=\"_blank\">ConfigMap</a>. </p><h2>Results </h2>  <p>This section presents the results for each of the above configurations and suggests which configuration is the best based on the environments we considered. As per Figure 1, the best configuration and the environmental setup is observed as 2 nodes, 8 <code>intra_op_parallelism_threads</code>, 8 <code>inter_op_parallelism_threads</code>, 8vCPUs, 16GB RAM based on the result. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN73u2NKv-JzaPZ-NaGtIlNOTYrx_JiQeob2CB1lHbfOoLbjyYsi9GRiV76sxZEeX4M5_0HPjZlawduh1hWdbE5vYkXmmABQkteqKR26EoqJVh26A1K31RNXEV2fAjGUC6flIBDRWb7cZr8mkW4Wiqa3f1Y7cwi3Fb_QpUssA4wp2wGjdmY7kW-aRC/s1600/TF%20Blog%201.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgN73u2NKv-JzaPZ-NaGtIlNOTYrx_JiQeob2CB1lHbfOoLbjyYsi9GRiV76sxZEeX4M5_0HPjZlawduh1hWdbE5vYkXmmABQkteqKR26EoqJVh26A1K31RNXEV2fAjGUC6flIBDRWb7cZr8mkW4Wiqa3f1Y7cwi3Fb_QpUssA4wp2wGjdmY7kW-aRC/s1600/TF%20Blog%201.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i><span style=\"text-align: start;\">Figure 1: Comparison between different configurations of TensorFlow Serving (</span><a href=\"https://i.ibb.co/wJ42g2Q/download-2.png\" style=\"text-align: start;\">original</a><span style=\"text-align: start;\">).<br /></span></i></td></tr></tbody></table><p>We have observed the following aspects by picking the best options. </p><ul> <li>TensorFlow Serving is more efficient when deployed on fewer, larger (more CPU and RAM) machines, but the RAM capacity doesn\u2019t have much impact on handling more requests. It is important to find the right number of <code>inter_op_parallelism_threads</code> with experimentation. With a higher number the better performance is not always guaranteed even when the nodes are equipped with high-capacity hardware. </li></ul><p>TensorFlow Serving focuses more on reliability than throughput performance. We believe it sacrifices some throughput performance to achieve reliability, but this is the expected behavior of TensorFlow Serving, as stated in the <a href=\"https://www.tensorflow.org/tfx/serving/performance#objectives\" target=\"_blank\">official document</a>. Even though handling as many requests as possible is important, keeping the server as reliable as possible is also substantially important when dealing with a production system.  </p><p>There is a trade-off between performance and reliability, so you must be careful to choose the right one. However, it seems like the throughput performance of TensorFlow Serving is close enough to <a href=\"https://github.com/sayakpaul/ml-deployment-k8s-fastapi\" target=\"_blank\">results from other frameworks such as FastAPI</a>, and if you want to factor in richer features such as dynamic batching and sharing GPU resources efficiently between models, we believe TensorFlow Serving is the right one to choose. </p><h2>Note on gRPC and TensorFlow Serving</h2>  <p>We are dealing with an image classification model for the deployments, and the input to the model will include images. Hence the size of the request payload can spiral up depending on the image resolution and fidelity. Therefore it\u2019s particularly important to ensure the message transmission is as lightweight as possible. Generally, message transmission is quite a bit faster in gRPC than REST. <a href=\"https://blog.dreamfactory.com/grpc-vs-rest-how-does-grpc-compare-with-traditional-rest-apis\" target=\"_blank\">This post</a> provides a good discussion on the main differences between REST and gRPC APIs. </p><p>TensorFlow Serving can <a href=\"https://www.tensorflow.org/tfx/serving/docker\" target=\"_blank\">serve a model with gRPC</a> seamlessly, but comparing the performance of a gRPC API and REST API is non-trivial. This is why we did not include that in this post. The interested readers can check out <a href=\"https://github.com/deep-diver/ml-deployment-k8s-tfserving\" target=\"_blank\">this repository</a> that follows a similar setup but uses a gRPC server instead.  </p><h2>Costs</h2>  <p>We used the <a href=\"https://cloud.google.com/products/calculator\" target=\"_blank\">GCP cost estimator</a> for this purpose. Pricing for each experiment configuration was assumed to be live for 24 hours per month (which was sufficient for our experiments).  </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table style=\"border-collapse: collapse; border: none; width: 468pt;\">        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Machine Configuration (E2 series)</span></p>                </td>                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Pricing (USD)</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2vCPUs, 4GB RAM, 8 Nodes</span></p>                </td>                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">11.15</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">4vCPUs, 8GB RAM, 4 Nodes</span></p>                </td>                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">11.15</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">8vCPUs, 16GB RAM, 2 Nodes</span></p>                </td>                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">11.15</span></p>                </td>            </tr>            <tr style=\"height: 0pt;\">                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">8vCPUs, 64GB RAM, 2 Nodes</span></p>                </td>                <td style=\"border-bottom: solid #000000 1pt; border-color: rgb(0, 0, 0); border-left: solid #000000 1pt; border-right: solid #000000 1pt; border-style: solid; border-top: solid #000000 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span style=\"background-color: transparent; color: black; font-family: Arial; font-size: 11pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">18.21</span></p>                </td>            </tr>        </tbody>    </table></div><h2>Conclusion</h2>  <p>In this post, we discussed some critical lessons we learned from our experience of load-testing a standard image classification model. We considered the industry-grade framework for exposing the model to the end-users \u2013 TensorFlow Serving. While our setup for performing the load tests may not fully resemble what happens in the wild, we hope that our findings will at least act as a good starting point for the community. Even though the post demonstrated our approaches with an image classification model, the approaches should be fairly task-agnostic.  </p><p>In the interest of brevity, we didn\u2019t do much to push further the efficiency aspects of the model in both the APIs. With modern CPUs, software stack, and OS-level optimizations, it\u2019s possible to improve the latency and throughput of the model. We redirect the interested reader to the following resources that might be relevant: </p><ul> <li><a href=\"https://huggingface.co/blog/bert-cpu-scaling-part-1\" target=\"_blank\">Scaling up BERT-like model Inference on modern CPU - Part 1</a> </li><li><a href=\"https://huggingface.co/blog/bert-cpu-scaling-part-2\" target=\"_blank\">Scaling up BERT-like model Inference on modern CPU - Part 2</a> </li><li><a href=\"https://cloud.google.com/architecture/load-testing-and-monitoring-aiplatform-models\" target=\"_blank\">Load testing and monitoring AI Platform models </a> </li><li><a href=\"https://cloud.google.com/architecture/best-practices-for-ml-performance-cost\" target=\"_blank\">Best practices for performance and cost optimization for machine learning</a></li></ul><h2>Acknowledgements</h2>  <p>We are grateful to the <a href=\"https://developers.google.com/community/experts\" target=\"_blank\">ML Ecosystem team</a> that provided GCP credits for supporting our experiments. We also thank <a href=\"https://www.linkedin.com/in/hanneshapke\" target=\"_blank\">Hannes Hapke</a> and <a href=\"https://www.linkedin.com/in/robert-crowe\" target=\"_blank\">Robert Crowe</a> for providing us with helpful feedback and guidance.  </p>",
            "pubdate": "Thu, 28 Jul 2022 17:33:00 +0000",
            "pubdate_parsed": [
                2022,
                7,
                28
            ],
            "email_sent": true
        },
        "Training tree-based models with TensorFlow in just a few lines of code": {
            "url": "https://blog.tensorflow.org/2022/08/training-tree-based-models-with-TensorFlow.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEit5NPpiHRl9CeAKbAxAc7-BPlZIdAGyWm4xQOICiq7r-JCaURdQn7x6pzjbz4VyM_PSLBnIVkvOvxf8NUkIE8FgkzvEA8-ALh_DCnGcdjlxjG7V-ko3NFbh8qFM-V_9jcNJqIjQrSf49ydz240yGZyLWEVQ0Oj15HLWexHxeROYjpagMISjvh9FQgr/s1600/TF%20Blog%20Social%20Asset.png\" style=\"display: none;\" /> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJKKsbTV_o5m76CtyNsZE51uUXZp8Tu-FvDTk568ZwyWqLO6vXRAWt4bBQvCD30ZEfX7z7VGhB_Nmg2529gnspRA-AHNtgN-BHNUEqru8cE75g0GOgIaUCsb9oZzaqu7jDq_s2wMktzP8U_SOuAJOZNNgqVFey1Dm0fWRPws0lfq7b_5JRfQUTK2Xr/s1600/TF%20Blog%20Header.png\"><img border=\"0\" id=\"imgFull\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJKKsbTV_o5m76CtyNsZE51uUXZp8Tu-FvDTk568ZwyWqLO6vXRAWt4bBQvCD30ZEfX7z7VGhB_Nmg2529gnspRA-AHNtgN-BHNUEqru8cE75g0GOgIaUCsb9oZzaqu7jDq_s2wMktzP8U_SOuAJOZNNgqVFey1Dm0fWRPws0lfq7b_5JRfQUTK2Xr/s1600/TF%20Blog%20Header.png\" /></a>  <p><em>A guest post by Dinko Franceschi, Broad Institute of MIT and Harvard </em></p><p> </p><a name=\"more\"></a><p></p> <p>Kaggle has become the go-to place to practice data science skills and participate in machine learning model-building competitions. This tutorial will provide an easy-to-follow walkthrough of how to get started with a Kaggle notebook using <a href=\"https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\" target=\"_blank\">TensorFlow Decision Forests</a>. It\u2019s a library that allows you to train tree-based models (like random forests and gradient-boosted trees) in TensorFlow. </p><p>Why should you be interested in decision forests? There are roughly two types of Kaggle competitions - and the winning solution (neural networks or decision forests) depends on the kind of data you\u2019re working with.  </p><p>If you\u2019re working with a <em>tabular data problem</em> (these involve training a model to classify data in a spreadsheet which is an extremely common scenario) - the winning solution is often a decision forest. However, if you\u2019re working with a <em>perception problem</em> that involves teaching a computer to see or hear (for example, image classification), the winning model is usually a neural network.  </p><p>Here\u2019s where the good news starts. You can implement a decision forest in TensorFlow with just a few lines of code. This relatively simple model often outperforms a neural network on many Kaggle problems.  </p><p>We will explore the decision forests library with a simple dataset from Kaggle, and we will build our model with <a href=\"https://www.kaggle.com/code/welcome\" target=\"_blank\">Kaggle Kernels</a> which allow you to completely build and train your models online using free cloud compute power - similar to <a href=\"https://colab.research.google.com/\" target=\"_blank\">Colab</a>. The <a href=\"https://www.kaggle.com/elikplim/car-evaluation-data-set\" target=\"_blank\">dataset</a> contains vehicle information such as cost, number of doors, occupancy, and maintenance costs which we will use to assign an evaluation on the car. </p><p>Kaggle Kernels can be accessed through your Kaggle account. If you do not have an account, please begin by <a href=\"http://www.kaggle.com/\" target=\"_blank\">signing up</a>. On the home page, select the \u201cCode\u201d option on the left menu and select \u201cNew Notebook,\u201d which will open a new Kaggle Kernel.  </p><p><br /></p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3sG3tN2tLCgvVQLoeztWlFAMBGfo7xJP84JCBVp-uKBQOxxTYZtdk6OfEluP0i1HTqKHr_xGk9fEJMaflHtGnRrqZI3KF5YR3-9CCgcv5xPMfBFTQj5JwxY0VAW2spwSJoIw0xgTlDpLaEAc9_ir_sRX_cN_Q--dHl3p8oIbVZ9ZdspaC26oG6SpA/s1600/TF%20Kaggle%201.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3sG3tN2tLCgvVQLoeztWlFAMBGfo7xJP84JCBVp-uKBQOxxTYZtdk6OfEluP0i1HTqKHr_xGk9fEJMaflHtGnRrqZI3KF5YR3-9CCgcv5xPMfBFTQj5JwxY0VAW2spwSJoIw0xgTlDpLaEAc9_ir_sRX_cN_Q--dHl3p8oIbVZ9ZdspaC26oG6SpA/s1600/TF%20Kaggle%201.png\" /></a></div><p><br /></p><p>Once we have opened a new notebook from Kaggle Kernels, we download the car evaluation dataset to our environment. Click \u201cAdd data\u201d near the top right corner of your notebook, search for \u201ccar evaluation,\u201d and add the dataset. </p><p><br /></p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQcVF5WVpn2cc70uxeEeBlTRwvJvIFMls1k9SkdnUUSB7atP9YKHAfB3_iv7W4wXHR26-ioVECvGtJCnENpalCPwFx7ZTaurZD1S6dD2gbki9I0_6iXPbmqqbUC0EIeO5CEue3SdqydtWWPbN4JkCGkj8d_MhL7ypW-soZUe4UHJOtox-BcxbPerrE/s1600/TF%20Kaggle%202.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQcVF5WVpn2cc70uxeEeBlTRwvJvIFMls1k9SkdnUUSB7atP9YKHAfB3_iv7W4wXHR26-ioVECvGtJCnENpalCPwFx7ZTaurZD1S6dD2gbki9I0_6iXPbmqqbUC0EIeO5CEue3SdqydtWWPbN4JkCGkj8d_MhL7ypW-soZUe4UHJOtox-BcxbPerrE/s1600/TF%20Kaggle%202.png\" /></a></div><p>Now we are ready to start writing code. Install the TensorFlow Decision Forests library and the necessary imports, as shown below. The code in this blog post has been obtained from the Build, train and evaluate models with the TensorFlow Decision Forests<a href=\"https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\"> tutorial</a> which contains additional examples to look at.  </p><pre><code class=\"\u201dlanguage-python\u201d\"><p>!pip install tensorflow_decision_forests</p><p>import numpy as np<br /></p><p>import pandas <br /></p><p>import tensorflow_decision_forests as tfdf</p></code></pre><p>  </p><p>We will now import the dataset. We should note that the dataset we downloaded did not contain headers, so we will add those first based on the information provided on the Kaggle page for the dataset. It is good practice to inspect your dataset before you start working with it by opening it up in your favorite text or spreadsheet editor.  </p><pre><code class=\"\u201dlanguage-python\u201d\"><p>df = pandas.read_csv(\"../input/car-evaluation-data-set/car_evaluation.csv\")</p><p>col_names =['buying price', 'maintenance price', 'doors', 'persons', 'lug_boot', 'safety', 'class']<br /></p><p>df.columns = col_names<br /></p><p>df.head()<br /></p></code></pre><p></p><p>We must then split the dataset into train and test:</p><pre><code class=\"\u201dlanguage-python\u201d\"><p>def split_dataset(dataset, test_ratio=0.30):</p><p>  test_indices = np.random.rand(len(dataset)) &lt; test_ratio<br /></p><p>  return dataset[~test_indices], dataset[test_indices]<br /></p><p><br /></p><p>train_ds_pd, test_ds_pd = split_dataset(df)</p><p>print(\"{} examples in training, {} examples for testing.\".format(<br /></p><p>    len(train_ds_pd), len(test_ds_pd)))<br /></p></code></pre> <p>And finally we will convert the dataset into tf.data format. This is a high-performance format that is used by TensorFlow to train models more efficiently, and with TensorFlow Decision Forests, you can convert your dataset to this format with one line of code:</p><pre><code class=\"\u201dlanguage-python\u201d\"><p><br /></p><p>train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"class\")<br /></p><p>test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=\"class\")<br /></p></code></pre> <p>Now you can go ahead and train your model right away by executing the following:</p><pre><code class=\"\u201dlanguage-python\u201d\"><p>model = tfdf.keras.RandomForestModel()</p><p>model.fit(train_ds)<br /></p></code></pre> <p>The library has good defaults which are a fine place to start for most problems. For advanced users, there are lots of options to choose from in the API<a href=\"https://www.tensorflow.org/decision_forests\"> doc</a> as random forests are configurable.</p><p>Once you have trained the model, you can see how it will perform on the test data. </p><pre><code class=\"\u201dlanguage-python\u201d\"><p>model.compile(metrics=[\"accuracy\"])</p><p>print(model.evaluate(test_ds))<br /></p></code></pre> <p>In just a few lines of code, you reached an accuracy of &gt;95% on this small dataset! This is a simple dataset, and one might argue that neural networks could also yield impressive results. And they absolutely can (and do), especially when you have very large datasets (think: hundreds of thousands of examples, or more). However, neural networks require more code and are resource intensive as they require significantly more compute power.</p><h3>Easy preprocessing</h3>  <p>Decision forests have another important advantage: there are fewer steps to preprocess the data. Notice in the code above that you were able to pass a dataset with both categorical and numeric values <em>directly</em> to the decision forests. You did not have to do any preprocessing like normalizing numeric values, converting strings to integers, and one-hot encoding them. This has major benefits. It makes decision forests simpler to work with (so you can train a model quickly), and there is less code that can go wrong. </p><p>Below, you will see some important differences between the two techniques.  </p><h3>Easy to interpret</h3>  <p>A significant advantage of decision forests is that they are easy to interpret. While the pipeline for decision trees differs significantly from that of training neural networks, there are major advantages for selecting these models for a given task. This is because feature importance is particularly straightforward to determine with decision forests (ensemble of decision trees). Notably, the TensorFlow Decision Forests library makes it possible to visualize feature importance with its model plotter function. Let\u2019s see below how this works!  </p><pre><code class=\"\u201dlanguage-python\u201d\"><p>tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)</p></code></pre> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhURgHiyrnk91zqNm68kOLoLN8DKTyJK3b_99D_8b2GK73y7z9F4p9yXr4bYVY-EJ9k2tZBjgr7spniFAUghUgrQZJ9fSIOvsnXLCkFdxyfVE79qx4vZWjISyNUOXlArdEWtba7GM2ZiUhk4amwNY9WWP1MQAmeYk958q6zql81TofdNqllf2NlwqoU/s1600/TF%20Kaggle%205.png\" style=\"display: block; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhURgHiyrnk91zqNm68kOLoLN8DKTyJK3b_99D_8b2GK73y7z9F4p9yXr4bYVY-EJ9k2tZBjgr7spniFAUghUgrQZJ9fSIOvsnXLCkFdxyfVE79qx4vZWjISyNUOXlArdEWtba7GM2ZiUhk4amwNY9WWP1MQAmeYk958q6zql81TofdNqllf2NlwqoU/s1600/TF%20Kaggle%205.png\" /></a></div><p>We see in the root of the tree on the left the number of examples (1728) and the corresponding distribution indicated by the different colors. Here our model is looking at the number of persons that the car can fit. The largest section indicated by green stands for 2 persons and the red for 4 persons. Furthermore, as we go down the tree we continue to see how the tree splits and the corresponding number of examples. Based on the condition, examples are branched to one of two paths. Interestingly, from here we can also determine the importance of a feature by examining all of the splits of a given feature and then computing how much this feature lowered the variance.  </p><h3>Decision Trees vs. Neural Networks </h3>  <p>Neural networks undoubtedly have incredible representation learning capabilities. While they are very powerful in this regard, it is important to consider whether they are the right tool for the problem at hand. When working with neural networks, one must think a lot about how they will construct the layers. In contrast, decision forests are ready to go out of the box (of course, advanced users can tune a variety of parameters). </p><p>Prior to even building a neural network layer by layer, in most cases one must perform feature pre-processing. For example, this could include normalizing the features to have mean around 0 and standard deviation of 1 and converting strings to numbers. This initial step can be skipped right away with Tree-based models which natively handle mixed data.  </p><p>As seen in the code above, we were able to obtain results in just a few steps. Once we have our desired metrics, we have to interpret them within the context of our problem. Perhaps one of the most significant strengths of Decision Trees is their interpretability. We see in the code above the diagrams that were outputted. Starting at the root, we can follow the branches and quickly get a good idea of how the model made its decisions. In contrast, neural networks are a \u201cblack box\u201d that can be difficult to interpret and to explain to a non-technical audience. </p><h3>Learning more</h3>  <p>If you\u2019d like to learn more about TensorFlow Decision Forests, the best place to start is with the<a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\"> project homepage</a>. You can also check out this<a href=\"https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\" target=\"_blank\"> previous article</a> for more background. And if you have any questions or feedback, the best place to ask them is on<a href=\"https://discuss.tensorflow.org/\" target=\"_blank\"> https://discuss.tensorflow.org/</a> using the tag \u201ctfdf\u201d. Thanks for reading! </p><p></p><p></p><p></p>",
            "pubdate": "Wed, 03 Aug 2022 19:00:00 +0000",
            "pubdate_parsed": [
                2022,
                8,
                3
            ],
            "email_sent": true
        },
        "Content moderation using machine learning: a dual approach": {
            "url": "https://blog.tensorflow.org/2022/08/content-moderation-using-machine-learning-a-dual-approach.html",
            "description": "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEht0dlvklEzGfGe8X6EHy8fCHtBY4norDtdVs6JJOTEVRVqsU--EBwm6YWxC5c2iRWYdEHKxTnZWI7_3JetB8F3r3K2J3Ep7SMUF_3jlQEmoTuHcCHAz1yRkLUB_8jM2VksCE0w0xXt0ynPcR1ZJHS1LVs7VtJDmDes1AaJZw1tYkGxG_Lxir8lSn0s/s1600/Android-DeepLinksCrashCourse_Pt1_1024x512.png\" style=\"display: none;\" /> <a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzog6lDA2x_ZqZiRfK4-u1PfKvHiSzO4n_En9df6wT3FtJYdCRpGQjAz6Fi6NxMwgLXhM2WROOELVEzDrPXWTe0vyaLdZryWtgTH34fjZpAF_3sfqqK8dVqDrYlBvMop7RUFNjB50ZRLUIGyCmwvFcGEqZoY1tWu2RevJ0bAjAYj9bjD16B4yhYl4z/s1600/tensorflow-content-moderation-using-machine-learning-a-dual-approach-01.png\"><img border=\"0\" height=\"191\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjzog6lDA2x_ZqZiRfK4-u1PfKvHiSzO4n_En9df6wT3FtJYdCRpGQjAz6Fi6NxMwgLXhM2WROOELVEzDrPXWTe0vyaLdZryWtgTH34fjZpAF_3sfqqK8dVqDrYlBvMop7RUFNjB50ZRLUIGyCmwvFcGEqZoY1tWu2RevJ0bAjAYj9bjD16B4yhYl4z/w686-h191/tensorflow-content-moderation-using-machine-learning-a-dual-approach-01.png\" width=\"686\" /></a> <p><em>Posted by Jen Person, Developer Advocate</em></p><p> </p><a name=\"more\"></a><p></p> <h2>Being kind: a perennial problem</h2>  <p>I've often wondered why anonymity drives people to say things that they'd never dare say in person, and it\u2019s unfortunate that comment sections for videos and articles are so often toxic! If you\u2019re interested in content moderation, you can use machine learning to help detect toxic posts which you consider for removal.  </p><h2>ML for web developers</h2>  <p>Machine learning is a powerful tool for all sorts of natural language-processing tasks, including translation, sentiment analysis, and predictive text. But perhaps it feels outside the scope of your work. After all, when you're building a website in JavaScript, you don't have time to collect and validate data, train a model using Python, and then implement some backend in Python on which to run said model. Not that there's anything wrong with Python\u2013it's just that, if you're a web developer, it's probably not your language of choice. </p><p>Fortunately, <a href=\"https://www.tensorflow.org/js\" target=\"_blank\">TensorFlow.js</a> allows you to run your machine learning model on your website in <a href=\"https://jsisweird.com/\" target=\"_blank\">everybody's favorite language</a>: JavaScript. Furthermore, TensorFlow.js offers several <a href=\"https://github.com/tensorflow/tfjs-models\" target=\"_blank\">pre-trained models</a> for common use cases on the web. You can add the power of ML to your website in just a few lines of code! There is even a pre-trained model to help you moderate written content, which is what we're looking at today. </p><h2>The text toxicity classifier ML model</h2>  <p>There is an existing pretrained model that works well for content moderation: the <a href=\"https://github.com/tensorflow/tfjs-models/tree/master/toxicity\" target=\"_blank\">TensorFlow.js text toxicity classifier model</a>. With this model, you can evaluate text on different labels of unwanted content, including identity attacks, insults, and obscenity. You can try out <a href=\"https://storage.googleapis.com/tfjs-models/demos/toxicity/index.html\" target=\"_blank\">the demo</a> to see the classifier in action. I admit that I had a bit of fun testing out what sort of content would be flagged as harmful. For example: </p> <div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIuGhiE2jKW5V6XkKoGVmRQRgivcMN_guzKClgY133bEvkLH94m6L_ETzXupMWWHJWrKUgvygL52BlPZSdMJWAGn2JyzetFVa1D6sdxjAEQ5J3UXuWEiq1a5uNazaAFGoH9euRMkxTXZzUhlbcHpXDoXKke5u5-Gcu1YjP7pRAwNpcs1lmFtCA3eSt/s1600/TF%20Blog%201%20copy.png\" style=\"clear: left; display: block; float: left; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIuGhiE2jKW5V6XkKoGVmRQRgivcMN_guzKClgY133bEvkLH94m6L_ETzXupMWWHJWrKUgvygL52BlPZSdMJWAGn2JyzetFVa1D6sdxjAEQ5J3UXuWEiq1a5uNazaAFGoH9euRMkxTXZzUhlbcHpXDoXKke5u5-Gcu1YjP7pRAwNpcs1lmFtCA3eSt/s1600/TF%20Blog%201%20copy.png\" /></a></div><p>I recommend stopping here and playing around with the text toxicity classifier demo. It's a good idea to see what categories of text the model checks for and determine which ones you would want to filter from your own website. Besides, if you want to know what categories the above quote got flagged for, you'll have to go to the demo to read the headings. </p><p>Once you've hurled sufficient insults at the text toxicity classifier model, come back to this blog post to find out how to use it in your own code. </p><h2>A dual approach</h2>  <p>This started as a single tutorial with client and server-side code, but it got a bit lengthy so I decided to split it up. Separating the tutorials also makes it easier to target the part that interests you if you just want to implement one part. In this post, I cover the implementation steps for <strong>client-side</strong> moderation with TensorFlow.js using a basic website. In part 2, I show how to implement the same model <strong>server-side</strong> using Cloud Functions for Firebase. </p><h2>Client-side moderation</h2>  <p>Moderating content client-side provides a quicker feedback loop for your users, allowing you to stop harmful discourse before it starts. It can also potentially save on backend costs since inappropriate comments don't have to be written to the database, evaluated, and then subsequently removed. </p><h3>Starter code</h3>  <p>I used the <a href=\"https://github.com/firebase/functions-samples/tree/main/text-moderation\" target=\"_blank\">Firebase text moderation example</a> as the foundation of my demo website. It looks like this: </p><div class=\"separator\" style=\"clear: both;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSH4Omn4E0kp2CPMrT7XaHt9TNrkltPFlA-sEJhT8Du8cN9FUI_d2iO45Q4WbgfgV0tQU15y9rKkhcIp0-q4fs2jEQHyjNuWRPc0znnOLKUWp-sHt5Zd0bp8fMdj8hS2Jc3nzUwlPrRI-fg4WhlrzOwEjv-k_7FrF7-SvJ6ZS60CwH8XtikZcYOS3W/s1600/TF%20Blog%202%20copy.png\" style=\"clear: left; display: block; float: left; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjSH4Omn4E0kp2CPMrT7XaHt9TNrkltPFlA-sEJhT8Du8cN9FUI_d2iO45Q4WbgfgV0tQU15y9rKkhcIp0-q4fs2jEQHyjNuWRPc0znnOLKUWp-sHt5Zd0bp8fMdj8hS2Jc3nzUwlPrRI-fg4WhlrzOwEjv-k_7FrF7-SvJ6ZS60CwH8XtikZcYOS3W/s1600/TF%20Blog%202%20copy.png\" /></a></div><p>Keep in mind TensorFlow.js doesn't require Firebase. You can use whatever hosting, database, and backend solutions that work best for your app's needs. I just tend to use Firebase because I'm pretty familiar with it already. And quite frankly, TensorFlow.js and Firebase work well together! The website in the Firebase demo showcases content moderation through a basic guestbook using a server-side content moderation system implemented through a <a href=\"https://firebase.google.com/docs/database/extend-with-functions\" target=\"_blank\">Realtime Database-triggered Cloud Function</a>. Don't worry if this sounds like a lot of jargon. I'll walk you through the specifics of what you need to know to use the TensorFlow.js model in your own code. That being said, if you want to build this specific example I made, it's helpful to take a look at the <a href=\"https://github.com/firebase/functions-samples/tree/main/text-moderation\" target=\"_blank\">Firebase example on GitHub</a>. </p><p>If you're building the example with me, clone the <a href=\"https://github.com/firebase/functions-samples\" target=\"_blank\">Cloud Functions samples</a> repo. Then change to the directory of the text moderation app. </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">cd text</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">moderation</span></span></p>                </td>            </tr>        </tbody>    </table></div><p>This project requires you to have the<a href=\"https://firebase.google.com/docs/cli\" target=\"_blank\"> Firebase CLI</a> installed. If you don't have it, you can install it using the following npm command: </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">npm install&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">g firebase</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">tools</span></span></p>                </td>            </tr>        </tbody>    </table></div><p>Once installed, use the following command to log in: </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: black; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">firebase login</span></p>                </td>            </tr>        </tbody>    </table></div><p>Run this command to connect the app to your Firebase project: </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">firebase&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">use</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">--</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">add</span></span></p>                </td>            </tr>        </tbody>    </table></div><p>From here, you can select your project in the list, connect Firebase to an existing Google Cloud project, or create a new Firebase project. Once the project is configured, use the following command to deploy Realtime Database security rules and Firebase Hosting:</p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">firebase deploy&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">--</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">only database</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">,</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">hosting</span></span></p>                </td>            </tr>        </tbody>    </table></div><p>There is no need to deploy Cloud Functions at this time since we will be changing the sample code entirely. </p><p>Note that the Firebase text moderation sample as written uses the <a href=\"https://firebase.google.com/docs/projects/billing/firebase-pricing-plans#blaze-pricing-plan\" target=\"_blank\">Blaze (pay as you go)</a> plan for Firebase. If you choose to follow this demo including the server-side component, your project might need to be upgraded from Spark to Blaze. If you have a billing account set on your project through Google Cloud, you are already upgraded and good to go! Most importantly, if you're not ready to upgrade your project, then do not deploy the Cloud Functions portion of the sample. You can still use the <strong>client-side moderation without Cloud Functions</strong>. </p><p>To implement client-side moderation in the sample, I added some code to the <code><span style=\"font-family: courier;\">index.html</span></code> and <code><span style=\"font-family: courier;\">main.js</span></code> files in the Firebase text moderation example. There are three main steps to implement when using a TensorFlow.js model: installing the required components, loading the model, and then running the prediction. Let's add the code for each of these steps. </p><h3>Install the scripts</h3>  <p>Add the required TensorFlow.js dependencies. I added the dependencies as script tags in the HTML, but you can <a href=\"https://www.tensorflow.org/js/guide/nodejs\">use Node.js</a> if you use a bundler/transpiler for your web app. </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;!-- &nbsp;index.html --&gt;</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;!-- scripts for TensorFlow.js --&gt;</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #00796b; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;script</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;src</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\"</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #00796b; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&gt;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #00796b; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;/script&gt;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #00796b; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;script</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;src</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\"https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity\"</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #00796b; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&gt;&lt;/script&gt;</span></span></p>                </td>            </tr>        </tbody>    </table></div><h3>Load the model</h3>  <p>Add the following code to load the text toxicity model in the <code><span style=\"font-family: courier;\">Guestbook()</span></code> function. The <code><span style=\"font-family: courier;\">Guestbook()</span></code> function is part of the original Firebase sample. It initializes the <code><span style=\"font-family: courier;\">Guestbook</span></code> components and is called on page load. </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// main.js</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// Initializes the Guestbook.</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">function</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">()</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p><span style=\"font-family: courier;\"><br />                    </span><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #455a64; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// The minimum prediction confidence.</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">const</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;threshold&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #c53929; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0.9</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #455a64; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// Load the model. Users optionally pass in a threshold and an array of</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #455a64; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// labels to include.</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; toxicity</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">load</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">threshold</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">).</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">then</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">model&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=&gt;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; toxicity_model&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;model</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">});</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">//\u2026</span></p>                </td>            </tr>        </tbody>    </table></div><p>The <code><span style=\"font-family: courier;\">threshold</span></code> of the model is the minimum prediction confidence you want to use to set the model's predictions to <code><span style=\"font-family: courier;\">true</span></code> or <code><span style=\"font-family: courier;\">false</span></code>--that is, how confident the model is that the text does or does not contain the given type of toxic content. The scale for the threshold is 0-1.0. In this case, I set the threshold to .9, which means the model will predict <code><span style=\"font-family: courier;\">true</span></code> or <code><span style=\"font-family: courier;\">false</span></code> if it is 90% confident in its findings.  It is up to you to decide what threshold works for your use case. You may even want to try out the <a href=\"https://storage.googleapis.com/tfjs-models/demos/toxicity/index.html\" target=\"_blank\">text toxicity classifier demo</a> with some phrases that could come up on your website to determine how the model handles them. </p><p><code><span style=\"font-family: courier;\">toxicity.load</span></code> loads the model, passing the threshold. Once loaded, it sets <code><span style=\"font-family: courier;\">toxicity_model</span></code> to the <code><span style=\"font-family: courier;\">model</span></code> value. </p><h3>Run the prediction</h3>  <p>Add a <code><span style=\"font-family: courier;\">checkContent</span></code> function that runs the model predictions on messages upon clicking \"Add message\": </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// main.js</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">checkContent&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">function</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">message</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">if</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(!</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">toxicity_model</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; console</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">log</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #0f9d58; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">'no model found'</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">);</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">false</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p><span style=\"font-family: courier;\"><br />                    </span><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">const</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;messages&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">[</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">message</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">];</span></span></p><span style=\"font-family: courier;\"><br />                    </span><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;toxicity_model</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">classify</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">messages</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">).</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">then</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">predictions&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=&gt;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p><span style=\"font-family: courier;\"><br />                    </span><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">let item of predictions</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">let i&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">in</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;item</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">results</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; &nbsp; console</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">log</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">item</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">results</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">[</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">i</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">].</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">match</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">if</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">item</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">results</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">[</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">i</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">].</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">match&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">===</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">true</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; console</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">log</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #0f9d58; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">'toxicity found'</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">);</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">true</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; console</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">log</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #0f9d58; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">'no toxicity found'</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">);</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">false</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">});</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #616161; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></p>                </td>            </tr>        </tbody>    </table></div><p>This function does the following: </p><ol> <li>Verifies that the model load has completed. If <code><span style=\"font-family: courier;\">toxicity_model</span></code> has a value, then the <code><span style=\"font-family: courier;\">load()</span></code> function has finished loading the model.  </li><li>Puts the message into an array called <code><span style=\"font-family: courier;\">messages</span></code>, as an array is the object type that the <code><span style=\"font-family: courier;\">classify</span></code> function accepts.  </li><li>Calls <code><span style=\"font-family: courier;\">classify</span></code> on the <code><span style=\"font-family: courier;\">messages</span></code> array.  </li><li>Iterates through the prediction results. <code><span style=\"font-family: courier;\">predictions</span></code> is an array of objects each representing a different language label. You may want to know about only specific labels rather than iterating through them all. For example, if your use case is a website for hosting the transcripts of rap battles, you probably don't want to detect and remove insults.  </li><li>Checks if the content is a match for that label. if the <code>match</code> value is <code><span style=\"font-family: courier;\">true</span></code>, then the model has detected the given type of unwanted language. If the unwanted language is detected, the function returns true. There's no need to keep checking the rest of the results, since the content has already been deemed inappropriate.  </li><li>If the function iterates through all the results and no label match is set to <code><span style=\"font-family: courier;\">true</span></code>, then the function returns <code><span style=\"font-family: courier;\">false</span></code> \u2013 meaning no undesirable language was found. The match label can also be <code><span style=\"font-family: courier;\">null</span></code>. In that case, its value isn't <code><span style=\"font-family: courier;\">true</span></code>, so it's considered acceptable language. I will talk more about the <code>null</code> option in a future post. </li></ol><p>  Add a call to the <code><span style=\"font-family: courier;\">checkContent</span></code> in the <code><span style=\"font-family: courier;\">saveMessage</span></code> function: </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0.75pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// main.js</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// Saves a new message on the Firebase DB.</span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">prototype</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">saveMessage&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">function</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">e</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; e</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">preventDefault</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">();</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">if</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(!</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">this</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">messageInput</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">value&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">||</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">!</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">this</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">nameInput</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">value</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p><span style=\"font-family: courier;\"><br />                    </span><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">checkContent</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">this</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">messageInput</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">value</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">).</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">then</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">((</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">toxic</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">=&gt;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">if</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">toxic&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">===</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">true</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">{</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #455a64; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// display a message to the user to be kind</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">displaySnackbar</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">();</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #455a64; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// clear the message field</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #3367d6; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Guestbook</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">resetMaterialTextfield</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">(</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">this</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">messageInput</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">);</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #9c27b0; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp;&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p>                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: #455a64; font-family: courier; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">//\u2026</span></p>                </td>            </tr>        </tbody>    </table></div><p>After a couple quick checks for input values, the contents of the message box is passed to the <code><span style=\"font-family: courier;\">checkContent</span></code> function. </p><p>If the content passes this check, the message is written to the Realtime Database. If not, a snack bar displays reminding the message author to be kind. The snack bar isn't anything special, so I'm not going to include the code here. You can see it in the full example code, or <a href=\"https://www.w3schools.com/howto/howto_js_snackbar.asp\" target=\"_blank\">implement a snack bar of your own</a>. </p><h2>Try it out</h2>  <p>If you've been following along in your own code, run this terminal command in your project folder to deploy the website: </p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">    <table>        <tbody>            <tr style=\"height: 0pt;\">                <td style=\"background-color: #fafafa; border-bottom: solid #e0e0e0 1pt; border-color: rgb(224, 224, 224); border-left: solid #e0e0e0 1pt; border-right: solid #e0e0e0 1pt; border-style: solid; border-top: solid #e0e0e0 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\">                    <p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">firebase deploy&nbsp;</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: #616161; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\u2013</span><span face=\"Consolas,sans-serif\" style=\"background-color: transparent; color: black; font-size: 10pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">only hosting</span></span></p>                </td>            </tr>        </tbody>    </table><br /></div><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\">You can view the completed example code&nbsp;<a href=\"https://github.com/jenperson/text-moderation\" target=\"_blank\">here</a>.</div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCudwB-q5bH0md-HHEcQ9ZAvMU5ABBEgQkjQx3rzkvvlpOw8WVA0uP_dmyjyE62W3V4_Hcd4CPhQirXyRwVJJCUJclvTnJb6dhtSHMi4v3T72h5M6UaNDKLnOZSU4xla_6BpuiHEFLL3seD-3Tq2hJRutrxjZJjkaZ5fXqFdie5KN1Saul2E3RWmGS/s1600/TF%20BLog%204.gif\" style=\"clear: left; display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCudwB-q5bH0md-HHEcQ9ZAvMU5ABBEgQkjQx3rzkvvlpOw8WVA0uP_dmyjyE62W3V4_Hcd4CPhQirXyRwVJJCUJclvTnJb6dhtSHMi4v3T72h5M6UaNDKLnOZSU4xla_6BpuiHEFLL3seD-3Tq2hJRutrxjZJjkaZ5fXqFdie5KN1Saul2E3RWmGS/s1600/TF%20BLog%204.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i><span style=\"text-align: start;\">A message that's not acceptable gets rejected<br /></span></i><table cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"float: left;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgghotkpgTBH5yNkpmpM_Ji0BfgqNSsexUQFTjyI4qPRD41gz_akoutZGscIuSewooRS0YcRPFVJik0hZ3fBvozlAChRElmIOgcwZW4z1b60aAKWWppvEIKl6zY1jQHUD89gMXTb9ZKwqt2begqJLSatmfkWOJqHeZbAbZPSeDTXPDUM63LH5pLSmCj/s1713/TF%20Blog%203.gif\" style=\"clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgghotkpgTBH5yNkpmpM_Ji0BfgqNSsexUQFTjyI4qPRD41gz_akoutZGscIuSewooRS0YcRPFVJik0hZ3fBvozlAChRElmIOgcwZW4z1b60aAKWWppvEIKl6zY1jQHUD89gMXTb9ZKwqt2begqJLSatmfkWOJqHeZbAbZPSeDTXPDUM63LH5pLSmCj/s16000/TF%20Blog%203.gif\" title=\"An acceptable message gets published to the guestbook\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: start;\"><i>An acceptable message gets published to the guestbook<br /></i></span></td></tr></tbody></table><br /></td></tr><tr><td class=\"tr-caption\"><div>Verifying that this code was working properly was really uncomfortable. I had to come up with an insult that the model would deem inappropriate, and then keep writing it on the website. From my work computer. I know nobody could actually see it, but still. That was one of the stranger parts of my job, to be sure!</div><div><h2><span style=\"text-align: left;\">Next steps</span></h2></div> </td></tr></tbody></table>Using client-side moderation like this could catch most issues before they occur. But a clever user might open developer tools and try to find a way to write obscenities directly to the database, circumventing the content check. That's where server-side moderation comes in. <br /><br />If you enjoyed this article and would like to learn more about TensorFlow.js, here are some things you can do: <br /><ul style=\"text-align: left;\"><li>Check out the <a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\">TensorFlow.js edX course</a> by Jason Mayes. If you are even remotely interested in using TensorFlow.js, I cannot recommend this enough. It might look like a lot at first, but the course is broken up into easy-to-follow manageable pieces.&nbsp;</li><li>View all the TensorFlow.js pretrained models in the <a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\">TFJS repository on GitHub</a>.&nbsp;</li><li>Play around with <a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\">TensorFlow.js projects on Glitch</a>.&nbsp;</li><li>To see an example of ML image moderation on the web, try out <a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss\">Gant Laborde's NSFW TFJS image checker</a>.</li></ul>",
            "pubdate": "Fri, 19 Aug 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                8,
                19
            ],
            "email_sent": true
        }
    },
    "Machine Learning Mastery Blog": {
        "Image Augmentation with Keras Preprocessing Layers and tf.image": {
            "url": "https://machinelearningmastery.com/image-augmentation-with-keras-preprocessing-layers-and-tf-image/",
            "description": "<p>Last Updated on August 6, 2022 When you work on a machine learning problem related to images, not only do you need to collect some images as training data, but you also need to employ augmentation to create variations in the image. It is especially true for more complex object recognition problems. There are many [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/image-augmentation-with-keras-preprocessing-layers-and-tf-image/\" rel=\"nofollow\">Image Augmentation with Keras Preprocessing Layers and tf.image</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Wed, 20 Jul 2022 02:10:31 +0000",
            "pubdate_parsed": [
                2022,
                7,
                20
            ],
            "email_sent": true
        },
        "Tepper Wants to Nerd Out On Data With You": {
            "url": "https://machinelearningmastery.com/tepper-wants-to-nerd-out-on-data-with-you/",
            "description": "<p>Last Updated on July 28, 2022 Sponsored Post There are many practical reasons why you should choose an online Masters in Business Analytics from the Tepper School of Business at Carnegie Mellon University. We can list facts like: our alumni average $103,000 in starting salary and 84% of our grads secured a promotion or new [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/tepper-wants-to-nerd-out-on-data-with-you/\" rel=\"nofollow\">Tepper Wants to Nerd Out On Data With You</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Wed, 27 Jul 2022 16:25:13 +0000",
            "pubdate_parsed": [
                2022,
                7,
                27
            ],
            "email_sent": true
        },
        "Mastering MLOps: Live Model Deployment & Inference Course with Stefan Krawczyk": {
            "url": "https://machinelearningmastery.com/mastering-mlops-live-model-deployment-inference-course-with-stefan-krawczyk/",
            "description": "<p>Last Updated on July 29, 2022 Sponsored Post AI &#38; Machine Learning now power most product experiences even beyond those of the big technology companies. Today, your models must perform and function correctly to ultimately deliver business value. The cost of deploying a slow or bad model, or not detecting undesirable behavior quickly, could significantly [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/mastering-mlops-live-model-deployment-inference-course-with-stefan-krawczyk/\" rel=\"nofollow\">Mastering MLOps: Live Model Deployment &#038; Inference Course with Stefan Krawczyk</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Thu, 28 Jul 2022 17:11:19 +0000",
            "pubdate_parsed": [
                2022,
                7,
                28
            ],
            "email_sent": true
        },
        "Using Depthwise Separable Convolutions in Tensorflow": {
            "url": "https://machinelearningmastery.com/using-depthwise-separable-convolutions-in-tensorflow/",
            "description": "<p>Last Updated on August 10, 2022 Looking at all of the very large convolutional neural networks such as ResNets, VGGs, and the like, it begs the question on how we can make all of these networks smaller with less parameters while still maintaining the same level of accuracy or even improving generalization of the model [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/using-depthwise-separable-convolutions-in-tensorflow/\" rel=\"nofollow\">Using Depthwise Separable Convolutions in Tensorflow</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Wed, 03 Aug 2022 18:41:58 +0000",
            "pubdate_parsed": [
                2022,
                8,
                3
            ],
            "email_sent": true
        },
        "Difference Between a Batch and an Epoch in a Neural Network": {
            "url": "https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/",
            "description": "<p>Last Updated on August 15, 2022 Stochastic gradient descent is a learning algorithm that has a number of hyperparameters. Two hyperparameters that often confuse beginners are the batch size and number of epochs. They are both integer values and seem to do the same thing. In this post, you will discover the difference between batches [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\" rel=\"nofollow\">Difference Between a Batch and an Epoch in a Neural Network</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Tue, 09 Aug 2022 19:00:43 +0000",
            "pubdate_parsed": [
                2022,
                8,
                9
            ],
            "email_sent": true
        },
        "When to Use MLP, CNN, and RNN Neural Networks": {
            "url": "https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/",
            "description": "<p>Last Updated on August 15, 2022 What neural network is appropriate for your predictive modeling problem? It can be difficult for a beginner to the field of deep learning to know what type of network to use. There are so many types of networks to choose from and new methods being published and discussed every [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/\" rel=\"nofollow\">When to Use MLP, CNN, and RNN Neural Networks</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Thu, 11 Aug 2022 19:00:29 +0000",
            "pubdate_parsed": [
                2022,
                8,
                11
            ],
            "email_sent": true
        },
        "Why Initialize a Neural Network with Random Weights?": {
            "url": "https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/",
            "description": "<p>Last Updated on August 15, 2022 The weights of artificial neural networks must be initialized to small random numbers. This is because this is an expectation of the stochastic optimization algorithm used to train the model, called stochastic gradient descent. To understand this approach to problem solving, you must first understand the role of nondeterministic [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\" rel=\"nofollow\">Why Initialize a Neural Network with Random Weights?</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Sat, 13 Aug 2022 19:00:29 +0000",
            "pubdate_parsed": [
                2022,
                8,
                13
            ],
            "email_sent": true
        },
        "How to Make Predictions with Keras": {
            "url": "https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/",
            "description": "<p>Last Updated on August 23, 2022 Once you choose and fit a final deep learning model in Keras, you can use it to make predictions on new data instances. There is some confusion amongst beginners about how exactly to do this. I often see questions such as: How do I make predictions with my model [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\" rel=\"nofollow\">How to Make Predictions with Keras</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Tue, 16 Aug 2022 19:00:52 +0000",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "How to Calculate Precision, Recall, F1, and More for Deep Learning Models": {
            "url": "https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/",
            "description": "<p>Last Updated on August 23, 2022 Once you fit a deep learning neural network model, you must evaluate its performance on a test dataset. This is critical, as the reported performance allows you to both choose between candidate models and to communicate to stakeholders about how good the model is at solving the problem. The [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\" rel=\"nofollow\">How to Calculate Precision, Recall, F1, and More for Deep Learning Models</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 19:00:54 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Last call: Stefan Krawcyzks Mastering MLOps Live Cohort": {
            "url": "https://machinelearningmastery.com/last-call-stefan-krawcyzks-mastering-mlops-live-cohort/",
            "description": "<p>Last Updated on August 19, 2022 Sponsored Post &#160; This is your last chance to sign up for Stefan Krawczyk&#8217;s exclusive live cohort, starting next week (August 22nd). We already have students enrolled from Apple, Amazon, Spotify, Nubank, Workfusion, Glassdoor, ServiceNow, and more. Stefan Krawczky has spent the last 15+ years working on MLOps at [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/last-call-stefan-krawcyzks-mastering-mlops-live-cohort/\" rel=\"nofollow\">Last call: Stefan Krawcyzk\u2019s &#8216;Mastering MLOps&#8217; Live Cohort</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">Machine Learning Mastery</a>.</p>",
            "pubdate": "Thu, 18 Aug 2022 17:30:04 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Building a Logistic Regression Classifier in PyTorch": {
            "url": "https://machinelearningmastery.com/building-a-logistic-regression-classifier-in-pytorch/",
            "description": "<p>Last Updated on December 30, 2022 Logistic regression is a type of regression that predicts the probability of an event. It is used for classification problems and has many applications in the fields of machine learning, artificial intelligence, and data mining. The formula of logistic regression is to apply a sigmoid function to the output [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/building-a-logistic-regression-classifier-in-pytorch/\" rel=\"nofollow\">Building a Logistic Regression Classifier in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Fri, 30 Dec 2022 09:11:28 +0000",
            "pubdate_parsed": [
                2022,
                12,
                30
            ],
            "email_sent": true
        },
        "Training Logistic Regression with Cross-Entropy Loss in PyTorch": {
            "url": "https://machinelearningmastery.com/training-logistic-regression-with-cross-entropy-loss-in-pytorch/",
            "description": "<p>Last Updated on December 30, 2022 In the previous session of our PyTorch series, we demonstrated how badly initialized weights can impact the accuracy of a classification model when mean square error (MSE) loss is used. We noticed that the model didn&#8217;t converge during training and its accuracy was also significantly reduced. In the following, [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/training-logistic-regression-with-cross-entropy-loss-in-pytorch/\" rel=\"nofollow\">Training Logistic Regression with Cross-Entropy Loss in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Fri, 30 Dec 2022 08:47:11 +0000",
            "pubdate_parsed": [
                2022,
                12,
                30
            ],
            "email_sent": true
        },
        "Initializing Weights for Deep Learning Models": {
            "url": "https://machinelearningmastery.com/initializing-weights-for-deep-learning-models/",
            "description": "<p>Last Updated on December 30, 2022 In order to build a classifier that accurately classifies the data samples and performs well on test data, you need to initialize the weights in a way that the model converges well. Usually we randomized the weights. But when we use mean square error (MSE) as loss for training [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/initializing-weights-for-deep-learning-models/\" rel=\"nofollow\">Initializing Weights for Deep Learning Models</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Fri, 30 Dec 2022 08:24:53 +0000",
            "pubdate_parsed": [
                2022,
                12,
                30
            ],
            "email_sent": true
        },
        "Building a Regression Model in PyTorch": {
            "url": "https://machinelearningmastery.com/building-a-regression-model-in-pytorch/",
            "description": "<p>PyTorch library is for deep learning. Some applications of deep learning models are to solve regression or classification problems. In this post, you will discover how to use PyTorch to develop and evaluate neural network models for regression problems. After completing this post, you will know: How to load data from scikit-learn and adapt it [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/building-a-regression-model-in-pytorch/\" rel=\"nofollow\">Building a Regression Model in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Sun, 05 Feb 2023 13:12:37 +0000",
            "pubdate_parsed": [
                2023,
                2,
                5
            ],
            "email_sent": true
        },
        "How to Grid Search Hyperparameters for PyTorch Models": {
            "url": "https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/",
            "description": "<p>The &#8220;weights&#8221; of a neural network is referred as &#8220;parameters&#8221; in PyTorch code and it is fine-tuned by optimizer during training. On the contrary, hyperparameters are the parameters of a neural network that is fixed by design and not tuned by training. Examples are the number of hidden layers and the choice of activation functions. [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/\" rel=\"nofollow\">How to Grid Search Hyperparameters for PyTorch Models</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Thu, 09 Feb 2023 13:35:17 +0000",
            "pubdate_parsed": [
                2023,
                2,
                9
            ],
            "email_sent": true
        },
        "Using Activation Functions in Deep Learning Models": {
            "url": "https://machinelearningmastery.com/using-activation-functions-in-deep-learning-models/",
            "description": "<p>A deep learning model in its simplest form are layers of perceptrons connected in tandem. Without any activation functions, they are just matrix multiplications with limited power, regardless how many of them. Activation is the magic why neural network can be an approximation to a wide variety of non-linear function. In PyTorch, there are many [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/using-activation-functions-in-deep-learning-models/\" rel=\"nofollow\">Using Activation Functions in Deep Learning Models</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Tue, 14 Feb 2023 13:36:34 +0000",
            "pubdate_parsed": [
                2023,
                2,
                14
            ],
            "email_sent": true
        },
        "Using Dropout Regularization in PyTorch Models": {
            "url": "https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/",
            "description": "<p>Dropout is a simple and powerful regularization technique for neural networks and deep learning models. In this post, you will discover the Dropout regularization technique and how to apply it to your models in PyTorch models. After reading this post, you will know: How the Dropout regularization technique works How to use Dropout on your [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/\" rel=\"nofollow\">Using Dropout Regularization in PyTorch Models</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Sun, 19 Feb 2023 13:23:22 +0000",
            "pubdate_parsed": [
                2023,
                2,
                19
            ],
            "email_sent": true
        },
        "Using Learning Rate Schedule in PyTorch Training": {
            "url": "https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/",
            "description": "<p>Training a neural network or large deep learning model is a difficult optimization task. The classical algorithm to train neural networks is called stochastic gradient descent. It has been well established that you can achieve increased performance and faster training on some problems by using a learning rate that changes during training. In this post, [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training/\" rel=\"nofollow\">Using Learning Rate Schedule in PyTorch Training</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Tue, 21 Feb 2023 13:33:16 +0000",
            "pubdate_parsed": [
                2023,
                2,
                21
            ],
            "email_sent": true
        },
        "Understand Model Behavior During Training by Visualizing Metrics": {
            "url": "https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/",
            "description": "<p>You can learn a lot about neural networks and deep learning models by observing their performance over time during training. For example, if you see the training accuracy went worse with training epochs, you know you have issue with the optimization. Probably your learning rate is too fast. In this post, you will discover how [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/understand-model-behavior-during-training-by-visualizing-metrics/\" rel=\"nofollow\">Understand Model Behavior During Training by Visualizing Metrics</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Sun, 26 Feb 2023 13:19:03 +0000",
            "pubdate_parsed": [
                2023,
                2,
                26
            ],
            "email_sent": true
        },
        "Visualizing a PyTorch Model": {
            "url": "https://machinelearningmastery.com/visualizing-a-pytorch-model/",
            "description": "<p>PyTorch is a deep learning library. You can build very sophisticated deep learning models with PyTorch. However, there are times you want to have a graphical representation of your model architecture. In this post, you will learn: How to save your PyTorch model in an exchange format How to use Netron to create a graphical [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/visualizing-a-pytorch-model/\" rel=\"nofollow\">Visualizing a PyTorch Model</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Thu, 02 Mar 2023 13:25:26 +0000",
            "pubdate_parsed": [
                2023,
                3,
                2
            ],
            "email_sent": true
        },
        "Building a Convolutional Neural Network in PyTorch": {
            "url": "https://machinelearningmastery.com/building-a-convolutional-neural-network-in-pytorch/",
            "description": "<p>Neural networks are built with layers connected to each other. There are many different kind of layers. For image related applications, you can always find convolutional layers. It is a layer with very few parameters but applied over a large sized input. It is powerful because it can preserve the spatial structure of the image. [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/building-a-convolutional-neural-network-in-pytorch/\" rel=\"nofollow\">Building a Convolutional Neural Network in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Sun, 05 Mar 2023 13:06:34 +0000",
            "pubdate_parsed": [
                2023,
                3,
                5
            ],
            "email_sent": true
        },
        "LSTM for Time Series Prediction in PyTorch": {
            "url": "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/",
            "description": "<p>Long Short-Term Memory (LSTM) is a structure that can be used in neural network. It is a type of recurrent neural network (RNN) that expects the input in the form of a sequence of features. It is useful for data such as time series or string of text. In this post, you will learn about [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/\" rel=\"nofollow\">LSTM for Time Series Prediction in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Fri, 10 Mar 2023 01:24:39 +0000",
            "pubdate_parsed": [
                2023,
                3,
                10
            ],
            "email_sent": true
        },
        "Text Generation with LSTM in PyTorch": {
            "url": "https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/",
            "description": "<p>Recurrent neural network can be used for time series prediction. In which, a regression neural network is created. It can also be used as generative model, which usually is a classification neural network model. A generative model is to learn certain pattern from data, such that when it is presented with some prompt, it can [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/\" rel=\"nofollow\">Text Generation with LSTM in PyTorch</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Mon, 13 Mar 2023 01:42:16 +0000",
            "pubdate_parsed": [
                2023,
                3,
                13
            ],
            "email_sent": true
        },
        "Deep Learning with PyTorch (9-Day Mini-Course)": {
            "url": "https://machinelearningmastery.com/deep-learning-with-pytorch-9-day-mini-course/",
            "description": "<p>Last Updated on April 4, 2023 Deep learning is a fascinating field of study and the techniques are achieving world class results in a range of challenging machine learning problems. It can be hard to get started in deep learning.Which library should you use and which techniques should you focus on? In this 9-part crash [&#8230;]</p>\n<p>The post <a href=\"https://machinelearningmastery.com/deep-learning-with-pytorch-9-day-mini-course/\" rel=\"nofollow\">Deep Learning with PyTorch (9-Day Mini-Course)</a> appeared first on <a href=\"https://machinelearningmastery.com\" rel=\"nofollow\">MachineLearningMastery.com</a>.</p>",
            "pubdate": "Tue, 04 Apr 2023 05:28:53 +0000",
            "pubdate_parsed": [
                2023,
                4,
                4
            ],
            "email_sent": true
        }
    },
    "Colah's Blog": {},
    "Amazon Science Blog": {
        "How deep learning is reducing Amazons packaging waste": {
            "url": "https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste",
            "description": "A combination of deep learning, natural language processing, and computer vision enables Amazon to hone in on the right amount of packaging for each product.",
            "pubdate": "Tue, 04 Jan 2022 13:38:23 GMT",
            "pubdate_parsed": [
                2022,
                1,
                4
            ],
            "email_sent": true
        },
        "Using NLU labels to improve an ASR rescoring model": {
            "url": "https://www.amazon.science/blog/using-nlu-labels-to-improve-an-automatic-speech-recognition-rescoring-model",
            "description": "Second-pass language models that rescore automatic-speech-recognition hypotheses benefit from multitask training on natural-language-understanding objectives.",
            "pubdate": "Wed, 05 Jan 2022 14:09:52 GMT",
            "pubdate_parsed": [
                2022,
                1,
                5
            ],
            "email_sent": true
        },
        "WACV: Transformers for video and contrastive learning": {
            "url": "https://www.amazon.science/blog/wacv-transformer-models-for-video-and-contrastive-learning",
            "description": "Amazon\u2019s Joe Tighe on the major trends he sees in the field of computer vision.",
            "pubdate": "Thu, 06 Jan 2022 14:12:55 GMT",
            "pubdate_parsed": [
                2022,
                1,
                6
            ],
            "email_sent": true
        },
        "A conversation with economics Nobelists": {
            "url": "https://www.amazon.science/latest-news/a-conversation-with-economics-nobelists-on-experimental-design",
            "description": "Amazon Scholar David Card and Amazon academic research consultant Guido Imbens talk about the past and future of empirical economics.",
            "pubdate": "Fri, 07 Jan 2022 16:13:47 GMT",
            "pubdate_parsed": [
                2022,
                1,
                7
            ],
            "email_sent": true
        },
        "Using computer vision to weed out product catalogue errors": {
            "url": "https://www.amazon.science/blog/using-computer-vision-to-weed-out-product-catalogue-errors",
            "description": "Method uses metric learning to determine whether images depict the same product.",
            "pubdate": "Mon, 10 Jan 2022 14:27:32 GMT",
            "pubdate_parsed": [
                2022,
                1,
                10
            ],
            "email_sent": true
        },
        "The science behind the next-gen FORMULA 1 car": {
            "url": "https://www.amazon.science/latest-news/the-science-behind-the-next-gen-2022-F1-car",
            "description": "Learn how the F1 engineering team collaborated with AWS to develop new design specifications to help make races more competitive.",
            "pubdate": "Tue, 11 Jan 2022 12:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                11
            ],
            "email_sent": true
        },
        "How new machine learning techniques could improve MRI scans": {
            "url": "https://www.amazon.science/research-awards/success-stories/how-new-machine-learning-techniques-could-improve-mri-machine-images",
            "description": "Amazon Research Award recipient Jonathan Tamir is focusing on deriving better images faster.",
            "pubdate": "Wed, 12 Jan 2022 13:49:37 GMT",
            "pubdate_parsed": [
                2022,
                1,
                12
            ],
            "email_sent": true
        },
        "Why a Brazilian robotics expert moved to West Virginia to work on robots": {
            "url": "https://www.amazon.science/research-awards/success-stories/autonomous-robots-why-a-brazilian-robotics-expert-moved-to-west-virginia-to-work-on-robots",
            "description": "Guilherme Pereira, a 2019 Amazon Research Award recipient, is researching methods to improve a robot\u2019s ability to work autonomously.",
            "pubdate": "Thu, 13 Jan 2022 13:30:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                13
            ],
            "email_sent": true
        },
        "Hierarchical representations improve image retrieval": {
            "url": "https://www.amazon.science/blog/hierarchical-representations-improve-image-retrieval",
            "description": "A new metric-learning loss function groups together superclasses and learns commonalities within them.",
            "pubdate": "Fri, 14 Jan 2022 14:31:05 GMT",
            "pubdate_parsed": [
                2022,
                1,
                14
            ],
            "email_sent": true
        },
        "Amazons DynamoDB  10 years later": {
            "url": "https://www.amazon.science/latest-news/amazons-dynamodb-10-years-later",
            "description": "Amazon DynamoDB was introduced 10 years ago today; one of its key contributors reflects on its origins, and discusses the 'never-ending journey' to make DynamoDB more secure, more available and more performant.",
            "pubdate": "Tue, 18 Jan 2022 19:47:10 GMT",
            "pubdate_parsed": [
                2022,
                1,
                18
            ],
            "email_sent": true
        },
        "Decisions, decisions: Lihong Li's Amazon Ads reinforcement learning research": {
            "url": "https://www.amazon.science/working-at-amazon/amazon-advertising-lihong-li-using-reinforcement-learning-algorithms",
            "description": "The scientist's work is driving practical outcomes within an exploding machine learning research field.",
            "pubdate": "Wed, 19 Jan 2022 18:26:06 GMT",
            "pubdate_parsed": [
                2022,
                1,
                19
            ],
            "email_sent": true
        },
        "How Alexa learned Arabic": {
            "url": "https://www.amazon.science/latest-news/how-alexa-learned-arabic",
            "description": "Arabic posed unique challenges for speech recognition, language understanding, and speech synthesis.",
            "pubdate": "Mon, 24 Jan 2022 17:10:24 GMT",
            "pubdate_parsed": [
                2022,
                1,
                24
            ],
            "email_sent": true
        },
        "On-device speech processing makes Alexa faster, lower-bandwidth": {
            "url": "https://www.amazon.science/blog/on-device-speech-processing-makes-alexa-faster-lower-bandwidth",
            "description": "Innovative training methods and model compression techniques combine with clever engineering to keep speech processing local.",
            "pubdate": "Tue, 25 Jan 2022 16:32:21 GMT",
            "pubdate_parsed": [
                2022,
                1,
                25
            ],
            "email_sent": true
        },
        "How to build a successful career as a scientist at Amazon": {
            "url": "https://www.amazon.science/working-at-amazon/how-to-build-a-successful-career-as-a-scientist-at-amazon",
            "description": "Belinda Zeng, head of applied science and engineering at Amazon Search Science and AI, shares her perspective.",
            "pubdate": "Wed, 26 Jan 2022 14:03:05 GMT",
            "pubdate_parsed": [
                2022,
                1,
                26
            ],
            "email_sent": true
        },
        "How Prime Video updates its app for more than 8,000 device types": {
            "url": "https://www.amazon.science/blog/how-prime-video-updates-its-app-for-more-than-8-000-device-types",
            "description": "The switch to WebAssembly increases stability, speed.",
            "pubdate": "Thu, 27 Jan 2022 18:24:26 GMT",
            "pubdate_parsed": [
                2022,
                1,
                27
            ],
            "email_sent": true
        },
        "How Amazon Music's recommender hits the right notes": {
            "url": "https://www.amazon.science/latest-news/how-amazon-music-uses-recommendation-system-machine-learning",
            "description": "Learn how the Amazon Music Conversations team is using pioneering machine learning to make Alexa's discernment better than ever.",
            "pubdate": "Fri, 28 Jan 2022 14:00:02 GMT",
            "pubdate_parsed": [
                2022,
                1,
                28
            ],
            "email_sent": true
        },
        "The engineering behind Alexa's contextual speech recognition": {
            "url": "https://www.amazon.science/latest-news/the-engineering-behind-alexas-contextual-speech-recognition",
            "description": "How Alexa scales machine learning models to millions of customers.",
            "pubdate": "Mon, 31 Jan 2022 15:45:55 GMT",
            "pubdate_parsed": [
                2022,
                1,
                31
            ],
            "email_sent": true
        },
        "Alexa AI co-organizes special sessions at Interspeech": {
            "url": "https://www.amazon.science/blog/alexa-ai-co-organizes-special-sessions-at-icassp-interspeech",
            "description": "Sessions on multidevice scenarios, inclusive and fair speech technologies, trustworthy speech processing, and speech intelligibility prediction seek paper submissions.",
            "pubdate": "Wed, 02 Feb 2022 17:31:13 GMT",
            "pubdate_parsed": [
                2022,
                2,
                2
            ],
            "email_sent": true
        },
        "Ren Vidal wins Edward J. McCluskey Technical Achievement Award": {
            "url": "https://www.amazon.science/latest-news/subspace-clustering-rene-vidal-2021-edward-j-mccluskey-technical-achievement-award",
            "description": "The Amazon Scholar and Johns Hopkins University professor was honored for \u201cpioneering contributions to subspace clustering\u201d.",
            "pubdate": "Wed, 02 Feb 2022 15:08:07 GMT",
            "pubdate_parsed": [
                2022,
                2,
                2
            ],
            "email_sent": true
        },
        "Josh Miele: Amazons resident MacArthur Fellow": {
            "url": "https://www.amazon.science/working-at-amazon/josh-miele-amazons-resident-macarthur-fellow",
            "description": "Miele has merged a lifelong passion for science with a mission to make the world more accessible for people with disabilities.",
            "pubdate": "Thu, 03 Feb 2022 14:47:12 GMT",
            "pubdate_parsed": [
                2022,
                2,
                3
            ],
            "email_sent": true
        },
        "CAIT announces new fellowships, faculty research awards": {
            "url": "https://www.amazon.science/academic-engagements/cait-announces-two-new-phd-student-fellowships-and-five-new-faculty-research-awards",
            "description": "The Columbia Center of AI Technology announced its inaugural recipients last year.",
            "pubdate": "Fri, 04 Feb 2022 14:01:37 GMT",
            "pubdate_parsed": [
                2022,
                2,
                4
            ],
            "email_sent": true
        },
        "How Margarita Chli is using drones to go where people cant": {
            "url": "https://www.amazon.science/research-awards/success-stories/autonomous-mobile-robots-margarita-chli-drones",
            "description": "When it comes to search-and-rescue missions, dogs are second to none, but an Amazon Research Award recipient says they might have competition from drones.",
            "pubdate": "Mon, 07 Feb 2022 15:59:27 GMT",
            "pubdate_parsed": [
                2022,
                2,
                7
            ],
            "email_sent": true
        },
        "Amazon expands SURE program to boost diversity in STEM education": {
            "url": "https://www.amazon.science/academic-engagements/amazon-expands-sure-program-to-boost-diversity-in-stem-education",
            "description": "New programs with Georgia Tech and the University of Southern California are established; existing Columbia University program expands.",
            "pubdate": "Tue, 08 Feb 2022 13:55:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                8
            ],
            "email_sent": true
        },
        "New UW-Amazon Science Hub launches": {
            "url": "https://www.amazon.science/academic-engagements/new-uw-amazon-science-hub-launches",
            "description": "The collaboration will focus on advancing innovation in core robotics and AI technologies and their applications.",
            "pubdate": "Wed, 09 Feb 2022 17:00:37 GMT",
            "pubdate_parsed": [
                2022,
                2,
                9
            ],
            "email_sent": true
        },
        "Automated reasoning's scientific frontiers": {
            "url": "https://www.amazon.science/blog/automated-reasonings-scientific-frontiers",
            "description": "Distributing proof search, reasoning about distributed systems, and automating regulatory compliance are just three fruitful research areas.",
            "pubdate": "Thu, 10 Feb 2022 14:22:40 GMT",
            "pubdate_parsed": [
                2022,
                2,
                10
            ],
            "email_sent": true
        },
        "Alexa AI team discusses NeurIPS workshop best paper award": {
            "url": "https://www.amazon.science/latest-news/alexa-ai-team-discusses-neurips-workshop-best-paper-award",
            "description": "Paper deals with detecting and answering out-of-domain requests for task-oriented dialogue systems.",
            "pubdate": "Fri, 11 Feb 2022 14:25:37 GMT",
            "pubdate_parsed": [
                2022,
                2,
                11
            ],
            "email_sent": true
        },
        "Amazon Scholar Ranjit Jhala named ACM Fellow": {
            "url": "https://www.amazon.science/latest-news/amazon-scholar-ranjit-jhala-chosen-as-acm-fellow-for-computer-science-contributions",
            "description": "Jhala received the ACM honor for lifetime contributions to software verification, developing innovative tools to help computer programmers test their code.",
            "pubdate": "Mon, 14 Feb 2022 13:59:18 GMT",
            "pubdate_parsed": [
                2022,
                2,
                14
            ],
            "email_sent": true
        },
        "Alexa Prize has a new home": {
            "url": "https://www.amazon.science/alexa-prize/alexa-prize-has-a-new-home",
            "description": "Amazon Science is now the destination for information on the SocialBot, TaskBot, and SimBot challenges, including FAQs, team updates, publications, and other program information.",
            "pubdate": "Tue, 15 Feb 2022 15:14:22 GMT",
            "pubdate_parsed": [
                2022,
                2,
                15
            ],
            "email_sent": true
        },
        "Amazon Robotics, Hampton University team up to establish robotics program": {
            "url": "https://www.amazon.science/academic-engagements/amazon-robotics-hampton-university-team-up-to-establish-robotics-program",
            "description": "Amazon funding will assist with a senior capstone course where students will receive mentorship from Amazon leading researchers, software developers, and engineers.",
            "pubdate": "Wed, 16 Feb 2022 17:04:38 GMT",
            "pubdate_parsed": [
                2022,
                2,
                16
            ],
            "email_sent": true
        },
        "Amazon at WSDM: The future of graph neural networks": {
            "url": "https://www.amazon.science/blog/amazon-at-wsdm-the-future-of-graph-neural-networks",
            "description": "Amazon\u2019s George Karypis will give a keynote address on graph neural networks, a field in which \u201cthere is some fundamental theoretical stuff that we still need to understand.\u201d",
            "pubdate": "Thu, 17 Feb 2022 14:54:21 GMT",
            "pubdate_parsed": [
                2022,
                2,
                17
            ],
            "email_sent": true
        },
        "Using hyperboloids to improve product retrieval": {
            "url": "https://www.amazon.science/blog/using-hyperboloids-to-improve-product-retrieval",
            "description": "Method using hyperboloid embeddings improves on methods that use vector embeddings by up to 33%.",
            "pubdate": "Fri, 18 Feb 2022 15:20:07 GMT",
            "pubdate_parsed": [
                2022,
                2,
                18
            ],
            "email_sent": true
        },
        "Finding  and preventing  vulnerabilities in machine learning models": {
            "url": "https://www.amazon.science/research-awards/success-stories/explainable-machine-learning-bo-li",
            "description": "Bo Li \u2014 a new Amazon Visiting Academic and former Amazon Research Award recipient \u2014 is making sure algorithms are not only smarter but more trustworthy.",
            "pubdate": "Mon, 21 Feb 2022 14:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                21
            ],
            "email_sent": true
        },
        "Whats next for deep learning?": {
            "url": "https://www.amazon.science/blog/whats-next-for-deep-learning",
            "description": "Integrating symbolic reasoning and learning efficiently from interactions with the world are two major remaining challenges, says vice president and distinguished scientist Nikko Str\u00f6m.",
            "pubdate": "Tue, 22 Feb 2022 14:32:38 GMT",
            "pubdate_parsed": [
                2022,
                2,
                22
            ],
            "email_sent": true
        },
        "George Michailidis: How to identify important changes in online networks": {
            "url": "https://www.amazon.science/working-at-amazon/george-michailidis-anomaly-detection-machine-learning",
            "description": "Amazon Scholar discusses the evolution of anomaly detection research.",
            "pubdate": "Wed, 23 Feb 2022 14:06:36 GMT",
            "pubdate_parsed": [
                2022,
                2,
                23
            ],
            "email_sent": true
        },
        "How chance encounters sparked a career in engineering and robotics": {
            "url": "https://www.amazon.science/working-at-amazon/how-chance-encounters-sparked-a-career-in-engineering-and-robotics",
            "description": "Jovonia Thibert, director of strategy for Amazon Robotics, has a career that spans two decades \u2014 thanks in part to a lesson from her parents.",
            "pubdate": "Thu, 24 Feb 2022 15:13:41 GMT",
            "pubdate_parsed": [
                2022,
                2,
                24
            ],
            "email_sent": true
        },
        "Improving question-answering models that use data from tables": {
            "url": "https://www.amazon.science/blog/improving-question-answering-models-that-use-data-from-tables",
            "description": "Novel pretraining method enables increases of 5% to 14% on five different evaluation metrics.",
            "pubdate": "Mon, 28 Feb 2022 15:03:33 GMT",
            "pubdate_parsed": [
                2022,
                2,
                28
            ],
            "email_sent": true
        },
        "How Haluk Demirkan is using machine learning to get devices to the right place at the right time": {
            "url": "https://www.amazon.science/working-at-amazon/haluk-demirkan-sales-forecast-demand-planning",
            "description": "Part-time sabbatical plan turns into full-time role for author of five books and more than 170 research articles.",
            "pubdate": "Tue, 01 Mar 2022 14:37:09 GMT",
            "pubdate_parsed": [
                2022,
                3,
                1
            ],
            "email_sent": true
        },
        "Amazon VP Babak Parviz appointed to AAAS Board of Directors": {
            "url": "https://www.amazon.science/latest-news/amazon-vp-babak-parviz-appointed-to-aaas-board-of-directors",
            "description": "Parviz will serve a three-year term as one of four appointed directors.",
            "pubdate": "Wed, 02 Mar 2022 18:51:28 GMT",
            "pubdate_parsed": [
                2022,
                3,
                2
            ],
            "email_sent": true
        },
        "Using natural language processing to understand and identify risks": {
            "url": "https://www.amazon.science/working-at-amazon/using-natural-language-processing-to-understand-and-identify-risks",
            "description": "As an applied science manager at Amazon, Muthu Chandrasekaran works on new tools to automate and build a risk technology.",
            "pubdate": "Thu, 03 Mar 2022 14:00:37 GMT",
            "pubdate_parsed": [
                2022,
                3,
                3
            ],
            "email_sent": true
        },
        "How Prime Video uses machine learning to ensure video quality": {
            "url": "https://www.amazon.science/blog/how-prime-video-uses-machine-learning-to-ensure-video-quality",
            "description": "Detectors for block corruption, audio artifacts, and errors in audio-video synchronization are just three of Prime Video\u2019s quality assurance tools.",
            "pubdate": "Fri, 04 Mar 2022 17:13:15 GMT",
            "pubdate_parsed": [
                2022,
                3,
                4
            ],
            "email_sent": true
        },
        "Amazon and Energy Dept. team up to change how we recycle plastic": {
            "url": "https://www.amazon.science/blog/amazon-and-energy-dept-team-up-to-change-how-we-recycle-plastic",
            "description": "Amazon joins the US DOE\u2019s Bio-Optimized Technologies to keep Thermoplastics out of Landfills and the Environment (BOTTLE\u2122) Consortium, focusing on materials and recycling innovation.",
            "pubdate": "Wed, 09 Mar 2022 12:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                9
            ],
            "email_sent": true
        },
        "25 years of QIP": {
            "url": "https://www.amazon.science/blog/25-years-of-qip",
            "description": "As the major quantum computing conference celebrates its anniversary, we ask the conference chair and the head of Amazon\u2019s quantum computing program to take stock.",
            "pubdate": "Thu, 10 Mar 2022 15:03:42 GMT",
            "pubdate_parsed": [
                2022,
                3,
                10
            ],
            "email_sent": true
        },
        "Amazon and Virginia Tech launch AI and ML research initiative": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-virginia-tech-launch-ai-and-ml-research-initiative",
            "description": "Initiative will be led by the Virginia Tech College of Engineering and directed by Thomas L. Phillips Professor of Engineering Naren Ramakrishnan.",
            "pubdate": "Thu, 10 Mar 2022 10:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                10
            ],
            "email_sent": true
        },
        "Bringing practical applications of quantum computing closer": {
            "url": "https://www.amazon.science/blog/bringing-practical-applications-of-quantum-computing-closer",
            "description": "New phase estimation technique reduces qubit count, while learning framework enables characterization of noisy quantum systems.",
            "pubdate": "Fri, 11 Mar 2022 15:58:31 GMT",
            "pubdate_parsed": [
                2022,
                3,
                11
            ],
            "email_sent": true
        },
        "Amazon Science celebrates Pi Day": {
            "url": "https://www.amazon.science/latest-news/pi-day-2022",
            "description": "Times Square display honors scientists, engineers, and mathematicians past, present, and future.",
            "pubdate": "Mon, 14 Mar 2022 12:59:11 GMT",
            "pubdate_parsed": [
                2022,
                3,
                14
            ],
            "email_sent": true
        },
        "Registration opens for Amazon re:MARS event": {
            "url": "https://www.amazon.science/latest-news/registration-opens-for-amazon-re-mars-event",
            "description": "In-person event featuring some of the brightest leaders in science, academia, and business is planned for June 21-24 in Las Vegas",
            "pubdate": "Tue, 15 Mar 2022 17:31:04 GMT",
            "pubdate_parsed": [
                2022,
                3,
                15
            ],
            "email_sent": true
        },
        "Real-world robotic-manipulation system": {
            "url": "https://www.amazon.science/research-awards/success-stories/real-world-robotic-manipulation-system",
            "description": "Amazon Research Award recipient Russ Tedrake is teaching robots to manipulate a wide variety of objects in unfamiliar and constantly changing contexts.",
            "pubdate": "Tue, 15 Mar 2022 14:16:52 GMT",
            "pubdate_parsed": [
                2022,
                3,
                15
            ],
            "email_sent": true
        },
        "The science behind Hunches: Deep device embeddings": {
            "url": "https://www.amazon.science/blog/the-science-behind-hunches-deep-device-embeddings",
            "description": "A machine learning model learns representations that cluster devices according to their usage patterns.",
            "pubdate": "Wed, 16 Mar 2022 17:51:14 GMT",
            "pubdate_parsed": [
                2022,
                3,
                16
            ],
            "email_sent": true
        },
        "Nadia Carlsten drives Amazon's quest for a quantum breakthrough": {
            "url": "https://www.amazon.science/working-at-amazon/nadia-carlsten-drives-amazons-quest-for-a-quantum-breakthrough",
            "description": "The senior product manager leading hardware and software product development at the Center for Quantum Computing wants to make fault-tolerant quantum computing a reality.",
            "pubdate": "Thu, 17 Mar 2022 14:11:49 GMT",
            "pubdate_parsed": [
                2022,
                3,
                17
            ],
            "email_sent": true
        },
        "Huseyin Topaloglu receives Cornell endowed faculty chair": {
            "url": "https://www.amazon.science/latest-news/amazon-scholar-huseyin-topaloglu-receives-endowed-faculty-chair-at-cornell",
            "description": "The Howard and Eleanor Morgan Professor is awarded to a Cornell faculty member who has made meaningful contributions to operations research.",
            "pubdate": "Fri, 18 Mar 2022 12:47:58 GMT",
            "pubdate_parsed": [
                2022,
                3,
                18
            ],
            "email_sent": true
        },
        "New Amazon graduate research fellows announced at Carnegie Mellon": {
            "url": "https://www.amazon.science/academic-engagements/new-amazon-graduate-research-fellows-announced-at-carnegie-mellon",
            "description": "Graduate Research Fellows Program, launched in 2021, supports research in automated reasoning, computer vision, robotics, language technology, machine learning, operations research, and data science.",
            "pubdate": "Mon, 21 Mar 2022 12:55:18 GMT",
            "pubdate_parsed": [
                2022,
                3,
                21
            ],
            "email_sent": true
        },
        "Monitoring and rewarding honest bids to increase auction revenue": {
            "url": "https://www.amazon.science/latest-news/monitoring-and-rewarding-honest-bids-to-increase-revenue-in-auctions",
            "description": "Amazon Scholar Alexandre Belloni discusses the implications of auction design on digital goods.",
            "pubdate": "Tue, 22 Mar 2022 14:29:51 GMT",
            "pubdate_parsed": [
                2022,
                3,
                22
            ],
            "email_sent": true
        },
        "Making DeepSpeed ZeRO run efficiently on more-affordable hardware": {
            "url": "https://www.amazon.science/blog/making-deepspeed-zero-run-efficiently-on-more-affordable-hardware",
            "description": "Amazon researchers optimize the distributed-training tool to run efficiently on the Elastic Fabric Adapter network interface.",
            "pubdate": "Wed, 23 Mar 2022 14:29:08 GMT",
            "pubdate_parsed": [
                2022,
                3,
                23
            ],
            "email_sent": true
        },
        "How AWS uses graph neural networks to meet customer needs": {
            "url": "https://www.amazon.science/blog/how-aws-uses-graph-neural-networks-to-meet-customer-needs",
            "description": "Information extraction, drug discovery, and software analysis are just a few applications of this versatile tool.",
            "pubdate": "Thu, 24 Mar 2022 16:17:39 GMT",
            "pubdate_parsed": [
                2022,
                3,
                24
            ],
            "email_sent": true
        },
        "Amazon to host StatML Oxford Imperial ML Workshop in Berlin office": {
            "url": "https://www.amazon.science/latest-news/amazon-to-host-statml-oxford-imperial-ml-workshop-in-berlin-office",
            "description": "Workshop provides opportunity for students to showcase their work and for connections to be established between academics and Amazon researchers.",
            "pubdate": "Mon, 28 Mar 2022 20:34:28 GMT",
            "pubdate_parsed": [
                2022,
                3,
                28
            ],
            "email_sent": true
        },
        "Edouard Belval: From AWS intern to research engineer": {
            "url": "https://www.amazon.science/working-at-amazon/edouard-belval-from-aws-intern-to-research-engineer",
            "description": "How he parlayed an internship to land an expanded role at Amazon while pursuing his master\u2019s degree.",
            "pubdate": "Wed, 30 Mar 2022 13:05:25 GMT",
            "pubdate_parsed": [
                2022,
                3,
                30
            ],
            "email_sent": true
        },
        "Postdoctoral Science Program": {
            "url": "https://www.amazon.science/postdoctoral-science-program",
            "description": "The program offers recent PhD graduates an opportunity to advance research while working alongside experienced scientists with backgrounds in industry and academia.",
            "pubdate": "Thu, 31 Mar 2022 20:59:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                31
            ],
            "email_sent": true
        },
        "Improving forecasting by learning quantile functions": {
            "url": "https://www.amazon.science/blog/improving-forecasting-by-learning-quantile-functions",
            "description": "Learning the complete quantile function, which maps probabilities to variable values, rather than building separate models for each quantile level, enables better optimization of resource trade-offs.",
            "pubdate": "Thu, 31 Mar 2022 14:21:48 GMT",
            "pubdate_parsed": [
                2022,
                3,
                31
            ],
            "email_sent": true
        },
        "Scalable framework lets multiple text-to-speech models coexist": {
            "url": "https://www.amazon.science/blog/text-to-speech-models-coexist-thanks-to-scalable-framework",
            "description": "Thanks to a set of simple abstractions, models with different architectures can be integrated and optimized for particular hardware accelerators.",
            "pubdate": "Mon, 04 Apr 2022 15:12:34 GMT",
            "pubdate_parsed": [
                2022,
                4,
                4
            ],
            "email_sent": true
        },
        "How applied math impacts forecasting at Amazon": {
            "url": "https://www.amazon.science/working-at-amazon/how-applied-math-impacts-forecasting-at-amazon",
            "description": "Danielle Maddix Robinson's mathematics background helps inform robust models that can predict everything from retail demand to epidemiology.",
            "pubdate": "Tue, 05 Apr 2022 13:09:14 GMT",
            "pubdate_parsed": [
                2022,
                4,
                5
            ],
            "email_sent": true
        },
        "Helping AWS customers accelerate success via machine learning": {
            "url": "https://www.amazon.science/working-at-amazon/helping-aws-customers-accelerate-success-via-machine-learning",
            "description": "Priya Ponnapalli leads the Amazon Machine Learning Solutions Lab, fostering inclusion and growth for her team along the way.",
            "pubdate": "Wed, 06 Apr 2022 13:58:50 GMT",
            "pubdate_parsed": [
                2022,
                4,
                6
            ],
            "email_sent": true
        },
        "Amazon helps create first conference on causal learning and reasoning": {
            "url": "https://www.amazon.science/blog/amazon-helps-create-first-conference-on-causal-learning-and-reasoning",
            "description": "Conference will be held April 11 \u2013 13 in Eureka, California, with virtual elements.",
            "pubdate": "Thu, 07 Apr 2022 18:53:47 GMT",
            "pubdate_parsed": [
                2022,
                4,
                7
            ],
            "email_sent": true
        },
        "Amazon and Johns Hopkins announce new AI institute": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-johns-hopkins-announce-new-ai-institute",
            "description": "The JHU + Amazon Initiative for Interactive AI (AI2AI) will be housed in the Whiting School of Engineering.",
            "pubdate": "Thu, 07 Apr 2022 14:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                7
            ],
            "email_sent": true
        },
        "At Amazon Robotics, simulation gains traction": {
            "url": "https://www.amazon.science/latest-news/at-amazon-robotics-simulation-gains-traction",
            "description": "Scientists and engineers are developing a new generation of simulation tools accurate enough to develop and test robots virtually.",
            "pubdate": "Fri, 08 Apr 2022 13:29:36 GMT",
            "pubdate_parsed": [
                2022,
                4,
                8
            ],
            "email_sent": true
        },
        "New method for \"editing\" fabricated chips enables more-efficient designs": {
            "url": "https://www.amazon.science/blog/new-method-for-editing-fabricated-chips-enables-more-efficient-designs",
            "description": "Reducing the energy of ion beams used for editing eliminates the need for \u201csacrificial\u201d areas between electrical components and improves precision.",
            "pubdate": "Tue, 12 Apr 2022 15:10:21 GMT",
            "pubdate_parsed": [
                2022,
                4,
                12
            ],
            "email_sent": true
        },
        "Luciana Buriols quest for scientific joy": {
            "url": "https://www.amazon.science/working-at-amazon/luciana-buriols-quest-for-scientific-joy",
            "description": "The principal research scientist shares lessons learned during her life journey from a small farm to working on optimizing Amazon\u2019s distribution network.",
            "pubdate": "Wed, 13 Apr 2022 13:30:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                13
            ],
            "email_sent": true
        },
        "Amazon Scholar contributes to best student paper award": {
            "url": "https://www.amazon.science/latest-news/amazon-scholar-contributes-to-best-student-paper-award",
            "description": "Paper proposes a method to better and more equitably place COVID vaccine clinics to encourage more vaccinations.",
            "pubdate": "Thu, 14 Apr 2022 13:03:37 GMT",
            "pubdate_parsed": [
                2022,
                4,
                14
            ],
            "email_sent": true
        },
        "Xiuli Chao appointed Ralph Disney Collegiate Professor at UM": {
            "url": "https://www.amazon.science/latest-news/xiuli-zack-chao-appointed-ralph-l-disney-collegiate-professor-of-industrial-and-operations-engineering",
            "description": "Professorship named after influential former University of Michigan professor.",
            "pubdate": "Fri, 15 Apr 2022 13:16:28 GMT",
            "pubdate_parsed": [
                2022,
                4,
                15
            ],
            "email_sent": true
        },
        "Robin deals with a world where things are changing all around it": {
            "url": "https://www.amazon.science/latest-news/robin-deals-with-a-world-where-things-are-changing-all-around-it",
            "description": "An advanced perception system, which detects and learns from its own mistakes, enables Robin robots to select individual objects from jumbled packages \u2014 at production scale.",
            "pubdate": "Mon, 18 Apr 2022 14:15:43 GMT",
            "pubdate_parsed": [
                2022,
                4,
                18
            ],
            "email_sent": true
        },
        "How does Astro localize itself in an ever-changing home?": {
            "url": "https://www.amazon.science/blog/how-does-astro-localize-itself-in-an-ever-changing-home",
            "description": "Deep learning to produce invariant representations, estimations of sensor reliability, and efficient map representations all contribute to Astro\u2019s superior spatial intelligence.",
            "pubdate": "Tue, 19 Apr 2022 14:23:24 GMT",
            "pubdate_parsed": [
                2022,
                4,
                19
            ],
            "email_sent": true
        },
        "Amazon releases 51-language dataset for language understanding": {
            "url": "https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding",
            "description": "MASSIVE dataset and Massively Multilingual NLU (MMNLU-22) competition and workshop will help researchers scale natural-language-understanding technology to every language on Earth.",
            "pubdate": "Wed, 20 Apr 2022 12:55:45 GMT",
            "pubdate_parsed": [
                2022,
                4,
                20
            ],
            "email_sent": true
        },
        "TheWebConf: Stable themes, new wrinkles": {
            "url": "https://www.amazon.science/blog/thewebconf-blurring-the-line-between-industry-and-academic-research",
            "description": "Amazon Scholar Eugene Agichtein on incorporating knowledge into natural-language-processing models, multimodal interactions, and more.",
            "pubdate": "Thu, 21 Apr 2022 14:31:22 GMT",
            "pubdate_parsed": [
                2022,
                4,
                21
            ],
            "email_sent": true
        },
        "How Zoox vehicles find themselves in an ever-changing world": {
            "url": "https://www.amazon.science/latest-news/how-zoox-vehicles-find-themselves-in-an-ever-changing-world",
            "description": "Advanced machine learning systems help autonomous vehicles react to unexpected changes.",
            "pubdate": "Mon, 25 Apr 2022 14:15:20 GMT",
            "pubdate_parsed": [
                2022,
                4,
                25
            ],
            "email_sent": true
        },
        "Amazon at ICLR: Graphs, time series, and more": {
            "url": "https://www.amazon.science/blog/amazon-at-iclr-graphs-time-series-and-more",
            "description": "Other paper topics include natural-language processing, dataset optimization, and the limits of existing machine learning techniques.",
            "pubdate": "Tue, 26 Apr 2022 15:42:26 GMT",
            "pubdate_parsed": [
                2022,
                4,
                26
            ],
            "email_sent": true
        },
        "Registration remains open for June Amazon re:MARS event": {
            "url": "https://www.amazon.science/latest-news/registration-remains-open-for-june-amazon-re-mars-event",
            "description": "Event\u2019s speaker roster expands for keynotes, innovation spotlights, and leadership sessions.",
            "pubdate": "Wed, 27 Apr 2022 15:27:19 GMT",
            "pubdate_parsed": [
                2022,
                4,
                27
            ],
            "email_sent": true
        },
        "Advances in trustworthy machine learning at Alexa AI": {
            "url": "https://www.amazon.science/blog/advances-in-trustworthy-machine-learning-at-alexa-ai",
            "description": "The team\u2019s latest research on privacy-preserving machine learning, federated learning, and bias mitigation.",
            "pubdate": "Thu, 28 Apr 2022 15:49:50 GMT",
            "pubdate_parsed": [
                2022,
                4,
                28
            ],
            "email_sent": true
        },
        "Improving unsupervised sentence-pair comparison": {
            "url": "https://www.amazon.science/blog/improving-unsupervised-sentence-pair-comparison",
            "description": "Method that captures advantages of cross-encoding and bi-encoding improves on predecessors by as much as 5%.",
            "pubdate": "Fri, 29 Apr 2022 13:38:23 GMT",
            "pubdate_parsed": [
                2022,
                4,
                29
            ],
            "email_sent": true
        },
        "\"An accidental project born out of our need to innovate": {
            "url": "https://www.amazon.science/working-at-amazon/an-accidental-project-born-out-of-our-need-to-innovate",
            "description": "Former Amazon intern George Boateng is using machine learning and mobile tech to bridge Africa\u2019s digital divide.",
            "pubdate": "Mon, 02 May 2022 13:52:01 GMT",
            "pubdate_parsed": [
                2022,
                5,
                2
            ],
            "email_sent": true
        },
        "Ankan Bansals long journey into the world of computer vision": {
            "url": "https://www.amazon.science/working-at-amazon/ankan-bansals-long-journey-into-the-world-of-computer-vision",
            "description": "How a math-loving student travelled 7,000 miles to pursue a passion and wound up becoming an applied scientist.",
            "pubdate": "Tue, 03 May 2022 14:18:26 GMT",
            "pubdate_parsed": [
                2022,
                5,
                3
            ],
            "email_sent": true
        },
        "The science behind ultrasonic motion sensing for Echo": {
            "url": "https://www.amazon.science/blog/the-science-behind-ultrasonic-motion-sensing-for-echo",
            "description": "Reducing false positives for rare events, adapting Echo hardware to ultrasound sensing, and enabling concurrent ultrasound sensing and music playback are just a few challenges Amazon researchers addressed.",
            "pubdate": "Wed, 04 May 2022 15:33:54 GMT",
            "pubdate_parsed": [
                2022,
                5,
                4
            ],
            "email_sent": true
        },
        "Amazon and UCLA announce recipients of gift awards, graduate fellowships": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-ucla-announce-recipients-of-gift-awards-graduate-fellowships",
            "description": "The UCLA Science Hub seeks to address challenges to humanity through research using artificial intelligence, bringing together academic and industry scientists.",
            "pubdate": "Thu, 05 May 2022 13:51:02 GMT",
            "pubdate_parsed": [
                2022,
                5,
                5
            ],
            "email_sent": true
        },
        "More-efficient caching for product retrieval": {
            "url": "https://www.amazon.science/blog/more-efficient-caching-for-product-retrieval",
            "description": "Locality-sensitive hashing enables cache to hold more than three times as many query results.",
            "pubdate": "Fri, 06 May 2022 13:50:40 GMT",
            "pubdate_parsed": [
                2022,
                5,
                6
            ],
            "email_sent": true
        },
        "Amazon scientist Sergey Menis contributes to development of vaccine approach against HIV": {
            "url": "https://www.amazon.science/working-at-amazon/amazon-scientist-sergey-menis-contributes-to-development-of-vaccine-approach-against-hiv",
            "description": "\"I hope we have accelerated HIV vaccine development by providing findings that we and others can build on.\"",
            "pubdate": "Mon, 09 May 2022 14:09:50 GMT",
            "pubdate_parsed": [
                2022,
                5,
                9
            ],
            "email_sent": true
        },
        "A quick guide to Amazons 50-plus ICASSP papers": {
            "url": "https://www.amazon.science/blog/a-quick-guide-to-amazons-50-plus-icassp-papers",
            "description": "Topics range from the predictable, such as speech recognition and signal processing, to time series forecasting and personalization.",
            "pubdate": "Tue, 10 May 2022 15:22:17 GMT",
            "pubdate_parsed": [
                2022,
                5,
                10
            ],
            "email_sent": true
        },
        "Alexas speech recognition research at ICASSP 2022": {
            "url": "https://www.amazon.science/blog/alexas-speech-recognition-research-at-icassp-2022",
            "description": "Multimodal training, signal-to-interpretation, and BERT rescoring are just a few topics covered by Amazon\u2019s 21 speech-related papers.",
            "pubdate": "Thu, 12 May 2022 15:21:30 GMT",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "Swami Sivasubramanian named to National AI Advisory Committee": {
            "url": "https://www.amazon.science/latest-news/swami-sivasubramanian-named-to-national-ai-advisory-committee",
            "description": "NAIAC will advise the president on a range of issues related to artificial intelligence.",
            "pubdate": "Thu, 12 May 2022 13:34:31 GMT",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "New textbook focuses on making better decisions in business and beyond": {
            "url": "https://www.amazon.science/latest-news/matt-taddy-business-analytics-data-science-interview",
            "description": "Matt Taddy, vice president of Amazon\u2019s Private Brands business, is the coauthor of Modern Business Analytics: Practical Data Science for Decision Making, a primer for those who want to gain the skills to use data science to help make decisions in business and beyond.",
            "pubdate": "Mon, 16 May 2022 13:36:49 GMT",
            "pubdate_parsed": [
                2022,
                5,
                16
            ],
            "email_sent": true
        },
        "Amazon Text-to-Speech group's research at ICASSP 2022": {
            "url": "https://www.amazon.science/blog/amazon-text-to-speech-groups-research-at-icassp-2022",
            "description": "Papers focus on speech conversion and data augmentation \u2014 and sometimes both at once.",
            "pubdate": "Tue, 17 May 2022 14:20:14 GMT",
            "pubdate_parsed": [
                2022,
                5,
                17
            ],
            "email_sent": true
        },
        "Amazon Redshift: Ten years of continuous reinvention": {
            "url": "https://www.amazon.science/latest-news/amazon-redshift-ten-years-of-continuous-reinvention",
            "description": "Two authors of Amazon Redshift research paper that will be presented at leading international forum for database researchers reflect on how far the first petabyte scale cloud data warehouse has advanced since it was announced ten years ago.",
            "pubdate": "Wed, 18 May 2022 13:02:19 GMT",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "Speeding database queries by rewriting redundancies": {
            "url": "https://www.amazon.science/blog/speeding-database-queries-by-rewriting-redundancies",
            "description": "Amazon Athena reduces query execution time by 14% by eliminating redundant operations.",
            "pubdate": "Thu, 19 May 2022 15:06:12 GMT",
            "pubdate_parsed": [
                2022,
                5,
                19
            ],
            "email_sent": true
        },
        "Amazon researchers honored by two esteemed academies": {
            "url": "https://www.amazon.science/latest-news/amazon-researchers-honored-by-two-esteemed-academies",
            "description": "Guido Imbens elected to the National Academy of Sciences; Alberto Abadie elected to American Academy of Arts and Sciences.",
            "pubdate": "Fri, 20 May 2022 12:29:26 GMT",
            "pubdate_parsed": [
                2022,
                5,
                20
            ],
            "email_sent": true
        },
        "Robotics at Amazon": {
            "url": "https://www.amazon.science/blog/icra-2022-robotics-at-amazon",
            "description": "Three of Amazon\u2019s leading roboticists \u2014 Sidd Srinivasa, Tye Brady, and Philipp Michel \u2014 discuss the challenges of building robotic systems that interact with human beings in real-world settings.",
            "pubdate": "Mon, 23 May 2022 16:38:34 GMT",
            "pubdate_parsed": [
                2022,
                5,
                23
            ],
            "email_sent": true
        },
        "How Amazon robots navigate congestion": {
            "url": "https://www.amazon.science/latest-news/how-amazon-robots-navigate-congestion",
            "description": "Amazon fulfillment centers use thousands of mobile robots. To keep products moving, Amazon Robotics researchers have crafted unique solutions.",
            "pubdate": "Tue, 24 May 2022 14:58:57 GMT",
            "pubdate_parsed": [
                2022,
                5,
                24
            ],
            "email_sent": true
        },
        "Amazon Robotics names 14 new Day One Fellowship recipients": {
            "url": "https://www.amazon.science/academic-engagements/amazon-robotics-expands-day-one-fellowship-program-and-selects-14-recipients-for-2022",
            "description": "Program empowers Black, Latinx, and Native American students to become industry leaders through scholarship, research, and career opportunities.",
            "pubdate": "Wed, 25 May 2022 18:02:29 GMT",
            "pubdate_parsed": [
                2022,
                5,
                25
            ],
            "email_sent": true
        },
        "Paper on translating images into maps wins ICRA best-paper award": {
            "url": "https://www.amazon.science/blog/translating-images-into-birds-eye-view-maps",
            "description": "Reformulating the mapping problem to take advantage of sequence-to-sequence Transformers improves performance by an average of 15%.",
            "pubdate": "Thu, 26 May 2022 14:51:18 GMT",
            "pubdate_parsed": [
                2022,
                5,
                26
            ],
            "email_sent": true
        },
        "ACL: What comes next for natural-language processing?": {
            "url": "https://www.amazon.science/blog/acl-what-comes-next-for-natural-language-processing",
            "description": "Amazon Scholar and Columbia professor Kathleen McKeown on model compression, data distribution shifts, language revitalization, and more.",
            "pubdate": "Fri, 27 May 2022 16:02:47 GMT",
            "pubdate_parsed": [
                2022,
                5,
                27
            ],
            "email_sent": true
        },
        "Amazon and Max Planck Society launch Science Hub": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-max-planck-society-launch-science-hub",
            "description": "The first Amazon Science Hub to exist outside the US will focus on driving AI research and development throughout Germany.",
            "pubdate": "Fri, 27 May 2022 11:10:27 GMT",
            "pubdate_parsed": [
                2022,
                5,
                27
            ],
            "email_sent": true
        },
        "AWS contributes novel causal machine learning algorithms to DoWhy": {
            "url": "https://www.amazon.science/blog/aws-contributes-novel-causal-machine-learning-algorithms-to-dowhy",
            "description": "New features go beyond conventional effect estimation by attributing events to individual components of complex systems.",
            "pubdate": "Tue, 31 May 2022 16:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                31
            ],
            "email_sent": true
        },
        "RescoreBERT: Using BERT models to improve ASR rescoring": {
            "url": "https://www.amazon.science/blog/rescorebert-using-bert-models-to-improve-asr-rescoring",
            "description": "Knowledge distillation and discriminative training enable efficient use of a BERT-based model to rescore automatic-speech-recognition hypotheses.",
            "pubdate": "Wed, 01 Jun 2022 14:04:39 GMT",
            "pubdate_parsed": [
                2022,
                6,
                1
            ],
            "email_sent": true
        },
        "Richard Zhang wins 2022 CHCCS Achievement Award": {
            "url": "https://www.amazon.science/latest-news/richard-zhang-wins-2022-canadian-human-computer-communications-society-achievement-award",
            "description": "The Amazon Scholar received the award for his seminal and sustained contributions to the fields of computer graphics and visual computing.",
            "pubdate": "Thu, 02 Jun 2022 12:54:28 GMT",
            "pubdate_parsed": [
                2022,
                6,
                2
            ],
            "email_sent": true
        },
        "Amazon Research Awards issues spring 2022 call for proposals": {
            "url": "https://www.amazon.science/research-awards/program-updates/amazon-research-awards-issues-spring-2022-call-for-proposals",
            "description": "Submission period extended to July 15.",
            "pubdate": "Fri, 03 Jun 2022 15:48:58 GMT",
            "pubdate_parsed": [
                2022,
                6,
                3
            ],
            "email_sent": true
        },
        "Compressing BART models for resource-constrained operation": {
            "url": "https://www.amazon.science/blog/compressing-bart-models-for-resource-constrained-operation",
            "description": "Combination of distillation and distillation-aware quantization compresses BART model to 1/16th its size.",
            "pubdate": "Mon, 06 Jun 2022 14:39:46 GMT",
            "pubdate_parsed": [
                2022,
                6,
                6
            ],
            "email_sent": true
        },
        "The next frontier in robotics": {
            "url": "https://www.amazon.science/latest-news/the-next-frontier-in-robotics",
            "description": "Zoox principal software engineer Olivier Toupet on company\u2019s autonomous robotaxi technology",
            "pubdate": "Tue, 07 Jun 2022 15:24:19 GMT",
            "pubdate_parsed": [
                2022,
                6,
                7
            ],
            "email_sent": true
        },
        "Simplifying BERT-based models to increase efficiency, capacity": {
            "url": "https://www.amazon.science/blog/simplifying-bert-based-models-to-increase-efficiency-capacity",
            "description": "New method would enable BERT-based natural-language-processing models to handle longer text strings, run in resource-constrained settings \u2014 or sometimes both.",
            "pubdate": "Wed, 08 Jun 2022 13:58:24 GMT",
            "pubdate_parsed": [
                2022,
                6,
                8
            ],
            "email_sent": true
        },
        "From petroleum engineering to machine learning": {
            "url": "https://www.amazon.science/working-at-amazon/from-petroleum-engineering-to-machine-learning",
            "description": "How Chukwudi Chukwudozie\u2019s path to Amazon was paved by a passion for problem-solving and growth.",
            "pubdate": "Thu, 09 Jun 2022 15:04:35 GMT",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "Alexa AIs natural-language-understanding papers at ICASSP 2022": {
            "url": "https://www.amazon.science/blog/alexa-ais-natural-language-understanding-papers-at-icassp-2022",
            "description": "Papers focus on learning previously unseen intents and personalization, both generally and in the specific case of recipe recommendation.",
            "pubdate": "Fri, 10 Jun 2022 14:26:10 GMT",
            "pubdate_parsed": [
                2022,
                6,
                10
            ],
            "email_sent": true
        },
        "Book demonstrates how to implement NLP business solutions": {
            "url": "https://www.amazon.science/latest-news/new-hands-on-guide-demonstrates-how-to-implement-natural-language-processing-business-solutions",
            "description": "Natural Language Processing with AWS AI Services seeks to demystify NLP for just about anyone.",
            "pubdate": "Mon, 13 Jun 2022 14:22:32 GMT",
            "pubdate_parsed": [
                2022,
                6,
                13
            ],
            "email_sent": true
        },
        "Amazon Robotics supports Georgia Tech startup incubator": {
            "url": "https://www.amazon.science/academic-engagements/amazon-robotics-supports-atdc-georgia-tech-startup-incubator",
            "description": "Funding will go toward assisting diverse entrepreneurs in the fields of robotics and automation.",
            "pubdate": "Tue, 14 Jun 2022 17:19:05 GMT",
            "pubdate_parsed": [
                2022,
                6,
                14
            ],
            "email_sent": true
        },
        "Three top performers emerge in inaugural Alexa Prize TaskBot Challenge": {
            "url": "https://www.amazon.science/alexa-prize/three-top-performers-emerge-in-inaugural-alexa-prize-taskbot-challenge",
            "description": "Digital assistants developed by competing teams helped users perform cooking and home-improvement tasks.",
            "pubdate": "Wed, 15 Jun 2022 14:00:27 GMT",
            "pubdate_parsed": [
                2022,
                6,
                15
            ],
            "email_sent": true
        },
        "CVPR: Understanding images means understanding the world": {
            "url": "https://www.amazon.science/blog/cvpr-understanding-images-means-understanding-the-world",
            "description": "Senior principal scientist Aleix M. Martinez on why computer vision research has only begun to scratch the surface.",
            "pubdate": "Thu, 16 Jun 2022 19:16:06 GMT",
            "pubdate_parsed": [
                2022,
                6,
                16
            ],
            "email_sent": true
        },
        "Amazon announces picks for best science books of 2022  so far": {
            "url": "https://www.amazon.science/latest-news/amazon-announces-picks-for-best-science-books-of-2022-so-far",
            "description": "Amazon yesterday announced its picks for 2022 Best Books of the Year So Far, including its top book within the general-interest science category, \u201cStolen Focus: Why You Can\u2019t Pay Attention \u2014 and How to Think Deeply Again\u201d.",
            "pubdate": "Thu, 16 Jun 2022 12:55:48 GMT",
            "pubdate_parsed": [
                2022,
                6,
                16
            ],
            "email_sent": true
        },
        "Calculating the differential cost of code changes": {
            "url": "https://www.amazon.science/blog/calculating-the-differential-cost-of-code-changes",
            "description": "Automated-reasoning method enables the calculation of tight bounds on the use of resources \u2014 such as computation or memory \u2014 that results from code changes.",
            "pubdate": "Fri, 17 Jun 2022 14:37:21 GMT",
            "pubdate_parsed": [
                2022,
                6,
                17
            ],
            "email_sent": true
        },
        "Anton van den Hengels journey from intellectual property law to computer vision pioneer": {
            "url": "https://www.amazon.science/working-at-amazon/anton-van-den-hengels-journey-from-intellectual-property-law-to-computer-vision-pioneer",
            "description": "Amazon\u2019s director of applied science in Adelaide, Australia, believes the economic value of computer vision has \u201cgone through the roof\".",
            "pubdate": "Mon, 20 Jun 2022 14:14:29 GMT",
            "pubdate_parsed": [
                2022,
                6,
                20
            ],
            "email_sent": true
        },
        "Olga Moskvyaks journey into the world of science": {
            "url": "https://www.amazon.science/working-at-amazon/olga-moskvyaks-journey-into-the-world-of-science",
            "description": "How she moved across the world to discover a passion for (and a career in) machine learning.",
            "pubdate": "Tue, 21 Jun 2022 13:45:33 GMT",
            "pubdate_parsed": [
                2022,
                6,
                21
            ],
            "email_sent": true
        },
        "Alexa's head scientist on conversational exploration, ambient AI": {
            "url": "https://www.amazon.science/blog/alexas-head-scientist-on-conversational-exploration-ambient-ai",
            "description": "Rohit Prasad on the pathway to generalizable intelligence and what excites him most about his re:MARS keynote.",
            "pubdate": "Wed, 22 Jun 2022 18:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                22
            ],
            "email_sent": true
        },
        "Prime Video's work on 3-D scene reconstruction, image representation": {
            "url": "https://www.amazon.science/blog/prime-videos-work-on-3-d-scene-reconstruction-image-representation",
            "description": "CVPR papers examine the recovery of 3-D information from camera movement and learning general representations from weakly annotated data.",
            "pubdate": "Wed, 22 Jun 2022 14:40:44 GMT",
            "pubdate_parsed": [
                2022,
                6,
                22
            ],
            "email_sent": true
        },
        "New workshop to help bring causal reasoning to recommendation systems": {
            "url": "https://www.amazon.science/blog/new-workshop-to-help-bring-causal-reasoning-to-recommendation-systems",
            "description": "Two-day RecSys workshop that extends the popular REVEAL to include CONSEQUENCES features Amazon organizers, speakers.",
            "pubdate": "Thu, 23 Jun 2022 20:34:45 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "Former Amazon intern Karsten Roth wins EMVA young professional award": {
            "url": "https://www.amazon.science/latest-news/former-amazon-intern-karsten-roth-wins-emva-young-professional-award",
            "description": "EMVA Young Professional Award honors \u201coutstanding and innovative work of a student or a young professional in the field of machine vision or image processing.\u201d",
            "pubdate": "Thu, 23 Jun 2022 17:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "Antia Lamas-Linaress path into the world of quantum": {
            "url": "https://www.amazon.science/working-at-amazon/antia-lamas-linaress-aws-quantum-networking",
            "description": "Among the \u2018first wave\u2019 of scientists to gain a PhD in quantum technology, the senior manager of research science discusses her two-decade-long career journey.",
            "pubdate": "Thu, 23 Jun 2022 14:18:44 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "A little public data makes privacy-preserving AI models more accurate": {
            "url": "https://www.amazon.science/blog/a-little-public-data-makes-privacy-preserving-ai-models-more-accurate",
            "description": "Technique that mixes public and private training data can meet differential-privacy criteria while cutting error increase by 60%-70%.",
            "pubdate": "Fri, 24 Jun 2022 21:38:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                24
            ],
            "email_sent": true
        },
        "How a passion for reinforcement learning guided Alexander Longs trajectory": {
            "url": "https://www.amazon.science/working-at-amazon/how-a-passion-for-reinforcement-learning-guided-alexander-longs-trajectory",
            "description": "The field motivated him to pursue a PhD, which eventually led him to Amazon.",
            "pubdate": "Fri, 24 Jun 2022 13:48:59 GMT",
            "pubdate_parsed": [
                2022,
                6,
                24
            ],
            "email_sent": true
        },
        "Near-linear scaling of gigantic-model training on AWS": {
            "url": "https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws",
            "description": "A new distributed-training library achieves near-linear efficiency in scaling from tens to hundreds of GPUs.",
            "pubdate": "Mon, 27 Jun 2022 19:01:57 GMT",
            "pubdate_parsed": [
                2022,
                6,
                27
            ],
            "email_sent": true
        },
        "Alexa Prize announces $1 million SocialBot Grand Challenge 5": {
            "url": "https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2022",
            "description": "Next round of competition will add a science and innovation prize.",
            "pubdate": "Tue, 28 Jun 2022 18:01:56 GMT",
            "pubdate_parsed": [
                2022,
                6,
                28
            ],
            "email_sent": true
        },
        "Bringing the power of deep learning to data in tables": {
            "url": "https://www.amazon.science/blog/bringing-the-power-of-deep-learning-to-data-in-tables",
            "description": "Amazon\u2019s TabTransformer model is now available through SageMaker JumpStart and the official release of the Keras open-source library.",
            "pubdate": "Tue, 28 Jun 2022 15:36:53 GMT",
            "pubdate_parsed": [
                2022,
                6,
                28
            ],
            "email_sent": true
        },
        "Amazon scientists welcome Icelands presidential delegation": {
            "url": "https://www.amazon.science/blog/amazon-scientists-welcome-icelands-presidential-delegation",
            "description": "President\u2019s visit part of a mission to preserve the Icelandic language in the digital age.",
            "pubdate": "Wed, 29 Jun 2022 16:49:11 GMT",
            "pubdate_parsed": [
                2022,
                6,
                29
            ],
            "email_sent": true
        },
        "Amazons tiny robot drives do the heavy lifting": {
            "url": "https://www.amazon.science/latest-news/amazon-robotics-autonomous-drive-units-hercules-pegasus-xanthus-xbot",
            "description": "Autonomous robots called drives play a critical role in making billions of shipments every year. Here\u2019s how they work.",
            "pubdate": "Thu, 30 Jun 2022 12:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                30
            ],
            "email_sent": true
        },
        "Better joint representations of image and text": {
            "url": "https://www.amazon.science/blog/better-joint-representations-of-image-and-text",
            "description": "Two methods presented at CVPR achieve state-of-the-art results by imposing additional structure on the representational space.",
            "pubdate": "Fri, 01 Jul 2022 15:30:39 GMT",
            "pubdate_parsed": [
                2022,
                7,
                1
            ],
            "email_sent": true
        },
        "Ten stories from the first half of 2022 that captivated readers": {
            "url": "https://www.amazon.science/latest-news/ten-stories-from-the-first-half-of-2022-that-captivated-readers",
            "description": "From Josh Miele's passion for making the world more accessible to improving forecasting by learning quantile functions, these stories resonated with our audience.",
            "pubdate": "Mon, 04 Jul 2022 04:01:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                4
            ],
            "email_sent": true
        },
        "My experience at Amazon while teaching at Stanford": {
            "url": "https://www.amazon.science/working-at-amazon/my-experience-at-amazon-while-teaching-at-stanford",
            "description": "Co-mingling industry experience and academic teaching.",
            "pubdate": "Tue, 05 Jul 2022 14:03:03 GMT",
            "pubdate_parsed": [
                2022,
                7,
                5
            ],
            "email_sent": true
        },
        "Second annual Machine Learning Summer School launches in India": {
            "url": "https://www.amazon.science/academic-engagements/second-annual-ml-summer-school-amazon-india",
            "description": "Expanded program aimed at engineering undergraduate and graduate students builds off the success of inaugural program.",
            "pubdate": "Wed, 06 Jul 2022 14:39:07 GMT",
            "pubdate_parsed": [
                2022,
                7,
                6
            ],
            "email_sent": true
        },
        "Anwar Walid receives 2022 IEEE INFOCOM Test of Time Paper Award": {
            "url": "https://www.amazon.science/latest-news/anwar-walid-receives-2022-ieee-infocom-test-of-time-paper-award",
            "description": "Walid\u2019s 2010 paper on distributed caching algorithms for content distribution networks cited for its \u201csignificant impact on the research community\u201d.",
            "pubdate": "Thu, 07 Jul 2022 19:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                7
            ],
            "email_sent": true
        },
        "A quick guide to Amazons 45-plus NAACL papers": {
            "url": "https://www.amazon.science/blog/a-quick-guide-to-amazons-45-plus-naacl-papers",
            "description": "The breadth and originality of Amazon\u2019s natural-language-processing research are on display at the annual meeting of the North American chapter of the Association for Computational Linguistics.",
            "pubdate": "Thu, 07 Jul 2022 16:09:54 GMT",
            "pubdate_parsed": [
                2022,
                7,
                7
            ],
            "email_sent": true
        },
        "NAACL: Industry track offers reality checks, new directions": {
            "url": "https://www.amazon.science/blog/naacl-industry-track-offers-reality-checks-new-directions",
            "description": "Industry track chair and Amazon principal research scientist Rashmi Gangadharaiah on trends in industry papers and the challenges of building practical dialogue systems.",
            "pubdate": "Fri, 08 Jul 2022 17:19:26 GMT",
            "pubdate_parsed": [
                2022,
                7,
                8
            ],
            "email_sent": true
        },
        "Improving entity linking between texts and knowledge bases": {
            "url": "https://www.amazon.science/blog/improving-entity-linking-between-texts-and-knowledge-bases",
            "description": "New model sets new standard in accuracy while enabling 60-fold speedups.",
            "pubdate": "Fri, 08 Jul 2022 14:15:01 GMT",
            "pubdate_parsed": [
                2022,
                7,
                8
            ],
            "email_sent": true
        },
        "How events like Prime Day helped Amazon navigate the pandemic": {
            "url": "https://www.amazon.science/latest-news/how-peak-events-like-prime-day-helped-amazon-navigate-the-pandemic",
            "description": "The SCOT science team used lessons from the past \u2014 and improved existing tools \u2014 to contend with \u201ca peak that lasted two years\u201d.",
            "pubdate": "Mon, 11 Jul 2022 13:12:35 GMT",
            "pubdate_parsed": [
                2022,
                7,
                11
            ],
            "email_sent": true
        },
        "Machine Learning University expands with MLU Explains": {
            "url": "https://www.amazon.science/latest-news/amazon-machine-learning-university-new-courses-mlu-explains",
            "description": "Fun visual essays explain key concepts of machine learning.",
            "pubdate": "Tue, 12 Jul 2022 13:43:35 GMT",
            "pubdate_parsed": [
                2022,
                7,
                12
            ],
            "email_sent": true
        },
        "Amazon and MIT announce Science Hub gift project awards": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-mit-announce-science-hub-gift-project-awards",
            "description": "Four MIT professors are the recipients of the inaugural call for research projects.",
            "pubdate": "Wed, 13 Jul 2022 18:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                13
            ],
            "email_sent": true
        },
        "Knowledge distillation for better convergence in multitask learning": {
            "url": "https://www.amazon.science/blog/knowledge-distillation-for-better-convergence-in-multitask-learning",
            "description": "Allowing separate tasks to converge on their own schedules and using knowledge distillation to maintain performance improves accuracy.",
            "pubdate": "Wed, 13 Jul 2022 15:28:28 GMT",
            "pubdate_parsed": [
                2022,
                7,
                13
            ],
            "email_sent": true
        },
        "Why ambient computing needs self-learning": {
            "url": "https://www.amazon.science/blog/why-ambient-computing-needs-self-learning",
            "description": "To become the interface for the Internet of things, conversational agents will need to learn on their own. Alexa has already started down that path.",
            "pubdate": "Thu, 14 Jul 2022 20:59:54 GMT",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "Joris Kinable wins IISE Transactions 2022 Best Application Award": {
            "url": "https://www.amazon.science/latest-news/amazon-scientist-joris-kinable-wins-iise-transactions-2022-best-application-award",
            "description": "Paper explains the use of constraint programming and mathematical optimization techniques in calculating the best routes for snowplows to clear Pittsburgh\u2019s roads.",
            "pubdate": "Thu, 14 Jul 2022 13:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "Filtering out \"forbidden\" documents during information retrieval": {
            "url": "https://www.amazon.science/blog/filtering-out-forbidden-documents-during-information-retrieval",
            "description": "New method optimizes the twin demands of retrieving relevant content and filtering out bad content.",
            "pubdate": "Fri, 15 Jul 2022 18:14:51 GMT",
            "pubdate_parsed": [
                2022,
                7,
                15
            ],
            "email_sent": true
        },
        "Amazon scientists Mike Hicks and Ren Vidal honored": {
            "url": "https://www.amazon.science/latest-news/amazon-scientists-mike-hicks-and-rene-vidal-honored",
            "description": "Hicks wins 2022 ACM SIGPLAN Distinguished Service Award for career contributions; Vidal wins IEEE Signal Processing Magazine Best Paper Award.",
            "pubdate": "Fri, 15 Jul 2022 16:32:31 GMT",
            "pubdate_parsed": [
                2022,
                7,
                15
            ],
            "email_sent": true
        },
        "74 Amazon Research Awards recipients announced": {
            "url": "https://www.amazon.science/research-awards/program-updates/74-amazon-research-awards-recipients-announced",
            "description": "The awardees represent 51 universities in 17 countries. Recipients have access to more than 300 Amazon public datasets, and can utilize AWS AI/ML services and tools.",
            "pubdate": "Mon, 18 Jul 2022 16:10:38 GMT",
            "pubdate_parsed": [
                2022,
                7,
                18
            ],
            "email_sent": true
        },
        "New method identifies the root causes of statistical outliers": {
            "url": "https://www.amazon.science/blog/new-method-identifies-the-root-causes-of-statistical-outliers",
            "description": "Amazon ICML paper proposes information-theoretic measurement of quantitative causal contribution.",
            "pubdate": "Tue, 19 Jul 2022 15:20:16 GMT",
            "pubdate_parsed": [
                2022,
                7,
                19
            ],
            "email_sent": true
        },
        "\"Among all sources of information, visual information may be the most interesting\"": {
            "url": "https://www.amazon.science/working-at-amazon/amazon-computer-vision-intern-to-applied-scientist-violetta-shevchenko",
            "description": "Violetta Shevchenko, an Amazon applied scientist and former intern, combines vision and language to create solutions to challenging problems.",
            "pubdate": "Wed, 20 Jul 2022 13:55:22 GMT",
            "pubdate_parsed": [
                2022,
                7,
                20
            ],
            "email_sent": true
        },
        "ICML: Where causality meets machine learning": {
            "url": "https://www.amazon.science/blog/icml-where-causality-meets-machine-learning",
            "description": "Amazon\u2019s Dominik Janzing on the history and promise of the young field of causal machine learning.",
            "pubdate": "Thu, 21 Jul 2022 13:43:23 GMT",
            "pubdate_parsed": [
                2022,
                7,
                21
            ],
            "email_sent": true
        },
        "Causal inference when treatments are continuous variables": {
            "url": "https://www.amazon.science/blog/causal-inference-when-treatments-are-continuous-variables",
            "description": "Combining a cutting-edge causal-inference technique and end-to-end machine learning reduces root-mean-square error by 27% to 38%.",
            "pubdate": "Fri, 22 Jul 2022 20:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                22
            ],
            "email_sent": true
        },
        "Massively Multilingual NLU 2022: Call for papers and shared-task entries": {
            "url": "https://www.amazon.science/blog/massively-multilingual-nlu-2022-call-for-papers-and-shared-task-entries",
            "description": "New EMNLP workshop will feature talks, papers, posters, and a competition built around the 50-plus-language, million-utterance MASSIVE dataset.",
            "pubdate": "Fri, 22 Jul 2022 16:39:17 GMT",
            "pubdate_parsed": [
                2022,
                7,
                22
            ],
            "email_sent": true
        },
        "Honorable mention to Amazon researchers for ICML test-of-time award": {
            "url": "https://www.amazon.science/blog/honorable-mention-to-amazon-researchers-for-icml-test-of-time-award",
            "description": "Amazon's Bernhard Sch\u00f6lkopf and Dominik Janzing are first and second authors on \"breakthrough 2012 paper\".",
            "pubdate": "Fri, 22 Jul 2022 14:21:28 GMT",
            "pubdate_parsed": [
                2022,
                7,
                22
            ],
            "email_sent": true
        },
        "Amazon-Columbia SURE students meet with Alexa AI VP": {
            "url": "https://www.amazon.science/academic-engagements/amazon-columbia-sure-students-meet-with-alexa-ai-vp",
            "description": "Prem Natarajan, Alexa AI vice president of natural understanding, visited the campus to engage with young STEM researchers from historically underrepresented backgrounds.",
            "pubdate": "Mon, 25 Jul 2022 19:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                25
            ],
            "email_sent": true
        },
        "How Amazon learned to cut its cardboard waste": {
            "url": "https://www.amazon.science/latest-news/amazon-cardboard-boxes-waste-reduction",
            "description": "Pioneering web-based PackOpt tool has resulted in an annual reduction in cardboard waste of 7% to 10% in North America, saving roughly 60,000 tons of cardboard annually.",
            "pubdate": "Mon, 25 Jul 2022 14:10:18 GMT",
            "pubdate_parsed": [
                2022,
                7,
                25
            ],
            "email_sent": true
        },
        "Preparing today for a post-quantum cryptographic future": {
            "url": "https://www.amazon.science/blog/preparing-today-for-a-post-quantum-cryptographic-future",
            "description": "Amazon is helping develop standards for post-quantum cryptography and deploying promising technologies for customers to experiment with.",
            "pubdate": "Tue, 26 Jul 2022 14:17:17 GMT",
            "pubdate_parsed": [
                2022,
                7,
                26
            ],
            "email_sent": true
        },
        "How silicon innovation became the secret sauce behind AWSs success": {
            "url": "https://www.amazon.science/how-silicon-innovation-became-the-secret-sauce-behind-awss-success",
            "description": "Nafea Bshara, AWS vice president and distinguished engineer, discusses Annapurna Lab\u2019s path to silicon success; Annapurna co-founder was a featured speaker at AWS Silicon Innovation Day virtual event.",
            "pubdate": "Wed, 27 Jul 2022 18:10:07 GMT",
            "pubdate_parsed": [
                2022,
                7,
                27
            ],
            "email_sent": true
        },
        "I didnt imagine I could grow and learn so much": {
            "url": "https://www.amazon.science/working-at-amazon/amazon-internships-summer-2022-experience-donato-crisostomi-science-intern",
            "description": "Donato Crisostomi talks about how his mother helped spark a love of knowledge that led him to two science internships at Amazon.",
            "pubdate": "Thu, 28 Jul 2022 18:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                28
            ],
            "email_sent": true
        },
        "Amazon hosts largest class of science interns": {
            "url": "https://www.amazon.science/working-at-amazon/amazon-hosts-largest-class-of-science-interns",
            "description": "This year\u2019s class includes applied science, research science, and data science interns.",
            "pubdate": "Thu, 28 Jul 2022 14:33:08 GMT",
            "pubdate_parsed": [
                2022,
                7,
                28
            ],
            "email_sent": true
        },
        "A hyperparameter optimization library for reproducible research": {
            "url": "https://www.amazon.science/blog/a-hyperparameter-optimization-library-for-reproducible-research",
            "description": "Syne Tune supports multiple backends, single-fidelity and multi-fidelity (early-exit) optimization algorithms, and hyperparameter transfer learning.",
            "pubdate": "Fri, 29 Jul 2022 14:16:25 GMT",
            "pubdate_parsed": [
                2022,
                7,
                29
            ],
            "email_sent": true
        },
        "Amazon Scholar Kathleen McKeown receives dual honors": {
            "url": "https://www.amazon.science/latest-news/amazon-scholar-kathleen-mckeown-receives-dual-honors",
            "description": "McKeown awarded IEEE Innovation in Societal Infrastructure Award and named a member of the American Philosophical Society.",
            "pubdate": "Mon, 01 Aug 2022 18:02:14 GMT",
            "pubdate_parsed": [
                2022,
                8,
                1
            ],
            "email_sent": true
        },
        "The path to carbon reductions in high-growth economic sectors": {
            "url": "https://www.amazon.science/blog/the-path-to-carbon-reductions-in-high-growth-economic-sectors",
            "description": "Confronting climate change requires the participation of governments, companies, academics, civil-society organizations, and the public.",
            "pubdate": "Mon, 01 Aug 2022 14:37:12 GMT",
            "pubdate_parsed": [
                2022,
                8,
                1
            ],
            "email_sent": true
        },
        "20B-parameter Alexa model sets new marks in few-shot learning": {
            "url": "https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning",
            "description": "With an encoder-decoder architecture \u2014 rather than decoder only \u2014 the Alexa Teacher Model excels other large language models on few-shot tasks such as summarization and machine translation.",
            "pubdate": "Tue, 02 Aug 2022 12:59:36 GMT",
            "pubdate_parsed": [
                2022,
                8,
                2
            ],
            "email_sent": true
        },
        "Amazon wins best-paper award at first AutoML conference": {
            "url": "https://www.amazon.science/blog/amazon-wins-best-paper-award-at-first-automl-conference",
            "description": "Paper presents a criterion for halting the hyperparameter optimization process.",
            "pubdate": "Wed, 03 Aug 2022 13:35:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                3
            ],
            "email_sent": true
        },
        "Data science could be used everywhere": {
            "url": "https://www.amazon.science/working-at-amazon/mlu-data-scientist-jared-wilber",
            "description": "How Jared Wilber is using his skills as a storyteller and data scientist to help others learn about machine learning.",
            "pubdate": "Thu, 04 Aug 2022 13:28:47 GMT",
            "pubdate_parsed": [
                2022,
                8,
                4
            ],
            "email_sent": true
        },
        "Domenico Giannones never-ending drive to learn more from economic data": {
            "url": "https://www.amazon.science/working-at-amazon/domenico-giannone-nowcasting-amazon-economics-forecasting",
            "description": "How the Amazon Supply Chain Optimization Technologies principal economist uses his expertise in time series econometrics to forecast aggregate demand.",
            "pubdate": "Fri, 05 Aug 2022 14:08:26 GMT",
            "pubdate_parsed": [
                2022,
                8,
                5
            ],
            "email_sent": true
        },
        "Automated reasoning at Amazon: a conversation": {
            "url": "https://www.amazon.science/blog/automated-reasoning-at-federated-logic-conference-floc",
            "description": "To mark the occasion of the eighth Federated Logic Conference (FloC), Amazon\u2019s Byron Cook, Daniel Kr\u00f6ning, and Marijn Heule discussed automated reasoning\u2019s prospects.",
            "pubdate": "Mon, 08 Aug 2022 18:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                8
            ],
            "email_sent": true
        },
        "National Science Foundation and Amazon announce latest Fairness in AI grant projects": {
            "url": "https://www.amazon.science/academic-engagements/national-science-foundation-amazon-2022-fairness-in-AI-grant-projects",
            "description": "Thirteen new projects focus on ensuring fairness in AI algorithms and the systems that incorporate them.",
            "pubdate": "Mon, 08 Aug 2022 15:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                8
            ],
            "email_sent": true
        },
        "How the Zoox robotaxi predicts everything, everywhere, all at once": {
            "url": "https://www.amazon.science/latest-news/how-the-zoox-robotaxi-predicts-everything-everywhere-all-at-once",
            "description": "A combination of cutting-edge hardware, sensor technology, and bespoke machine learning approaches can predict trajectories of vehicles, people, and even animals, as far as 8 seconds into the future.",
            "pubdate": "Tue, 09 Aug 2022 14:56:10 GMT",
            "pubdate_parsed": [
                2022,
                8,
                9
            ],
            "email_sent": true
        },
        "Amazon and UW announce inaugural Science Hub faculty research awards": {
            "url": "https://www.amazon.science/academic-engagements/amazon-and-university-of-washington-announce-inaugural-science-hub-faculty-research-awards",
            "description": "Six UW professors will advance artificial intelligence and robotics research with new grants.",
            "pubdate": "Wed, 10 Aug 2022 16:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Reid Blackman: The ethics of AI": {
            "url": "https://www.amazon.science/latest-news/reid-blackman-ethical-machines-the-ethics-of-ai",
            "description": "The author of Ethical Machines explains why companies pursuing ethical AI must ultimately place the responsibility with their senior leadership.",
            "pubdate": "Wed, 10 Aug 2022 14:10:50 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "KDD: Graph neural networks, fairness, and inclusivity": {
            "url": "https://www.amazon.science/blog/kdd-graph-neural-networks-fairness-and-inclusivity",
            "description": "The general chair of this year\u2019s Conference on Knowledge Discovery and Data Mining on what excites him most about the conference program.",
            "pubdate": "Thu, 11 Aug 2022 16:36:23 GMT",
            "pubdate_parsed": [
                2022,
                8,
                11
            ],
            "email_sent": true
        },
        "Erran Li receives 2022 SIGMOBILE test-of-time award": {
            "url": "https://www.amazon.science/latest-news/erran-li-receives-2022-sigmobile-test-of-time-award",
            "description": "Li and co-authors honored for creating an antenna design that was essential to the growth of mobile devices.",
            "pubdate": "Fri, 12 Aug 2022 16:31:47 GMT",
            "pubdate_parsed": [
                2022,
                8,
                12
            ],
            "email_sent": true
        },
        "Amazon wins contest to control \"formality\" in machine translation": {
            "url": "https://www.amazon.science/blog/amazon-wins-contest-to-control-formality-in-machine-translation",
            "description": "Data augmentation and post-editing strategies lift Amazon\u2019s submission above competitors.",
            "pubdate": "Mon, 15 Aug 2022 14:00:55 GMT",
            "pubdate_parsed": [
                2022,
                8,
                15
            ],
            "email_sent": true
        },
        "Ying Dings human-centered approach to AI-enhanced medical imaging diagnosis": {
            "url": "https://www.amazon.science/research-awards/success-stories/ying-dings-human-centered-approach-to-ai-enhanced-medical-imaging-diagnosis",
            "description": "ARA recipient is using artificial intelligence to help doctors make decisions based on radiological data.",
            "pubdate": "Tue, 16 Aug 2022 15:10:11 GMT",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "Scaling graph-neural-network training with CPU-GPU clusters": {
            "url": "https://www.amazon.science/blog/scaling-graph-neural-network-training-with-cpu-gpu-clusters",
            "description": "In tests, new approach is 15 to 18 times as fast as predecessors.",
            "pubdate": "Wed, 17 Aug 2022 14:57:33 GMT",
            "pubdate_parsed": [
                2022,
                8,
                17
            ],
            "email_sent": true
        },
        "A billion SMT queries a day": {
            "url": "https://www.amazon.science/blog/a-billion-smt-queries-a-day",
            "description": "CAV keynote lecture by the director of applied science for AWS Identity explains how AWS is making the power of automated reasoning available to all customers.",
            "pubdate": "Thu, 18 Aug 2022 17:00:35 GMT",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Ozge Sahin on the art and science of studying consumer behavior": {
            "url": "https://www.amazon.science/working-at-amazon/ozge-sahin-on-the-art-and-science-of-studying-consumer-behavior",
            "description": "The Johns Hopkins business school professor and Amazon Scholar focuses on enhancing customer experiences.",
            "pubdate": "Fri, 19 Aug 2022 19:15:05 GMT",
            "pubdate_parsed": [
                2022,
                8,
                19
            ],
            "email_sent": true
        },
        "Why Amazon Scholar Yossi Keshet remains \"excited about speech\"": {
            "url": "https://www.amazon.science/working-at-amazon/why-amazon-scholar-yossi-keshet-remains-excited-about-speech",
            "description": "New speech representations and self-supervised learning are two of the recent trends that most intrigue him.",
            "pubdate": "Tue, 23 Aug 2022 16:16:25 GMT",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "The science behind grouping package deliveries": {
            "url": "https://www.amazon.science/latest-news/the-science-behind-grouping-amazon-package-deliveries",
            "description": "How Customer Order and Network Density OptimizeR (CONDOR) has led to improved delivery routes.",
            "pubdate": "Wed, 24 Aug 2022 14:35:36 GMT",
            "pubdate_parsed": [
                2022,
                8,
                24
            ],
            "email_sent": true
        },
        "Amazon product query competition draws more than 9,200 submissions": {
            "url": "https://www.amazon.science/blog/amazon-product-query-competition-draws-more-than-9-200-submissions",
            "description": "Launched under the auspices of the KDD Cup at KDD 2022, the competition included the release of a new product query dataset.",
            "pubdate": "Thu, 25 Aug 2022 18:30:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "Using data science to help improve NFL quarterback passing scores": {
            "url": "https://www.amazon.science/working-at-amazon/elena-ehrlich-data-science-nfl-quarterback-passing-ratings",
            "description": "Principal data scientist Elena Ehrlich uses her skills to help a wide variety of customers \u2014 including the National Football League.",
            "pubdate": "Fri, 26 Aug 2022 13:58:02 GMT",
            "pubdate_parsed": [
                2022,
                8,
                26
            ],
            "email_sent": true
        },
        "The science behind NFL Next Gen Stats new passing metric": {
            "url": "https://www.amazon.science/blog/the-science-behind-nfl-next-gen-stats-new-passing-metric",
            "description": "Spliced binned-Pareto distributions are flexible enough to handle symmetric, asymmetric, and multimodal distributions, offering a more consistent metric.",
            "pubdate": "Fri, 26 Aug 2022 13:57:48 GMT",
            "pubdate_parsed": [
                2022,
                8,
                26
            ],
            "email_sent": true
        },
        "I always knew that my main interest was in supply chain optimization": {
            "url": "https://www.amazon.science/working-at-amazon/alp-muharremoglu-columbia-university-of-texas-operations-professor-amazon-scot",
            "description": "After 15 years in academia, Alp Muharremoglu became a senior principal senior scientist within Amazon\u2019s Supply Chain Optimization Technologies organization, and says his teaching skills are indispensable.",
            "pubdate": "Mon, 29 Aug 2022 13:36:27 GMT",
            "pubdate_parsed": [
                2022,
                8,
                29
            ],
            "email_sent": true
        },
        "Model assesses the validity of tips offered in product reviews": {
            "url": "https://www.amazon.science/blog/model-assesses-the-validity-of-tips-offered-in-product-reviews",
            "description": "Method would enable customers to evaluate supporting evidence for tip reliability.",
            "pubdate": "Fri, 02 Sep 2022 13:31:04 GMT",
            "pubdate_parsed": [
                2022,
                9,
                2
            ],
            "email_sent": true
        },
        "Automatically optimizing execution of unfamiliar tensor operations": {
            "url": "https://www.amazon.science/blog/automatically-optimizing-execution-of-unfamiliar-tensor-operations",
            "description": "New auto-scheduler speeds optimization process sixfold while improving performance of resulting code up to 70%.",
            "pubdate": "Thu, 08 Sep 2022 13:46:52 GMT",
            "pubdate_parsed": [
                2022,
                9,
                8
            ],
            "email_sent": true
        },
        "Interspeech 2022: The growth of interdisciplinary research": {
            "url": "https://www.amazon.science/blog/interspeech-2022-the-growth-of-interdisciplinary-research",
            "description": "Cyclic training of speech synthesis and speech recognition models and language understanding for better speech prosody are just a few examples of cross-pollination in speech-related fields.",
            "pubdate": "Wed, 14 Sep 2022 14:04:46 GMT",
            "pubdate_parsed": [
                2022,
                9,
                14
            ],
            "email_sent": true
        },
        "Jaco Geldenhuys and Willem Visser win ISSTA Impact Paper Award": {
            "url": "https://www.amazon.science/latest-news/amazon-scientists-jaco-geldenhuys-and-willem-visser-win-2022-issta-impact-paper-award",
            "description": "Scientists\u2019 paper on probabilistic symbolic execution has significantly influenced software testing and analysis.",
            "pubdate": "Wed, 21 Sep 2022 13:31:46 GMT",
            "pubdate_parsed": [
                2022,
                9,
                21
            ],
            "email_sent": true
        },
        "Amazon releases dataset for complex, multilingual question answering": {
            "url": "https://www.amazon.science/blog/amazon-releases-dataset-for-complex-multilingual-question-answering",
            "description": "Dataset that requires question-answering models to look up multiple facts and perform comparisons bridges a significant gap in the field.",
            "pubdate": "Wed, 05 Oct 2022 13:44:37 GMT",
            "pubdate_parsed": [
                2022,
                10,
                5
            ],
            "email_sent": true
        },
        "The science behind the new Alexa, what should I watch? Fire TV experience": {
            "url": "https://www.amazon.science/latest-news/the-science-behind-the-new-alexa-what-should-i-watch-fire-tv-experience",
            "description": "The phrase launches a feature built to help customers navigate an increasingly complex and diverse world of content.",
            "pubdate": "Thu, 06 Oct 2022 13:48:49 GMT",
            "pubdate_parsed": [
                2022,
                10,
                6
            ],
            "email_sent": true
        },
        "TRIPP explores the potential of VRpowered meditation": {
            "url": "https://www.amazon.science/latest-news/tripp-explores-the-potential-of-virtual-reality-powered-meditation",
            "description": "Alexa Fund portfolio company\u2019s science-led program could change how we approach mental wellness \u2014 and how we use VR.",
            "pubdate": "Fri, 07 Oct 2022 13:57:11 GMT",
            "pubdate_parsed": [
                2022,
                10,
                7
            ],
            "email_sent": true
        },
        "Maximizing the efficiency of Amazon's own delivery networks": {
            "url": "https://www.amazon.science/blog/maximizing-the-efficiency-of-amazons-own-delivery-networks",
            "description": "INFORMS talk explores techniques Amazon\u2019s Supply Chain Optimization Technologies organization is testing to fulfill customer orders more efficiently.",
            "pubdate": "Fri, 14 Oct 2022 13:44:31 GMT",
            "pubdate_parsed": [
                2022,
                10,
                14
            ],
            "email_sent": true
        },
        "Exploring the uncertainty of predictions": {
            "url": "https://www.amazon.science/latest-news/amazon-scholar-tatevik-sekhposyan-exploring-the-uncertainty-of-predictions",
            "description": "Tatevik Sekhposyan, Amazon Scholar and Texas A&amp;M University professor, enjoys the flexibility of economics and how embracing uncertainty can enhance prediction.",
            "pubdate": "Mon, 17 Oct 2022 13:38:13 GMT",
            "pubdate_parsed": [
                2022,
                10,
                17
            ],
            "email_sent": true
        },
        "reMARS revisited: Amazons supply chain optimization": {
            "url": "https://www.amazon.science/remars-revisited-amazons-supply-chain-optimization",
            "description": "How the SCOT team implemented a system that leverages operations research and machine learning to decide what products to buy, how much to buy, where to place them, and more.",
            "pubdate": "Tue, 18 Oct 2022 14:00:00 GMT",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "The quest to deploy autonomous robots within Amazon fulfillment centers": {
            "url": "https://www.amazon.science/latest-news/the-quest-to-deploy-autonomous-robots-within-amazon-fulfillment-centers",
            "description": "Company is testing a new class of robots that use artificial intelligence and computer vision to move freely throughout facilities.",
            "pubdate": "Mon, 24 Oct 2022 13:51:48 GMT",
            "pubdate_parsed": [
                2022,
                10,
                24
            ],
            "email_sent": true
        },
        "Amazon Robotics welcomes students to inaugural Day One Fellowship Summit": {
            "url": "https://www.amazon.science/latest-news/amazon-robotics-welcomes-students-to-inaugural-day-one-fellowship-summit",
            "description": "Summit offered Day One fellows the opportunity to interact with leaders in the robotics field.",
            "pubdate": "Tue, 25 Oct 2022 13:44:20 GMT",
            "pubdate_parsed": [
                2022,
                10,
                25
            ],
            "email_sent": true
        },
        "Amazon SCOT announces 2022 INFORMS Scholars": {
            "url": "https://www.amazon.science/latest-news/amazon-scot-announces-2022-informs-scholars",
            "description": "Program was established to help expand the pipeline of operations research, management science, and analytics talent from underrepresented backgrounds.",
            "pubdate": "Fri, 28 Oct 2022 13:32:13 GMT",
            "pubdate_parsed": [
                2022,
                10,
                28
            ],
            "email_sent": true
        },
        "David Schusters quest to make practical quantum computers a reality": {
            "url": "https://www.amazon.science/working-at-amazon/david-schusters-quest-to-make-practical-quantum-computers-a-reality",
            "description": "With quantum computers poised to take a big step forward, we speak to an Amazon Scholar who has spent two decades driving the technology to realize its enormous potential.",
            "pubdate": "Mon, 31 Oct 2022 14:00:17 GMT",
            "pubdate_parsed": [
                2022,
                10,
                31
            ],
            "email_sent": true
        },
        "Jens Lehmann receives Semantic Web journal 10-year award for influential paper": {
            "url": "https://www.amazon.science/latest-news/jens-lehmann-receives-semantic-web-journal-10-year-award-for-influential-paper",
            "description": "The work of Lehmann and three co-authors helped demonstrate the feasibility of large-scale virtual knowledge graphs.",
            "pubdate": "Fri, 04 Nov 2022 13:49:31 GMT",
            "pubdate_parsed": [
                2022,
                11,
                4
            ],
            "email_sent": true
        },
        "Honors and awards presented to Amazon researchers": {
            "url": "https://www.amazon.science/latest-news/honors-and-awards-presented-to-amazon-researchers-november-2022",
            "description": "Omar Javed, Steven Flammia, Michael I. Jordan, and Daniela Witten recently recognized for their contributions to science.",
            "pubdate": "Mon, 14 Nov 2022 13:30:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                14
            ],
            "email_sent": true
        },
        "How Amazon integrated Alexa into NASAs Orion spacecraft": {
            "url": "https://www.amazon.science/latest-news/the-science-behind-alexa-on-artemis-and-orion-spacecraft-nasa",
            "description": "From physical constraints to acoustic challenges, learn how Amazon collaborated with NASA and Lockheed Martin to get Alexa to work in space.",
            "pubdate": "Wed, 16 Nov 2022 10:50:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                16
            ],
            "email_sent": true
        },
        "How Amazon Robotics is working to eliminate the need for barcodes": {
            "url": "https://www.amazon.science/latest-news/how-amazon-robotics-is-working-on-new-ways-to-eliminate-the-need-for-barcodes",
            "description": "Why multimodal identification is a crucial step in automating item identification at Amazon scale.",
            "pubdate": "Fri, 09 Dec 2022 12:55:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                9
            ],
            "email_sent": true
        },
        "AmazonNext program hosts final project presentations at Virginia HQ2": {
            "url": "https://www.amazon.science/academic-engagements/amazonnext-program-hosts-final-project-presentations-at-virginia-hq2",
            "description": "Program focuses on diversifying tech-industry talent.",
            "pubdate": "Fri, 23 Dec 2022 13:19:31 GMT",
            "pubdate_parsed": [
                2022,
                12,
                23
            ],
            "email_sent": true
        },
        "Improving automatic discrimination of logos with similar texts": {
            "url": "https://www.amazon.science/blog/improving-automatic-discrimination-of-logos-with-similar-texts",
            "description": "Combining contrastive training and selection of hard negative examples establishes new benchmarks.",
            "pubdate": "Tue, 27 Dec 2022 01:52:08 GMT",
            "pubdate_parsed": [
                2022,
                12,
                27
            ],
            "email_sent": true
        },
        "Stories that inspired us in 2022": {
            "url": "https://www.amazon.science/latest-news/stories-that-inspired-us-in-2022",
            "description": "From the remarkable story of Josh Miele and his passion for improving accessibility, to the challenges overcome by scientists and engineers getting Alexa to work in space, these five stories touched both emotional and intellectual chords.",
            "pubdate": "Fri, 30 Dec 2022 12:00:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                30
            ],
            "email_sent": true
        },
        "Amazon and IIT Bombay launch multiyear collaboration": {
            "url": "https://www.amazon.science/news-and-features/amazon-and-iit-bombay-launch-multiyear-collaboration",
            "description": "Initiative will advance artificial intelligence and machine learning research within speech, language, and multimodal-AI domains.",
            "pubdate": "Mon, 27 Mar 2023 13:50:39 GMT",
            "pubdate_parsed": [
                2023,
                3,
                27
            ],
            "email_sent": true
        }
    },
    "The Berkeley Artificial Intelligence Research Blog": {
        "Unsupervised Skill Discovery with Contrastive Intrinsic Control": {
            "url": "http://bair.berkeley.edu/blog/2022/02/23/cic/",
            "description": "<!--\nThese are comments in HTML. The above header text is needed to format the\ntitle, authors, etc. The \"example_post\" is an example representative image (not\nGIF) that we use for each post for tweeting (see below as well) and for the\nemails to subscribers. Please provide this image (and any other images and\nGIFs) in the blog to the BAIR Blog editors directly.\n\nThe text directly below gets tweets to work. Please adjust according to your\npost.\n\nThe `static/blog` directory is a location on the blog server which permanently\nstores the images/GIFs in BAIR Blog posts. Each post has a subdirectory under\nthis for its images (titled `example_post` here, please change).\n\nKeeping the post visbility as False will mean the post is only accessible if\nyou know the exact URL.\n\nYou can also turn on Disqus comments, but we recommend disabling this feature.\n-->\n\n<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<!--\nThe actual text for the post content appears below.  Text will appear on the\nhomepage, i.e., https://bair.berkeley.edu/blog/ but we only show part of the\nposts on the homepage. The rest is accessed via clicking 'Continue'. This is\nenforced with the `more` excerpt separator.\n-->\n\n<p><img alt=\"Main Image\" src=\"https://bair.berkeley.edu/static/blog/cic/img1.png\" /></p>\n\n<p>Unsupervised Reinforcement Learning (RL), where RL agents pre-train with self-supervised rewards, is an emerging paradigm for developing RL agents that are capable of generalization. Recently, we released the Unsupervised RL Benchmark (URLB) which we covered in a <a href=\"https://bair.berkeley.edu/blog/2021/12/15/unsupervised-rl/\">previous post</a>. URLB benchmarked many unsupervised RL algorithms across three categories \u2014 competence-based, knowledge-based, and data-based algorithms. A surprising finding was that competence-based algorithms significantly underperformed other categories. In this post we will demystify what has been holding back competence-based methods and introduce Contrastive Intrinsic Control (CIC), a new competence-based algorithm that is the first to achieve leading results on URLB.</p>\n\n<!--more-->\n\n<h2 id=\"results-from-benchmarking-unsupervised-rl-algorithms\">Results from benchmarking unsupervised RL algorithms</h2>\n\n<p>To recap, competence-based methods (which we will cover in detail) maximize the mutual information between states and skills (e.g. DIAYN), knowledge-based methods maximize the error of a predictive model (e.g. Curiosity), and data-based methods maximize the diversity of observed data (e.g. APT). Evaluating these algorithms on URLB by reward-free pre-training for 2M steps followed by 100k steps of finetuning across 12 downstream tasks, we previously found the following stack ranking of algorithms from the three categories.</p>\n\n<p><img alt=\"URLB results\" src=\"https://bair.berkeley.edu/static/blog/cic/img2.png\" /></p>\n\n<p>In the above figure competence-based methods (in green) do substantially worse than the other two types of unsupervised RL algorithms. Why is this the case and what can we do to resolve it?</p>\n\n<h2 id=\"competence-based-exploration\">Competence-based exploration</h2>\n\n<p>As a quick primer, competence-based algorithms maximize the mutual information between some observed variable such as a state and a latent skill vector, which is usually sampled from noise.</p>\n\n<p><img alt=\"Competence-based Exploration\" src=\"https://bair.berkeley.edu/static/blog/cic/img3.png\" /></p>\n\n<p>The mutual information is usually an intractable quantity and since we want to maximize it, we are usually better off maximizing a variational lower bound.</p>\n\n<p><img alt=\"Mutual Info Decomposition\" src=\"https://bair.berkeley.edu/static/blog/cic/img4.png\" /></p>\n\n<p>The quantity <code class=\"language-plaintext highlighter-rouge\">q(z|\\tau)</code> is referred to as the discriminator. In prior works, the discriminators are either classifiers over discrete skills or regressors over continuous skills. The problem is that classification and regression tasks need an exponential number of diverse data samples to be accurate. In simple environments where the number of potential behaviors is small, current competence-based methods work but not in environments where the set of potential behaviors is large and diverse.</p>\n\n<h2 id=\"how-environment-design-influences-performance\">How environment design influences performance</h2>\n\n<p>To illustrate this point, let\u2019s run three algorithms on the OpenAI Gym and DeepMind Control (DMC) Hopper. Gym Hopper resets when the agent loses balance while DMC episodes have fixed length regardless if the agent falls over. By resetting early, Gym Hopper constrains the agent to a small number of behaviors that can be achieved by remaining balanced. We run three algorithms \u2014 DIAYN and ICM, popular competence-based and knowledge-based algorithms, as well as a \u201cFixed\u201d agent which gets a reward of +1 for each timestep, and measure the zero-shot extrinsic reward for hopping during self-supervised pre-training.</p>\n\n<p><img alt=\"OpenAI Gym vs DMC\" src=\"https://bair.berkeley.edu/static/blog/cic/img5.png\" /></p>\n\n<p>On OpenAI Gym both DIAYN and the Fixed agent receive higher extrinsic rewards relative to ICM, but on the DeepMind Control Hopper both algorithms collapse. The only significant difference between the two environments is that OpenAI Gym resets early whereas DeepMind Control does not. This supports the hypothesis that when an environment supports many behaviors prior competence-based approaches struggle to learn useful skills.</p>\n\n<p>Indeed, if we visualize behaviors learned by DIAYN on other DeepMind Control environments, we see that it learns a small set of static skills.</p>\n\n<h3 id=\"prior-methods-fail-to-learn-diverse-behaviors\">Prior methods fail to learn diverse behaviors</h3>\n\n<p><img alt=\"diaynw1.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynw1.gif\" />\n<img alt=\"diaynw2.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynw2.gif\" />\n<img alt=\"diaynw3.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynw3.gif\" />\n<img alt=\"diaynq1.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynq1.gif\" />\n<img alt=\"diaynq2.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynq2.gif\" />\n<img alt=\"diaynq3.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/diaynq3.gif\" /></p>\n\n<p><em>Skills learned by DIAYN after 2M steps of training.</em></p>\n\n<h2 id=\"effective-competence-based-exploration-with-contrastive-intrinsic-control-cic\">Effective competence-based exploration with Contrastive Intrinsic Control (CIC)</h2>\n\n<p>As illustrated in the above example - complex environments support a large number of skills and we therefore need discriminators capable of supporting large skill spaces. This tension between the need to support large skill spaces and the limitation of current discriminators leads us to propose Contrastive Intrinsic Control (CIC).</p>\n\n<p>Contrastive Intrinsic Control (CIC) introduces a new contrastive density estimator to approximate the conditional entropy (the discriminator). Unlike visual contrastive learning, this contrastive objective operates over <strong>state transitions</strong> and <strong>skill vectors</strong>. This allows us to bring powerful representation learning machinery from vision to unsupervised skill discovery.</p>\n\n<p><img alt=\"CIC Decomposition\" src=\"https://bair.berkeley.edu/static/blog/cic/img6.png\" /></p>\n\n<p>For a practical algorithm, we use the CIC contrastive skill learning as an auxiliary loss during pre-training. The self-supervised intrinsic reward is the value of the entropy estimate computed over the CIC embeddings. We also analyze other forms of intrinsic rewards in the paper, but this simple variant performs well with minimal complexity. The CIC architecture has the following form:</p>\n\n<p><img alt=\"CIC Architecture\" src=\"https://bair.berkeley.edu/static/blog/cic/img7.png\" /></p>\n\n<p>Qualitatively the behaviors from CIC after 2M steps of pre-training are quite diverse.</p>\n\n<h3 id=\"diverse-behaviors-learned-with-cic\">Diverse Behaviors learned with CIC</h3>\n\n<p><img alt=\"cicw1.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicw1.gif\" />\n<img alt=\"cicw2.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicw2.gif\" />\n<img alt=\"cicw3.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicw3.gif\" />\n<img alt=\"cicq1.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicq1.gif\" />\n<img alt=\"cicq2.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicq2.gif\" />\n<img alt=\"cicq3.gif\" src=\"https://bair.berkeley.edu/static/blog/cic/cicq3.gif\" /></p>\n\n<p><em>Skills learned by CIC after 2M steps of training.</em></p>\n\n<p>With explicit exploration through the state-transition entropy term and the contrastive skill discriminator for representation learning CIC adapts extremely efficiently to downstream tasks - outperforming prior competence-based approaches by <strong>1.78x</strong> and all prior exploration methods by <strong>1.19x</strong> on state-based URLB.</p>\n\n<p><img alt=\"Results\" src=\"https://bair.berkeley.edu/static/blog/cic/img8.png\" /></p>\n\n<p>We provide more information in the CIC paper about how architectural details and skill dimension affect the performance of the CIC paper. The main takeaway from CIC is that there is nothing wrong with the competence-based objective of maximizing mutual information. However, what matters is how well we approximate this objective, especially in environments that support a large number of behaviors. CIC is the first competence-based algorithm to achieve leading performance on URLB. Our hope is that our approach encourages other researchers to work on new unsupervised RL algorithms</p>\n\n<h2 id=\"links\">Links</h2>\n\n<p><strong>Paper:</strong> <a href=\"https://arxiv.org/abs/2202.00161\">CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery</a>\nMichael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, Pieter Abbeel</p>\n\n<p><strong>Code:</strong> <a href=\"https://github.com/rll-research/cic\">https://github.com/rll-research/cic</a></p>",
            "pubdate": "Wed, 23 Feb 2022 04:00:00 -0800",
            "pubdate_parsed": [
                2022,
                2,
                23
            ],
            "email_sent": true
        },
        "Accelerating Ukraine Intelligence Analysis with Computer Vision on Synthetic Aperture Radar Imagery": {
            "url": "http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-maers/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- body -->\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/ukraine-clouds-optim.gif\" width=\"120%\" />\n    <br />\n    <i><b>Figure 1:</b> Airmass measurements (clouds) over Ukraine from February 18, 2022 - March 01, 2022 from the SEVIRI instrument. Data accessed via the <a href=\"https://view.eumetsat.int/productviewer?v=default\">EUMETSAT Viewer</a>.</i>\n</p>\n\n<p>Satellite imagery is a critical source of information during the current invasion of Ukraine. Military strategists, journalists, and researchers use this imagery to make decisions, unveil violations of international agreements, and inform the public of the stark realities of war. With Ukraine experiencing a large amount of cloud cover and attacks often occuring during night-time, many forms of satellite imagery are hindered from seeing the ground. <a href=\"https://earthdata.nasa.gov/learn/backgrounders/what-is-sar\">Synthetic Aperture Radar (SAR)</a> imagery penetrates cloud cover, but requires special training to interpret. Automating this tedious task would enable real-time insights, but current computer vision methods developed on typical RGB imagery do not properly account for the phenomenology of SAR. This leads to suboptimal performance on this critical modality. Improving the access to and availability of SAR-specific methods, codebases, datasets, and pretrained models will benefit intelligence agencies, researchers, and journalists alike during this critical time for Ukraine.</p>\n\n<p>In this post, we present a baseline method and pretrained models that enable the interchangeable use of RGB and SAR for downstream classification, semantic segmentation, and change detection pipelines.</p>\n\n<!--more-->\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>We live in a rapidly changing world, one that experiences natural disasters, civic upheaval, war, and all sorts of chaotic events which leave unpredictable\u2014and often permanent\u2014marks on the face of the planet. Understanding this change has historically been difficult. Surveyors were sent out to explore our new reality, and their distributed findings were often noisily integrated into a source of reality. Maintaining a constant state of vigilance has been a goal of mankind since we were able to conceive such a thought, all the way from when <a href=\"https://time.com/longform/aerial-photography-drones-history/\">Nadar took the first aerial photograph</a> to when <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0273117715001623\">Sputnik 1\u2019s radio signals were used to analyze the ionosphere</a>.</p>\n\n<p>Vigilance, or to the French, <em>surveillance</em>, has been a part of human history for millenia. As with any tool, it has been a double-edged sword. Historically, surveillance without checks and balances has been detrimental to society. Conversely, the proper and responsible surveillance has allowed us to learn deep truths about our world which have resulted in advances in the <a href=\"https://www.nasa.gov/mission_pages/icebridge/instruments/index.html\">scientific</a> and <a href=\"https://web.archive.org/web/20211001071654/https://news.un.org/en/story/2006/04/176152-un-launches-new-enhanced-tool-use-satellite-data-fighting-hunger-poverty\">humanitarian</a> domains. With the amount of satellites in orbit today, our understanding of the environment is updated almost daily. We have rapidly transitioned from having very little information to now having more data than we can meaningfully extract knowledge from. Storing this information, let alone understanding, is an engineering challenge that is of growing urgency.</p>\n\n<h2 id=\"machine-learning-and-remote-sensing\">Machine Learning and Remote Sensing</h2>\n\n<p>With <a href=\"https://datacenterfrontier.com/terabytes-from-space-satellite-imaging-is-filling-data-centers/\">hundreds of terabytes</a> of data being downlinked from satellites to data centers every day, gaining knowledge and actionable insights from that data with manual processing has already become an impossible task. The most widely used form of remote sensing data is electro-optical (EO) satellite imagery. EO imagery is commonplace\u2014anyone who has used Google Maps or similar mapping software has interacted with EO satellite imagery.</p>\n\n<p>Machine learning (ML) on EO imagery is used in a wide variety of scientific and commercial applications. From <a href=\"https://journals.ametsoc.org/view/journals/hydr/17/3/jhm-d-15-0075_1.xml\">improving precipitation predictions</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S0924271618300479\">analyzing human slavery by identifying brick kilns</a>, to <a href=\"https://blog.google/products/maps/google-maps-101-ai-power-new-features-io-2021/\">classifying entire cities to improve traffic routing</a>, the outputs of ML on EO imagery have been integrated into almost every facet of human society.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/bridge-eo-kyiv.jpeg\" width=\"100%\" />\n    <br />\n    <i><a href=\"https://www.cnn.com/europe/live-news/ukraine-russia-putin-news-03-03-22/h_ed1c79ce964585a1d044c2dd50e2997a\"><b>Figure 2:</b> VHR EO imagery over the Kyiv region as acquired by Maxar on February 28, 2022</a>.</i>\n</p>\n\n<p>Commonly used satellite constellations for EO imagery include the <a href=\"https://landsat.gsfc.nasa.gov/\">Landsat</a> series of satellites operated by the United States Geological Survey and the <a href=\"https://sentinel.esa.int/web/sentinel/missions/sentinel-2\">Copernicus Sentinel-2</a> constellation operated by the European Space Agency. These constellations provide imagery at resolutions between 10-60 meters which is good enough for many use cases, but preclude the observation of finer details.</p>\n\n<h2 id=\"the-advent-of-very-high-resolution-commercial-electro-optical-satellite-imagery\">The Advent of Very High Resolution, Commercial Electro-Optical Satellite Imagery</h2>\n\n<p>Over the last few years, very high resolution (VHR) EO imagery has been made available through a variety of commercial sources. Ranging from between 0.3 - 2.0 meter resolution<sup id=\"fnref:1\"><a class=\"footnote\" href=\"https://bair.berkeley.edu/blog/feed.xml#fn:1\" rel=\"footnote\">1</a></sup>, companies such as <a href=\"https://www.planet.com/\">Planet</a>, <a href=\"https://www.maxar.com/\">Maxar</a>, <a href=\"https://www.airbus.com/en/products-services/space/earth-observation\">Airbus</a>, and others are providing extremely precise imagery with high revisit rates, <a href=\"https://www.fastcompany.com/40498033/every-day-this-satellite-company-takes-a-snapshot-of-the-entire-planet\">imaging the entire planet every day</a>.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maxar-ships.jpeg\" width=\"100%\" />\n    <br />\n    <i><a href=\"https://blog.maxar.com/earth-intelligence/2022/enhancing-maritime-domain-awareness-with-maxars-crows-nest-solution\"><b>Figure 3:</b> An example of Maxar VHR EO imagery showing floating production, storage and off-loading units and a tanker</a>.</i>\n</p>\n\n<p>The increased resolution provided by VHR imagery enables a litany of downstream use cases. <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/ldr.1094\">Erosion can be detected at finer scales</a>, and the <a href=\"https://xview2.org/\">building damage can be classified after natural disasters</a><strong>.</strong></p>\n\n<p>Machine learning methods have had to adapt in response to VHR satellite imagery. With an increased acuity, the amount of pixels and the <a href=\"http://xviewdataset.org/\">amount of classes that can be discerned</a> has increased by orders of magnitude. Computer vision research has responded by <a href=\"https://www.nature.com/articles/s41467-021-24638-z\">reducing the computational cost to learn efficient representation of satellite imagery</a>, creating <a href=\"https://arxiv.org/abs/2108.09186\">methods to alleviate the increased burden on labelers</a>, and even <a href=\"https://arxiv.org/abs/2111.08872\">engineering large software frameworks</a> to allow computer vision practitioners to handle this abundant source of imagery.</p>\n\n<p>In general, existing computer vision methods on other, non-aerial RGB imagery <a href=\"https://arxiv.org/abs/1510.00098\">transfer very well</a> to satellite imagery. This has allowed commercial VHR imagery to be immediately useful with highly accurate results.</p>\n\n<h2 id=\"the-problem-with-electro-optical-imagery\">The Problem with Electro-Optical Imagery</h2>\n\n<p>For highly turbulent and risky situations such as war and natural disasters, having constant, reliable access to the Earth is paramount.  Unfortunately, EO imagery cannot solve all of our surveillance needs. EO can only detect light sources during daytime, and as it turns out, <a href=\"https://earthobservatory.nasa.gov/images/85843/cloudy-earth\">nearly 2/3rds of the Earth is covered by clouds at any given time</a>. Unless you care about clouds, this blockage of the surface of the planet is problematic when understanding what happens on the ground is of critical importance. Machine learning methods attempt to sidestep this problem by <a href=\"https://hal-enpc.archives-ouvertes.fr/hal-01832797/document\">predicting what the world would look like without clouds</a>. However, the loss of information is fundamentally irrecoverable.</p>\n\n<h2 id=\"synthetic-aperture-radar-imagery\">Synthetic Aperture Radar Imagery</h2>\n\n<p>Synthetic aperture radar (SAR) imagery is an active form of remote sensing in which a satellite transmits pulses of microwave radar waves down to the surface of the Earth. These radar waves reflect off the ground and any objects on it and are returned back to the satellite. By processing these pulses over time and space, a SAR image is formed where each pixel is the superposition of different radar scatters.</p>\n\n<p>Radar waves penetrate clouds, and since the satellite is actively producing the radar waves, it illuminates the surface of the Earth even during the night. Synthetic aperture radar has a wide variety of uses, being used to <a href=\"https://ieeexplore.ieee.org/abstract/document/134087\">estimate the roughness of the Earth</a>, <a href=\"https://unitar.org/about/news-stories/news/unosat-introduces-ai-its-flood-rapid-mapping-operations-benefit-national-disaster-management\">mapping the extent of flooding over large areas</a>, and to <a href=\"https://iuu.xview.us/\">detect the presence of illegal fishing vessels in protected waters</a>.</p>\n\n<p>There are multiple SAR satellite constellations in operation at the moment. The <a href=\"https://sentinel.esa.int/web/sentinel/missions/sentinel-1\">Copernicus Sentinel-1</a> constellation provides imagery to the public at large with resolutions ranging from 10 - 80 meters (10 meter imagery being the most common. Most commercial SAR providers, such as <a href=\"https://www.iceye.com/\">ICEYE</a> and <a href=\"https://www.capellaspace.com/\">Capella Space</a>, provide imagery down to 0.5 meter resolution. In upcoming launches, other commercial vendors aim to produce SAR imagery with sub-0.5 meter resolution with high revisit rates as satellite constellations grow and government regulations evolve.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/sar-ukraine-belarus.jpg\" width=\"100%\" />\n    <br />\n    <i><a href=\"https://www.wired.co.uk/article/ukraine-russia-satellites\"><b>Figure 4:</b> A VHR SAR image provided by Capella Space over the Ukraine-Belarus border</a>.</i>\n</p>\n\n<h2 id=\"the-wacky-world-of-synthetic-aperture-radar-imagery\">The Wacky World of Synthetic Aperture Radar Imagery</h2>\n\n<p>While SAR imagery, at a quick glance, may look very similar to EO imagery, the underlying physics is quite different, which leads to many interesting effects in the imagery product which can be counterintuitive and incompatible with modern computer vision. Three common effects are termed polarization, layover, and multi-path effects.</p>\n\n<p>Radar antennas on SAR satellites often transmit polarized radar waves. The direction of polarization is the orientation of the wave\u2019s electric field. Objects on the ground exhibit different responses to the different polarizations of radar waves. Therefore, SAR satellites often operate in dual or quad-polarization modes, broadcasting horizontally (H) or vertically (V) polarized waves and reading either polarization back, resulting in HH, HV, VH, and VV bands. You can contrast this with RGB bands in EO imagery, but the fundamental physics are different.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/sentinel1-vv-vh.png\" width=\"100%\" />\n    <br />\n    <i><b>Figure 5:</b> Difference between VH (left) and VV (right) polarizations over the same region in Dnipro, Ukraine from Sentinel-1 radiometric terrain corrected imagery. As seen here, the radar returns in corresponding local regions can be different.</i>\n</p>\n\n<p>Layover is an effect in which radar beams reach the top of a structure before they reach the bottom, resulting in the top of the object being presented as overlapping with the bottom. This happens when objects are particularly tall. Visually, tall buildings appear as if they are laying on their side, while mountains will have their peaks intersecting with their bases.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/capella-layover.jpeg\" width=\"100%\" />\n    <br />\n    <i><a href=\"https://twitter.com/capellaspace/status/1367865023587049474/photo/1\"><b>Figure 6</b>: Example of layover in Capella\u2019s VHR SAR imagery.</a> The upper portion of the stadium is intersecting, seemingly, with the parking lot behind it.</i>\n</p>\n\n<p>Multi-path effects occur when radar waves reflect off of objects on the ground and incur multiple bounces before returning to the SAR sensor. Multi-path effects result in objects appearing in the imagery in various transformations in the resulting image. This effect can be seen everywhere in SAR imagery, but is particularly noticeable in urban areas, forests, and other dense environments.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/multipath.png\" width=\"100%\" />\n    <br />\n    <i><a href=\"https://discovery.ucl.ac.uk/id/eprint/10053908/\"><b>Figure 7:</b> Example of a multi-path effect on a bridge from oblique SAR imagery</a>.</i>\n</p>\n\n<p>Existing computer vision methods that are built on traditional RGB imagery are not built with these effects in mind. Object detectors trained on EO satellite imagery assume that a unique object will only appear once, or that the object will appear relatively similar in different contexts, rather than potentially mirrored or scattered or interwoven with surrounding objects. The very nature of occlusion and the vision principles underlying the assumptions of occlusion in EO imagery do not transfer to SAR. Taken together, existing computer vision techniques can transfer to SAR imagery, but with reduced performance and a set of systematic errors that can be addressed through SAR-specific methodology.</p>\n\n<p><strong>Computer Vision on SAR Imagery for Ukraine</strong></p>\n\n<p>Imagery analysts are currently relying on both EO and SAR imagery where available over Ukraine. When EO imagery is available, existing computer vision tooling built for that modality is used to expedite the process of intelligence gathering. However, when only SAR imagery is available, these toolchains cannot be used. Imagery analysts have to resort to manual analysis which is time consuming and can be prone to mistakes. This topic is being explored by some other institutions internationally, however, it still remains an understudied area with respect to the amount of data available.</p>\n\n<p>At Berkeley AI Research, we have created an initial set of methods and models that have learned robust representations for RGB, SAR, and co-registered RGB + SAR imagery from the publicly released <a href=\"https://bigearth.net\">BigEarthNet-MM dataset</a> and the data from <a href=\"https://www.capellaspace.com/community/capella-open-data/\">Capella\u2019s Open Data</a>, which consists of both RGB and SAR imagery. As such, using our models, imagery analysts are able to interchangeably use RGB, SAR, or co-registered RGB+SAR imagery for downstream tasks such as image classification, semantic segmentation, object detection, or change detection.</p>\n\n<p>Given that SAR is a phenomenologically different data source than EO imagery, we have found that the Vision Transformer (ViT) is a particularly effective architecture for representation learning with SAR as it removes the scale and shift invariant inductive biases built into convolutional neural networks. Our top performing method, MAERS, for representation learning on RGB, SAR, and co-registered RGB + SAR builds upon the <a href=\"https://arxiv.org/abs/2111.06377\">Masked Autoencoder</a> (MAE) recently introduced by He et. al., where the network learns to encode the input data by taking a masked version of the data as input, encoding the data, and then learning to decode the data in such a way that it reconstructs the unmasked input data.</p>\n\n<p>Contrary to popular <a href=\"https://arxiv.org/abs/2002.05709\">classes of contrastive learning techniques</a>, the MAE does not presuppose certain augmentation invariances in the data that may be incorrect for SAR features. Instead, it solely relies on reconstructing the original input, which is agnostic to RGB, SAR, or co-registered modalities. As shown in Figure 8, MAERS further extends MAE by learning independent input projection layers for RGB, SAR, and RGB+SAR channels, encoding the output of these projected layers using a shared ViT, and then decoding to the RGB, SAR, or RGB+SAR channels using independent output projection layers. The input projection layers and shared ViT can then be transferred to downstream tasks, such as object detection or change detection, where the input encoder can then take RGB, SAR, or RGB+SAR as input.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maers.png\" width=\"100%\" />\n    <br />\n    <i><b>Figure 8:</b> (top) A visualization of MAERS to learn a joint representation and encoder that can be used for a (bottom) downstream task, such as object detection on either, or both, modalities.</i>\n</p>\n\n<p>Learning representations for RGB, SAR, and co-registered modalities can benefit a range of downstream tasks, such as content-based image retrieval, classification, segmentation, and detection. To demonstrate the effectiveness of our learned representations, we perform experiments on the well-established benchmarks of 1) multi-label classification of co-registered EO and SAR scenes from the <a href=\"https://bigearth.net/\">BigEarthNet-MM dataset</a>, and 2) semantic segmentation on the VHR EO and SAR <a href=\"https://spacenet.ai/sn6-challenge/\">SpaceNet 6 dataset</a>.</p>\n\n<h2 id=\"multi-label-classification-on-bigearth-mm\">Multi-Label Classification on BigEarth-MM</h2>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maers-benmm.png\" width=\"100%\" />\n    <br />\n    <i><b>Figure 9:</b> (left) co-registered Sentinel-2 EO and Sentinel-1 SAR imagery are patchified and used to perform a multi-label classification task as specified by the BigEarth-MM challenge. A linear layer is added to our multi-modal encoder and then fine-tuned end-to-end.</i>\n</p>\n\n<p>MAERS is initialized with a set of ImageNet weights for a ViT-Base encoder, followed by pretraining on the BigEarthNet-MM dataset for 20 epochs with RGB, SAR, and RGB+SAR imagery. We append a single linear layer to the MAERS encoder and learn the multi-label classification task by fine-tuning the entire model for 20 epochs (linear probing experiments obtain similar results, as we will show in our upcoming paper). Our results are shown in Table 1. MAERS with fine-tuning outperforms the best RGB+SAR results as presented in the BigEarthNet-MM paper, and show that adapting the State-of-the-Art MAE architecture for representation learning for RGB, SAR, and RGB+SAR input modalities leads to State-of-the-Art results.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maers-table.png\" width=\"100%\" />\n    <br />\n    <i><b>Table 1:</b> Reported per-class F2 scores on the test set of BigEarthNet-MM.</i>\n</p>\n\n<h2 id=\"semantic-segmentation-on-vhr-eo-and-sar-spacenet-6\">Semantic Segmentation on VHR EO and SAR SpaceNet 6</h2>\n\n<p>We further experimented with transfer learning for a timely task that will aid imagery analysts aiming to understand the destruction in Ukraine: semantic segmentation of buildings footprints, which is a precursor task to performing building damage assessment. Building damage assessment is of direct interest to government officials, journalists, and human rights organizations aiming to understand the scope and severity of Russia\u2019s attacks against infrastructure and civilian populations.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maers-vhr-sar-segmentation.png\" width=\"100%\" />\n    <br />\n    <i><b>Figure 10:</b> Example of building SAR-based MAERS segmentation taken from SpaceNet6, where the image on the left shows the RGB image, and the image on the right shows the SAR image with overlaid segmentation results. The SAR image is displayed in false color with VV, VH, and VV/VH bands.</i>\n</p>\n\n<p>For this experiment, we used the SpaceNet 6 dataset as an open and public benchmark to illustrate the effectiveness of our learned representations for building footprint detection with VHR SAR from Capella Space. We used this encoder in tandem with the <a href=\"https://arxiv.org/abs/1807.10221\">UperNet</a> architecture for semantic segmentation. Figure 11 shows the IoU performance of segmenting building footprints in a held-out validation component of the SpaceNet 6 with <strong>only SAR input imagery</strong>, on a segmentation model that was trained to use either SAR or RGB imagery. The MAERS pretrained model leads to a ~13 point improvement compared to training the RGB+SAR model from scratch or adapting ImageNet weights with the exact same architecture.</p>\n\n<p>This demonstrates that MAERS can learn robust RGB+SAR representations that allow a practitioner to use EO or SAR imagery interchangeably to accomplish downstream tasks. It is important to note that the phenomenology of SAR imagery is not fully conducive for building segmentation and that using EO imagery for this task leads to IoU scores &gt; 90. This leaves a substantial gap yet to be covered by SAR techniques, something we hope to cover in our following paper. However, getting this performance out of SAR is essential when environmental conditions are not conducive to EO imagery capture.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maers/maers-segmentation-iou.png\" width=\"100%\" />\n    <br />\n    <i><b>Figure 11:</b> Building segmentation IoU on the SpaceNet 6 Challenge, using an UperNet segmentation model with a ViT backbone. MAERS pretraining leads to ~13 point gain in IoU performance compared to training from scratch or adapting ImageNet pretrained weights.</i>\n</p>\n\n<p>These results are preliminary, but compelling. We will follow up this effort with a publication with a detailed set of experiments and benchmarks. Furthermore, we will aid in the transition of our models to our humanitarian partners to enable them to perform change detection over residential and other civilian areas to enable better tracking of war crimes being committed in Ukraine.</p>\n\n<p>These models are created with the goal of increasing the efficacy of organizations involved in humanitarian missions that are keeping a watchful eye on the war in Ukraine. However, as with any technology, it is our responsibility to understand how this technology could be misused. Therefore, we have designed these models with input from partners who perform intelligence and imagery analysis in humanitarian settings. By taking into account their thoughts, comments, and critiques, we are releasing a capability we are confident will be used for the good of humanity and with processes which dictate their safe and responsible use.</p>\n\n<h2 id=\"call-to-action\">Call to Action</h2>\n\n<p>As citizens of free democracies who develop technologies which help us make sense of the complicated, chaotic, and counter-intuitive world that we live in, we have a responsibility to act when acts of injustice occur. Our colleagues and friends in Ukraine are facing extreme uncertainties and danger. We possess skills in the cyber domain that can aid in the fight against Russian forces. By focusing our time and efforts, whether that be through targeted research or volunteering our time in <a href=\"https://ukrainenow.org/\">helping keep track of processing times at border crossings</a>, we can make a small dent in an otherwise difficult situation.</p>\n\n<p>We urge our fellow computer scientists to partner with government and humanitarian organizations and listen to their needs as difficult times persist. Simple things can make large differences.</p>\n\n<h2 id=\"model-and-weights\">Model and Weights</h2>\n\n<p>The models are not being made publicly accessible at this time. We are releasing our models to qualified researchers and partners through this <a href=\"https://forms.gle/8rB4wvzair1t8qqz9\">form</a>. Full distribution will follow once we have completed a thorough assessment of our models.</p>\n\n<h2 id=\"acknowledgements\">Acknowledgements</h2>\n\n<p>Thank you to <a href=\"https://www.diu.mil/team/Steven-Butow\">Gen. Steve Butow</a> and  <a href=\"https://scholar.google.com/citations?user=bJ51bBQAAAAJ&amp;hl=en\">Dr. Nirav Patel</a> at the Department of Defense\u2019s <a href=\"https://diu.mil/\">Defense Innovation Unit</a> for reviewing this post and providing their expertise on the future of commercial SAR constellations.</p>\n\n<!-- Footnotes themselves at the bottom. -->\n<h2 id=\"footnotes\">Footnotes</h2>\n\n<div class=\"footnotes\">\n  <ol>\n    <li id=\"fn:1\">\n\n      <p>It\u2019s interesting to note that the definition of VHR imagery has changed over time. In the 80s, <a href=\"https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JC093iC06p06735\">20 kilometer resolution was \u201cVHR\u201d</a>. Perhaps, in the future, 0.3m resolution imagery will no longer be VHR.\u00a0<a class=\"reversefootnote\" href=\"https://bair.berkeley.edu/blog/feed.xml#fnref:1\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>",
            "pubdate": "Mon, 21 Mar 2022 05:00:00 -0700",
            "pubdate_parsed": [
                2022,
                3,
                21
            ],
            "email_sent": true
        },
        "Offline RL Made Easier: No TD Learning, Advantage Reweighting, or Transformers": {
            "url": "http://bair.berkeley.edu/blog/2022/04/20/rvs/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/rvs/rvs-kitchen.gif\" width=\"70%\" />\n<br />\n<i>A demonstration of the RvS policy we learn with just supervised learning and a depth-two MLP. It uses no TD learning, advantage reweighting, or Transformers!</i>\n</p>\n\n<p>Offline reinforcement learning (RL) is conventionally approached using value-based methods based on temporal difference (TD) learning. However, many recent algorithms reframe RL as a supervised learning problem. These algorithms learn <em>conditional policies</em> by conditioning on goal states (Lynch <em>et al.</em>, 2019; Ghosh <em>et al.</em>, 2021), reward-to-go (Kumar <em>et al.</em>, 2019; Chen <em>et al.</em>, 2021), or language descriptions of the task (Lynch and Sermanet, 2021).</p>\n\n<p>We find the simplicity of these methods quite appealing. If supervised learning is enough to solve RL problems, then offline RL could become widely accessible and (relatively) easy to implement. Whereas TD learning must delicately balance an actor policy with an ensemble of critics, these supervised learning methods train just one (conditional) policy, and nothing else!</p>\n\n<!--more-->\n\n<p>So, how can we use these methods to effectively solve offline RL problems? Prior work puts forward a number of clever tips and tricks, but these tricks are sometimes contradictory, making it challenging for practitioners to figure out how to successfully apply these methods. For example, RCPs (Kumar <em>et al.</em>, 2019) require carefully reweighting the training data, GCSL (Ghosh <em>et al.</em>, 2021) requires iterative, online data collection, and Decision Transformer (Chen <em>et al.</em>, 2021) uses a Transformer sequence model as the policy network.</p>\n\n<p>Which, if any, of these hypotheses are correct? Do we need to reweight our training data based on estimated advantages? Are Transformers necessary to get a high-performing policy? Are there other critical design decisions that have been left out of prior work?</p>\n\n<p>Our work aims to answer these questions by trying to identify the <em>essential elements</em> of offline RL via supervised learning. We run experiments across 4 suites, 26 environments, and 8 algorithms. When the dust settles, we get competitive performance in every environment suite we consider using remarkably simple elements. The video above shows the complex behavior we learn using just supervised learning with a depth-two MLP \u2013 no TD learning, data reweighting, or Transformers!</p>\n\n<h1 id=\"rl-via-supervised-learning\">RL via Supervised Learning</h1>\n\n<p>Let\u2019s begin with an overview of the algorithm we study. While lots of prior work (Kumar <em>et al.</em>, 2019; Ghosh <em>et al.</em>, 2021; and Chen <em>et al.</em>, 2021) share the same core algorithm, it lacks a common name. To fill this gap, we propose the term <em>RL via Supervised Learning (RvS)</em>. We are not proposing any new algorithm but rather showing how prior work can be viewed from a unifying framework; see Figure 1.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/rvs/rvs-data.png\" width=\"70%\" />\n<br />\n<i><b>Figure 1.</b> (Left) A replay buffer of experience (Right) Hindsight relabelled training data</i>\n</p>\n\n<p>RL via Supervised Learning takes as input a replay buffer of experience including states, actions, and outcomes. The outcomes can be an arbitrary function of the trajectory, including a goal state, reward-to-go, or language description. Then, RvS performs hindsight relabeling to generate a dataset of state, action, and outcome triplets. The intuition is that the actions that are observed provide supervision for the outcomes that are reached. With this training dataset, RvS performs supervised learning by maximizing the likelihood of the actions given the states and outcomes. This yields a conditional policy that can condition on arbitrary outcomes at test time.</p>\n\n<h1 id=\"experimental-results\">Experimental Results</h1>\n\n<p>In our experiments, we focus on the following three key questions.</p>\n<ol>\n  <li>Which design decisions are critical for RL via supervised learning?</li>\n  <li>How well does RL via supervised learning actually work? We can do RL via supervised learning, but would using a different offline RL algorithm perform better?</li>\n  <li>What type of outcome variable should we condition on? (And does it even matter?)</li>\n</ol>\n\n<h1 id=\"network-architecture\">Network Architecture</h1>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/rvs/rvs-architecture.png\" width=\"70%\" />\n<br />\n<i><b>Figure 2.</b> Our RvS architecture. A depth-two MLP suffices in every environment suite we consider.</i>\n</p>\n\n<p>We get good performance using just a depth-two multi-layer perceptron. In fact, this is competitive with all previously published architectures we\u2019re aware of, including a Transformer sequence model. We just concatenate the state and outcome before passing them through two fully-connected layers (see Figure 2). The keys that we identify are having a network with large capacity \u2013 we use width 1024 \u2013 as well as dropout in some environments. We find that this works well without reweighting the training data or performing any additional regularization.</p>\n\n<h1 id=\"overall-performance\">Overall Performance</h1>\n\n<p>After identifying these key design decisions, we study the overall performance of RvS in comparison to previous methods. This blog post will overview results from two of the suites we consider in the paper.</p>\n\n<h1 id=\"d4rl-gym\">D4RL Gym</h1>\n\n<p><img align=\"right\" hspace=\"20\" src=\"https://bair.berkeley.edu/static/blog/rvs/gym-env.png\" width=\"40%\" />\nThe first suite is D4RL Gym, which contains the standard MuJoCo halfcheetah, hopper, and walker robots. The challenge in D4RL Gym is to learn locomotion policies from offline datasets of varying quality. For example, one offline dataset contains rollouts from a totally random policy. Another dataset contains rollouts from a \u201cmedium\u201d policy trained partway to convergence, while another dataset is a mixture of rollouts from medium and expert policies.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/rvs/gym-performance.png\" width=\"70%\" />\n<br />\n<i><b>Figure 3.</b> Overall performance in D4RL Gym.</i>\n</p>\n\n<p>Figure 3 shows our results in D4RL Gym. RvS-R is our implementation of RvS conditioned on rewards (illustrated in Figure 2). On average across all 12 tasks in the suite, we see that RvS-R, which uses just a depth-two MLP, is competitive with Decision Transformer (DT; Chen <em>et al.</em>, 2021). We also see that RvS-R is competitive with the methods that use temporal difference (TD) learning, including CQL-R (Kumar <em>et al.</em>, 2020), TD3+BC (Fujimoto <em>et al.</em>, 2021), and Onestep (Brandfonbrener <em>et al.</em>, 2021). However, the TD learning methods have an edge because they perform especially well on the random datasets. This suggests that one might prefer TD learning over RvS when dealing with low-quality data.</p>\n\n<h1 id=\"d4rl-antmaze\">D4RL AntMaze</h1>\n\n<p><img align=\"right\" hspace=\"20\" src=\"https://bair.berkeley.edu/static/blog/rvs/antmaze-env.png\" width=\"27%\" />\nThe second suite is D4RL AntMaze. This suite requires a quadruped to navigate to a target location in mazes of varying size. The challenge of AntMaze is that many trajectories contain only pieces of the full path from the start to the goal location. Learning from these trajectories requires stitching together these pieces to get the full, successful path.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/rvs/antmaze-performance.png\" width=\"70%\" />\n<br />\n<i><b>Figure 4.</b> Overall performance in D4RL AntMaze.</i>\n</p>\n\n<p>Our AntMaze results in Figure 4 highlight the importance of the conditioning variable. Whereas conditioning RvS on rewards (RvS-R) was the best choice of the conditioning variable in D4RL Gym, we find that in D4RL AntMaze, it is much better to condition RvS on $(x, y)$ goal coordinates (RvS-G). When we do this, we see that RvS-G compares favorably to TD learning! This was surprising to us because TD learning explicitly performs dynamic programming using the Bellman equation.</p>\n\n<p>Why does goal-conditioning perform better than reward conditioning in this setting? Recall that AntMaze is designed so that simple imitation is not enough: optimal methods must stitch together parts of suboptimal trajectories to figure out how to reach the goal. In principle, TD learning can solve this with <em>temporal</em> compositionality. With the Bellman equation, TD learning can combine a path from A to B with a path from B to C, yielding a path from A to C. RvS-R, along with other behavior cloning methods, does not benefit from this temporal compositionality. We hypothesize that RvS-G, on the other hand, benefits from <em>spatial compositionality</em>. This is because, in AntMaze, the policy needed to reach one goal is similar to the policy needed to reach a nearby goal. We see correspondingly that RvS-G beats RvS-R.</p>\n\n<p>Of course, conditioning RvS-G on $(x, y)$ coordinates represents a form of prior knowledge about the task. But this also highlights an important consideration for RvS methods: the choice of conditioning information is critically important, and it may depend significantly on the task.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>Overall, we find that in a diverse set of environments, RvS works well without needing any fancy algorithmic tricks (such as data reweighting) or fancy architectures (such as Transformers). Indeed, our simple RvS setup can match, and even outperform, methods that utilize (conservative) TD learning. The keys for RvS that we identify are model capacity, regularization, and the conditioning variable.</p>\n\n<p>In our work, we handcraft the conditioning variable, such as $(x, y)$ coordinates in AntMaze. Beyond the standard offline RL setup, this introduces an additional assumption, namely, that we have some prior information about the structure of the task. We think an exciting direction for future work would be to remove this assumption by automating the learning of the goal space.</p>\n\n<hr />\n\n<h1 id=\"reproducing-experiments\">Reproducing Experiments</h1>\n\n<p>We packaged our <a href=\"https://github.com/scottemmons/rvs\">open-source code</a> so that it can automatically handle all the dependencies for you. After downloading the code, you can run these five commands to reproduce our experiments:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>docker build -t rvs:latest .\ndocker run -it --rm -v $(pwd):/rvs rvs:latest bash\ncd rvs\npip install -e .\nbash experiments/launch_gym_rvs_r.sh\n</code></pre></div></div>\n\n<hr />\n\n<p>This post is based on the paper:</p>\n\n<p><strong><a href=\"https://arxiv.org/abs/2112.10751\">RvS: What is Essential for Offline RL via Supervised Learning?</a></strong><br />\n<a href=\"http://scottemmons.com/\">Scott Emmons</a>, <a href=\"https://ben-eysenbach.github.io/\">Benjamin Eysenbach</a>, <a href=\"https://www.kostrikov.xyz/\">Ilya Kostrikov</a>, <a href=\"https://people.eecs.berkeley.edu/~svlevine/\">Sergey Levine</a><br />\nInternational Conference on Learning Representations (ICLR), 2022<br />\n<a href=\"https://arxiv.org/abs/2112.10751\">[Paper]</a> <a href=\"https://github.com/scottemmons/rvs\">[Code]</a></p>",
            "pubdate": "Wed, 20 Apr 2022 02:00:00 -0700",
            "pubdate_parsed": [
                2022,
                4,
                20
            ],
            "email_sent": true
        },
        "Should I Use Offline RL or Imitation Learning?": {
            "url": "http://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- body -->\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649378429759_Screenshot+2022-04-07+at+5.40.23+PM.png\" width=\"100%\" />\n<br />\n<i>Figure 1: Summary of our recommendations for when a practitioner should BC and various imitation learning style methods, and when they should use offline RL approaches.</i>\n</p>\n\n<p>Offline reinforcement learning allows learning policies from previously collected data, which has profound implications for applying RL in domains where running trial-and-error learning is impractical or dangerous, such as safety-critical settings like autonomous driving or medical treatment planning. In such scenarios, online exploration is simply too risky, but <a href=\"https://arxiv.org/abs/2005.01643\">offline RL</a> methods can learn effective policies from logged data collected by <a href=\"https://arxiv.org/abs/2109.10813\">humans</a> or <a href=\"https://arxiv.org/abs/2010.14500\">heuristically designed controllers</a>.  Prior learning-based control methods have also approached learning from existing data as imitation learning: if the data is generally \u201cgood enough,\u201d simply copying the behavior in the data can lead to good results, and if it\u2019s not good enough, then filtering or reweighting the data and then copying can work well.  <a href=\"https://arxiv.org/abs/2106.01345\">Several</a> <a href=\"https://arxiv.org/abs/2108.03298\">recent</a> <a href=\"https://arxiv.org/abs/2110.09470\">works</a> suggest that this is a viable alternative to modern offline RL methods.</p>\n\n<p>This brings about several questions: <strong>when should we use offline RL? Are there fundamental limitations to methods that rely on some form of imitation (BC, conditional BC, filtered BC) that offline RL addresses?</strong> While it might be clear that offline RL should enjoy a large advantage over imitation learning when learning from diverse datasets that contain a lot of suboptimal behavior, we will also discuss how even cases that might seem BC-friendly can still allow offline RL to attain <a href=\"https://arxiv.org/abs/2204.05618\">significantly better results</a>. Our goal is to help explain when and why you should use each method and provide guidance to practitioners on the benefits of each approach. Figure 1 concisely summarizes our findings and we will discuss each component.</p>\n\n<!--more-->\n\n<h2 id=\"methods-for-learning-from-offline-data\">Methods for Learning from Offline Data</h2>\n\n<p>Let\u2019s start with a brief recap of various methods for learning policies from data that we will discuss. The learning algorithm is provided with an offline dataset \\(\\mathcal{D}\\), consisting of trajectories \\(\\{\\tau_i\\}_{i=1}^N\\) generated by some behavior policy. Most offline RL methods perform some sort of dynamic programming (e.g., Q-learning) updates on the provided data, aiming to obtain a value function. This typically requires adjusting for <a href=\"https://arxiv.org/abs/2005.01643\">distributional</a> <a href=\"https://arxiv.org/abs/2006.04779\">shift</a> to work well, but when this is done properly, it leads to good results.</p>\n\n<p>On the other hand, methods based on imitation learning attempt to simply clone the actions observed in the dataset if the dataset is good enough, or perform some kind of filtering or conditioning to extract useful behavior when the dataset is not good. For instance, recent work <a href=\"https://arxiv.org/abs/2106.01345\">filters trajectories</a> based on their return, or directly <a href=\"https://arxiv.org/abs/2106.08909\">filters individual transitions</a> based on how advantageous these could be under the behavior policy and then clones them. Conditional BC methods are based on the idea that every transition or trajectory is optimal when conditioned on the right variable. This way, after conditioning, the data becomes optimal given the value of the conditioning variable, and in principle we could then condition on the desired task, such as a high reward value, and get a near-optimal trajectory. For example, a trajectory that attains a return of \\(R_0\\) is <em>optimal</em> if our goal is to attain return \\(R = R_0\\) (<a href=\"https://arxiv.org/abs/1912.13465\">RCPs</a>, <a href=\"https://arxiv.org/abs/2106.01345\">decision transformer</a>); a trajectory that reaches goal \\(g\\) is optimal for reaching \\(g=g_0\\) (<a href=\"https://openreview.net/forum?id=rALA0Xo6yNJ\">GCSL</a>, <a href=\"https://arxiv.org/abs/2112.10751\">RvS</a>). Thus, one can perform perform reward-conditioned BC or goal-conditioned BC, and execute the learned policies with the desired value of return or goal during evaluation. This approach to offline RL bypasses learning value functions or dynamics models entirely, which can make it simpler to use. However, does it actually solve the general offline RL problem?</p>\n\n<h2 id=\"what-we-already-know-about-rl-vs-imitation-methods\">What We Already Know About RL vs Imitation Methods</h2>\n\n<p>Perhaps a good place to start our discussion is to review the performance of offline RL and imitation-style methods on benchmark tasks. In the figure below, we review the performance of some recent methods for learning from offline data on a subset of the <a href=\"https://arxiv.org/abs/2004.07219\">D4RL</a> benchmark.</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1650480950123_Screenshot+2022-04-20+at+11.54.39+AM.png\" width=\"100%\" />\n<br />\n<i>Table 1: Dichotomy of empirical results on several tasks in D4RL. While imitation-style methods (decision transformer, %BC, one-step RL, conditional BC) perform at par with and can outperform offline RL methods (CQL, IQL) on the locomotion tasks, these methods simply break down on the more complex maze navigation tasks.</i>\n</p>\n\n<p>Observe in the table that while imitation-style methods perform at par with offline RL methods across the span of the locomotion tasks, offline RL approaches vastly outperform these methods (except, goal-conditioned BC, which we will discuss towards the end of this post) by a large margin on the antmaze tasks. <strong>What explains this difference?</strong> As we will discuss in this blog post, methods that rely on imitation learning are often quite effective when the behavior in the offline dataset consists of some complete trajectories that perform well. This is true for most replay-buffer style datasets, and all of the locomotion datasets in D4RL are generated from replay buffers of online RL algorithms. In such cases, simply filtering good trajectories, and executing the mode of the filtered trajectories will work well. This explains why %BC, one-step RL and decision transformer work quite well. However, offline RL methods can vastly outperform BC methods when this stringent requirement is not met because they benefit from a form of \u201ctemporal compositionality\u201d which enables them to learn from suboptimal data. This explains the enormous difference between RL and imitation results on the antmazes.</p>\n\n<h2 id=\"offline-rl-can-solve-problems-that-conditional-filtered-or-weighted-bc-cannot\">Offline RL Can Solve Problems that Conditional, Filtered or Weighted BC Cannot</h2>\n\n<p>To understand why offline RL can solve problems that the aforementioned BC methods cannot, let\u2019s ground our discussion in a simple, didactic example. Let\u2019s consider the navigation task shown in the figure below, where the goal is to navigate from the starting location A to the goal location D in the maze. This is directly representative of several real-world decision-making scenarios in mobile robot navigation and provides an abstract model for an RL problem in domains such as robotics or recommender systems. Imagine you are provided with data that shows how the agent can navigate from location A to B and how it can navigate from C to E, but no single trajectory in the dataset goes from A to D. Obviously, the offline dataset shown below provides enough information for discovering a way to navigate to D: by combining different paths that cross each other at location E. But, can various offline learning methods find a way to go from A to D?</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649378795135_Screenshot+2022-04-07+at+5.46.30+PM.png\" width=\"70%\" />\n<br />\n<i>Figure 2: Illustration of the base case of temporal compositionality or stitching that is needed find optimal trajectories in various problem domains.</i>\n</p>\n\n<p>It turns out that, while offline RL methods are able to discover the path from A to D, various imitation-style methods cannot. This is because offline RL algorithms can <strong>\u201cstitch\u201d</strong> suboptimal trajectories together: while the trajectories \\(\\tau_i\\) in the offline dataset might attain poor return, a better policy can be obtained by combining good segments of trajectories (A\u2192E + E\u2192D = A\u2192D).  This ability to stitch segments of trajectories temporally is the hallmark of value-based offline RL algorithms that utilize Bellman backups, but cloning (a subset of) the data or trajectory-level sequence models are unable to extract this information, since such no single trajectory from A to D is observed in the offline dataset!</p>\n\n<p><strong>Why should you care about stitching and these mazes?</strong> One might now wonder if this stitching phenomenon is only useful in some esoteric edge cases or if it is an actual, practically-relevant phenomenon. Certainly stitching appears very explicitly in <a href=\"https://arxiv.org/abs/2010.14500\">multi-stage robotic manipulation</a> tasks and also in <a href=\"https://arxiv.org/abs/2012.09812\">navigation tasks</a>. However, stitching is not limited to just these domains \u2014 it turns out that the need for stitching implicitly appears even in tasks that do not appear to contain a maze. In practice, effective policies would often require finding an \u201cextreme\u201d but high-rewarding action, very different from an action that the behavior policy would prescribe, at <em>every</em> state and learning to stitch such actions to obtain a policy that performs well overall. This form of <em>implicit</em> stitching appears in many practical applications: for example, one might want to find an HVAC control policy that minimizes the carbon footprint of a building with a dataset collected from distinct control policies run historically in different buildings, each of which is suboptimal in one manner or the other. In this case, one can still get a much better policy by stitching extreme actions at every state. In general this implicit form of stitching is required in cases where we wish to find really good policies that maximize a continuous value (e.g., maximize rider comfort in autonomous driving;  maximize profits in automatic stock trading) using a dataset collected from a mixture of suboptimal policies (e.g., data from different human drivers; data from different human traders who excel and underperform under different situations) that never execute extreme actions at each decision. However, by stitching such extreme actions at each decision, one can obtain a much better policy. Therefore, naturally succeeding at many problems requires learning to either explicitly or implicitly stitch trajectories, segments or even single decisions, and offline RL is good at it.</p>\n\n<p>The next natural question to ask is: <strong>Can we resolve this issue by adding an RL-like component in BC methods?</strong> One recently-studied approach is to perform a limited number of policy improvement steps beyond behavior cloning. That is, while full offline RL performs multiple rounds of policy improvement untill we find an optimal policy, one can just find a policy by running <a href=\"https://arxiv.org/abs/2106.08909\">one step of policy improvement</a> beyond behavioral cloning. This policy improvement is performed by incorporating some sort of a value function, and one might hope that utilizing some form of Bellman backup equips the method with the ability to \u201c<strong>stitch</strong>\u201d. Unfortunately, even this approach is unable to fully close the gap against offline RL. This is because while the one-step approach can stitch trajectory segments, it would often end up stitching the wrong segments! One step of policy improvement only myopically improves the policy, without taking into account the impact of updating the policy on the future outcomes, the policy may fail to identify truly optimal behavior. For example, in our maze example shown below, it might appear better for the agent to find a solution that decides to go upwards and attain mediocre reward compared to going towards the goal, since under the behavior policy going downwards might appear highly suboptimal.</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648609753495_Screenshot+2022-03-29+at+8.09.10+PM.png\" width=\"100%\" />\n<br />\n<i>Figure 3: Imitation-style methods that only perform a limited steps of policy improvement may still fall prey to choosing suboptimal actions, because the optimal action assuming that the agent will follow the behavior policy in the future may actually not be optimal for the full sequential decision making problem.</i>\n</p>\n\n<h2 id=\"is-offline-rl-useful-when-stitching-is-not-a-primary-concern\">Is Offline RL Useful When Stitching is Not a Primary Concern?</h2>\n\n<p>So far, our analysis reveals that offline RL methods are better due to good \u201cstitching\u201d properties. But one might wonder, if stitching is critical when provided with good data, such as demonstration data in <a href=\"https://arxiv.org/abs/2108.03298\">robotics</a> or data from good policies in <a href=\"https://arxiv.org/abs/1908.08796\">healthcare</a>. However, in our <a href=\"http://link here\">recent paper,</a> we find that even when temporal compositionality is not a primary concern, offline RL does provide benefits over imitation learning.</p>\n\n<p><strong>Offline RL can teach the agent what to \u201cnot do\u201d.</strong> Perhaps one of the biggest benefits of offline RL algorithms is that running RL on noisy datasets generated from stochastic policies can not only teach the agent what it should do to maximize return, but also what shouldn\u2019t be done and how actions at a given state would influence the chance of the agent ending up in undesirable scenarios in the future. In contrast, any form of conditional or weighted BC which only teach the policy \u201cdo X\u201d, without explicitly discouraging particularly low-rewarding or unsafe behavior. This is especially relevant in open-world settings such as robotic manipulation in diverse settings or making decisions about patient admission in an ICU, where knowing what to not do very clearly is essential. In our <a href=\"https://arxiv.org/abs/2204.05618\">paper</a>, we quantify the gain of accurately inferring \u201cwhat not to do and how much it hurts\u201d and describe this intuition pictorially below. Often obtaining such noisy data is easy \u2014 one could augment expert demonstration data with additional \u201cnegatives\u201d or \u201cfake data\u201d generated from a simulator (e.g., robotics, autonomous driving), or by first running an imitation learning method and creating a dataset for offline RL that augments data with evaluation rollouts from the imitation learned policy.</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648634037765_Screenshot+2022-03-30+at+2.53.52+AM.png\" width=\"90%\" />\n<br />\n<i>Figure 4: By leveraging noisy data, offline RL algorithms can learn to figure out what shouldn\u2019t be done in order to explicitly avoid regions of low reward, and how the agent could be overly cautious much before that.</i>\n</p>\n\n<p><strong>Is offline RL useful at all when I</strong> <strong>actually</strong> <strong>have near-expert demonstrations?</strong>  As the final scenario, let\u2019s consider the case where we actually have only near-expert demonstrations \u2014 perhaps, the perfect setting for imitation learning. In such a setting, there is no opportunity for stitching or leveraging noisy data to learn what not to do. Can offline RL still improve upon imitation learning? Unfortunately, one can show that, in the worst case, no algorithm can perform better than standard behavioral cloning. However, if the task admits some structure then offline RL policies can be more robust. For example, if there are multiple states where it is easy to identify a good action using reward information, offline RL approaches can quickly converge to a good action at such states, whereas a standard BC approach that does not utilize rewards may fail to identify a good action, leading to policies that are non-robust and fail to solve the task. Therefore, offline RL is a preferred option for tasks with an abundance of such \u201cnon-critical\u201d states where long-term reward can easily identify a good action. An illustration of this idea is shown below, and we formally prove a theoretical result quantifying these intuitions in the <a href=\"https://arxiv.org/abs/2204.05618\">paper</a>.</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1648635140775_Screenshot+2022-03-30+at+3.12.16+AM.png\" width=\"90%\" />\n<br />\n<i>Figure 5: An illustration of the idea of non-critical states: the abundance of states where reward information can easily identify good actions at a given state can help offline RL \u2014 even when provided with expert demonstrations \u2014  compared to standard BC, that does not utilize any kind of reward information,</i>\n</p>\n\n<h2 id=\"so-when-is-imitation-learning-useful\">So, When Is Imitation Learning Useful?</h2>\n\n<p>Our discussion has so far highlighted that offline RL methods can be robust and effective in many scenarios where conditional and weighted BC might fail. Therefore, we now seek to understand if conditional or weighted BC are useful in certain problem settings. This question is easy to answer in the context of standard behavioral cloning, if your data consists of expert demonstrations that you wish to mimic, standard behavioral cloning is a relatively simple, good choice.  However this approach fails when the data is noisy or suboptimal or when the task changes (e.g., when the distribution of initial states changes). And offline RL may still be preferred in settings with some structure (as we discussed above). Some failures of BC can be resolved by utilizing filtered BC \u2014 if the data consists of a mixture of good and bad trajectories, filtering trajectories based on return can be a good idea. Similarly, one could use one-step RL if the task does not require any form of stitching. However, in all of these cases, offline RL might be a better alternative especially if the task or the environment satisfies some conditions, and might be worth trying at least.</p>\n\n<p>Conditional BC performs well on a problem when one can obtain a conditioning variable well-suited to a given task. For example, empirical results on the antmaze domains from <a href=\"https://arxiv.org/abs/2112.10751\">recent work</a> indicate that conditional BC with a goal as a conditioning variable is quite effective in goal-reaching problems, however, conditioning on returns is not (compare Conditional BC (goals) vs Conditional BC (returns) in Table 1). Intuitively, this \u201cwell-suited\u201d conditioning variable essentially enables stitching \u2014 for instance, a navigation problem naturally decomposes into a sequence of intermediate goal-reaching problems and then stitch solutions to a cleverly chosen subset of intermediate goal-reaching problems to solve the complete task. At its core, the success of conditional BC requires some domain knowledge about the compositionality structure in the task. On the other hand, offline RL methods extract the underlying stitching structure by running dynamic programming, and work well more generally. Technically, one could combine these ideas and utilize dynamic programming to learn a value function and then obtain a policy by running conditional BC with the value function as the conditioning variable, and this can work quite well (compare RCP-A to RCP-R <a href=\"https://arxiv.org/abs/1912.13465\">here</a>, where RCP-A uses a value function for conditioning; compare TT+Q and TT <a href=\"https://arxiv.org/abs/2106.02039\">here</a>)!</p>\n\n<h1 id=\"empirical-results-comparing-offline-rl-and-bc\">Empirical Results Comparing Offline RL and BC</h1>\n\n<p>In our discussion so far, we have already studied settings such as the antmazes, where offline RL methods can significantly outperform imitation-style methods due to stitching. We will now quickly discuss some empirical results that compare the performance of offline RL and BC on tasks where we are provided with near-expert, demonstration data.</p>\n\n<p style=\"text-align: center; float: right;\">\n<img src=\"https://paper-attachments.dropbox.com/s_A60F7B4D130EBF1556762D7CF6FF295A033F16EE9EE529F257434EE2272F2C22_1649381175454_image.png\" width=\"90%\" />\n<br />\n<i>Figure 6: Comparing full offline RL (CQL) to imitation-style methods (One-step RL and BC) averaged over 7 Atari games, with expert demonstration data and noisy-expert data. Empirical details here.</i>\n</p>\n\n<p>In our final experiment, we compare the performance of offline RL methods to imitation-style methods on an average over seven Atari games. We use <a href=\"https://sites.google.com/view/cql-offline-rl\">conservative Q-learning</a> (CQL) as our representative offline RL method. Note that naively running offline RL (\u201cNaive CQL (Expert)\u201d), without proper cross-validation to prevent overfitting and underfitting does not improve over BC. However, offline RL equipped with a reasonable cross-validation procedure (\u201cTuned CQL (Expert)\u201d) is able to clearly improve over BC. This highlights the need for <a href=\"https://arxiv.org/abs/2109.10813\">understanding how offline RL methods must be tuned</a>, and at least, in part explains the poor performance of offline RL when learning from demonstration data in prior works. Incorporating a bit of noisy data that can inform the algorithm of what it shouldn\u2019t do, further improves performance (\u201cCQL (Noisy Expert)\u201d vs \u201cBC (Expert)\u201d) within an identical data budget. Finally, note that while one would expect that while one step of policy improvement can be quite effective, we found that it is quite sensitive to hyperparameters and fails to improve over BC significantly. These observations validate the findings discussed earlier in the blog post. We discuss results on other domains in our <a href=\"https://arxiv.org/abs/2204.05618\">paper</a>, that we encourage practitioners to check out.</p>\n\n<h1 id=\"discussion-and-takeaways\">Discussion and Takeaways</h1>\n\n<p>In this blog post, we aimed to understand if, when and why offline RL is a better approach for tackling a variety of sequential decision-making problems. Our discussion suggests that offline RL methods that learn value functions can leverage the benefits of stitching, which can be crucial in many problems. Moreover, there are even scenarios with expert or near-expert demonstration data, where running offline RL is a good idea. We summarize our recommendations for practitioners in Figure 1, shown right at the beginning of this blog post. We hope that our analysis improves the understanding of the benefits and properties of offline RL approaches.</p>\n\n<hr />\n\n<p>This blog post is primarily based on the paper:</p>\n\n<p><strong>When Should Offline RL Be Preferred Over Behavioral Cloning?</strong><br />\nAviral Kumar*, Joey Hong*, Anikait Singh, Sergey Levine       [<a href=\"https://arxiv.org/abs/2204.05618\">arxiv</a>].<br />\nIn International Conference on Learning Representations (ICLR), 2022.</p>\n\n<p>In addition, the empirical results discussed in the blog post are taken from various papers, in particular from <a href=\"https://arxiv.org/abs/2112.10751\">RvS</a> and <a href=\"https://arxiv.org/abs/2110.06169\">IQL</a>.</p>",
            "pubdate": "Mon, 25 Apr 2022 05:00:00 -0700",
            "pubdate_parsed": [
                2022,
                4,
                25
            ],
            "email_sent": true
        },
        "Designing Societally Beneficial Reinforcement Learning Systems": {
            "url": "http://bair.berkeley.edu/blog/2022/04/29/reward-reports/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- body -->\n\n<p>Deep reinforcement learning (DRL) is transitioning from a research field focused on game playing to a technology with real-world applications. Notable examples include DeepMind\u2019s work on <a href=\"https://www.nature.com/articles/s41586-021-04301-9\">controlling a nuclear reactor</a> or on improving <a href=\"https://arxiv.org/abs/2202.06626\">Youtube video compression</a>, or Tesla <a href=\"https://www.youtube.com/watch?v=j0z4FweCy4M&amp;t=4802s\">attempting to use a method inspired by MuZero</a> for autonomous vehicle behavior planning. But the exciting potential for real world applications of RL should also come with a healthy dose of caution - for example RL policies are well known to be vulnerable to <a href=\"https://robotic.substack.com/p/rl-exploitation?s=w\">exploitation</a>, and methods for safe and <a href=\"https://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/\">robust policy development</a> are an active area of research.</p>\n\n<p>At the same time as the emergence of powerful RL systems in the real world, the public and researchers are expressing an increased appetite for fair, aligned, and safe machine learning systems. The focus of these research efforts to date has been to account for shortcomings of datasets or supervised learning practices that can harm individuals. However the unique ability of RL systems to leverage temporal feedback in learning complicates the types of risks and safety concerns that can arise.</p>\n\n<p>This post expands on our recent <a href=\"https://cltc.berkeley.edu/2022/02/08/reward-reports/\">whitepaper</a> and <a href=\"https://arxiv.org/abs/2204.10817\">research paper</a>, where we aim to illustrate the different modalities harms can take when augmented with the temporal axis of RL. To combat these novel societal risks, we also propose a new kind of documentation for dynamic Machine Learning systems which aims to assess and monitor these risks both before and after deployment.</p>\n\n<!--more-->\n\n<h1 id=\"whats-special-about-rl-a-taxonomy-of-feedback\">What\u2019s Special About RL? A Taxonomy of Feedback</h1>\n\n<p>Reinforcement learning systems are often spotlighted for their ability to act in an environment, rather than passively make predictions. Other supervised machine learning systems, such as computer vision, consume data and return a prediction that can be used by some decision making rule. In contrast, the appeal of RL is in its ability to not only (a) directly model the impact of actions, but also to (b) improve policy performance automatically. These key properties of acting upon an environment, and learning within that environment can be understood as by considering the different types of feedback that come into play when an RL agent acts within an environment. We classify these feedback forms in a taxonomy of (1) Control, (2) Behavioral, and (3) Exogenous feedback. The first two notions of feedback, Control and Behavioral, are directly within the formal mathematical definition of an RL agent while Exogenous feedback is induced as the agent interacts with the broader world.</p>\n\n<h2 id=\"1-control-feedback\">1. Control Feedback</h2>\n\n<p>First is control feedback - in the control systems engineering sense - where the action taken depends on the current measurements of the state of the system. RL agents choose actions based on an observed state according to a policy, which generates environmental feedback. For example, a thermostat turns on a furnace according to the current temperature measurement. Control feedback gives an agent the ability to react to unforeseen events (e.g. a sudden snap of cold weather) autonomously.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/fb-control.png\" width=\"60%\" />\n<br />\n<i>Figure 1: Control Feedback.</i>\n</p>\n\n<h2 id=\"2-behavioral-feedback\">2. Behavioral Feedback</h2>\n\n<p>Next in our taxonomy of RL feedback is \u2018behavioral feedback\u2019: the trial and error learning that enables an agent to improve its policy through interaction with the environment. This could be considered the defining feature of RL, as compared to e.g. \u2018classical\u2019 control theory. Policies in RL can be defined by a set of parameters that determine the actions the agent takes in the future. Because these parameters are updated through behavioral feedback, these are actually a reflection of the data collected from executions of past policy versions. RL agents are not fully \u2018memoryless\u2019 in this respect\u2013the current policy depends on stored experience, and impacts newly collected data, which in turn impacts future versions of the agent. To continue the thermostat example - a \u2018smart home\u2019 thermostat might analyze historical temperature measurements and adapt its control parameters in accordance with seasonal shifts in temperature, for instance to have a more aggressive control scheme during winter months.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/fb-behavioral.png\" width=\"70%\" />\n<br />\n<i>Figure 2: Behavioral Feedback.</i>\n</p>\n\n<h2 id=\"3-exogenous-feedback\">3. Exogenous Feedback</h2>\n\n<p>Finally, we can consider a third form of feedback external to the specified RL environment, which we call Exogenous (or \u2018exo\u2019) feedback. While RL benchmarking tasks may be static environments, every action in the real world impacts the dynamics of both the target deployment environment, as well as adjacent environments. For example, a news recommendation system that is optimized for clickthrough may change the way editors write headlines towards attention-grabbing\u00a0 clickbait. In this RL formulation, the set of articles to be recommended would be considered part of the environment and expected to remain static, but exposure incentives cause a shift over time.</p>\n\n<p>To continue the thermostat example, as a \u2018smart thermostat\u2019 continues to adapt its behavior over time, the behavior of other adjacent systems in a household might change in response - for instance other appliances might consume more electricity due to increased heat levels, which could impact electricity costs. Household occupants might also change their clothing and behavior patterns due to different temperature profiles during the day. In turn, these secondary effects could also influence the temperature which the thermostat monitors, leading to a longer timescale feedback loop.</p>\n\n<p>Negative costs of these external effects will not be specified in the agent-centric reward function, leaving these external environments to be manipulated or exploited. Exo-feedback is by definition difficult for a designer to predict. Instead, we propose that it should be addressed by documenting the evolution of the agent, the targeted environment, and adjacent environments.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/fb-exo.png\" width=\"80%\" />\n<br />\n<i>Figure 3: Exogenous (exo) Feedback.</i>\n</p>\n\n<hr />\n\n<h1 id=\"how-can-rl-systems-fail\">How can RL systems fail?</h1>\n\n<p>Let\u2019s consider how two key properties can lead to failure modes specific to RL systems: direct action selection (via control feedback) and autonomous data collection (via behavioral feedback).</p>\n\n<p>First is decision-time safety. One current practice in RL research to create safe decisions is to augment the agent\u2019s reward function with a penalty term for certain harmful or undesirable states and actions. For example, in a robotics domain we might penalize certain actions (such as extremely large torques) or state-action tuples (such as carrying a glass of water over sensitive equipment). However it is difficult to anticipate where on a pathway an agent may encounter a crucial action, such that failure would result in an unsafe event. This aspect of how reward functions interact with optimizers is especially problematic for deep learning systems, where numerical guarantees are challenging.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/decision.png\" width=\"90%\" />\n<br />\n<i>Figure 4: Decision time failure illustration.</i>\n</p>\n\n<p>As an RL agent collects new data and the policy adapts, there is a complex interplay between current parameters, stored data, and the environment that governs evolution of the system. Changing any one of these three sources of information will change the future behavior of the agent, and moreover these three components are deeply intertwined. This uncertainty makes it difficult to back out the cause of failures or successes.</p>\n\n<p>In domains where many behaviors can possibly be expressed, the RL specification leaves a lot of factors constraining behavior unsaid. For a robot learning locomotion over an uneven environment, it would be useful to know what signals in the system indicate it will learn to find an easier route rather than a more complex gait. In complex situations with less well-defined reward functions, these intended or unintended behaviors will encompass a much broader range of capabilities, which may or may not have been accounted for by the designer.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/behavior.png\" width=\"80%\" />\n<br />\n<i>Figure 5: Behavior estimation failure illustration.</i>\n</p>\n\n<p>While these failure modes are closely related to control and behavioral feedback, Exo-feedback does not map as clearly to one type of error and introduces risks that do not fit into simple categories. Understanding exo-feedback requires that stakeholders in the broader communities (machine learning, application domains, sociology, etc.) work together on real world RL deployments.</p>\n\n<h1 id=\"risks-with-real-world-rl\">Risks with real-world RL</h1>\n\n<p>Here, we discuss four types of design choices an RL designer must make, and how these choices can have an impact upon the socio-technical failures that an agent might exhibit once deployed.</p>\n\n<h2 id=\"scoping-the-horizon\">Scoping the Horizon</h2>\n\n<p>Determining the timescale on which aRL agent can plan impacts the possible and actual behavior of that agent. In the lab, it may be common to tune the horizon length until the desired behavior is achieved. But in real world systems, optimizations will externalize costs depending on the defined horizon. For example, an RL agent controlling an autonomous vehicle will have very different goals and behaviors if the task is to stay in a lane,\u00a0 navigate a contested intersection, or route across a city to a destination. This is true even if the objective (e.g. \u201cminimize travel time\u201d) remains the same.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/horizon.png\" width=\"100%\" />\n<br />\n<i>Figure 6: Scoping the horizon example with an autonomous vehicle.</i>\n</p>\n\n<h2 id=\"defining-rewards\">Defining Rewards</h2>\n\n<p>A second design choice is that of actually specifying the reward function to be maximized. This immediately raises the well-known risk of RL systems, reward hacking, where the designer and agent negotiate behaviors based on specified reward functions. In a deployed RL system, this often results in unexpected exploitative behavior \u2013 from <a href=\"https://openai.com/blog/faulty-reward-functions/\">bizarre video game agents</a> to <a href=\"https://bair.berkeley.edu/blog/2021/04/19/mbrl/\">causing errors in robotics simulators</a>. For example, if an agent is presented with the problem of navigating a maze to reach the far side, a mis-specified reward might result in the agent avoiding the task entirely to minimize the time taken.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/reward-shaping.png\" width=\"100%\" />\n<br />\n<i>Figure 7: Defining rewards example with maze navigation.</i>\n</p>\n\n<h2 id=\"pruning-information\">Pruning Information</h2>\n\n<p>A common practice in RL research is to redefine the environment to fit one\u2019s needs \u2013 RL designers make numerous explicit and implicit assumptions to model tasks in a way that makes them amenable to virtual RL agents. In highly structured domains, such as video games, this can be rather benign.However, in the real world redefining the environment amounts to changing the ways information can flow between the world and the RL agent. This can dramatically change the meaning of the reward function and offload risk to external systems. For example, an autonomous vehicle with sensors focused only on the road surface shifts the burden from AV designers to pedestrians. In this case, the designer is pruning out information about the surrounding environment that is actually crucial to robustly safe integration within society.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/info-shaping.png\" width=\"80%\" />\n<br />\n<i>Figure 8: Information shaping example with an autonomous vehicle.</i>\n</p>\n\n<h2 id=\"training-multiple-agents\">Training Multiple Agents</h2>\n\n<p>There is growing interest in the problem of <a href=\"https://bair.berkeley.edu/blog/2021/07/14/mappo/\">multi-agent RL</a>, but as an emerging research area, little is known about how learning systems interact within dynamic environments. When the relative concentration of autonomous agents increases within an environment, the terms these agents optimize for can actually re-wire norms and values encoded in that specific application domain. An example would be the changes in behavior that will come if the majority of vehicles are autonomous and communicating (or not) with each other. In this case, if the agents have autonomy to optimize toward a goal of minimizing transit time (for example), they could crowd out the remaining human drivers and heavily disrupt accepted societal norms of transit.</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/multi-agent.png\" width=\"80%\" />\n<br />\n<i>Figure 9: The risks of multi-agency example on autonomous vehicles.</i>\n</p>\n\n<hr />\n\n<h1 id=\"making-sense-of-applied-rl-reward-reporting\">Making sense of applied RL: Reward Reporting</h1>\n\n<p>In our recent <a href=\"https://cltc.berkeley.edu/2022/02/08/reward-reports/\">whitepaper</a> and <a href=\"https://arxiv.org/abs/2204.10817\">research paper</a>, we proposed <a href=\"https://rewardreports.github.io/\">Reward Reports</a>, a new form of ML documentation that foregrounds the societal risks posed by sequential data-driven optimization systems, whether explicitly constructed as an RL agent or <a href=\"https://robotic.substack.com/p/ml-becomes-rl?s=w\">implicitly construed</a> via data-driven optimization and feedback. Building on proposals to document datasets and models, we focus on reward functions: the objective that guides optimization decisions in feedback-laden systems. Reward Reports comprise questions that highlight the promises and risks entailed in defining what is being optimized in an AI system, and are intended as living documents that dissolve the distinction between ex-ante (design) specification and ex-post (after the fact) harm. As a result, Reward Reports provide a framework for ongoing deliberation and accountability before and after a system is deployed.</p>\n\n<p>Our proposed template for a Reward Reports consists of several sections, arranged to help the reporter themselves understand and document the system. A Reward Report begins with (1) system details that contain the information context for deploying the model. From there, the report documents (2) the optimization intent, which questions the goals of the system and why RL or ML may be a useful tool. The designer then documents (3) how the system may affect different stakeholders in the institutional interface. The next two sections contain technical details on (4) the system implementation and (5) evaluation. Reward reports conclude with (6) plans for system maintenance as additional system dynamics are uncovered.</p>\n\n<p>The most important feature of a Reward Report is that it allows documentation to evolve over time, in step with the temporal evolution of an online, deployed RL system! This is most evident in the change-log, which is we locate at the end of our Reward Report template:</p>\n\n<p style=\"text-align: center; float: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/reward-reports/rr-contents.png\" width=\"80%\" />\n<br />\n<i>Figure 10: Reward Reports contents.</i>\n</p>\n\n<h2 id=\"what-would-this-look-like-in-practice\">What would this look like in practice?</h2>\n\n<p>As part of our research, we have developed a reward report <a href=\"https://github.com/RewardReports/reward-reports\">LaTeX template, as well as several example reward reports</a> that aim to illustrate the kinds of issues that could be managed by this form of documentation. These examples include the temporal evolution of the MovieLens recommender system, the DeepMind MuZero game playing system, and a hypothetical deployment of an RL autonomous vehicle policy for managing merging traffic, based on the <a href=\"https://flow-project.github.io/\">Project Flow simulator</a>.</p>\n\n<p>However, these are just examples that we hope will serve to inspire the RL community\u2013as more RL systems are deployed in real-world applications, we hope the research community will build on our ideas for Reward Reports and refine the specific content that should be included. To this end, we hope that you will join us at our (un)-workshop.</p>\n\n<h2 id=\"work-with-us-on-reward-reports-an-unworkshop\">Work with us on Reward Reports: An (Un)Workshop!</h2>\n\n<p>We are hosting an \u201cun-workshop\u201d at the upcoming conference on Reinforcement Learning and Decision Making (<a href=\"https://rldm.org/rldm-2022-workshops/\">RLDM</a>) on June 11th from 1:00-5:00pm EST at Brown University, Providence, RI. We call this an un-workshop because we are looking for the attendees to help create the content! We will provide templates, ideas, and discussion as our attendees build out example reports. We are excited to develop the ideas behind Reward Reports with real-world practitioners and cutting-edge researchers.</p>\n\n<p>For more information on the workshop, visit the <a href=\"https://rewardreports.github.io/workshop.html\">website</a> or contact the organizers at <a href=\"mailto:geese-org@lists.berkeley.edu\">geese-org@lists.berkeley.edu</a>.</p>\n\n<hr />\n\n<p>This post is based on the following papers:</p>\n\n<ul>\n  <li><a href=\"https://cltc.berkeley.edu/2022/02/08/reward-reports/\">Choices, Risks, and Reward Reports: Charting Public Policy for Reinforcement Learning Systems</a> by Thomas Krendl Gilbert, Sarah Dean, Tom Zick, Nathan Lambert. Center for Long Term Cybersecurity Whitepaper Series 2022.</li>\n  <li><a href=\"https://arxiv.org/abs/2204.10817\">Reward Reports for Reinforcement Learning</a> by Thomas Krendl Gilbert, Sarah Dean, Nathan Lambert, Tom Zick and Aaron Snoswell. ArXiv Preprint 2022.</li>\n</ul>",
            "pubdate": "Fri, 29 Apr 2022 05:00:00 -0700",
            "pubdate_parsed": [
                2022,
                4,
                29
            ],
            "email_sent": true
        },
        "Rethinking Human-in-the-Loop for Artificial Augmented Intelligence": {
            "url": "http://bair.berkeley.edu/blog/2022/05/03/human-in-the-loop/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- body -->\n\n<p style=\"text-align: center;\">\n    <img src=\"http://bair.berkeley.edu/static/blog/human-in-the-loop/image3.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 1: In real-world applications, we think there exist a human-machine loop where humans and machines are mutually augmenting each other. We call it Artificial Augmented Intelligence.\n</i>\n</p>\n\n<p>How do we build and evaluate an AI system for real-world applications? In most AI research, the evaluation of AI methods involves a training-validation-testing process. The experiments usually stop when the models have good testing performance on the reported datasets because real-world data distribution is assumed to be modeled by the validation and testing data. However, real-world applications are usually more complicated than a single training-validation-testing process. The biggest difference is the ever-changing data. For example, wildlife datasets change in class composition all the time because of animal invasion, re-introduction, re-colonization, and seasonal animal movements. A model trained, validated, and tested on existing datasets can easily be broken when newly collected data contain novel species. Fortunately, we have out-of-distribution detection methods that can help us detect samples of novel species. However, when we want to expand the recognition capacity (i.e., being able to recognize novel species in the future), the best we can do is fine-tuning the models with new ground-truthed annotations. In other words, we need to incorporate human effort/annotations regardless of how the models perform on previous testing sets.</p>\n\n<!--more-->\n\n<h1 id=\"inevitable-human-in-the-loop\">Inevitable human-in-the-loop</h1>\n\n<p>When human annotations are inevitable, real-world recognition systems become a never-ending loop of <strong>data collection \u2192 annotation \u2192 model fine-tuning</strong> (Figure 2). As a result, the performance of one single step of model evaluation does not represent the actual generalization of the whole recognition system because the model will be updated with new data annotations, and a new round of evaluation will be conducted. With this loop in mind, we think that instead of building a model with <strong><em>better testing performance</em></strong>, focusing on <strong><em>how much human effort can be saved</em></strong> is a more generalized and practical goal in real-world applications.</p>\n\n<p style=\"text-align: center;\">\n    <img height=\"\" src=\"http://bair.berkeley.edu/static/blog/human-in-the-loop/image1.png\" />\n    <br />\n<i>\nFigure 2: In the loop of data collection, annotation, and model update, the goal of optimization becomes minimizing the requirement of human annotation rather than single-step recognition performance.\n</i>\n</p>\n\n<h1 id=\"a-case-study-on-wildlife-recognition\">A case study on wildlife recognition</h1>\n\n<p>In the paper we published last year in Nature-Machine Intelligence [1], we discussed the incorporation of human-in-the-loop into wildlife recognition and proposed to examine human effort efficiency in model updates instead of simple testing performance. For demonstration, we designed a recognition framework that was a combination of active learning, semi-supervised learning, and human-in-the-loop (Figure 3). We also incorporated a time component into this framework to indicate that the recognition models did not stop at any single time step. Generally speaking, in the framework, at each time step, when new data are collected, a recognition model actively selects which data should be annotated based on a prediction confidence metric. Low-confidence predictions are sent for human annotation, and high-confidence predictions are trusted for downstream tasks or pseudo-labels for model updates.</p>\n\n<p style=\"text-align: center;\">\n    <img height=\"\" src=\"http://bair.berkeley.edu/static/blog/human-in-the-loop/image2.png\" />\n    <br />\n<i>\nFigure 3: Here, we present an iterative recognition framework that can both maximize the utility of modern image recognition methods and minimize the dependence on manual annotations for model updating.  \n</i>\n</p>\n\n<p>In terms of human annotation efficiency for model updates, we split the evaluation into 1) the percentage of high-confidence predictions on validation (i.e., saved human effort for annotation); 2) the accuracy of high-confidence predictions (i.e., reliability); and 3) the percentage of novel categories that are detected as low-confidence predictions (i.e., sensitivity to novelty). With these three metrics, the optimization of the framework becomes minimizing human efforts (i.e., to maximize high-confidence percentage) and maximizing model update performance and high-confidence accuracy.</p>\n\n<p>We reported a two-step experiment on a large-scale wildlife camera trap dataset collected from Mozambique National Park for demonstration purposes. The first step was an initialization step to initialize a model with only part of the dataset. In the second step, a new set of data with known and novel classes was applied to the initialized model. Following the framework, the model made predictions on the new dataset with confidence, where high-confidence predictions were trusted as pseudo-labels, and low-confidence predictions were provided with human annotations. Then, the model was updated with both pseudo-labels and annotations and ready for the future time steps. As a result, the percentage of high-confidence predictions on second step validation was 72.2%, the accuracy of high-confidence predictions was 90.2%, and the percentage of novel classes detected as low-confidence was 82.6%. In other words, our framework saved 72% of human effort on annotating all the second step data. As long as the model was confident, 90% of the predictions were correct. In addition, 82% of novel samples were successfully detected. Details of the framework and experiments can be found in the original paper.</p>\n\n<h1 id=\"artificial-augmented-intelligence-a2i\">Artificial Augmented Intelligence (A<sup>2</sup>I)</h1>\n\n<p>By taking a closer look at Figure 3, besides the <strong>data collection - human annotation - model update</strong> loop, there is another <strong>human-machine</strong> loop hidden in the framework (Figure 1). This is a loop where both humans and machines are constantly improving each other through model updates and human intervention. For example, when AI models cannot recognize novel classes, human intervention can provide information to expand the model\u2019s recognition capacity. On the other hand, when AI models get more and more generalized, the requirement for human effort gets less. In other words, the use of human effort gets more efficient.</p>\n\n<p>In addition, the confidence-based human-in-the-loop framework we proposed is not limited to novel class detection but can also help with issues like long-tailed distribution and multi-domain discrepancies. As long as AI models feel less confident, human intervention comes in to help improve the model. Similarly, human effort is saved as long as AI models feel confident, and sometimes human errors can even be corrected (Figure 4). In this case, the relationship between humans and machines becomes synergistic. Thus, the goal of AI development changes from replacing human intelligence to mutually augmenting both human and machine intelligence. We call this type of AI: <strong>Artificial Augmented Intelligence (A<sup>2</sup>I)</strong>.</p>\n\n<p>Ever since we started working on artificial intelligence, we have been asking ourselves, what do we create AI for? At first, we believed that, ideally, AI should fully replace human effort in simple and tedious tasks such as large-scale image recognition and car driving. Thus, we have been pushing our models to an idea called \u201chuman-level performance\u201d for a long time. However, this goal of replacing human effort is intrinsically building up opposition or a mutually exclusive relationship between humans and machines. In real-world applications, the performance of AI methods is just limited by so many affecting factors like long-tailed distribution, multi-domain discrepancies, label noise, weak supervision, out-of-distribution detection, etc. Most of these problems can be somehow relieved with proper human intervention. The framework we proposed is just one example of how these separate problems can be summarized into high- versus low-confidence prediction problems and how human effort can be introduced into the whole AI system. We think it is not cheating or surrendering to hard problems. It is a more human-centric way of AI development, where the focus is on how much human effort is saved rather than how many testing images a model can recognize. Before the realization of Artificial General Intelligence (AGI), we think it is worthwhile to further explore the direction of machine-human interactions and A<sup>2</sup>I such that AI can start making more impacts in various practical fields.</p>\n\n<p style=\"text-align: center;\">\n    <img height=\"\" src=\"http://bair.berkeley.edu/static/blog/human-in-the-loop/image4.png\" />\n    <br />\n<i>\nFigure 4: Examples of high-confidence predictions that did not match the original annotations. Many high-confidence predictions that were flagged as incorrect based on validation labels (provided by students and citizen scientists) were in fact correct upon closer inspection by wildlife experts.  \n</i>\n</p>\n\n<p><em>Acknowledgements: We thank all co-authors of the paper \u201cIterative Human and Automated Identification of Wildlife Images\u201d for their contributions and discussions in preparing this blog. The views and opinions expressed in this blog are solely of the authors of this paper.</em></p>\n\n<p>This blog post is based on the following paper which is published at Nature - Machine Intelligence:<br />\n[1] Miao, Zhongqi, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S. Palmer, Stella X. Yu, and Wayne M. Getz. \u201cIterative human and automated identification of wildlife images.\u201d Nature Machine Intelligence 3, no. 10 (2021): 885-895.(Link to <a href=\"https://arxiv.org/pdf/2105.02320.pdf\">Pre-print</a>)</p>",
            "pubdate": "Tue, 03 May 2022 06:55:00 -0700",
            "pubdate_parsed": [
                2022,
                5,
                3
            ],
            "email_sent": true
        },
        "The Berkeley Crossword Solver": {
            "url": "http://bair.berkeley.edu/blog/2022/05/20/crosswords/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>We recently built the Berkeley Crossword Solver (BCS), the first computer program to beat every human competitor in the world\u2019s top crossword tournament. The BCS combines neural question answering and probabilistic inference to achieve near-perfect performance on most American-style crossword puzzles, like the one shown below:</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/crosswords/fig1.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 1: Example American-style crossword puzzle\n</i>\n</p>\n\n<p>Crosswords are challenging for humans and computers alike. Many clues are vague or underspecified and can\u2019t be answered until crossing constraints are taken into account. While some clues are similar to factoid question answering, others require relational reasoning or understanding difficult wordplay.</p>\n\n<!--more-->\n\n<p>Here are a handful of example clues from our dataset (answers at the bottom of this post):</p>\n<ul>\n  <li>They\u2019re given out at Berkeley\u2019s HAAS School (4)</li>\n  <li>Winter hrs. in Berkeley (3)</li>\n  <li>Domain ender that UC Berkeley was one of the first schools to adopt (3)</li>\n  <li>Angeleno at Berkeley, say (8)</li>\n</ul>\n\n<h1 id=\"our-approach\">Our Approach</h1>\n<p>The BCS uses a two-step process to solve crossword puzzles. First, it generates a probability distribution over possible answers to each clue using a question answering (QA) model; second, it uses probabilistic inference, combined with local search and a generative language model, to handle conflicts between proposed intersecting answers.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/crosswords/fig2.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 2: Architecture diagram of the Berkeley Crossword Solver\n</i>\n</p>\n\n<p>The BCS\u2019s question answering model is based on DPR [Karpukhin et al., 2020], which is a bi-encoder model typically used to retrieve passages that are relevant to a given question. Rather than passages, however, our approach maps both questions and answers into a shared embedding space and finds answers directly. Compared to the previous state-of-the-art method for answering crossword clues, this approach obtained a 13.4% absolute improvement in top-1000 QA accuracy. We conducted a manual error analysis and found that our QA model typically performed well on questions involving knowledge, commonsense reasoning, and definitions, but it often struggled to understand wordplay or theme-related clues.</p>\n\n<p>After running the QA model on each clue, the BCS runs loopy belief propagation to iteratively update the answer probabilities in the grid. This allows information from high confidence predictions to propagate to more challenging clues. After belief propagation converges, the BCS obtains an initial puzzle solution by greedily taking the highest likelihood answer at each position.</p>\n\n<p>The BCS then refines this solution using a local search that tries to replace low confidence characters in the grid. Local search works by using a guided proposal distribution in which characters that had lower marginal probabilities during belief propagation are iteratively replaced until a locally optimal solution is found. We score these alternate characters using a character-level language model (ByT5, Xue et al., 2022), that handles novel answers better than our closed-book QA model.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/crosswords/fig3.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 3: Example changes made by our local search procedure\n</i>\n</p>\n\n<h1 id=\"results\">Results</h1>\n<p>We evaluated the BCS on puzzles from five major crossword publishers, including The New York Times. Our system obtains 99.7% letter accuracy on average, which jumps to 99.9% if you ignore puzzles that involve rare themes. It solves 81.7% of puzzles without a single mistake, which is a 24.8% improvement over the previous state-of-the-art system.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/crosswords/fig4.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 4: Results compared to previous state-of-the-art Dr. Fill\n</i>\n</p>\n\n<h1 id=\"winning-the-american-crossword-puzzle-tournament\">Winning The American Crossword Puzzle Tournament</h1>\n<p>The American Crossword Puzzle Tournament (ACPT) is the largest and longest-running crossword tournament and is organized by Will Shortz, the New York Times crossword editor. Two prior approaches to computer crossword solving gained mainstream attention and competed in the ACPT: Proverb and Dr. Fill. Proverb is a 1998 system that ranked 213th out of 252 competitors in the tournament. Dr. Fill\u2019s first competition was in ACPT 2012, and it ranked 141st out of 650 competitors. We teamed up with Dr. Fill\u2019s creator Matt Ginsberg and combined an early version of our QA system with Dr. Fill\u2019s search procedure to win first place in the 2021 ACPT against over a thousand competitors. Our submission solved all seven puzzles in under a minute, missing just three letters across two puzzles.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/crosswords/fig5.png\" width=\"90%\" />\n    <br />\n<i>\nFigure 5: Results from the 2021 American Crossword Puzzle Tournament (ACPT)\n</i>\n</p>\n\n<p>We are really excited about the challenges that remain in crosswords, including handling difficult themes and more complex wordplay. To encourage future work, we are releasing a dataset of 6.4M question answer clues, a demo of the Berkeley Crossword Solver, and our code at <a href=\"http://berkeleycrosswordsolver.com\">http://berkeleycrosswordsolver.com</a>.</p>\n\n<p>Answers to clues: MBAS, PST, EDU, INSTATER</p>",
            "pubdate": "Fri, 20 May 2022 03:00:00 -0700",
            "pubdate_parsed": [
                2022,
                5,
                20
            ],
            "email_sent": true
        },
        "FIGS: Attaining XGBoost-level performance with the interpretability and speed of CART": {
            "url": "http://bair.berkeley.edu/blog/2022/06/30/figs/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p style=\"text-align: center;\">\n    <a href=\"https://arxiv.org/abs/2201.11931\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_intro.gif\" width=\"90%\" /></a>\n<br />\n<b>FIGS (Fast Interpretable Greedy-tree Sums): </b><i>A method for building interpretable models by simultaneously growing an ensemble of decision trees in competition with one another.</i>\n</p>\n\n<p>Recent machine-learning advances have led to increasingly complex predictive models, often at the cost of interpretability. We often need interpretability, particularly in high-stakes applications such as in clinical decision-making; interpretable models help with all kinds of things, such as identifying errors, leveraging domain knowledge, and making speedy predictions.</p>\n\n<p>In this blog post we\u2019ll cover <a href=\"https://arxiv.org/abs/2201.11931\">FIGS</a>, a new method for fitting an <em>interpretable model</em> that takes the form of a sum of trees. Real-world experiments and theoretical results show that FIGS can effectively adapt to a wide range of structure in data, achieving state-of-the-art performance in several settings, all without sacrificing interpretability.\n<!--more--></p>\n\n<h2 id=\"how-does-figs-work\">How does FIGS work?</h2>\n\n<p>Intuitively, FIGS works by extending CART, a typical greedy algorithm for growing a decision tree, to consider growing a <em>sum</em> of trees <em>simultaneously</em> (see Fig 1). At each iteration, FIGS may grow any existing tree it has already started or start a new tree; it greedily selects whichever rule reduces the total unexplained variance (or an alternative splitting criterion) the most. To keep the trees in sync with one another, each tree is made to predict the <em>residuals</em> remaining after summing the predictions of all other trees (see <a href=\"https://arxiv.org/abs/2201.11931\">the paper</a> for more details).</p>\n\n<p>FIGS is intuitively similar to ensemble approaches such as gradient boosting / random forest, but importantly since all trees are grown to compete with each other the model can adapt more to the underlying structure in the data. The number of trees and size/shape of each tree emerge automatically from the data rather than being manually specified.</p>\n\n<p style=\"text-align: center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_fitting.gif\" width=\"90%\" /></a>\n<br />\n<b>Fig 1. </b><i>High-level intuition for how FIGS fits a model.</i>\n</p>\n\n<h2 id=\"an-example-using-figs\">An example using <code class=\"language-plaintext highlighter-rouge\">FIGS</code></h2>\n\n<p>Using FIGS is extremely simple. It is easily installable through the <a href=\"https://github.com/csinva/imodels\">imodels package</a> (<code class=\"language-plaintext highlighter-rouge\">pip install imodels</code>) and then can be used in the same way as standard scikit-learn models: simply import a classifier or regressor and use the <code class=\"language-plaintext highlighter-rouge\">fit</code> and <code class=\"language-plaintext highlighter-rouge\">predict</code> methods. Here\u2019s a full example of using it on a sample clinical dataset in which the target is risk of cervical spine injury (CSI).</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">imodels</span> <span class=\"kn\">import</span> <span class=\"n\">FIGSClassifier</span><span class=\"p\">,</span> <span class=\"n\">get_clean_dataset</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n\n<span class=\"c1\"># prepare data (in this a sample clinical dataset)\n</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">feat_names</span> <span class=\"o\">=</span> <span class=\"n\">get_clean_dataset</span><span class=\"p\">(</span><span class=\"s\">'csi_pecarn_pred'</span><span class=\"p\">)</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.33</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># fit the model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FIGSClassifier</span><span class=\"p\">(</span><span class=\"n\">max_rules</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>  <span class=\"c1\"># initialize a model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>   <span class=\"c1\"># fit model\n</span><span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"c1\"># discrete predictions: shape is (n_test, 1)\n</span><span class=\"n\">preds_proba</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"c1\"># predicted probabilities: shape is (n_test, n_classes)\n</span>\n<span class=\"c1\"># visualize the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">feature_names</span><span class=\"o\">=</span><span class=\"n\">feat_names</span><span class=\"p\">,</span> <span class=\"n\">filename</span><span class=\"o\">=</span><span class=\"s\">'out.svg'</span><span class=\"p\">,</span> <span class=\"n\">dpi</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>This results in a simple model \u2013 it contains only 4 splits (since we specified that the model should have no more than 4 splits (<code class=\"language-plaintext highlighter-rouge\">max_rules=4</code>). Predictions are made by dropping a sample down every tree, and <i>summing</i> the risk adjustment values obtained from the resulting leaves of each tree. This model is extremely interpretable, as a physician can now (i) easily make predictions using the 4 relevant features and (ii) vet the model to ensure it matches their domain expertise. Note that this model is just for illustration purposes, and achieves ~84\\% accuracy.</p>\n\n<p style=\"text-align: center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_csi_model_small.svg\" width=\"85%\" /></a>\n<br />\n<i><b>Fig 2.</b> Simple model learned by FIGS for predicting risk of cervical spinal injury. </i>\n</p>\n\n<p>If we want a more flexible model, we can also remove the constraint on the number of rules (changing the code to <code class=\"language-plaintext highlighter-rouge\">model = FIGSClassifier()</code>), resulting in a larger model (see Fig 3). Note that the number of trees and how balanced they are emerges from the structure of the data \u2013 only the total number of rules may be specified.</p>\n\n<p style=\"text-align: center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_csi_model_large.svg\" width=\"100%\" /></a>\n<br />\n<i><b>Fig 3.</b> Slightly larger model learned by FIGS for predicting risk of cervical spinal injury. </i>\n</p>\n\n<h2 id=\"how-well-does-figs-perform\">How well does FIGS perform?</h2>\n\n<p>In many cases when interpretability is desired, such as <a href=\"https://arxiv.org/abs/2205.15135\">clinical-decision-rule modeling</a>, FIGS is able to achieve state-of-the-art performance. For example, Fig 4 shows different datasets where FIGS achieves excellent performance, particularly when limited to using very few total splits.</p>\n\n<p style=\"text-align: center;\">\n    <a href=\"https://github.com/csinva/imodels\"><img src=\"https://bair.berkeley.edu/static/blog/figs/figs_classification.png\" width=\"100%\" /></a>\n<br />\n<i><b>Fig 4.</b> FIGS predicts well with very few splits. </i>\n</p>\n\n<h2 id=\"why-does-figs-perform-well\">Why does FIGS perform well?</h2>\n\n<p>FIGS is motivated by the observation that single decision trees often have splits that are repeated in different branches, which may occur when there is <a href=\"https://proceedings.mlr.press/v151/shuo-tan22a/shuo-tan22a.pdf\">additive structure</a> in the data. Having multiple trees helps to avoid this by disentangling the additive components into separate trees.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Overall, interpretable modeling offers an alternative to common black-box modeling, and in many cases can offer massive improvements in terms of efficiency and transparency without suffering from a loss in performance.</p>\n\n<hr />\n\n<p><em>This post is based on two papers: <a href=\"https://arxiv.org/abs/2201.11931\">FIGS</a> and <a href=\"https://arxiv.org/abs/2205.15135\">G-FIGS</a> \u2013 all code is available through the <a href=\"https://github.com/csinva/imodels\">imodels package</a>. This is joint work with <a href=\"https://www.linkedin.com/in/nasseri/\">Keyan Nasseri</a>, <a href=\"https://www.linkedin.com/in/abhineet-agarwal-126171185/\">Abhineet Agarwal</a>, <a href=\"https://www.linkedin.com/in/james-pc-duncan/\">James Duncan</a>, <a href=\"https://www.linkedin.com/in/omer-ronen-48ba9412a/?originalSubdomain=il\">Omer Ronen</a>, and <a href=\"https://profiles.ucsf.edu/aaron.kornblith\">Aaron Kornblith</a>.</em></p>",
            "pubdate": "Thu, 30 Jun 2022 02:00:00 -0700",
            "pubdate_parsed": [
                2022,
                6,
                30
            ],
            "email_sent": true
        },
        "Why do Policy Gradient Methods work so well in Cooperative MARL? Evidence from Policy Representation": {
            "url": "http://bair.berkeley.edu/blog/2022/07/10/pg-ar/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>In cooperative multi-agent reinforcement learning (MARL), due to its <em>on-policy</em> nature, policy gradient (PG) methods are typically believed to be less sample efficient than value decomposition (VD) methods, which are <em>off-policy</em>. However, some <a href=\"https://arxiv.org/abs/2103.01955\">recent</a> <a href=\"https://arxiv.org/abs/2011.09533\">empirical</a> <a href=\"https://arxiv.org/abs/2006.07869\">studies</a> demonstrate that with proper input representation and hyper-parameter tuning, multi-agent PG can achieve <a href=\"http://bair.berkeley.edu/blog/2021/07/14/mappo/\">surprisingly strong performance</a> compared to off-policy VD methods.</p>\n\n<p><strong>Why could PG methods work so well?</strong> In this post, we will present concrete analysis to show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, VD can be problematic and lead to undesired outcomes. By contrast, PG methods with individual policies can converge to an optimal policy in these cases. In addition, PG methods with auto-regressive (AR) policies can learn multi-modal policies.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/ar.png\" width=\"80%\" />\n    <br />\n<i>\nFigure 1: different policy representation for the 4-player permutation game.\n</i>\n</p>\n\n<!--more-->\n\n<h2 id=\"ctde-in-cooperative-marl-vd-and-pg-methods\">CTDE in Cooperative MARL: VD and PG methods</h2>\n\n<p>Centralized training and decentralized execution (<a href=\"https://arxiv.org/abs/1706.02275\">CTDE</a>) is a popular framework in cooperative MARL. It leverages <em>global</em> information for more effective training while keeping the representation of individual policies for testing. CTDE can be implemented via value decomposition (VD) or policy gradient (PG), leading to two different types of algorithms.</p>\n\n<p>VD methods learn local Q networks and a mixing function that mixes the local Q networks to a global Q function. The mixing function is usually enforced to satisfy the Individual-Global-Max (<a href=\"https://arxiv.org/abs/1905.05408\">IGM</a>) principle, which guarantees the optimal joint action can be computed by greedily choosing the optimal action locally for each agent.</p>\n\n<p>By contrast, PG methods directly apply policy gradient to learn an individual policy and a centralized value function for each agent. The value function takes as its input the global state (e.g., <a href=\"https://arxiv.org/abs/2103.01955\">MAPPO</a>) or the concatenation of all the local observations (e.g., <a href=\"https://arxiv.org/abs/1706.02275\">MADDPG</a>), for an accurate global value estimate.</p>\n\n<h2 id=\"the-permutation-game-a-simple-counterexample-where-vd-fails\">The permutation game: a simple counterexample where VD fails</h2>\n\n<p>We start our analysis by considering a stateless cooperative game, namely the permutation game. In an $N$-player permutation game, each agent can output $N$ actions ${ 1,\\ldots, N }$. Agents receive $+1$ reward  if their actions are mutually different, i.e., the joint action is a permutation over $1, \\ldots, N$; otherwise, they receive $0$ reward. Note that there are $N!$ symmetric optimal strategies in this game.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/permutation_game.png\" width=\"70%\" />\n    <br />\n<i>\nFigure 2: the 4-player permutation game.\n</i>\n</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/vd_pg.png\" width=\"90%\" />\n    <br />\n    <i>\nFigure 3: high-level intuition on why VD fails in the 2-player permutation game.\n    </i>\n</p>\n<p>Let us focus on the 2-player permutation game now and apply VD to the game. In this stateless setting, we use  $Q_1$ and $Q_2$ to denote  the local Q-functions, and use $Q_\\textrm{tot}$ to denote  the global Q-function. The IGM principle requires that</p>\n\n\\[\\arg\\max_{a^1,a^2}Q_\\textrm{tot}(a^1,a^2)=\\{\\arg\\max_{a^1}Q_1(a^1),\\arg\\max_{a^2}Q_2(a^2)\\}.\\]\n\n<p>We prove that VD cannot represent the payoff of the 2-player permutation game by contradiction. If VD methods were able to represent the payoff, we would have</p>\n\n\\[Q_\\textrm{tot}(1, 2)=Q_\\textrm{tot}(2,1)=1\\quad \\text{and}\\quad Q_\\textrm{tot}(1, 1)=Q_\\textrm{tot}(2,2)=0.\\]\n\n<p>If either of these two agents has different local Q values (e.g. $Q_1(1)&gt; Q_1(2)$), we have $\\arg\\max_{a^1}Q_1(a^1)=1$. Then according to the IGM principle, <em>any</em> optimal joint action</p>\n\n\\[(a^{1\\star},a^{2\\star})=\\arg\\max_{a^1,a^2}Q_\\textrm{tot}(a^1,a^2)=\\{\\arg\\max_{a^1}Q_1(a^1),\\arg\\max_{a^2}Q_2(a^2)\\}\\]\n\n<p>satisfies $a^{1\\star}=1$ and $a^{1\\star}\\neq 2$, so the joint action $(a^1,a^2)=(2,1)$ is sub-optimal, i.e., $Q_\\textrm{tot}(2,1)&lt;1$.</p>\n\n<p>Otherwise, if $Q_1(1)=Q_1(2)$ and $Q_2(1)=Q_2(2)$, then</p>\n\n\\[Q_\\textrm{tot}(1, 1)=Q_\\textrm{tot}(2,2)=Q_\\textrm{tot}(1, 2)=Q_\\textrm{tot}(2,1).\\]\n\n<p>As a result, value decomposition cannot represent the payoff matrix of the 2-player permutation game.</p>\n\n<p>What about PG methods? Individual policies can indeed represent an optimal policy for the permutation game. Moreover, stochastic gradient descent can guarantee PG to converge to one of these optima <a href=\"https://arxiv.org/abs/1802.06175\">under mild assumptions</a>. This suggests that, even though PG methods are less popular in MARL compared with VD methods, they can be preferable in certain cases that are common in real-world applications, e.g., games with multiple strategy modalities.</p>\n\n<p>We also remark that in the permutation game, in order to represent an optimal joint policy, each agent must choose distinct actions. <strong>Consequently, a successful implementation of PG must ensure that the policies are agent-specific.</strong> This can be done by using either individual policies with unshared parameters (referred to as PG-Ind in our paper), or an agent-ID conditioned policy (<a href=\"http://bair.berkeley.edu/blog/2021/07/14/mappo/\">PG-ID</a>).</p>\n\n<h2 id=\"pg-outperforms-existing-vd-methods-on-popular-marl-testbeds\">PG outperforms existing VD methods on popular MARL testbeds</h2>\n\n<p>Going beyond the simple illustrative example of the permutation game, we extend our study to popular and more realistic MARL benchmarks. In addition to StarCraft Multi-Agent Challenge (<a href=\"https://github.com/oxwhirl/smac\">SMAC</a>), where the effectiveness of PG and agent-conditioned policy input <a href=\"http://bair.berkeley.edu/blog/2021/07/14/mappo/\">has been verified</a>, we show new results in Google Research Football (<a href=\"https://github.com/google-research/football\">GRF</a>) and multi-player <a href=\"https://github.com/deepmind/hanabi-learning-environment\">Hanabi Challenge</a>.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/football.png\" width=\"48%\" />\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/hanabi.png\" width=\"45%\" />\n    <br />\n<i>\nFigure 4: (left) winning rates of PG methods on GRF; (right) best and average evaluation scores on Hanabi-Full.\n</i>\n</p>\n\n<p>In GRF, PG methods outperform the state-of-the-art VD baseline (<a href=\"https://arxiv.org/abs/2106.02195\">CDS</a>) in 5 scenarios. Interestingly, we also notice that individual policies (PG-Ind) without parameter sharing achieve comparable, sometimes even higher winning rates, compared to agent-specific policies (PG-ID) in all 5 scenarios. We evaluate PG-ID in the full-scale Hanabi game with varying numbers of players (2-5 players) and compare them to <a href=\"https://arxiv.org/abs/1912.02288\">SAD</a>, a strong off-policy Q-learning variant in Hanabi, and Value Decomposition Networks (<a href=\"https://arxiv.org/abs/1706.05296\">VDN</a>). As demonstrated in the above table, PG-ID is able to produce results comparable to or better than the best and average rewards achieved by SAD and VDN with varying numbers of players using the same number of environment steps.</p>\n\n<h2 id=\"beyond-higher-rewards-learning-multi-modal-behavior-via-auto-regressive-policy-modeling\">Beyond higher rewards: learning multi-modal behavior via auto-regressive policy modeling</h2>\n\n<p>Besides learning higher rewards, we also study how to learn multi-modal policies in cooperative MARL. Let\u2019s go back to the permutation game. Although we have proved that PG can effectively learn an optimal policy, the strategy mode that it finally reaches can highly depend on the policy initialization. Thus, a natural question will be:</p>\n\n<p style=\"text-align: center;\">\n    <i>\nCan we learn a single policy that can cover all the optimal modes?\n    </i>\n</p>\n\n<p>In the decentralized PG formulation, the factorized representation of a joint policy can only represent one particular mode. Therefore, we propose an enhanced way to parameterize the policies for stronger expressiveness \u2014 the auto-regressive (AR) policies.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/pg-ar/permutation_ar.gif\" width=\"80%\" />\n<br />\n<i>\nFigure 5: comparison between individual policies (PG) and auto-regressive  policies (AR) in the 4-player permutation game.\n</i>\n</p>\n\n<p>Formally, we factorize the joint policy of $n$ agents into the form of</p>\n\n\\[\\pi(\\mathbf{a} \\mid \\mathbf{o}) \\approx \\prod_{i=1}^n \\pi_{\\theta^{i}} \\left( a^{i}\\mid o^{i},a^{1},\\ldots,a^{i-1} \\right),\\]\n\n<p>where the action produced by agent $i$ depends on its own observation $o_i$ and all the actions from previous agents $1,\\dots,i-1$. The auto-regressive factorization can represent <em>any</em> joint policy in a centralized MDP. The <em>only</em> modification to each agent\u2019s policy is the input dimension, which is slightly enlarged by including previous actions; and the output dimension of each agent\u2019s policy remains unchanged.</p>\n\n<p>With such a minimal parameterization overhead, AR policy substantially improves the representation power of PG methods. We remark that PG with AR policy (PG-AR) can simultaneously represent all optimal policy modes in the permutation game.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/heatmap.png\" width=\"70%\" />\n    <br />\n<i>\nFigure: the heatmaps of actions for policies learned by PG-Ind (left) and PG-AR (middle), and the heatmap for rewards (right); while PG-Ind only converge to a specific mode in the 4-player permutation game, PG-AR successfully discovers all the optimal modes.\n</i>\n</p>\n\n<p>In more complex environments, including SMAC and GRF, PG-AR can learn interesting emergent behaviors that require strong intra-agent coordination that may never be learned by PG-Ind.</p>\n\n<p style=\"text-align: center;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/2m1z.gif\" width=\"45%\" />\n    <img src=\"https://bair.berkeley.edu/static/blog/pg-ar/3v1.gif\" width=\"45%\" />\n    <br />\n<i>\nFigure 6: (left) emergent behavior induced by PG-AR in SMAC and GRF. On the 2m_vs_1z map of SMAC, the marines keep standing and attack alternately while ensuring there is only one attacking marine at each timestep; (right) in the academy_3_vs_1_with_keeper scenario of GRF, agents learn a \"Tiki-Taka\" style behavior: each player keeps passing the ball to their teammates.\n</i>\n</p>\n\n<h2 id=\"discussions-and-takeaways\">Discussions and Takeaways</h2>\n\n<p>In this post, we provide a concrete analysis of VD and PG methods in cooperative MARL. First, we reveal the limitation on the expressiveness of popular VD methods, showing that they could not represent optimal policies even in a simple permutation game. By contrast, we show that PG methods are provably more expressive. We empirically verify the expressiveness advantage of PG on popular MARL testbeds, including SMAC, GRF, and Hanabi Challenge. We hope the insights from this work could benefit the community towards more general and more powerful cooperative MARL algorithms in the future.</p>\n\n<hr />\n\n<p><em>This post is based on our paper: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (<a href=\"https://arxiv.org/abs/2206.07505\">paper</a>, <a href=\"https://sites.google.com/view/revisiting-marl\">website</a>).</em></p>",
            "pubdate": "Sun, 10 Jul 2022 02:00:00 -0700",
            "pubdate_parsed": [
                2022,
                7,
                10
            ],
            "email_sent": true
        },
        "Keeping Learning-Based Control Safe by Regulating Distributional Shift": {
            "url": "http://bair.berkeley.edu/blog/2022/09/19/ldm-control/",
            "description": "<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/header.jpg\" width=\"80%\" />\n<br />\n<i> To regulate the distribution shift experience by learning-based controllers, we seek a mechanism for constraining the agent to regions of high data density throughout its trajectory (left). Here, we present an approach which achieves this goal by combining features of density models (middle) and Lyapunov functions (right).</i>\n</p>\n\n<p>In order to make use of machine learning and reinforcement learning in controlling real world systems, we must design algorithms which not only achieve good performance, but also interact with the system in a safe and reliable manner. Most prior work on safety-critical control focuses on maintaining the safety of the <em>physical  system</em>, e.g. avoiding falling over for legged robots, or colliding into obstacles for autonomous vehicles. However, for learning-based controllers, there is another source of safety concern: because machine learning models are only optimized to output correct predictions on the training data, they are prone to outputting erroneous predictions when evaluated on out-of-distribution inputs. Thus, if an agent visits a state or takes an action that is very different from those in the training data, a learning-enabled controller may \u201cexploit\u201d the inaccuracies in its learned component and output actions that are suboptimal or even dangerous.</p>\n\n<!--more-->\n\n<p>To prevent these potential \u201cexploitations\u201d of model inaccuracies, we propose a new framework to reason about the safety of a learning-based controller with respect to its <em>training distribution</em>. The central idea behind our work is to view the training data distribution as a safety constraint, and to draw on tools from control theory to control the distributional shift experienced by the agent during closed-loop control. More specifically, we\u2019ll discuss how Lyapunov stability can be unified with density estimation to produce Lyapunov density models, a new kind of safety \u201cbarrier\u201d function which can be used to synthesize controllers with guarantees of keeping the agent in regions of high data density. Before we introduce our new framework, we will first give an overview of existing techniques for guaranteeing physical safety via barrier function.</p>\n\n<h1 id=\"guaranteeing-safety-via-barrier-functions\">Guaranteeing Safety via Barrier Functions</h1>\n<p>In control theory, a central topic of study is: given <em>known</em> system dynamics, $s_{t+1}=f(s_t, a_t)$, and <em>known</em> system constraints, $s \\in C$, how can we design a controller that is guaranteed to keep the system within the specified constraints? Here, $C$ denotes the set of states that are deemed safe for the agent to visit. This problem is challenging because the specified constraints need to be satisfied over the agent\u2019s entire trajectory horizon ($s_t \\in C$  $\\forall 0\\leq t \\leq T$). If the controller uses a simple \u201cgreedy\u201d strategy of avoiding constraint violations in the next time step (not taking $a_t$ for which $f(s_t, a_t) \\notin C$), the system may still end up in an \u201cirrecoverable\u201d state, which itself is considered safe, but will inevitably lead to an unsafe state in the future regardless of the agent\u2019s future actions. In order to avoid visiting these \u201cirrecoverable\u201d states, the controller must employ a more \u201clong-horizon\u201d strategy which involves predicting the agent\u2019s entire future trajectory to avoid safety violations at any point in the future (avoid $a_t$ for which all possible $\\{ a_{\\hat{t}} \\}_{\\hat{t}=t+1}^H$ lead to some $\\bar{t}$ where $s_{\\bar{t}} \\notin C$ and $t&lt;\\bar{t} \\leq T$). However, predicting the agent\u2019s full trajectory at every step is extremely computationally intensive, and often infeasible to perform online during run-time.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_1.jpg\" width=\"40%\" />\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_2.jpg\" width=\"40%\" />\n<br />\n<i> Illustrative example of a drone whose goal is to fly as straight as possible while avoiding obstacles. Using the \u201cgreedy\u201d strategy of avoiding safety violations (left), the drone flies straight because there\u2019s no obstacle in the next timestep, but inevitably crashes in the future because it can\u2019t turn in time. In contrast, using the \u201clong-horizon\u201d strategy (right), the drone turns early and successfully avoids the tree, by considering the entire future horizon future of its trajectory.</i>\n</p>\n\n<p>Control theorists tackle this challenge by designing \u201cbarrier\u201d functions, $v(s)$, to constrain the controller at each step (only allow $a_t$ which satisfy $v(f(s_t, a_t)) \\leq 0$). In order to ensure the agent remains safe throughout its entire trajectory, the constraint induced by barrier functions ($v(f(s_t, a_t))\\leq 0$) prevents the agent from visiting both unsafe states and irrecoverable states which inevitably lead to unsafe states in the future. This strategy essentially amortizes the computation of looking into the future for inevitable failures when designing the safety barrier function, which only needs to be done once and can be computed offline. This way, at runtime, the policy only needs to employ the greedy constraint satisfaction strategy on the barrier function $v(s)$ in order to ensure safety for all future timesteps.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/blog_fig_4.jpg\" width=\"50%\" />\n<br />\n<i> The blue region denotes the of states allowed by the barrier function constraint, $\\{s | v(s) \\leq 0\\}$. Using a \u201clong-horizon\u201d barrier function, the drone only needs to greedily ensure that the barrier function constraint $v(s) \\leq 0$ is satisfied for the next state, in order to avoid safety violations for all future timesteps. </i>\n</p>\n\n<p>Here, we used the notion of a \u201cbarrier\u201d function as an umbrella term to describe a number of different kinds of functions whose functionalities are to constrain the controller in order to make long-horizon guarantees. Some specific examples include <a href=\"https://link.springer.com/chapter/10.1007/978-1-4757-3108-8_5\">control Lyapunov functions</a> for guaranteeing stability, <a href=\"https://arxiv.org/abs/1903.11199\">control barrier functions</a> for guaranteeing general safety constraints, and the value function in <a href=\"https://arxiv.org/abs/1709.07523\">Hamilton-Jacobi reachability</a> for guaranteeing general safety constraints under external disturbances. More recently, there has also been <a href=\"https://arxiv.org/abs/1705.08551\">some</a> <a href=\"https://arxiv.org/abs/1805.07708\">work</a> on learning barrier functions, for settings where the system is unknown or where barrier functions are difficult to design. However, prior works in both traditional and learning-based barrier functions are mainly focused on making guarantees of physical safety. In the next section, we will discuss how we can extend these ideas to regulate the distribution shift experienced by the agent when using a learning-based controller.</p>\n\n<h1 id=\"lyapunov-density-models\">Lyapunov Density Models</h1>\n<p>To prevent model exploitation due to distribution shift, many learning-based control algorithms constrain or regularize the controller to prevent the agent from taking low-likelihood actions or visiting low likelihood states, for instance in <a href=\"https://arxiv.org/abs/2006.04779\">offline RL</a>, <a href=\"https://arxiv.org/abs/2005.13239\">model-based RL</a>, and <a href=\"https://arxiv.org/abs/1606.03476\">imitation learning</a>. However, most of these methods only constrain the controller with a single-step estimate of the data distribution, akin to the \u201cgreedy\u201d strategy of keeping an autonomous drone safe by preventing actions which causes it to crash in the next timestep. As we saw in the illustrative figures above, this strategy is not enough to guarantee that the drone will not crash (or go out-of-distribution) in another future timestep.</p>\n\n<p>How can we design a controller for which the agent is guaranteed to stay in-distribution for its entire trajectory? Recall that barrier functions can be used to guarantee constraint satisfaction for all future timesteps, which is exactly the kind of guarantee we hope to make with regards to the data distribution. Based on this observation, we propose a new kind of barrier function: the Lyapunov density model (LDM), which merges the dynamics-aware aspect of a Lyapunov function with the data-aware aspect of a density model (it is in fact a generalization of both types of function). Analogous to how Lyapunov functions keeps the system from becoming physically unsafe, our Lyapunov density model keeps the system from going out-of-distribution.</p>\n\n<p>An LDM ($G(s, a)$) maps state and action pairs to negative log densities, where the values of $G(s, a)$ represent the best data density the agent is able to stay above throughout its trajectory. It can be intuitively thought of as a \u201cdynamics-aware, long-horizon\u201d transformation on a single-step density model ($E(s, a)$), where $E(s, a)$ approximates the negative log likelihood of the data distribution. Since a single-step density model constraint ($E(s, a) \\leq -\\log(c)$ where $c$ is a cutoff density) might still allow the agent to visit \u201cirrecoverable\u201d states which inevitably causes the agent to go out-of-distribution, the LDM transformation increases the value of those \u201cirrecoverable\u201d states until they become \u201crecoverable\u201d with respect to their updated value. As a result, the LDM constraint ($G(s, a) \\leq -\\log(c)$) restricts the agent to a smaller set of states and actions which excludes the \u201cirrecoverable\u201d states, thereby ensuring the agent is able to remain in high data-density regions throughout its entire trajectory.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/jason.jpg\" width=\"80%\" />\n<br />\n<i> Example of data distributions (middle) and their associated LDMs (right) for a 2D linear system (left). LDMs can be viewed as \"dynamics-aware, long-horizon\" transformations on density models. </i>\n</p>\n\n<p>How exactly does this \u201cdynamics-aware, long-horizon\u201d transformation work? Given a data distribution $P(s, a)$ and dynamical system $s_{t+1} = f(s_t, a_t)$, we define the following as the LDM operator: $\\mathcal{T}G(s, a) = \\max\\{-\\log P(s, a), \\min_{a\u2019} G(f(s, a), a\u2019)\\}$. Suppose we initialize $G(s, a)$ to be $-\\log P(s, a)$. Under one iteration of the LDM operator, the value of a state action pair, $G(s, a)$, can either remain at $-\\log P(s, a)$ or increase in value, depending on whether the value at the best state action pair in the next timestep, $\\min_{a\u2019} G(f(s, a), a\u2019)$, is larger than $-\\log P(s, a)$. Intuitively, if the value at the best next state action pair is larger than the current $G(s, a)$ value, this means that the agent is unable to remain at the current density level regardless of its future actions, making the current state \u201cirrecoverable\u201d with respect to the current density level. By increasing the current the value of $G(s, a)$, we are \u201ccorrecting\u201d the LDM such that its constraints would not include \u201cirrecoverable\u201d states. Here, one LDM operator update captures the effect of looking into the future for one timestep. If we repeatedly apply the LDM operator on $G(s, a)$ until convergence, the final LDM will be free of \u201cirrecoverable\u201d states in the agent\u2019s entire future trajectory.</p>\n\n<p>To use an LDM in control, we can train an LDM and learning-based controller on the same training dataset and constrain the controller\u2019s action outputs with an LDM constraint ($G(s, a)) \\leq -\\log(c)$). Because the LDM constraint prevents both states with low density and \u201cirrecoverable\u201d states, the learning-based controller will be able to avoid out-of-distribution inputs throughout the agent\u2019s entire trajectory. Furthermore, by choosing the cutoff density of the LDM constraint, $c$, the user is able to control the tradeoff between protecting against model error vs. flexibility for performing the desired task.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/hopper.gif\" width=\"80%\" />\n<br />\n<i> Example evaluation of ours and baseline methods on a hopper control task for different values of constraint thresholds (x- axis). On the right, we show example trajectories from when the threshold is too low (hopper falling over due to excessive model exploitation), just right (hopper successfully hopping towards target location), or too high (hopper standing still due to over conservatism). </i>\n</p>\n\n<p>So far, we have only discussed the properties of a \u201cperfect\u201d LDM, which can be found if we had oracle access to the data distribution and dynamical system. In practice, though, we approximate the LDM using only data samples from the system. This causes a problem to arise: even though the role of the LDM is to prevent distribution shift, the LDM itself can also suffer from the negative effects of distribution shift, which degrades its effectiveness for preventing distribution shift. To understand the degree to which the degradation happens, we analyze this problem from both a theoretical and empirical perspective. Theoretically, we show even if there are errors in the LDM learning procedure, an LDM constrained controller is still able to maintain guarantees of keeping the agent in-distribution. Albeit, this guarantee is a bit weaker than the original guarantee provided by a perfect LDM, where the amount of degradation depends on the scale of the errors in the learning procedure. Empirically, we approximate the LDM using deep neural networks, and show that using a learned LDM to constrain the learning-based controller still provides performance improvements compared to using single-step density models on several domains.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ldm-control/bar.jpg\" width=\"80%\" />\n<br />\n<i> Evaluation of our method (LDM) compared to constraining a learning-based controller with a density model, the variance over an ensemble of models, and no constraint at all on several domains including hopper, lunar lander, and glucose control. </i>\n</p>\n\n<h1 id=\"conclusion-and-takeaways\">Conclusion and Takeaways</h1>\n<p>Currently, one of the biggest challenges in deploying learning-based controllers on real world systems is their potential brittleness to out-of-distribution inputs, and lack of guarantees on performance. Conveniently, there exists a large body of work in control theory focused on making guarantees about how systems evolve. However, these works usually focus on making guarantees with respect to physical safety requirements, and assume access to an accurate dynamics model of the system as well as physical safety constraints. The central idea behind our work is to instead view the training data distribution as a safety constraint. This allows us to make use of these ideas in controls in our design of learning-based control algorithms, thereby inheriting both the scalability of machine learning and the rigorous guarantees of control theory.</p>\n\n<p><i>This post is based on the paper \u201cLyapunov Density Models: Constraining Distribution Shift in Learning-Based Control\u201d, presented at ICML 2022. You\nfind more details in <a href=\"https://arxiv.org/abs/2206.10524\">our paper</a> and on our <a href=\"https://sites.google.com/berkeley.edu/ldm/\">website</a>. We thank Sergey Levine, Claire Tomlin, Dibya Ghosh, Jason Choi, Colin Li, and Homer Walke for their valuable feedback on this blog post.</i></p>",
            "pubdate": "Mon, 19 Sep 2022 02:00:00 -0700",
            "pubdate_parsed": [
                2022,
                9,
                19
            ],
            "email_sent": true
        },
        "Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation": {
            "url": "http://bair.berkeley.edu/blog/2023/01/20/relmm/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>Reinforcement learning provides a conceptual framework for autonomous agents to learn from experience, analogously to how one might train a pet with treats. But practical applications of reinforcement learning are often far from natural: instead of using RL to learn through trial and error by actually attempting the desired task, typical RL applications use a separate (usually simulated) training phase. For example, <a href=\"https://deepmind.com/research/case-studies/alphago-the-story-so-far\">AlphaGo</a> did not learn to play Go by competing against thousands of humans, but rather by playing against itself in simulation. While this kind of simulated training is appealing for games where the rules are perfectly known, applying this to real world domains such as robotics can require a range of complex approaches, such as <a href=\"https://www.youtube.com/watch?v=XUW0cnvqbwM\">the use of simulated data</a>, or instrumenting real-world environments in various ways to make training feasible <a href=\"https://bair.berkeley.edu/blog/2020/04/27/ingredients/\">under laboratory conditions</a>. Can we instead devise reinforcement learning systems for robots that allow them to learn directly \u201con-the-job\u201d, while performing the task that they are required to do? In this blog post, we will discuss ReLMM, a system that we developed that learns to clean up a room directly with a real robot via continual learning.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image8.gif\" width=\"48%\" />\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image12.gif\" width=\"48%\" />\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image3.gif\" width=\"48%\" />\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image2.gif\" width=\"48%\" />\n<br />\n<i>We evaluate our method on different tasks that range in difficulty. The top-left task has uniform white blobs to pickup with no obstacles, while other rooms have objects of diverse shapes and colors, obstacles that increase navigation difficulty and obscure the objects and patterned rugs that make it difficult to see the objects against the ground.</i>\n</p>\n\n<!--more-->\n\n<p>To enable \u201con-the-job\u201d training in the real world, the difficulty of collecting more experience is prohibitive. If we can make training in the real world easier, by making the data gathering process more autonomous without requiring human monitoring or intervention, we can further benefit from the simplicity of agents that learn from experience. In this work, we design an \u201con-the-job\u201d mobile robot training system for cleaning by learning to grasp objects throughout different rooms.</p>\n\n<h1 id=\"lesson-1-the-benefits-of-modular-policies-for-robots\">Lesson 1: The Benefits of Modular Policies for Robots.</h1>\n\n<p>People are not born one day and performing job interviews the next. There are many levels of tasks people learn before they apply for a job as we start with the easier ones and build on them. In ReLMM, we make use of this concept by allowing robots to train common-reusable skills, such as grasping, by first encouraging the robot to prioritize training these skills before learning later skills, such as navigation. Learning in this fashion has two advantages for robotics. The first advantage is that when an agent focuses on learning a skill, it is more efficient at collecting data around the local state distribution for that skill.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image13.png\" width=\"50%\" />\n<br />\n</p>\n\n<p>That is shown in the figure above, where we evaluated the amount of prioritized grasping experience needed to result in efficient mobile manipulation training. The second advantage to a multi-level learning approach is that we can inspect the models trained for different tasks and ask them questions, such as, \u201ccan you grasp anything right now\u201d which is helpful for navigation training that we describe next.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image14.png\" width=\"50%\" />\n<br />\n</p>\n\n<p>Training this multi-level policy was not only more efficient than learning both skills at the same time but it allowed for the grasping controller to inform the navigation policy. Having a model that estimates the uncertainty in its grasp success (<strong>Ours</strong> above) can be used to improve navigation exploration by skipping areas without graspable objects, in contrast to <strong>No Uncertainty Bonus</strong> which does not use this information. The model can also be used to relabel data during training so that in the unlucky case when the grasping model was unsuccessful trying to grasp an object within its reach, the grasping policy can still provide some signal by indicating that an object was there but the grasping policy has not yet learned how to grasp it. Moreover, learning modular models has engineering benefits. Modular training allows for reusing skills that are easier to learn and can enable building intelligent systems one piece at a time. This is beneficial for many reasons, including safety evaluation and understanding.</p>\n\n<h1 id=\"lesson-2-learning-systems-beat-hand-coded-systems-given-time\">Lesson 2: Learning systems beat hand-coded systems, given time</h1>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/relmm/image15.png\" width=\"50%\" />\n<br />\n</p>\n\n<p>Many robotics tasks that we see today can be solved to varying levels of success using hand-engineered controllers. For our room cleaning task, we designed a hand-engineered controller that locates objects using image clustering and turns towards the nearest detected object at each step. This expertly designed controller performs very well on the visually salient balled socks and takes reasonable paths around the obstacles <strong>but it can not learn an optimal path to collect the objects quickly, and it struggles with visually diverse rooms</strong>. As shown in video 3 below, the scripted policy gets distracted by the white patterned carpet while trying to locate more white objects to grasp.</p>\n\n<p style=\"text-align: center;\">\n1) <img src=\"https://bair.berkeley.edu/static/blog/relmm/image5.gif\" width=\"45%\" />\n2) <img src=\"https://bair.berkeley.edu/static/blog/relmm/image6.gif\" width=\"45%\" />\n<br />\n3) <img src=\"https://bair.berkeley.edu/static/blog/relmm/image1.gif\" width=\"45%\" />\n4) <img src=\"https://bair.berkeley.edu/static/blog/relmm/image9.png\" width=\"45%\" />\n<br />\n<i>We show a comparison between (1) our policy at the beginning of training (2) our policy at the end of training (3) the scripted policy. In (4) we can see the robot's performance improve over time, and eventually exceed the scripted policy at quickly collecting the objects in the room.</i>\n</p>\n\n<p>Given we can use experts to code this hand-engineered controller, what is the purpose of learning? An important limitation of hand-engineered controllers is that they are tuned for a particular task, for example, grasping white objects. When diverse objects are introduced, which differ in color and shape, the original tuning may no longer be optimal. Rather than requiring further hand-engineering, our learning-based method is able to adapt itself to various tasks by collecting its own experience.</p>\n\n<p>However, the most important lesson is that even if the hand-engineered controller is capable, the learning agent eventually surpasses it given enough time. This learning process is itself autonomous and takes place while the robot is performing its job, making it comparatively inexpensive. This shows the capability of learning agents, which can also be thought of as working out a general way to perform an \u201cexpert manual tuning\u201d process for any kind of task. Learning systems have the ability to create the entire control algorithm for the robot, and are not limited to tuning a few parameters in a script. The key step in this work allows these real-world learning systems to autonomously collect the data needed to enable the success of learning methods.</p>\n\n<p><i>This post is based on the paper \u201cFully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation\u201d, presented at CoRL 2021. You can find more details in <a href=\"https://arxiv.org/abs/2107.13545\">our paper</a>, on our <a href=\"https://sites.google.com/view/relmm\">website</a> and the on the <a href=\"https://drive.google.com/file/d/1BsqXvxv0ByGIXxGb3zBYBncL9pKaxWuX/view?usp=sharing\">video</a>. We provide <a href=\"https://github.com/charlesjsun/ReLMM\">code</a> to reproduce our experiments. We thank Sergey Levine for his valuable feedback on this blog post.</i></p>",
            "pubdate": "Fri, 20 Jan 2023 01:00:00 -0800",
            "pubdate_parsed": [
                2023,
                1,
                20
            ],
            "email_sent": true
        },
        "Interactive Fleet Learning": {
            "url": "http://bair.berkeley.edu/blog/2023/04/06/ifl/",
            "description": "<!-- twitter -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ifl/figure1.gif\" width=\"75%\" />\n<br />\n<i>Figure 1: \u201cInteractive Fleet Learning\u201d (IFL) refers to robot fleets in industry and academia that fall back on human teleoperators when necessary and continually learn from them over time.</i>\n</p>\n\n<p>In the last few years we have seen an exciting development in robotics and artificial intelligence: large fleets of robots have left the lab and entered the real world. <a href=\"https://waymo.com/\">Waymo</a>, for example, has over 700 self-driving cars operating in Phoenix and San Francisco and is <a href=\"https://blog.waymo.com/2022/10/next-stop-for-waymo-one-los-angeles.html\">currently expanding to Los Angeles</a>. Other industrial deployments of robot fleets include applications like e-commerce order fulfillment at <a href=\"https://www.amazon.com/\">Amazon</a> and <a href=\"https://www.ambirobotics.com/\">Ambi Robotics</a> as well as food delivery at <a href=\"https://www.nuro.ai/\">Nuro</a> and <a href=\"https://www.kiwibot.com/\">Kiwibot</a>.</p>\n\n<!--more-->\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ifl/figure2.jpg\" width=\"95%\" />\n<br />\n<i>Commercial and industrial deployments of robot fleets: package delivery (top left), food delivery (bottom left), e-commerce order fulfillment at Ambi Robotics (top right), autonomous taxis at Waymo (bottom right).</i>\n</p>\n\n<p>These robots use recent advances in deep learning to operate autonomously in unstructured environments. By pooling data from all robots in the fleet, the entire fleet can efficiently learn from the experience of each individual robot. Furthermore, due to advances in <a href=\"https://ieeexplore.ieee.org/document/7006734\">cloud robotics</a>, the fleet can offload data, memory, and computation (e.g., training of large models) to the cloud via the Internet. This approach is known as \u201cFleet Learning,\u201d a term popularized by Elon Musk in <a href=\"https://electrek.co/2016/09/11/transcript-elon-musks-press-conference-about-tesla-autopilot-under-v8-0-update-part-1/\">2016 press releases about Tesla Autopilot</a> and used in press communications by <a href=\"https://www.tri.global/news/tri-teaching-robots-help-people-their-homes\">Toyota Research Institute</a>, <a href=\"https://wayve.ai/technology/fleet-learning-technology/\">Wayve AI</a>, and others. A robot fleet is a modern analogue of a fleet of ships, where the word <em>fleet</em> has an etymology tracing back to <em>fl\u0113ot</em> (\u2018ship\u2019) and <em>fl\u0113otan</em> (\u2018float\u2019) in Old English.</p>\n\n<p>Data-driven approaches like fleet learning, however, face the problem of the <a href=\"https://www.forbes.com/sites/lanceeliot/2021/07/13/whether-those-endless-edge-or-corner-cases-are-the-long-tail-doom-for-ai-self-driving-cars/?sh=573981be5933\">\u201clong tail\u201d</a>: the robots inevitably encounter new scenarios and edge cases that are not represented in the dataset. Naturally, we can\u2019t expect the future to be the same as the past! How, then, can these robotics companies ensure sufficient reliability for their services?</p>\n\n<p>One answer is to fall back on remote humans over the Internet, who can interactively take control and \u201ctele-operate\u201d the system when the robot policy is unreliable during task execution. Teleoperation has a rich history in robotics: <a href=\"https://goldberg.berkeley.edu/pubs/Nature-Robots-and-Return-to-Collaborative-Intelligence.pdf\">the world\u2019s first robots were teleoperated</a> during WWII to handle radioactive materials, and the <a href=\"https://en.wikipedia.org/wiki/Telegarden\">Telegarden</a> pioneered robot control over the Internet in 1994. With continual learning, the human teleoperation data from these interventions can iteratively improve the robot policy and reduce the robots\u2019 reliance on their human supervisors over time. Rather than a discrete jump to full robot autonomy, this strategy offers a continuous alternative that approaches full autonomy over time while simultaneously enabling reliability in robot systems <em>today</em>.</p>\n\n<p>The use of human teleoperation as a fallback mechanism is increasingly popular in modern robotics companies: Waymo calls it <a href=\"https://www.theatlantic.com/technology/archive/2018/08/waymos-robot-cars-and-the-humans-who-tend-to-them/568051/\">\u201cfleet response,\u201d</a> Zoox calls it <a href=\"https://twitter.com/zoox/status/1415737908112203776\">\u201cTeleGuidance,\u201d</a> and Amazon calls it <a href=\"https://www.amazon.science/latest-news/robin-deals-with-a-world-where-things-are-changing-all-around-it\">\u201ccontinual learning.\u201d</a> Last year, a software platform for remote driving called <a href=\"https://phantom.auto/\">Phantom Auto</a> was recognized by Time Magazine as one of their <a href=\"https://time.com/collection/best-inventions-2022/6224834/phantom-auto-remote-operation-platform-for-logistics/\">Top 10 Inventions of 2022</a>. And just last month, <a href=\"https://www.therobotreport.com/john-deere-acquires-sparkais-human-in-the-loop-tech/\">John Deere acquired SparkAI</a>, a startup that develops software for resolving edge cases with humans in the loop.</p>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ifl/figure3.jpg\" width=\"95%\" />\n<br />\n<i>A remote human teleoperator at Phantom Auto, a software platform for enabling remote driving over the Internet.</i>\n</p>\n\n<p>Despite this growing trend in industry, however, there has been comparatively little focus on this topic in academia. As a result, robotics companies have had to rely on ad hoc solutions for determining when their robots should cede control. The closest analogue in academia is <a href=\"https://arxiv.org/abs/2211.00600\">interactive imitation learning (IIL)</a>, a paradigm in which a robot intermittently cedes control to a human supervisor and learns from these interventions over time. There have been a number of IIL algorithms in recent years for the single-robot, single-human setting including <a href=\"https://arxiv.org/abs/1011.0686\">DAgger</a> and variants such as <a href=\"https://arxiv.org/abs/1810.02890\">HG-DAgger</a>, <a href=\"https://arxiv.org/abs/1605.06450\">SafeDAgger</a>, <a href=\"https://arxiv.org/abs/1807.08364\">EnsembleDAgger</a>, and <a href=\"https://arxiv.org/abs/2109.08273\">ThriftyDAgger</a>; nevertheless, when and how to switch between robot and human control is still an open problem. This is even less understood when the notion is generalized to robot fleets, with multiple robots and multiple human supervisors.</p>\n\n<h2 id=\"ifl-formalism-and-algorithms\">IFL Formalism and Algorithms</h2>\n\n<p>To this end, in a <a href=\"https://proceedings.mlr.press/v205/hoque23a.html\">recent paper at the Conference on Robot Learning</a> we introduced the paradigm of <em>Interactive Fleet Learning (IFL)</em>, the first formalism in the literature for interactive learning with multiple robots and multiple humans. As we\u2019ve seen that this phenomenon already occurs in industry, we can now use the phrase \u201cinteractive fleet learning\u201d as unified terminology for robot fleet learning that falls back on human control, rather than keep track of the names of every individual corporate solution (\u201cfleet response\u201d, \u201cTeleGuidance\u201d, etc.). IFL scales up robot learning with four key components:</p>\n\n<ol>\n  <li><strong>On-demand supervision.</strong> Since humans cannot effectively monitor the execution of multiple robots at once and are prone to fatigue, the allocation of robots to humans in IFL is automated by some allocation policy $\\omega$. Supervision is requested \u201con-demand\u201d by the robots rather than placing the burden of continuous monitoring on the humans.</li>\n  <li><strong>Fleet supervision.</strong> On-demand supervision enables effective allocation of limited human attention to large robot fleets. IFL allows the number of robots to significantly exceed the number of humans (e.g., by a factor of 10:1 or more).</li>\n  <li><strong>Continual learning.</strong> Each robot in the fleet can learn from its own mistakes as well as the mistakes of the other robots, allowing the amount of required human supervision to taper off over time.</li>\n  <li><strong>The Internet.</strong> Thanks to mature and ever-improving Internet technology, the human supervisors do not need to be physically present. Modern computer networks enable <a href=\"https://venturebeat.com/business/how-teleoperation-could-enable-remote-work-for-more-industries/\">real-time remote teleoperation</a> at vast distances.</li>\n</ol>\n\n<p style=\"text-align: center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ifl/figure4.jpg\" width=\"95%\" />\n<br />\n<i>In the Interactive Fleet Learning (IFL) paradigm, M humans are allocated to the robots that need the most help in a fleet of N robots (where N can be much larger than M). The robots share policy $\\pi_{\\theta_t}$ and learn from human interventions over time.</i>\n</p>\n\n<p>We assume that the robots share a common control policy $\\pi_{\\theta_t}$ and that the humans share a common control policy $\\pi_H$. We also assume that the robots operate in independent environments with identical state and action spaces (but not identical states). Unlike a robot <em>swarm</em> of typically low-cost robots that coordinate to achieve a common objective in a shared environment, a robot <em>fleet</em> simultaneously executes a shared policy in distinct parallel environments (e.g., different bins on an assembly line).</p>\n\n<p>The goal in IFL is to find an optimal supervisor allocation policy $\\omega$, a mapping from $\\mathbf{s}^t$ (the state of all robots at time <em>t</em>) and the shared policy $\\pi_{\\theta_t}$ to a binary matrix that indicates which human will be assigned to which robot at time <em>t</em>. The IFL objective is a novel metric we call the \u201creturn on human effort\u201d (ROHE):</p>\n\n\\[\\max_{\\omega \\in \\Omega} \\mathbb{E}_{\\tau \\sim p_{\\omega, \\theta_0}(\\tau)} \\left[\\frac{M}{N} \\cdot \\frac{\\sum_{t=0}^T \\bar{r}( \\mathbf{s}^t, \\mathbf{a}^t)}{1+\\sum_{t=0}^T \\|\\omega(\\mathbf{s}^t, \\pi_{\\theta_t}, \\cdot) \\|^2 _F} \\right]\\]\n\n<p>where the numerator is the total reward across robots and timesteps and the denominator is the total amount of human actions across robots and timesteps. Intuitively, the ROHE measures the performance of the fleet normalized by the total human supervision required. See the <a href=\"https://arxiv.org/abs/2206.14349\">paper</a> for more of the mathematical details.</p>\n\n<p>Using this formalism, we can now instantiate and compare IFL algorithms (i.e., allocation policies) in a principled way. We propose a family of IFL algorithms called Fleet-DAgger, where the policy learning algorithm is interactive imitation learning and each Fleet-DAgger algorithm is parameterized by a unique priority function $\\hat p: (s, \\pi_{\\theta_t}) \\rightarrow [0, \\infty)$ that each robot in the fleet uses to assign itself a priority score. Similar to scheduling theory, higher priority robots are more likely to receive human attention. Fleet-DAgger is general enough to model a wide range of IFL algorithms, including IFL adaptations of existing single-robot, single-human IIL algorithms such as <a href=\"https://arxiv.org/abs/1807.08364\">EnsembleDAgger</a> and <a href=\"https://arxiv.org/abs/2109.08273\">ThriftyDAgger</a>. Note, however, that the IFL formalism isn\u2019t limited to Fleet-DAgger: policy learning could be performed with a reinforcement learning algorithm like <a href=\"https://arxiv.org/abs/1707.06347\">PPO</a>, for instance.</p>\n\n<h2 id=\"ifl-benchmark-and-experiments\">IFL Benchmark and Experiments</h2>\n\n<p>To determine how to best allocate limited human attention to large robot fleets, we need to be able to empirically evaluate and compare different IFL algorithms. To this end, we introduce the <a href=\"https://github.com/BerkeleyAutomation/ifl_benchmark\">IFL Benchmark</a>, an open-source Python toolkit available on Github to facilitate the development and standardized evaluation of new IFL algorithms. We extend <a href=\"https://developer.nvidia.com/isaac-gym\">NVIDIA Isaac Gym</a>, a highly optimized software library for end-to-end GPU-accelerated robot learning released in 2021, without which the simulation of hundreds or thousands of learning robots would be computationally intractable. Using the IFL Benchmark, we run large-scale simulation experiments with <em>N</em> = 100 robots, <em>M</em> = 10 algorithmic humans, 5 IFL algorithms, and 3 high-dimensional continuous control environments (Figure 1, left).</p>\n\n<p>We also evaluate IFL algorithms in a real-world image-based block pushing task with <em>N</em> = 4 robot arms and <em>M</em> = 2 remote human teleoperators (Figure 1, right). The 4 arms belong to 2 bimanual ABB YuMi robots operating simultaneously in 2 separate labs about 1 kilometer apart, and remote humans in a third physical location perform teleoperation through a keyboard interface when requested. Each robot pushes a cube toward a unique goal position randomly sampled in the workspace; the goals are programmatically generated in the robots\u2019 overhead image observations and automatically resampled when the previous goals are reached. Physical experiment results suggest trends that are approximately consistent with those observed in the benchmark environments.</p>\n\n<h2 id=\"takeaways-and-future-directions\">Takeaways and Future Directions</h2>\n\n<p>To address the gap between the theory and practice of robot fleet learning as well as facilitate future research, we introduce new formalisms, algorithms, and benchmarks for Interactive Fleet Learning. Since IFL does not dictate a specific form or architecture for the shared robot control policy, it can be flexibly synthesized with other promising research directions. For instance, <a href=\"https://arxiv.org/abs/2303.04137\">diffusion policies</a>, recently demonstrated to gracefully handle multimodal data, can be used in IFL to allow heterogeneous human supervisor policies. Alternatively, multi-task language-conditioned Transformers like <a href=\"https://arxiv.org/abs/2212.06817\">RT-1</a> and <a href=\"https://arxiv.org/abs/2209.05451\">PerAct</a> can be effective \u201cdata sponges\u201d that enable the robots in the fleet to perform heterogeneous tasks despite sharing a single policy. The systems aspect of IFL is another compelling research direction: recent developments in cloud and <a href=\"https://arxiv.org/abs/2205.09778\">fog robotics</a> enable robot fleets to offload all supervisor allocation, model training, and crowdsourced teleoperation to centralized servers in the cloud with minimal network latency.</p>\n\n<p>While <a href=\"https://en.wikipedia.org/wiki/Moravec%27s_paradox\">Moravec\u2019s Paradox</a> has so far prevented robotics and embodied AI from fully enjoying the recent spectacular success that Large Language Models (LLMs) like <a href=\"https://openai.com/research/gpt-4\">GPT-4</a> have demonstrated, the <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">\u201cbitter lesson\u201d</a> of LLMs is that supervised learning at unprecedented scale is what ultimately leads to the emergent properties we observe. Since we don\u2019t yet have a supply of robot control data nearly as plentiful as all the text and image data on the Internet, the IFL paradigm offers one path forward for scaling up supervised robot learning and deploying robot fleets reliably in today\u2019s world.</p>\n\n<p><em>This post is based on the paper \u201cFleet-DAgger: Interactive Robot Fleet Learning with Scalable Human Supervision\u201d by Ryan Hoque, Lawrence Chen, Satvik Sharma, Karthik Dharmarajan, Brijen Thananjeyan, Pieter Abbeel, and Ken Goldberg, presented at the Conference on Robot Learning (CoRL) 2022. For more details, see the <a href=\"https://arxiv.org/abs/2206.14349\">paper</a> on arXiv, <a href=\"https://www.youtube.com/watch?v=USr_iICRgvk\">CoRL presentation video</a> on YouTube, open-source <a href=\"https://github.com/BerkeleyAutomation/ifl_benchmark\">codebase</a> on Github, <a href=\"https://twitter.com/ryan_hoque/status/1542932195949432832?s=20\">high-level summary</a> on Twitter, and <a href=\"https://sites.google.com/berkeley.edu/fleet-dagger/home\">project website</a>.</em></p>\n\n<p><em>If you would like to cite this article, please use the following bibtex:</em></p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>@article{ifl_blog,\n    title={Interactive Fleet Learning},\n    author={Hoque, Ryan},\n    url={https://bair.berkeley.edu/blog/2023/04/06/ifl/},\n    journal={Berkeley Artificial Intelligence Research Blog},\n    year={2023} \n}\n</code></pre></div></div>",
            "pubdate": "Thu, 06 Apr 2023 02:00:00 -0700",
            "pubdate_parsed": [
                2023,
                4,
                6
            ],
            "email_sent": true
        }
    },
    "Google AI Blog": {
        "The Check Up: our latest health AI developments": {
            "url": "https://blog.google/technology/health/check-up-ai-developments-2022/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Over the years, teams across Google have focused on how technology \u2014 specifically artificial intelligence and hardware innovations \u2014 can improve access to high-quality, equitable healthcare across the globe.</p><p>Accessing the right healthcare can be challenging depending on where people live and whether local caregivers have specialized equipment or training for tasks like disease screening. To help, Google Health has expanded its research and applications to focus on improving the care clinicians provide and allow care to happen outside hospitals and doctor\u2019s offices.</p><p>Today, at our Google Health event <a href=\"https://www.youtube.com/watch?v=2XQZQR477fg\">The Check Up</a>, we\u2019re sharing new areas of AI-related research and development and how we\u2019re providing clinicians with easy-to-use tools to help them better care for patients. Here\u2019s a look at some of those updates.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Smartphone cameras\u2019 potential to protect cardiovascular health and preserve eyesight</h3><p>One of our earliest Health AI projects, <a href=\"https://health.google/caregivers/arda/\">ARDA</a>, aims to help address screenings for diabetic retinopathy \u2014 a complication of diabetes that, if undiagnosed and untreated, can cause blindness.</p><p>Today, we screen 350 patients daily, resulting in close to 100,000 patients screened to date. We <a href=\"https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00017-6/fulltext\">recently completed a prospective study</a> with the Thailand national screening program that further shows ARDA is accurate and capable of being deployed safely across multiple regions to support more accessible eye screenings.</p><p>In addition to diabetic eye disease, we\u2019ve previously also shown how <a href=\"https://ai.googleblog.com/2018/02/assessing-cardiovascular-risk-factors.html\">photos of eyes\u2019 interiors (or fundus) can reveal cardiovascular risk factors</a>, such as high blood sugar and cholesterol levels, with assistance from deep learning. <a href=\"http://ai.googleblog.com/2022/03/detecting-signs-of-disease-from.html\">Our recent research</a> tackles detecting diabetes-related diseases from photos of the exterior of the eye, using existing tabletop cameras in clinics. Given the <a href=\"https://arxiv.org/abs/2011.11732\">early promising results</a>, we\u2019re looking forward to clinical research with partners, including EyePACS and Chang Gung Memorial Hospital (CGMH), to investigate if photos from smartphone cameras can help detect diabetes and non-diabetes diseases from external eye photos as well. While this is in the early stages of research and development, our engineers and scientists envision a future where people, with the help of their doctors, can better understand and make decisions about health conditions from their own homes.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Recording and translating heart sounds with smartphones</h3><p>We\u2019ve previously shared how mobile sensors combined with machine learning can democratize health metrics and <a href=\"https://blog.google/technology/health/take-pulse-health-and-wellness-your-phone/\">give people insights into daily health and wellness</a>. Our feature that allows you to measure your heart rate and respiratory rate with your phone\u2019s camera is now available on over 100 models of Android devices, as well as iOS devices. Our <a href=\"https://www.medrxiv.org/content/10.1101/2021.03.08.21252408v1\">manuscript</a> describing the prospective validation study has been accepted for publication.</p><p>Today, we\u2019re sharing a new area of research that explores how a smartphone\u2019s built-in microphones could record heart sounds when placed over the chest. Listening to someone\u2019s heart and lungs with a stethoscope, known as auscultation, is a critical part of a physical exam. It can help clinicians detect heart valve disorders, such as aortic stenosis which is important to detect early. Screening for aortic stenosis typically requires specialized equipment, like a stethoscope or an ultrasound, and an in-person assessment.</p><p>Our latest research investigates whether a smartphone can detect heartbeats and murmurs. We're currently in the early stages of clinical study testing, but we hope that our work can empower people to use the smartphone as an additional tool for accessible health evaluation.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Partnering with Northwestern Medicine to apply AI to improve maternal health</h3><p>Ultrasound is a noninvasive diagnostic imaging method that uses high-frequency sound waves to create real-time pictures or videos of internal organs or other tissues, such as blood vessels and fetuses.</p><p>Research shows that ultrasound is safe for use in prenatal care and effective in identifying issues early in pregnancy. However, <a href=\"https://www.who.int/reproductivehealth/publications/maternal-mortality-2000-2017/en/\">more than half</a> of all birthing parents in low-to-middle-income countries don\u2019t receive ultrasounds, in part due to a shortage of expertise in reading ultrasounds. We believe that Google\u2019s expertise in machine learning can help solve this and allow for healthier pregnancies and better outcomes for parents and babies.</p><p>We are working on foundational, open-access <a href=\"https://arxiv.org/abs/2203.10139\">research</a> <a href=\"https://arxiv.org/abs/2203.11903\">studies</a> that validate the use of AI to help providers conduct ultrasounds and perform assessments. We\u2019re excited to partner with Northwestern Medicine to further develop and test these models to be more generalizable across different levels of experience and technologies. With more automated and accurate evaluations of maternal and fetal health risks, we hope to lower barriers and help people get timely care in the right settings.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>To learn more about the health efforts we shared at <a href=\"https://health.google/\">The Check Up with Google Health</a>, check out <a href=\"https://blog.google/technology/health/check-up-consumer-developments-2022/\">this blog post</a> from our Chief Health Officer Dr. Karen DeSalvo. And stay tuned for more health-related research milestones from us.</p></div></div>",
            "pubdate": "Thu, 24 Mar 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                24
            ],
            "email_sent": true
        },
        "Lift as you lead: Meet 2 women defining responsible AI": {
            "url": "https://blog.google/technology/ai/lift-as-you-lead-meet-2-women-defining-responsible-ai/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>At Google, <a href=\"https://blog.google/technology/ai/marian-croak-inventors-hall-fame/\">Marian Croak</a>\u2019s technical research team, The Center for Responsible AI and Human-Centered Technology, and Jen Gennai\u2019s operations and governance team, Responsible Innovation, collaborate often on creating a fairer future for AI systems.</p><p>The teams complement each other to support computer scientists, UX researchers and designers, product managers and subject matter experts in the social sciences, human rights and civil rights. Collectively, their teams include more than 200 people around the globe focused on putting our <a href=\"http://ai.google/principles\">AI Principles</a> \u2013 Google\u2019s ethical charter \u2013 into practice.</p><p>\u201cThe intersection of AI systems and society is a critical area of my team\u2019s technical research,\u201d Marian says. \u201cOur approach includes working directly with people who use and are impacted by AI systems. Working together with Jen\u2019s central operations team, the idea is to make AI more useful and reduce potential harm before products launch.\u201d</p><p>For Women\u2019s History Month, we wanted to talk to them both about this incredibly meaningful work and how they bring their lived experiences to it.</p><p><b>How do you define \u201cresponsible AI\u201d?</b></p><p><b>Marian:</b> It\u2019s the <i>technical</i> realization of our <a href=\"http://ai.google/principles\">AI Principles</a>. We need to understand how AI systems are performing in respect to fairness, transparency, interpretability, robustness and privacy. When gaps occur, we fix them. We benchmark and evaluate how product teams are adopting what Jen and I call smart practices. These are trusted practices based on patterns we see across Google as we\u2019re developing new AI applications, and the data-driven results of applying these practices over time.</p><p><b>Jen:</b> There are enormous opportunities to use AI for positive impact \u2014 and the potential for harm, too. The key is ethical deployment. \u201cResponsible AI\u201d for me means taking deliberate steps to ensure technology works the way it\u2019s intended to and doesn\u2019t lead to malicious or unintended negative consequences. This involves applying the smart practices Marian mentioned through repeatable processes and a governance structure for accountability.</p><p><b>How do your teams work together?</b></p><p><b>Marian</b>: They work hand in hand. My team conducts scientific research and creates open source tools like <a href=\"https://www.tensorflow.org/responsible_ai/fairness_indicators/guide\">Fairness Indicators</a> and <a href=\"https://knowyourdata.withgoogle.com/\">Know Your Data</a>. A large portion of our technical research and product work is centered in societal context and human and civil rights, so Jen\u2019s team is integral to understanding the problems we seek to help solve.</p><p><b>Jen:</b> The team I lead defines Google policies, handles day-to-day operations and central governance structure, and conducts ethical assessments. We\u2019re made up of user researchers, social scientists, ethicists, human rights specialists, policy and privacy advisors and legal experts.</p><p>One team can\u2019t work without the other! This complementary relationship allows many different perspectives and lived experiences to inform product design decisions. Here\u2019s an example, which was led by women from a variety of global backgrounds: Marian\u2019s team designed a streamlined, open source format for documenting technical details of datasets, called <a href=\"https://datacentricai.org/neurips21/papers/112_CameraReady_Data_Cards.pdf\">data cards</a>. When researchers on the <a href=\"https://translate.google.com/\">Translate</a> team, led by product manager Romina Stella, recently developed a <a href=\"https://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html\">new dataset</a> for studying and preventing gender bias in machine learning, members of my team, Anne P., N\u2019Mah Y. and Reena Jana, reviewed the dataset for alignment with the AI Principles. They recommended that the Translate researchers publish a data card for details on how the dataset was created and tested. The Translate team then worked with UX designer Mahima Pushkarna on Marian\u2019s team to create and launch the <a href=\"https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Data%20Card.pdf\">card</a> alongside the <a href=\"https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html\">dataset</a>.</p></div></div><div class=\"block-pull_quote\"><div class=\"uni-pull-quote h-c-page\"><section class=\"h-c-grid\"><div class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"><div class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"><q class=\"uni-pull-quote__text\">I\u2019m inspired most when someone tells me I can\u2019t do something. No matter what obstacles you face, believe you have the skills, the knowledge and the passion to make your dreams come true.</q></div></div></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>How did you end up working in this very new field?</b></p><p><b>Marian</b><i>:</i> I\u2019ve always been drawn to hard problems. This is a very challenging area! It\u2019s so multifaceted and constantly evolving. That excites me. It\u2019s an honor to work with so many passionate people who care so deeply about our world and understanding how to use technology for social good.</p><p>I\u2019ll always continue to seek out solutions to these problems because I understand the profound impact this work will have on our society and our world, especially communities underrepresented in the tech industry.</p><p><b>Jen:</b> I spent many years leading User Research and User Advocacy on Google\u2019s Trust and Safety team. An area I focused on was ML Fairness. I never thought I\u2019d get to work on it full time. But in 2016 my leadership team wanted to have a company-wide group concentrating on worldwide positive social benefits of AI. In 2017, I joined the team that was writing and publishing the AI Principles. Today, I apply my operational knowledge to make sure that as a company, we meet the obligations we laid out in the Principles.</p><p><b>What advice do you have for girls and women interested in pursuing careers in responsible tech?</b></p><p><b>Marian</b>: I\u2019m inspired most when someone tells me I <i>can\u2019t</i> do something. No matter what obstacles you face, believe you have the skills, the knowledge and the passion to make your dreams come true. Find motivation in the small moments, find motivation in those who <i>doubt</i> you, but most importantly, never forget to believe in the greatness of you.</p><p><b>Jen:</b> Don\u2019t limit yourself even if you don\u2019t have a computer science degree. I don\u2019t. I was convinced I\u2019d work in sustainability and environmental non-profits, and now I lead a team working to make advanced technologies work better for everyone. This space requires so many different skills, whether in program management, policy, engineering, UX or business and strategy.</p><p>My mantra is \u201clift as you lead.\u201d Don\u2019t just build a network for yourself; build a supportive network to empower everyone who works with you \u2014 and those who come after you, especially those who are currently underrepresented in the tech sector. Your collective presence in this space makes a positive impact! And it\u2019s even stronger when you build a better future together.</p></div></div>",
            "pubdate": "Tue, 29 Mar 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                29
            ],
            "email_sent": true
        },
        "Go with the flow state: What music and AI have in common": {
            "url": "https://blog.google/technology/ai/go-flow-state-what-music-and-ai-have-common/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Carrie Cai, Ben Zevenbergen and Johnny Soraker all work on developing artificial intelligence (AI) responsibly at Google, in the larger research community and across the technology industry. Carrie is a research scientist focusing on human-AI interaction, Ben is an ethicist and policy advisor and Johnny is an <a href=\"https://ai.google/principles/\">AI Principles</a> ethicist. They all work within a global team of experts from a variety of fields, including the social sciences and humanities, focused on the ethical development of AI. They\u2019re driven to make systems that are fair, inclusive and focused on people.</p><p>But they have more than their work in common: They\u2019re all accomplished musicians who\u2019ve studied music, composed and published pieces and even played at the professional level. We wanted to know more about their musical backgrounds, and how this creative interest informs their work building AI systems that take everyone into account.</p><p><b>What instrument \u2014 or instruments \u2014 do you play?</b></p><p><b>Ben:</b> Guitar, bass and drums.</p><p><b>Johnny:</b> Mainly drums these days, but I\u2019ve also done ambient and electronica.</p><p><b>Carrie:</b> I play piano and I also compose music.</p><p><b>Where did your interest in playing music come from?</b></p><p><b>Ben:</b> I grew up in a musical family where instruments were always lying around. My parents\u2019 friends would bring their instruments when they came to visit and our house would turn into a music venue. I enrolled in a music degree in my late teens to become a professional drummer. Then, a year later, I serendipitously became a bassist: I went to law school in the Netherlands, and the university band already had someone who was a better drummer than I was \u2014 but they needed a bassist, so I grabbed the opportunity.</p><p><b>Carrie:</b> I started out in the Yamaha music program when I was six, where rather than learning technical piano playing skills you focus on ear training, hearing the music and how to play as an ensemble. I think that foundation led me to be a lot more creative with my music than I would have been otherwise. I spent part of my childhood years composing music, too \u2014 here are <a href=\"https://people.csail.mit.edu/ccai/media/September.mp3\">some of</a> <a href=\"https://people.csail.mit.edu/ccai/media/SilverMountain.m4a\">my early</a> <a href=\"https://people.csail.mit.edu/ccai/media/WindHarmony.mp3\">compositions</a> from my high school days!</p><p><b>Johnny:</b> I\u2019ve played lots of instruments since I was a child, but never had the tenacity to get very good at any of them. Perhaps as a result of this, I got involved with a highly experimental ambient scene in the early 2000s and started the <a href=\"http://metusmortuus.bandcamp.com/\">one-man project Metus Mortuus</a>, using samples and DIY equipment to create often disturbing soundscapes. It was really only when I got hooked on the video game \u201cRock Band,\u201d where you play \"fake\" instruments along with the notation on screen, that I put in the hours needed to get some basic limb independence and with that a platform for learning real drums.</p></div></div><div class=\"block-image_carousel\"><div class=\"h-c-page article-module\"><div class=\"article-module glue-pagination h-c-carousel h-c-carousel--simple h-c-carousel--dark ng-cloak\"><div class=\"h-c-carousel__wrap\"><ul class=\"glue-carousel ng-cloak\"><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">Carrie stands in front of a group of people speaking while holding a microphone. A presentation behind her reads: \u201cNeural substrates of Musical Creativity.\u201d</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Carrie speaking at a conference about musical creativity and brain science.</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">Ben performing on stage. He is playing a bass and wearing a red and white track suit.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Ben playing the bass during a concert in Amsterdam.</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">Johnny speaking on stage to an audience in a dimly lit room.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Johnny speaking at <a href=\"https://www.youtube.com/watch?v=unVzwxmyeqs\">a TedX talk</a>.</p></div></figcaption></figure></li></ul><div class=\"h-c-carousel__paginate glue-pagination-previous uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-left\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div><div class=\"h-c-carousel__paginate glue-pagination-next uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-right\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div></div><div class=\"h-c-carousel__navigation\"><div class=\"glue-pagination-page-list uni-click-tracker\"></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>Did you gravitate toward the drums in the game?</b></p><p><b>Johnny:</b> No, I hardly ever touched them \u2014 I simply couldn\u2019t make my left arm do something my right arm wasn't doing, but one day I decided to try an experiment: Can I make these stale neural pathways of mine actually learn something new well into adulthood? I started practicing on these toy drums every day, which was painful and frustrating, but occasional breakthroughs kept me going. Eventually I achieved a level of limb independence I hadn't thought I was capable of. I invested in proper e-drums and I\u2019ve played almost every day since.</p></div></div><div class=\"block-pull_quote\"><div class=\"uni-pull-quote h-c-page\"><section class=\"h-c-grid\"><div class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"><div class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"><q class=\"uni-pull-quote__text\">This [work] often requires you to think creatively. And I feel that the way in which drumming almost literally rewired my brain has made me much better at doing that.</q></div></div></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>What\u2019s your favorite thing about playing?</b></p><p><b>Johnny:</b> It's really the ultimate flow experience, where you're fully immersed in an activity to the extent you lose track of time and only focus on the present moment. <a href=\"https://en.wikipedia.org/wiki/Flow_(psychology)\">There\u2019s lots of empirical research</a> in the field of positive psychology suggesting that regular flow experiences promote better well-being.</p><p><b>Ben:</b> I love playing the bass with a band because it\u2019s the glue between the rhythm section and the melody sections. It\u2019s fun when you purposefully come in a beat later, you really see people not sure whether to dance or not. When you start playing, suddenly the whole audience understands what\u2019s going on. And then they have the audacity to say they never hear the bass!</p><p><b>How has music made its way into your work, if at all?</b></p><p><b>Carrie:</b> It\u2019s certainly affected how I think about my work today, particularly around how to make AI more <i>controllable</i> to everyday people. For example, we\u2019re now seeing these new, highly capable generative AI models that can compose music sounding indistinguishable from something written by Bach. But we discovered that, just because an AI model is capable of making beautiful music, doesn\u2019t mean humans can always make beautiful music using AI.</p><p>When I create music, I\u2019m thinking, \u201cI want the beginning of the song to sound cheerful, then I want it to build tension, before ending in a somber way.\u201d When I\u2019m creating with AI, it can be difficult to express that \u2014 I can\u2019t easily say, \u201cHey AI, make the first part happy and then build tension here\u201d This can make it difficult for people to feel a sense of artistic agency and authorship when they\u2019re creating any kind of content with AI.</p><p>Recently, I collaborated with other human-computer interaction (HCI) and machine learning (ML) researchers at Google to <a href=\"https://magenta-staging.tensorflow.org/people-first-hci-ml-collaborations\">create new tools</a> enabling people to steer and control AI as they compose music with it. We found that these \u201csteering tools\u201d significantly boost users\u2019 sense of creative ownership and trust as they compose with AI.</p></div></div><div class=\"block-video\"><div class=\"h-c-page h-c-page--mobile-full-bleed\"><div class=\"h-c-grid\"><div class=\"h-c-grid__col h-c-grid__col-l--10 h-c-grid__col-l--offset-1\"><div class=\"article-module uni-article-video uni-article-video--body\"><div class=\"uni-article-video__embed-container hidden\"><div id=\"uni-article-yt-player-ls_CCMbMyLU\"></div></div><figure><a class=\"h-c-video h-c-video--marquee uni-article-video__custom-wrapper\" tabindex=\"0\"><div class=\"uni-article-video__aspect-image\"><img alt=\"A video demonstration of composing music with AI.\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screenshot_2022-03-28_5.43.44_PM.max-1000x1000.png\" /><div class=\"uni-article-video__dimmer\"></div><svg class=\"uni-article-video__play-button--active\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button_no_hole\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><svg class=\"uni-article-video__play-button\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><div class=\"uni-article-video__duration loading\"><svg class=\"uni-article-video__duration-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_duration\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><span class=\"uni-article-video__duration-time\">10:25</span></div></div></a></figure></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>Do you think there\u2019s anything about the sort of work you do that exercises the same sort of \u201cmental muscles\u201d as music does?</b></p><p><b>Johnny:</b> Yes, I think the key to ethics \u2014 and especially ethics of AI where there often is no precedent \u2014 is to be able to approach a problem from different angles and draw connections between the case at hand and relevant, similar cases from the past. This requires you to think creatively. And I feel that the way in which drumming almost literally rewired my brain has made me much better at doing that.</p><p><b>Ben:</b> When you learn to play the drums, one of the hardest things is learning you must separate the movements of your limbs in your mind. It's pretty difficult to process \u2014 which makes it a very nice experience once your mind can asynchronously control parts of your thinking to create interesting rhythms that are still in time. I think for my work on ethics of technical design, I have to frequently understand many interacting but very different disciplines. I'm not sure if it has anything to do with drumming, but I find that I can think about these things in tandem, while they are completely different.</p></div></div><div class=\"block-pull_quote\"><div class=\"uni-pull-quote h-c-page\"><section class=\"h-c-grid\"><div class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"><div class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"><q class=\"uni-pull-quote__text\">Once when I was little, I woke up and without even changing out of my pajamas, spent the entire day composing a piece of music.</q></div></div></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>Carrie:</b> I remember once when I was little, I woke up and without even changing out of my pajamas, spent the entire day composing a piece of music. I realize now that <i>that</i> was a flow state \u2014 I was working on something that was challenging yet doable. I think that\u2019s a key property of creativity and it\u2019s affected how I work in general. It's easiest for me to be productive when I'm in that state \u2014 working on something that\u2019s challenging, but not so difficult that I won't want to start it or keep going. That\u2019s helpful in research because there\u2019s so much uncertainty \u2014 you never know if your experiments are going to work! But I can take a lesson from how I got into that flow state with music and apply it to research: How can I as a research scientist enter a flow state?</p></div></div>",
            "pubdate": "Tue, 29 Mar 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                29
            ],
            "email_sent": true
        },
        "How AI and imagery build a self-updating map": {
            "url": "https://blog.google/products/maps/how-ai-and-imagery-build-self-updating-map/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Building a map is complex, and keeping it up-to-date is even more challenging. Think about how often your city, town or neighborhood changes on a day-to-day basis. Businesses and shops open and close, stretches of highway are added, and roadways change. In today\u2019s Maps 101 installment, we\u2019ll dive into two ways Google Maps uses advancements in AI and imagery to help you see the latest information about the world around you every single day.</p><p><b>Automatically updating business hours</b></p><p>Over the past few years, businesses have experienced <i>a lot</i> of change \u2014 including constantly updating operating hours based on changing pandemic-related restrictions. To keep up with this pace of change, we developed a machine learning model that automatically identifies if business hours are likely wrong, then instantly updates them with AI-generated predictions.</p><p>Let\u2019s look at Liam\u2019s Lemonade Shop as an example. To start, our systems consider multiple factors \u2014 such as when Liam last updated their business profile, what we know about other shops' hours, and the <a href=\"https://blog.google/products/maps/maps101-popular-times-and-live-busyness-information/\">Popular Times information</a> for the shop, which uses location trends to show when it tends to be busiest. Since it appears that Liam\u2019s business profile hasn\u2019t been updated in over a year and its busiest hours are typically Thursday afternoons \u2014 even though Google Maps says that it's closed at that time \u2014 Liam\u2019s business hours are likely out of date.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Still images of a business' hours and Popular Times information on Google Maps\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Frame_515_2.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>To see if business hours need updating, we check a store\u2019s Popular Times information and when its business profile was last updated.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>So what\u2019s next? Our algorithms analyze the business hours of other nearby lemonade shops, information from Liam\u2019s website, and Street View images of Liam\u2019s storefront that look specifically for business hour signs to determine the most accurate business hour prediction. At the same time, we enlist the help of the Google Maps community \u2014 including Local Guides and even the business owners themselves through their <a href=\"https://www.google.com/business/\">Google Business Profile</a> \u2014 to verify the information we predicted. In Argentina, Australia, Chile, France, Japan, Mexico, New Zealand, Peru, and the United States, we also use <a href=\"https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html\">Duplex conversational technology</a> to call businesses just like Liam\u2019s and ask for their hours directly. With this new AI-first approach, we\u2019re on track to update the hours for over 20 million businesses around the globe in the next six months - helping you know exactly when your favorite store, restaurant or cafe is open for business .</p><p><b>Road information that reflects the real world</b></p><p>We\u2019re also experimenting with ways we can use imagery to make updates to other helpful information. For instance, starting in the U.S., we\u2019re launching a third-party imagery pilot to let you see the most up-to-date speed limit information in your town, which can help keep you safe while driving. Here\u2019s how it works:</p><p>Say our systems think that the speed limit information on a particular highway needs to be updated. With the help of third-party imagery partners that already gather roadway imagery to improve delivery routes, we can request a photo of the specific stretch of road that also includes a speed limit sign. If the partner has this photo available, we then use a combination of AI and help from our operations team to identify the sign in the image, extract the new speed limit information, and update Google Maps.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Picture of an intersection that has a speed limit sign\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI__Street_Signs.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Representative imagery featuring a speed limit sign, with license plates blurred</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Over time, this technology will bring more details to the map that can help make your drives safer and more efficient \u2014 like where potholes and school zones are or where new construction is happening. And as with all Google Maps features, we designed this pilot with privacy top of mind. For instance, we only reference images taken on public roads, and partners are required to blur information (like faces and license plates) to avoid potentially identifying someone. For an extra layer of privacy, we blur the photo again when we receive it and delete the photo after we use it to update the map.</p><p>AI, imagery and Duplex technology will continue to play a critical role in helping make Google Maps the most comprehensive and useful map possible. For more behind-the-scenes looks at the technology that powers Google Maps, check out the rest of our <a href=\"https://blog.google/products/maps/maps-101/\">Maps 101 blog series.</a></p></div></div>",
            "pubdate": "Thu, 07 Apr 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                7
            ],
            "email_sent": true
        },
        "Investing in Eastern Europes AI future": {
            "url": "https://blog.google/technology/ai/investing-in-eastern-europes-ai-future/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>It was an honor and a privilege to attend a special event in the Bulgarian capital, Sofia, today to launch <a href=\"https://insait.ai/\">INSAIT</a>, the Institute for Computer Science, Artificial Intelligence and Technology. INSAIT is a new AI and computer science research institute that will provide truly world-class facilities.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>It\u2019s fantastic to see the country where I was born leading the charge in bridging Eastern Europe to the world-stage in computer science research.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>The institute is modeled on the computer science departments of renowned institutions such as MIT, UC Berkeley and the <a href=\"https://www.mpg.de/en\">Max-Planck Institute</a>, and is backed by the Bulgarian government with an endowment fund of nearly $100 million. Its computer science and AI research will span topics such as machine learning, quantum computing, information security, robotics and many more. Within two years, INSAIT expects faculty and students to publish papers in top conferences.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Google is investing $3 million over the next three years to provide INSAIT with cloud computing resources and access to its<a href=\"https://sites.research.google/trc/about/\">Tensor Processing Unit Research Cloud</a>, a specialized infrastructure for running high-performance machine learning models. Supported with additional investment from DeepMind and Amazon Web Services, INSAIT aims to attract and develop the best researchers, engineers and top PhD and MSc students.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>I know there\u2019s no shortage of talented researchers, computer scientists and engineers in Eastern Europe \u2013 indeed, Sofia is already ranked as<a href=\"https://www.fdiintelligence.com/article/77846\">one of Europe\u2019s top tech cities</a> \u2013 but historically, the lack of local facilities, funding and support has meant limited opportunities for basic research. INSAIT has been created in partnership with two of the world\u2019s leading technology universities, ETH Zurich and EPFL Lausanne, and its supervisory and advisory boards consist of leading researchers who are committed to help the institute achieve its ambitious goals.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>INSAIT opens in September, and I know the team is particularly keen to receive applications from women and other groups that are often underrepresented in the world of tech.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Google is delighted to support these efforts, and I cannot wait to see what new innovation emerges from this promising venture.</p></div></div>",
            "pubdate": "Mon, 11 Apr 2022 10:30:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                11
            ],
            "email_sent": true
        },
        "Improving skin tone representation across Google": {
            "url": "https://blog.google/products/search/monk-skin-tone-scale/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Seeing yourself reflected in the world around you \u2014 in real life, media or online \u2014 is so important. And we know that challenges with image-based technologies and representation on the web have historically left people of color feeling overlooked and misrepresented. Last year, we announced <a href=\"https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/\">Real Tone for Pixel</a>, which is just one example of our efforts to improve representation of diverse skin tones across Google products.</p><p>Today, we're introducing a next step in our commitment to image equity and improving representation across our products. In partnership with Harvard professor and sociologist <a href=\"https://www.ellismonk.com/\">Dr. Ellis Monk</a>, we\u2019re releasing a new skin tone scale designed to be more inclusive of the spectrum of skin tones we see in our society. Dr. Monk has been studying how skin tone and colorism affect people\u2019s lives for more than 10 years.</p><p>The culmination of Dr. Monk\u2019s research is the <a href=\"http://skintone.google/\">Monk Skin Tone (MST) Scale</a>, a 10-shade scale that will be incorporated into various Google products over the coming months. We\u2019re openly releasing the scale so anyone can use it for research and product development. Our goal is for the scale to support inclusive products and research across the industry \u2014 we see this as a chance to share, learn and evolve our work with the help of others.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Ten circles in a row, ranging from dark to light.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Monk_Skin_Tone.max-1000x1000.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The 10 shades of the Monk Skin Tone Scale.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>This scale was designed to be easy-to-use for development and evaluation of technology while representing a broader range of skin tones. In fact, our research found that amongst participants in the U.S., people found the Monk Skin Tone Scale to be more representative of their skin tones compared to the <a href=\"http://skintone.google/the-scale\">current tech industry standard</a>. This was especially true for people with darker skin tones.</p><p>\u201cIn our research, we found that a lot of the time people feel they\u2019re lumped into racial categories, but there\u2019s all this heterogeneity with ethnic and racial categories,\u201d Dr. Monk says. \u201cAnd many methods of categorization, including past skin tone scales, don\u2019t pay attention to this diversity. That\u2019s where a lack of representation can happen\u2026we need to fine-tune the way we measure things, so people feel represented.\u201d</p></div></div><div class=\"block-video\"><div class=\"h-c-page h-c-page--mobile-full-bleed\"><div class=\"h-c-grid\"><div class=\"h-c-grid__col h-c-grid__col-l--10 h-c-grid__col-l--offset-1\"><div class=\"article-module uni-article-video uni-article-video--body\"><div class=\"uni-article-video__embed-container hidden\"><div id=\"uni-article-yt-player-crD1tzTlE4o\"></div></div><figure><a class=\"h-c-video h-c-video--marquee uni-article-video__custom-wrapper\" tabindex=\"0\"><div class=\"uni-article-video__aspect-image\"><img alt=\"Video of Dr. Ellis Monk on why he developed the Monk Skin Tone Scale\" src=\"https://img.youtube.com/vi/crD1tzTlE4o/maxresdefault.jpg\" /><div class=\"uni-article-video__dimmer\"></div><svg class=\"uni-article-video__play-button--active\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button_no_hole\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><svg class=\"uni-article-video__play-button\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><div class=\"uni-article-video__duration loading\"><svg class=\"uni-article-video__duration-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_duration\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><span class=\"uni-article-video__duration-time\">10:25</span></div></div></a></figure></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Using the Monk Skin Tone Scale to improve Google products</h3><p>Updating our approach to skin tone can help us better understand representation in imagery, as well as evaluate whether a product or feature works well across a range of skin tones. This is especially important for computer vision, a type of AI that allows computers to see and understand images. When not built and tested intentionally to include a broad range of skin-tones, computer vision systems have been found to not perform as well for people with darker skin.</p><p>The MST Scale will help us and the tech industry at large build more representative datasets so we can train and evaluate AI models for fairness, resulting in features and products that work better for everyone \u2014 of all skin tones. For example, we use the scale to evaluate and improve the models that detect faces in images.</p><p>Here are other ways you\u2019ll see this show up in Google products.</p><h3>Improving skin tone representation in Search</h3><p>Every day, millions of people search the web expecting to find images that reflect their specific needs. That\u2019s why we\u2019re also introducing new features using the MST Scale to make it easier for people of all backgrounds to find more relevant and helpful results.</p><p>For example, now when you search for makeup related queries in Google Images, you'll see an option to further refine your results by skin tone. So if you\u2019re looking for \u201ceveryday eyeshadow\u201d or \u201cbridal makeup looks\u201d you\u2019ll more easily find results that work better for your needs.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Animated GIF showing a Google Images search for \u201cbridal makeup looks.\u201d The results include an option to filter by skin tone; the cursor selects a darker skin tone, which adjusts to results that are more relevant to this choice.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Bridal_makeup_search_refinements_v20.gif\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Seeing yourself represented in results can be key to finding information that's truly relevant and useful, which is why we\u2019re also rolling out improvements to show a greater range of skin tones in image results for broad searches about people, or ones where people show up in the results. In the future, we\u2019ll incorporate the MST Scale to better detect and rank images to include a broader range of results, so everyone can find what they're looking for.</p><p>Creating a more representative Search experience isn\u2019t something we can do alone, though. How content is labeled online is a key factor in how our systems surface relevant results. In the coming months, we'll also be developing a standardized way to label web content. Creators, brands and publishers will be able to use this new inclusive schema to label their content with attributes like skin tone, hair color and hair texture. This will make it possible for content creators or online businesses to label their imagery in a way that search engines and other platforms can easily understand.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"A photograph of a Black person looking into the camera. Tags hover over various areas of the photo; one over their skin says \u201cSkin tone\u201d with a circle matching their skin tone. Two additional tags over their hair read \u201cHair color\u201d and \u201cHair texture.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/inclusive_schema_0509_anUYk5H.max-1000x1000.png\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Improving skin tone representation in Google Photos</h3><p>We\u2019ll also be using the MST Scale to improve Google Photos. Last year, we introduced an <a href=\"https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/\">improvement to our auto enhance feature</a> in partnership with professional image makers. Now we\u2019re launching a new set of Real Tone filters that are designed to work well across skin tones and evaluated using the MST Scale. We worked with a diverse range of renowned image makers, like Kennedi Carter and Joshua Kissi, who are celebrated for beautiful and accurate depictions of their subjects, to evaluate, test and build these filters. These new Real Tone filters allow you to choose from a wider assortment of looks and find one that reflects your style. Real Tone filters will be rolling out on Google Photos across Android, iOS and Web in the coming weeks.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Animated video showing before and after photos of images with the Real Tone Filter.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Real_Tone_filter_v20.gif\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>What\u2019s next?</h3><p>We\u2019re openly releasing the Monk Skin Tone Scale so that others can use it in their own products, and learn from this work \u2014<i>and</i> so that we can partner with and learn from them. We want to get feedback, drive more interdisciplinary research, and make progress together. We encourage you to share your thoughts <a href=\"https://google.qualtrics.com/jfe/form/SV_eFJF7qguvcWvdFs\">here</a>. We\u2019re continuing to collaborate with Dr. Monk to evaluate the MST Scale across different regions and product applications, and we\u2019ll iterate and improve on it to make sure the scale works for people and use cases all over the world. And, we\u2019ll continue our efforts to make Google\u2019s products work even better for every user.</p><p>The best part of working on this project is that it isn\u2019t just ours \u2014 while we\u2019re committed to making Google products better and more inclusive, we\u2019re also excited about all the possibilities that exist as we work together to build for everyone across the web.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/technology/research/ai-monk-scale-skin-tone-story/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">A closer look at the research to help AI see more skin tones</h4><p class=\"uni-related-article-tout__body\">Meet the Googlers who\u2019ve led this latest work, and learn more about why and how they\u2019ve been dedicated to this AI advancement.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div>",
            "pubdate": "Wed, 11 May 2022 17:32:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "Google Translate learns 24 new languages": {
            "url": "https://blog.google/products/translate/24-new-languages/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>For years, Google Translate has helped break down language barriers and connect communities all over the world. And we want to make this possible for even more people \u2014 especially those whose languages aren\u2019t represented in most technology. So today we\u2019ve added 24 languages to Translate, now supporting a total of 133 used around the globe.</p><p>Over 300 million people speak these newly added languages \u2014 like Mizo, used by around 800,000 people in the far northeast of India, and Lingala, used by over 45 million people across Central Africa. As part of this update, Indigenous languages of the Americas (Quechua, Guarani and Aymara) and an English dialect (Sierra Leonean Krio) have also been added to Translate for the first time.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"The Google Translate bar translates the phrase &quot;Our mission: to enable everyone, everywhere to understand the world and express themselves across languages&quot; into different languages.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Translate_New-Languages.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Translate's mission translated into some of our newly added languages</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Here\u2019s a complete list of the new languages now available in Google Translate:</p><ul><li><b>Assamese</b>, used by about 25 million people in Northeast India</li><li><b>Aymara</b>, used by about two million people in Bolivia, Chile and Peru</li><li><b>Bambara</b>, used by about 14 million people in Mali</li><li><b>Bhojpuri</b>, used by about 50 million people in northern India, Nepal and Fiji</li><li><b>Dhivehi</b>, used by about 300,000 people in the Maldives</li><li><b>Dogri</b>, used by about three million people in northern India</li><li><b>Ewe</b>, used by about seven million people in Ghana and Togo</li><li><b>Guarani</b>, used by about seven million people in Paraguay and Bolivia, Argentina and Brazil</li><li><b>Ilocano</b>, used by about 10 million people in northern Philippines</li><li><b>Konkani</b>, used by about two million people in Central India</li><li><b>Krio</b>, used by about four million people in Sierra Leone</li><li><b>Kurdish (Sorani)</b>, used by about 15 million people in Iraq and Iran</li><li><b>Lingala</b>, used by about 45 million people in the Democratic Republic of the Congo, Republic of the Congo, Central African Republic, Angola and the Republic of South Sudan</li><li><b>Luganda</b>, used by about 20 million people in Uganda and Rwanda</li><li><b>Maithili</b>, used by about 34 million people in northern India</li><li><b>Meiteilon (Manipuri)</b>, used by about two million people in Northeast India</li><li><b>Mizo</b>, used by about 830,000 people in Northeast India</li><li><b>Oromo</b>, used by about 37 million people in Ethiopia and Kenya</li><li><b>Quechua</b>, used by about 10 million people in Peru, Bolivia, Ecuador and surrounding countries</li><li><b>Sanskrit</b>, used by about 20,000 people in India</li><li><b>Sepedi</b>, used by about 14 million people in South Africa</li><li><b>Tigrinya</b>, used by about eight million people in Eritrea and Ethiopia</li><li><b>Tsonga</b>, used by about seven million people in Eswatini, Mozambique, South Africa and Zimbabwe</li><li><b>Twi</b>, used by about 11 million people in Ghana</li></ul><p>This is also a technical milestone for Google Translate. These are the first languages we\u2019ve added using Zero-Shot Machine Translation, where a machine learning model only sees monolingual text \u2014 meaning, it learns to translate into another language without ever seeing an example. While this technology is impressive, it isn't perfect. And we\u2019ll keep improving these models to deliver the same experience you\u2019re used to with a Spanish or German translation, for example. If you want to dig into the technical details, check out our <a href=\"http://ai.googleblog.com/2022/05/24-new-languages-google-translate.html\">Google AI blog post</a> and <a href=\"https://arxiv.org/pdf/2205.03983.pdf\">research paper</a>.</p><p>We\u2019re grateful to the many native speakers, professors and linguists who worked with us on this latest update and kept us inspired with their passion and enthusiasm. If you want to help us support your language in a future update, contribute evaluations or translations through <a href=\"https://support.google.com/translate/answer/2534530?hl=en\">Translate Contribute</a>.</p></div></div>",
            "pubdate": "Wed, 11 May 2022 17:16:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "Google I/O 2022: Advancing knowledge and computing": {
            "url": "https://blog.google/technology/developers/io-2022-keynote/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>[TL;DR]</b></p><p>Nearly 24 years ago, Google started with two graduate students, one product, and a big mission: to organize the world\u2019s information and make it universally accessible and useful. In the decades since, we\u2019ve been developing our technology to deliver on that mission.</p><p>The progress we've made is because of our years of investment in advanced technologies, from AI to the technical infrastructure that powers it all. And once a year \u2014 on my favorite day of the year :) \u2014 we share an update on how it\u2019s going at <a href=\"https://io.google/2022/\">Google I/O</a>.</p><p>Today, I talked about how we\u2019re advancing two fundamental aspects of our mission \u2014 knowledge and computing \u2014 to create products that are built to help. It\u2019s exciting to build these products; it\u2019s even more exciting to see what people do with them.</p><p>Thank you to everyone who helps us do this work, and most especially our Googlers. We are grateful for the opportunity.</p><p>- Sundar</p><p></p><hr /><p><i>Editor\u2019s note: Below is an edited transcript of Sundar Pichai's keynote address during the opening of today's Google I/O Developers Conference.</i></p><p>Hi, everyone, and welcome. Actually, let\u2019s make that welcome back! It\u2019s great to return to Shoreline Amphitheatre after three years away. To the thousands of developers, partners and Googlers here with us, it\u2019s great to see all of you. And to the millions more joining us around the world \u2014 we\u2019re so happy you\u2019re here, too.</p><p>Last year, we <a href=\"https://blog.google/technology/developers/io-2021/\">shared</a> how new breakthroughs in some of the most technically challenging areas of computer science are making Google products more helpful in the moments that matter. All this work is in service of our timeless mission: to organize the world's information and make it universally accessible and useful.</p><p>I'm excited to show you how we\u2019re driving that mission forward in two key ways: by deepening our understanding of information so that we can turn it into knowledge; and advancing the state of computing, so that knowledge is easier to access, no matter who or where you are.</p><p>Today, you'll see how progress on these two parts of our mission ensures Google products are built to help. I\u2019ll start with a few quick examples. Throughout the pandemic, Google has focused on delivering accurate information to help people stay healthy. Over the last year, people used Google Search and Maps to find where they could get a COVID vaccine nearly two billion times.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"A visualization of Google\u2019s flood forecasting system, with three 3D maps stacked on top of one another, showing landscapes and weather patterns in green and brown colors. The maps are floating against a gray background.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Flood_option_1.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Google\u2019s flood forecasting technology sent flood alerts to 23 million people in India and Bangladesh last year.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019ve also expanded our flood forecasting technology to help people stay safe in the face of natural disasters. During last year\u2019s monsoon season, our flood alerts notified more than 23 million people in India and Bangladesh. And we estimate this supported the timely evacuation of hundreds of thousands of people.</p><p>In Ukraine, we worked with the government to rapidly deploy air raid alerts. To date, we\u2019ve delivered hundreds of millions of alerts to help people get to safety. In March <a href=\"https://blog.google/inside-google/company-announcements/warsaw-announcing-more-support-ukraine/\">I was in Poland</a>, where millions of Ukrainians have sought refuge. Warsaw\u2019s population has increased by nearly 20% as families host refugees in their homes, and schools welcome thousands of new students. Nearly every Google employee I spoke with there was hosting someone.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Adding 24 more languages to Google Translate</h3><p>In countries around the world, Google Translate has been a crucial tool for newcomers and residents trying to communicate with one another. We\u2019re proud of how it\u2019s helping Ukrainians find a bit of hope and connection until they are able to return home again.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Two boxes, one showing a question in English \u2014 \u201cWhat\u2019s the weather like today?\u201d \u2014 the other showing its translation in Quechua. There is a microphone symbol below the English question and a loudspeaker symbol below the Quechua answer.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Translate_Quechua.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>With machine learning advances, we're able to add languages like Quechua to Google Translate.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Real-time translation is a testament to how knowledge and computing come together to make people's lives better. More people are using Google Translate than ever before, but we still have work to do to make it universally accessible. There\u2019s a long tail of languages that are underrepresented on the web today, and translating them is a hard technical problem. That\u2019s because translation models are usually trained with bilingual text \u2014 for example, the same phrase in both English and Spanish. However, there's not enough publicly available bilingual text for every language.</p><p>So with advances in machine learning, we\u2019ve developed a monolingual approach where the model learns to translate a new language without ever seeing a direct translation of it. By collaborating with native speakers and institutions, we found these translations were of sufficient quality to be useful, and we'll continue to improve them.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"A list of the 24 new languages Google Translate now has available.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Translate_24_option_1.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>We\u2019re adding 24 new languages to Google Translate.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today, I\u2019m excited to announce that we\u2019re <a href=\"https://blog.google/products/translate/24-new-languages\">adding 24 new languages to Google Translate</a>, including the first indigenous languages of the Americas. Together, these languages are spoken by more than 300 million people. Breakthroughs like this are powering a radical shift in how we access knowledge and use computers.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Taking Google Maps to the next level</h3><p>So much of what\u2019s knowable about our world goes beyond language \u2014 it\u2019s in the physical and geospatial information all around us. For more than 15 years, Google Maps has worked to create rich and useful representations of this information to help us navigate. Advances in AI are taking this work to the next level, whether it\u2019s expanding our coverage to remote areas, or reimagining how to explore the world in more intuitive ways.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"An overhead image of a map of a dense urban area, showing gray roads cutting through clusters of buildings outlined in blue.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Open_Buildings.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Advances in AI are helping to map remote and rural areas.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Around the world, we\u2019ve mapped around 1.6 billion buildings and over 60 million kilometers of roads to date. Some remote and rural areas have previously been difficult to map, due to scarcity of high-quality imagery and distinct building types and terrain. To address this, we\u2019re using computer vision and neural networks to detect buildings at scale from satellite images. As a result, we have increased the number of buildings on Google Maps in Africa by 5X since July 2020, from 60 million to nearly 300 million.</p><p>We\u2019ve also doubled the number of buildings mapped in India and Indonesia this year. Globally, over 20% of the buildings on Google Maps have been detected using these new techniques. We\u2019ve gone a step further, and made the dataset of buildings in Africa publicly available. International organizations like the United Nations and the World Bank are already using it to better understand population density, and to provide support and emergency assistance.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><video alt=\"Immersive view London\" class=\"article-image__media\" loop=\"\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Karto_Sundar_Blog_02_Entering_the_Matrix.mp4\" tabindex=\"0\" title=\"An animation of an immersive look at the Houses of Parliament in London. That animation then moves to a smartphone screen, showing how it would appear in a Google Maps listing, then zooming in on Westminster Abbey.\" type=\"video/mp4\">Video format not supported</video></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Immersive view in Google Maps fuses together aerial and street level images.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re also bringing new capabilities into Maps. Using advances in 3D mapping and machine learning, we\u2019re fusing billions of aerial and street level images to create a new, high-fidelity representation of a place. These breakthrough technologies are coming together to power a <a href=\"https://blog.google/products/maps/three-maps-updates-io-2022\">new experience in Maps called immersive view</a>: it allows you to explore a place like never before.</p><p>Let\u2019s go to London and take a look. Say you\u2019re planning to visit Westminster with your family. You can get into this immersive view straight from Maps on your phone, and you can pan around the sights\u2026 here\u2019s Westminster Abbey. If you\u2019re thinking of heading to Big Ben, you can check if there's traffic, how busy it is, and even see the weather forecast. And if you\u2019re looking to grab a bite during your visit, you can check out restaurants nearby and get a glimpse inside.</p><p>What's amazing is that isn't a drone flying in the restaurant \u2014 we use neural rendering to create the experience from images alone. And Google Cloud Immersive Stream allows this experience to run on just about any smartphone. This feature will start rolling out in Google Maps for select cities globally later this year.</p><p>Another big improvement to Maps is eco-friendly routing. Launched last year, it shows you the most fuel-efficient route, giving you the choice to save money on gas and reduce carbon emissions. Eco-friendly routes have already rolled out in the U.S. and Canada \u2014 and people have used them to travel approximately 86 billion miles, helping save an estimated half million metric tons of carbon emissions, the equivalent of taking 100,000 cars off the road.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Still image of eco-friendly routing on Google Maps \u2014 a 53-minute driving route in Berlin is pictured, with text below the map showing it will add three minutes but save 18% more fuel.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Eco_Routes.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Eco-friendly routes will expand to Europe later this year.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>I\u2019m happy to share that we\u2019re expanding this feature to more places, including Europe later this year. In this Berlin example, you could reduce your fuel consumption by 18% taking a route that\u2019s just three minutes slower. These small decisions have a big impact at scale. With the expansion into Europe and beyond, we estimate carbon emission savings will double by the end of the year.</p><p>And we\u2019ve added a similar feature to Google Flights. When you search for flights between two cities, we also show you carbon emission estimates alongside other information like price and schedule, making it easy to choose a greener option. These eco-friendly features in Maps and Flights are part of our goal to empower 1 billion people to make more sustainable choices through our products, and we\u2019re excited about the progress here.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>New YouTube features to help people easily access video content</h3><p>Beyond Maps, video is becoming an even more fundamental part of how we share information, communicate, and learn. Often when you come to YouTube, you are looking for a specific moment in a video and we want to help you get there faster.</p><p>Last year we launched auto-generated chapters to make it easier to jump to the part you\u2019re most interested in.</p><p>This is also great for creators because it saves them time making chapters. We\u2019re now applying multimodal technology from DeepMind. It simultaneously uses text, audio and video to auto-generate chapters with greater accuracy and speed. With this, we now have a goal to 10X the number of videos with auto-generated chapters, from eight million today, to 80 million over the next year.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Often the fastest way to get a sense of a video\u2019s content is to read its transcript, so we\u2019re also using speech recognition models to transcribe videos. Video transcripts are now available to all Android and iOS users.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Animation showing a video being automatically translated. Then text reads &quot;Now available in sixteen languages.&quot;\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/5.10.22_YT_Auto-Translate.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Auto-translated captions on YouTube.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Next up, we\u2019re bringing auto-translated captions on YouTube to mobile. Which means viewers can now auto-translate video captions in 16 languages, and creators can grow their global audience. We\u2019ll also be expanding auto-translated captions to Ukrainian YouTube content next month, part of our larger effort to increase access to accurate information about the war.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Helping people be more efficient with Google Workspace</h3><p>Just as we\u2019re using AI to improve features in YouTube, we\u2019re <a href=\"https://cloud.google.com/blog/products/workspace/introducing-new-ai-to-help-people-thrive-in-hybrid-work\">building it into our Workspace products</a> to help people be more efficient. Whether you work for a small business or a large institution, chances are you spend a lot of time reading documents. Maybe you\u2019ve felt that wave of panic when you realize you have a 25-page document to read ahead of a meeting that starts in five minutes.</p><p>At Google, whenever I get a long document or email, I look for a TL;DR at the top \u2014 TL;DR is short for \u201cToo Long, Didn\u2019t Read.\u201d And it got us thinking, wouldn\u2019t life be better if more things had a TL;DR?</p><p>That\u2019s why we\u2019ve introduced automated summarization for Google Docs. Using one of our machine learning models for text summarization, Google Docs will automatically parse the words and pull out the main points.</p><p>This marks a big leap forward for natural language processing. Summarization requires understanding of long passages, information compression and language generation, which used to be outside of the capabilities of even the best machine learning models.</p><p>And docs are only the beginning. We\u2019re launching summarization for other products in Workspace. It will come to Google Chat in the next few months, providing a helpful digest of chat conversations, so you can jump right into a group chat or look back at the key highlights.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Animation showing summary in Google Chat\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/5.10.22_Chat_Summarization.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>We\u2019re bringing summarization to Google Chat in the coming months.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>And we\u2019re working to bring transcription and summarization to Google Meet as well so you can catch up on some important meetings you missed.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Visual improvements on Google Meet</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Of course there are many moments where you really want to be in a virtual room with someone. And that\u2019s why we continue to improve audio and video quality, inspired by <a href=\"https://blog.google/technology/research/project-starline/\">Project Starline</a>. We introduced Project Starline at I/O last year. And we\u2019ve been testing it across Google offices to get feedback and improve the technology for the future. And in the process, we\u2019ve learned some things that we can apply right now to Google Meet.</p><p>Starline inspired machine learning-powered image processing to automatically improve your image quality in Google Meet. And it works on all types of devices so you look your best wherever you are.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"An animation of a man looking directly at the camera then waving and smiling. A white line sweeps across the screen, adjusting the image quality to make it brighter and clearer.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/5.10.22_Portrait_Restore.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Machine learning-powered image processing automatically improves image quality in Google Meet.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re also bringing studio quality virtual lighting to Meet. You can adjust the light position and brightness, so you\u2019ll still be visible in a dark room or sitting in front of a window. We\u2019re testing this feature to ensure everyone looks like their true selves, continuing the work we\u2019ve done with Real Tone on Pixel phones and the <a href=\"https://blog.google/products/search/monk-skin-tone-scale/\">Monk Scale</a>.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/products/search/monk-skin-tone-scale/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Improving skin tone representation across Google</h4><p class=\"uni-related-article-tout__body\">We're introducing a next step in our commitment to image equity and improving representation across our products.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>These are just some of the ways AI is improving our products: making them more helpful, more accessible, and delivering innovative new features for everyone.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col-l--6 h-c-grid__col--8 h-c-grid__col-l--offset-3 h-c-grid__col--offset-2\"><img alt=\"Gif shows a phone camera pointed towards a rack of shelves, generating helpful information about food items. Text on the screen shows the words \u2018dark\u2019, \u2018nut-free\u2019 and \u2018highly-rated\u2019.\" class=\"article-image--large\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Scene_exploration_1.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Today at I/O Prabhakar Raghavan <a href=\"https://blog.google/products/search/search-io22/\">shared</a> how we\u2019re helping people find helpful information in more intuitive ways on Search.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Making knowledge accessible through computing</h3><p>We\u2019ve talked about how we\u2019re advancing access to knowledge as part of our mission: from better language translation to improved Search experiences across images and video, to richer explorations of the world using Maps.</p><p>Now we\u2019re going to focus on how we make that knowledge even more accessible through computing. The journey we\u2019ve been on with computing is an exciting one. Every shift, from desktop to the web to mobile to wearables and ambient computing has made knowledge more useful in our daily lives.</p><p>As helpful as our devices are, we\u2019ve had to work pretty hard to adapt to them. I\u2019ve always thought computers should be adapting to people, not the other way around. We continue to push ourselves to make progress here.</p><p>Here\u2019s how we\u2019re making computing more natural and intuitive with the <a href=\"https://blog.google/products/assistant/assistant-io-2022/\">Google Assistant</a>.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/products/assistant/assistant-io-2022/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Have more natural conversations with Google Assistant</h4><p class=\"uni-related-article-tout__body\">Google Assistant announces more natural and conversational ways to interact with devices.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Introducing LaMDA 2 and AI Test Kitchen</h3></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Animation shows demos of how LaMDA can converse on any topic and how AI Test Kitchen can help create lists.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/AI_TestKitchen_BlogPost_10fps.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>A demo of LaMDA, our generative language model for dialogue application, and the AI Test Kitchen.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We're continually working to <a href=\"https://blog.google/technology/ai/understanding-the-world-through-language/\">advance our conversational capabilities</a>. Conversation and natural language processing are powerful ways to make computers more accessible to everyone. And large language models are key to this.</p><p>Last year, we <a href=\"https://blog.google/technology/ai/lamda/\">introduced LaMDA</a>, our generative language model for dialogue applications that can converse on any topic. Today, we are excited to announce LaMDA 2, our most advanced conversational AI yet.</p><p>We are at the beginning of a journey to make models like these useful to people, and we feel a deep responsibility to get it right. To make progress, we need people to experience the technology and provide feedback. We opened LaMDA up to thousands of Googlers, who enjoyed testing it and seeing its capabilities. This yielded significant quality improvements, and led to a reduction in inaccurate or offensive responses.</p><p>That\u2019s why we\u2019ve made AI Test Kitchen. It\u2019s a new way to explore AI features with a broader audience. Inside the AI Test Kitchen, there are a few different experiences. Each is meant to give you a sense of what it might be like to have LaMDA in your hands and use it for things you care about.</p><p>The first is called \u201cImagine it.\u201d This demo tests if the model can take a creative idea you give it, and generate imaginative and relevant descriptions. These are not products, they are quick sketches that allow us to explore what LaMDA can do with you. The user interfaces are very simple.</p><p>Say you\u2019re writing a story and need some inspirational ideas. Maybe one of your characters is exploring the deep ocean. You can ask what that might feel like. Here LaMDA describes a scene in the Mariana Trench. It even generates follow-up questions on the fly. You can ask LaMDA to imagine what kinds of creatures might live there. Remember, we didn\u2019t hand-program the model for specific topics like submarines or bioluminescence. It synthesized these concepts from its training data. That\u2019s why you can ask about almost any topic: Saturn\u2019s rings or even being on a planet made of ice cream.</p><p>Staying on topic is a challenge for language models. Say you\u2019re building a learning experience \u2014 you want it to be open-ended enough to allow people to explore where curiosity takes them, but stay safely on topic. Our second demo tests how LaMDA does with that.</p><p>In this demo, we\u2019ve primed the model to focus on the topic of dogs. It starts by generating a question to spark conversation, \u201cHave you ever wondered why dogs love to play fetch so much?\u201d And if you ask a follow-up question, you get an answer with some relevant details: it\u2019s interesting, it thinks it might have something to do with the sense of smell and treasure hunting.</p><p>You can take the conversation anywhere you want. Maybe you\u2019re curious about how smell works and you want to dive deeper. You\u2019ll get a unique response for that too. No matter what you ask, it will try to keep the conversation on the topic of dogs. If I start asking about cricket, which I probably would, the model brings the topic back to dogs in a fun way.</p><p>This challenge of staying on-topic is a tricky one, and it\u2019s an important area of research for building useful applications with language models.</p><p>These experiences show the potential of language models to one day help us with things like planning, learning about the world, and more.</p><p>Of course, there are significant challenges to solve before these models can truly be useful. While we have improved safety, the model might still generate inaccurate, inappropriate, or offensive responses. That\u2019s why we are inviting feedback in the app, so people can help report problems.</p><p>We will be doing all of this work in accordance with our AI Principles. Our process will be iterative, opening up access over the coming months, and carefully assessing feedback with a broad range of stakeholders \u2014 from AI researchers and social scientists to human rights experts. We\u2019ll incorporate this feedback into future versions of LaMDA, and share our findings as we go.</p><p>Over time, we intend to continue adding other emerging areas of AI into AI Test Kitchen. You can learn more at: <a href=\"http://g.co/AITestKitchen\">g.co/AITestKitchen</a>.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Advancing AI language models</h3><p>LaMDA 2 has incredible conversational capabilities. To explore other aspects of natural language processing and AI, we recently announced a new model. It\u2019s called <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html?m=1\">Pathways Language Model</a>, or PaLM for short. It\u2019s our largest model to date and trained on 540 billion parameters.</p><p>PaLM demonstrates breakthrough performance on many natural language processing tasks, such as generating code from text, answering a math word problem, or even explaining a joke.</p><p>It achieves this through greater scale. And when we combine that scale with a new technique called chain-of- thought prompting, the results are promising. Chain-of-thought prompting allows us to describe multi-step problems as a series of intermediate steps.</p><p>Let\u2019s take an example of a math word problem that requires reasoning. Normally, how you use a model is you prompt it with a question and answer, and then you start asking questions. In this case: How many hours are in the month of May? So you can see, the model didn\u2019t quite get it right.</p><p>In chain-of-thought prompting, we give the model a question-answer pair, but this time, an explanation of how the answer was derived. Kind of like when your teacher gives you a step-by-step example to help you understand how to solve a problem. Now, if we ask the model again \u2014 how many hours are in the month of May \u2014 or other related questions, it actually answers correctly and even shows its work.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"There are two boxes below a heading saying \u2018chain-of-thought prompting\u2019. A box headed \u2018input\u2019 guides the model through answering a question about how many tennis balls a person called Roger has. The output box shows the model correctly reasoning through and answering a separate question (\u2018how many hours are in the month of May?\u2019)\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_Chain_of_Thought.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Chain-of-thought prompting leads to better reasoning and more accurate answers.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Chain-of-thought prompting increases accuracy by a large margin. This leads to state-of-the-art performance across several reasoning benchmarks, including math word problems. And we can do it all without ever changing how the model is trained.</p><p>PaLM is highly capable and can do so much more. For example, you might be someone who speaks a language that\u2019s not well-represented on the web today \u2014 which makes it hard to find information. Even more frustrating because the answer you are looking for is probably out there. PaLM offers a new approach that holds enormous promise for making knowledge more accessible for everyone.</p><p>Let me show you an example in which we can help answer questions in a language like Bengali \u2014 spoken by a quarter billion people. Just like before we prompt the model with two examples of questions in Bengali with both Bengali and English answers.</p><p>That\u2019s it, now we can start asking questions in Bengali: \u201cWhat is the national song of Bangladesh?\u201d The answer, by the way, is \u201cAmar Sonar Bangla\u201d \u2014 and PaLM got it right, too. This is not that surprising because you would expect that content to exist in Bengali.</p><p>You can also try something that is less likely to have related information in Bengali such as: \u201cWhat are popular pizza toppings in New York City?\u201d The model again answers correctly in Bengali. Though it probably just stirred up a debate amongst New Yorkers about how \u201ccorrect\u201d that answer really is.</p><p>What\u2019s so impressive is that PaLM has never seen parallel sentences between Bengali and English. Nor was it ever explicitly taught to answer questions or translate at all! The model brought all of its capabilities together to answer questions correctly in Bengali. And we can extend the techniques to more languages and other complex tasks.</p><p>We're so optimistic about the potential for language models. One day, we hope we can answer questions on more topics in any language you speak, making knowledge even more accessible, in Search and across all of Google.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Introducing the world\u2019s largest, publicly available machine learning hub</h3><p>The advances we\u2019ve shared today are possible only because of our continued innovation in our infrastructure. Recently we announced plans to invest $9.5 billion in data centers and offices across the U.S.</p><p>One of our state-of-the-art data centers is in Mayes County, Oklahoma. I\u2019m excited to announce that, there, we are launching the world\u2019s largest, publicly-available machine learning hub for our Google Cloud customers.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Still image of a data center with Oklahoma map pin on bottom left corner.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.10.22_TPU_Oklahoma.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>One of our state-of-the-art data centers in Mayes County, Oklahoma.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>This machine learning hub has eight Cloud TPU v4 pods, custom-built on the same networking infrastructure that powers Google\u2019s largest neural models. They provide nearly nine exaflops of computing power in aggregate \u2014 bringing our customers an unprecedented ability to run complex models and workloads. We hope this will fuel innovation across many fields, from medicine to logistics, sustainability and more.</p><p>And speaking of sustainability, this machine learning hub is already operating at 90% carbon-free energy. This is helping us make progress on our goal to become the first major company to operate all of our data centers and campuses globally on 24/7 carbon-free energy by 2030.</p><p>Even as we invest in our data centers, we are working to innovate on our mobile platforms so more processing can happen locally on device. Google Tensor, our custom system on a chip, was an important step in this direction. It\u2019s already running on Pixel 6 and Pixel 6 Pro, and it brings our AI capabilities \u2014 including the best speech recognition we\u2019ve ever deployed \u2014 right to your phone. It\u2019s also a big step forward in making those devices more secure. Combined with Android\u2019s Private Compute Core, it can run data-powered features directly on device so that it\u2019s private to you.</p><p>People turn to our products every day for help in moments big and small. Core to making this possible is protecting your private information each step of the way. Even as technology grows increasingly complex, we <a href=\"https://blog.google/technology/safety-security/io-safer-with-google/\">keep more people safe online</a> than anyone else in the world, with products that are secure by default, private by design and that put you in control.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/technology/safety-security/how-we-make-every-day-safer-with-google/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">How we make every day safer with Google</h4><p class=\"uni-related-article-tout__body\">An update on how Google keeps more people safe online than anyone else in the world.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We also spent time today sharing updates to platforms like <a href=\"https://www.blog.google/products/android/io22-multideviceworld\">Android</a>. They\u2019re delivering access, connectivity, and information to billions of people through their smartphones and other connected devices like TVs, cars and watches.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/products/android/io22-multideviceworld/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Living in a multi-device world with Android</h4><p class=\"uni-related-article-tout__body\">At I/O, Android announced updates to your phone, to your watch and tablet devices, and to help all your devices work better together.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>And we shared our new <a href=\"https://blog.google/products/pixel/hardware-updates-io-2022\">Pixel Portfolio,</a> including the Pixel 6a, Pixel Buds Pro, Google Pixel Watch, Pixel 7, and Pixel tablet all built with ambient computing in mind. We\u2019re excited to share a family of devices that work better together \u2014 for you.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/products/pixel/hardware-updates-io-2022/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Take a look at our new Pixel portfolio, made to be helpful</h4><p class=\"uni-related-article-tout__body\">The new Pixel portfolio furthers our work in ambient computing, making your hardware work better together for you.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>The next frontier of computing: augmented reality</h3><p>Today we talked about all the technologies that are changing how we use computers and access knowledge. We see devices working seamlessly together, exactly when and where you need them and with conversational interfaces that make it easier to get things done.</p><p>Looking ahead, there's a new frontier of computing, which has the potential to extend all of this even further, and that is augmented reality. At Google, we have been heavily invested in this area. We\u2019ve been building augmented reality into many Google products, from Google Lens to multisearch, scene exploration, and Live and immersive views in Maps.</p><p>These AR capabilities are already useful on phones and the magic will really come alive when you can use them in the real world without the technology getting in the way.</p><p>That potential is what gets us most excited about AR: the ability to spend time focusing on what matters in the real world, in our real lives. Because the real world is pretty amazing!</p><p>It\u2019s important we design in a way that is built for the real world \u2014 and doesn\u2019t take you away from it. And AR gives us new ways to accomplish this.</p><p>Let\u2019s take language as an example. Language is just so fundamental to connecting with one another. And yet, understanding someone who speaks a different language, or trying to follow a conversation if you are deaf or hard of hearing can be a real challenge. Let's see what happens when we take our advancements in translation and transcription and deliver them in your line of sight in one of the early prototypes we\u2019ve been testing.</p></div></div><div class=\"block-video\"><div class=\"h-c-page h-c-page--mobile-full-bleed\"><div class=\"h-c-grid\"><div class=\"h-c-grid__col h-c-grid__col-l--10 h-c-grid__col-l--offset-1\"><div class=\"article-module uni-article-video uni-article-video--body\"><div class=\"uni-article-video__embed-container hidden\"><div id=\"uni-article-yt-player-lj0bFX9HXeE\"></div></div><figure><a class=\"h-c-video h-c-video--marquee uni-article-video__custom-wrapper\" tabindex=\"0\"><div class=\"uni-article-video__aspect-image\"><img alt=\"Breaking down language barriers with augmented reality | Google\" src=\"https://img.youtube.com/vi/lj0bFX9HXeE/maxresdefault.jpg\" /><div class=\"uni-article-video__dimmer\"></div><svg class=\"uni-article-video__play-button--active\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button_no_hole\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><svg class=\"uni-article-video__play-button\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><div class=\"uni-article-video__duration loading\"><svg class=\"uni-article-video__duration-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_duration\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><span class=\"uni-article-video__duration-time\">10:25</span></div></div></a></figure></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>You can see it in their faces: the joy that comes with speaking naturally to someone. That moment of connection. To understand and be understood. That\u2019s what our focus on knowledge and computing is all about. And it\u2019s what we strive for every day, with products that are built to help.</p><p>Each year we get a little closer to delivering on our timeless mission. And we still have so much further to go. At Google, we genuinely feel a sense of excitement about that. And we are optimistic that the breakthroughs you just saw will help us get there. Thank you to all of the developers, partners and customers who joined us today. We look forward to building the future with all of you.</p></div></div>",
            "pubdate": "Wed, 11 May 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "Understanding the world through language": {
            "url": "https://blog.google/technology/ai/understanding-the-world-through-language/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Language is at the heart of how people communicate with each other. It\u2019s also proving to be powerful in advancing AI and building helpful experiences for people worldwide.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>From the beginning, we set out to connect words in your search to words on a page so we could make the web\u2019s information more accessible and useful. Over 20 years later, as the web changes, and the ways people consume information expand from text to images to videos and more \u2014 the one constant is that language remains a surprisingly powerful tool for understanding information.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In recent years, we\u2019ve seen an incredible acceleration in the field of natural language understanding. While our systems still don\u2019t understand language the way people do, they\u2019re increasingly able to spot patterns in information, identify complex concepts and even draw implicit connections between them. We\u2019re even finding that many of our advanced models can understand information across languages or in non-language-based formats like images and videos.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Building the next generation of language models</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In 2017, Google researchers developed the Transformer, the neural network that underlies major advancements like <a href=\"https://blog.google/products/search/introducing-mum/\">MUM</a> and <a href=\"https://blog.google/technology/ai/lamda/\">LaMDA</a>. Last year, we shared our thinking on a <a href=\"https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\">new architecture</a> called Pathways, which is loosely inspired by the sparse patterns of neural activity in the brain. When you read a blog post like this one, only the critical parts of your brain needed to process this information fire up \u2014 not every single neuron. With Pathways, we\u2019re now able to train AI models to be similarly effective.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Using this system, we recently introduced <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\">PaLM</a>, a new model that achieves state-of-the-art performance on challenging language modeling tasks. It can solve complex math word problems, and answer questions in new languages with very little additional training data.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>PaLM also shows improvements in understanding and expressing logic. This is significant because it allows the model to express its reasoning through words. Remember your algebra problem sets? It wasn\u2019t enough to just get the right answer \u2014 you had to explain how you got there. PaLM is able to prompt a \u201c<a href=\"http://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\">Chain of Thought</a>\u201d to explain its thought process, step-by-step. This emerging capability helps improve accuracy and our understanding of how a model arrives at answers.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Flow chart for the difference between &quot;Standard Prompting&quot; and &quot;Chain of Thought Prompting&quot;\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Chain_of_Thought_Prompting.max-1000x1000.png\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Translating the languages of the world</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Pathways-related models are enabling us to break down language barriers in a way never before possible. Nowhere is this clearer than in our <a href=\"https://blog.google/products/translate/24-new-languages\">recently added support for 24 new languages</a> in Google Translate, spoken by over 300 million people worldwide \u2014 including the first indigenous languages of the Americas. The amazing part is that the neural model did this using only <a href=\"http://ai.googleblog.com/2022/05/24-new-languages-google-translate.html\">monolingual text</a> with no translation pairs \u2014 which allows us to help communities and languages underrepresented by technology. Machine translation at this level helps the world feel a bit smaller, while allowing us to <a href=\"https://www.youtube.com/watch?v=lj0bFX9HXeE\">dream bigger</a>.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Unlocking knowledge about the world across modalities</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today, people consume information through webpages, images, videos, and more. Our advanced language and Pathways-related models are learning to make sense of information stemming from these different modalities through language. With these multimodal capabilities, we\u2019re <a href=\"https://blog.google/products/search/search-io22/\">expanding multisearch</a> in the Google app so you can search more naturally than ever before. As the saying goes \u2014 \u201ca picture is worth a thousand words\u201d \u2014 it turns out, words are really the key to sharing information about the world.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"&quot;Scene exploration&quot; GIF of a store shelf demonstrating multisearch\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Chocolate_Scene.gif\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Improving conversational AI</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Despite these advancements, human language continues to be one of the most complex undertakings for computers.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In everyday conversation, we all naturally say \u201cum,\u201d pause to find the right words, or correct ourselves \u2014 and yet other people have no trouble understanding what we\u2019re saying. That\u2019s because people can react to conversational cues in as little as 200 milliseconds. Moving our speech model from data centers to run on the device made things faster, but we wanted to push the envelope even more.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Computers aren\u2019t there yet \u2014 so we\u2019re introducing <a href=\"https://blog.google/products/assistant/assistant-io-2022\">improvements to responsiveness on the Assistant</a> with unified neural networks, combining many models into smarter ones capable of understanding more \u2014 like when someone pauses but is not finished speaking. Getting closer to the fluidity of real-time conversation is finally possible with Google's Tensor chip, which is custom-engineered to handle on-device machine learning tasks super fast.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re also investing in building models that are capable of carrying more natural, sensible and specific conversations. Since introducing LaMDA to the world last year, we\u2019ve made great progress, improving the model in key areas of <a href=\"https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\">quality, safety and groundedness</a> \u2014 areas where we know conversational AI models can struggle. We\u2019ll be releasing the next iteration, LaMDA 2, as a part of the <a href=\"http://g.co/aitestkitchen\">AI Test Kitchen</a>, which we\u2019ll be opening up to small groups of people gradually. Our goal with AI Test Kitchen is to learn, improve, and innovate responsibly on this technology together. It\u2019s still early days for LaMDA, but we want to continue to make progress and do so responsibly with feedback from the community.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"GIF showing LaMDA 2 on device\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/AI-TestKitchen-BlogPost-v5.gif\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Responsible development of AI models</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>While language is a remarkably powerful and versatile tool for understanding the world around us, we also know it comes with its limitations and challenges. In 2018, we published our <a href=\"https://ai.google/principles/\">AI Principles</a> as guidelines to help us avoid bias, test rigorously for safety, design with privacy top of mind and make technology accountable to people. We\u2019re investing in research across disciplines to understand the types of harms language models can affect, and to develop the frameworks and methods to ensure we bring in a diversity of perspectives and make meaningful improvements. We also build and use <a href=\"https://pair-code.github.io/lit/\">tools</a> that can help us better understand our models (e.g., identifying how different words affect a prediction, tracing an error back to training data and even measuring correlations within a model). And while we work to improve underlying models, we also test rigorously before and after any kind of product deployment.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019ve come a long way since introducing the world to the Transformer. We\u2019re proud of the tremendous value that it and its predecessors have brought not only to everyday Google products like Search and Translate, but also the breakthroughs they\u2019ve powered in natural language understanding. Our work advancing the future of AI is driven by something as old as time: the power language has to bring people together.</p></div></div>",
            "pubdate": "Wed, 11 May 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "Immersive view coming soon to Maps  plus more updates": {
            "url": "https://blog.google/products/maps/three-maps-updates-io-2022/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Google Maps helps over one billion people navigate and explore. And over the past few years, our investments in AI have supercharged the ability to bring you the most helpful information about the real world, including when a <a href=\"https://blog.google/products/maps/how-ai-and-imagery-build-self-updating-map/\">business is open</a> and <a href=\"https://blog.google/products/maps/new-normals-with-google-maps/\">how crowded your bus is</a>. Today at Google I/O, we announced new ways the latest advancements in AI are transforming Google Maps \u2014 helping you explore with an all-new immersive view of the world, find the most fuel-efficient route, and use the magic of Live View in your favorite third-party apps.</p><h3>A more immersive, intuitive map</h3><p>Google Maps first launched to help people navigate to their destinations. Since then, it\u2019s evolved to become much more \u2014 it\u2019s a handy companion when you need to find the perfect restaurant or get information about a local business. Today \u2014 thanks to advances in computer vision and AI that allow us to fuse together billions of Street View and aerial images to create a rich, digital model of the world \u2014 we\u2019re introducing a whole new way to explore with Maps. With our new immersive view, you\u2019ll be able to experience what a neighborhood, landmark, restaurant or popular venue is like \u2014 and even feel like you\u2019re right there before you ever set foot inside. So whether you\u2019re traveling somewhere new or scoping out hidden local gems, immersive view will help you make the most informed decisions before you go.</p><p>Say you\u2019re planning a trip to London and want to figure out the best sights to see and places to eat. With a quick search, you can virtually soar over Westminster to see the neighborhood and stunning architecture of places, like Big Ben, up close. With Google Maps\u2019 helpful information layered on top, you can use the time slider to check out what the area looks like at different times of day and in various weather conditions, and see where the busy spots are. Looking for a spot for lunch? Glide down to street level to explore nearby restaurants and see helpful information, like live busyness and nearby traffic. You can even look inside them to quickly get a feel for the vibe of the place before you book your reservation.</p><p>The best part? Immersive view will work on just about any phone and device. It starts rolling out in Los Angeles, London, New York, San Francisco and Tokyo later this year with more cities coming soon.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><video alt=\"Immersive View\" class=\"article-image__media\" loop=\"\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Karto_Geo_Blog_Edit_1080p_zoXr3if.mp4\" tabindex=\"0\" title=\"A multidimensional video soaring over London, showing the Westminster area, Big Ben and nearby restaurants up close\" type=\"video/mp4\">Video format not supported</video></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Immersive view lets you explore and understand the vibe of a place before you go</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>An update on eco-friendly routing</h3><p>In addition to making places easier to explore, we want to help you get there more sustainably. We recently launched eco-friendly routing in the U.S. and Canada, which lets you see and choose the most fuel-efficient route when looking for driving directions \u2014 helping you save money on gas. Since then, people have used it to travel 86 billion miles, saving more than an estimated half a million metric tons of carbon emissions \u2014 equivalent to taking 100,000 cars off the road. We\u2019re on track to double this amount as we expand to more places, like Europe.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Still image of eco-friendly routing on Google Maps\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Eco-friendly_routing_I_O.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Eco-friendly routing has helped save more than an estimated half a million metric tons of carbon emissions</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>The magic of Live View \u2014 now in your favorite apps</h3><p><a href=\"https://blog.google/products/maps/new-sense-direction-live-view/\">Live View</a> helps you find your way when walking around, using AR to display helpful arrows and directions right on top of your world. It's especially helpful when navigating tricky indoor areas, like <a href=\"https://blog.google/products/maps/redefining-what-map-can-be-new-information-and-ai/\">airports, malls and train stations</a>. Thanks to our AI-based technology called <a href=\"https://ai.googleblog.com/2019/02/using-global-localization-to-improve.html\">global localization,</a> Google Maps can point you where you need to go in a matter of seconds. As part of our efforts to bring the helpfulness of Google Maps to more places, we\u2019re now making this technology available to developers at no cost with the new ARCore Geospatial API.</p><p>Developers are already using the API to make apps that are even more useful and provide an easy way to interact with both the digital and physical worlds at once. Shared electric vehicle company Lime is piloting the API in London, Paris, Tel Aviv, Madrid, San Diego, and Bordeaux to help riders park their e-bikes and e-scooters responsibly and out of pedestrians\u2019 right of way. Telstra and Accenture are using it to help sports fans and concertgoers find their seats, concession stands and restrooms at Marvel Stadium in Melbourne. DOCOMO and Curiosity are building a new game that lets you fend off virtual dragons with robot companions in front of iconic Tokyo landmarks, like the Tokyo Tower. The new Geospatial API is available now to ARCore developers, wherever Street View is available.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col-l--2 h-c-grid__col--4 h-c-grid__col-l--offset-5 h-c-grid__col--offset-4\"><img alt=\"DOCOMO and Curiosity game showing an AR dragon, alien and spaceship interacting on top of a real-world image, powered by the ARCore Geospatial API.\" class=\"article-image--small\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/docomo_01_1.gif\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Live View technology is now available to ARCore developers around the world</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>AI will continue to play a critical role in making Google Maps the most comprehensive and helpful map possible for people everywhere.</p></div></div>",
            "pubdate": "Wed, 11 May 2022 15:17:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "A closer look at the research to help AI see more skin tones": {
            "url": "https://blog.google/technology/research/ai-monk-scale-skin-tone-story/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today at I/O <a href=\"https://www.blog.google/products/search/monk-skin-tone-scale/\">we released the Monk Skin Tone (MST) Scale</a> in partnership with Harvard professor and sociologist <a href=\"https://www.ellismonk.com/\">Dr. Ellis Monk</a>. The MST Scale, developed by Dr. Monk, is a 10-shade scale designed to be more inclusive of the spectrum of skin tones in our society. We\u2019ll be incorporating the MST Scale into various Google products over the coming months, and we are openly releasing the scale so that anyone can use it for research and product development.</p><p>The MST Scale is an important next step in a collective effort to improve skin tone inclusivity in technology. For Google, it will help us make progress in our commitment to image equity and improving representation across our products. And in releasing the MST Scale for all to use, we hope to make it easier for others to do the same, so we can learn and evolve together.</p><p>Addressing skin tone equity in technology poses an interesting research challenge because it isn\u2019t just a technical question, it\u2019s also a social one. Making progress requires the combined expertise of a wide range of people \u2014 from academics in the social sciences who have spent years studying social inequality and skin tone stratification through their research, to product and technology users, who provide necessary nuance and feedback borne of their lived experiences, to ethicists and civil rights activists, who guide on application frameworks to ensure we preserve and honor the social nuances. The ongoing and iterative work from this wider community has led us to the knowledge and understanding that we have today, and will be key to the continued path forward.</p><p>Teams within Google have been contributing to this body of work for years now. Here\u2019s a deeper look at how Googlers have been thinking about and working on skin tone representation efforts, particularly as it relates to the MST Scale \u2014 and what might come next.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/products/search/monk-skin-tone-scale/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Read Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Improving skin tone representation across Google</h4><p class=\"uni-related-article-tout__body\">We're introducing a next step in our commitment to image equity and improving representation across our products.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Building technology that sees more people</h3><p>\u201cPersistent inequities exist globally due to prejudice or discrimination against individuals with darker skin tones, also known as colorism,\u201d says Dr. Courtney Heldreth, a social psychologist and user experience (UX) researcher in Google\u2019s Responsible AI Human-Centered Technology UX (RAI-HCT UX) department, which is part of Google Research. \u201cThe <a href=\"https://direct.mit.edu/daed/article-pdf/150/2/76/1897805/daed_a_01847.pdf\">academic literature</a> demonstrates that skin tone plays a significant role in how people are treated across a wide variety of outcomes including health, wealth, well-being, and more.\u201d And one example of colorism is when technology doesn\u2019t see skin tone accurately, potentially exacerbating existing inequities.</p><p>Machine learning, a type of AI, is the bedrock of so many products we use every day. Cameras use ML for security reasons, to unlock a phone or register that someone is at the door. ML helps categorize your photos by similar faces, or adjust the brightness on a picture.</p><p>To do this well, engineers and researchers need diverse training datasets to train models, and to extensively test the resulting models across a diverse range of images. Importantly, in order to ensure that datasets used to develop technologies relating to understanding people are more inclusive, we need a scale that represents a wide range of skin tones.</p><p>\u201cIf you're saying, <i>I tested my model for fairness to make sure it works well for darker skin tones</i>, but you\u2019re using a scale that doesn't represent most people with those skin tones, you don't know how well it actually works,\u201d says Xango Eye\u00e9, a Product Manager working on Responsible AI.</p><p>\u201cIf not developed with intention, the skin tone measure we use to understand whether our models are fair and representative can affect how products are experienced by users. Downstream, these decisions can have the biggest impacts on people who are most vulnerable to unfair treatment, people with darker skin tones,\u201d Dr. Heldreth says.</p><p>Eye\u00e9 and Dr. Heldreth are both core members of Google\u2019s research efforts focused on building more skin tone equity into AI development, a group that includes an interdisciplinary set of product managers, researchers and engineers who specialize in computer vision and social psychology. The team also works across Google with image equity teams building more representation into products like cameras, photos, and emojis.</p><p>\u201cWe take a human-centered approach to understanding how AI can influence and help people around the world,\u201d Dr. Heldreth says, \u201cfocusing on improving inclusivity in AI, to ensure that technology reflects and empowers globally and culturally diverse communities, especially those who are historically marginalized and underserved.\u201d A more inclusive skin tone scale is a core part of this effort.</p><p>The team operates with a guiding objective: To keep improving technology so that it works well for more people. Doing that has involved two major tasks: \u201cThe first was figuring out what was already built and why it wasn't working,\u201d Eye\u00e9 says. \u201cAnd the second was figuring out what we needed to build instead.\u201d</p></div></div><div class=\"block-image_carousel\"><div class=\"h-c-page article-module\"><div class=\"article-module glue-pagination h-c-carousel h-c-carousel--simple h-c-carousel--dark ng-cloak\"><div class=\"h-c-carousel__wrap\"><ul class=\"glue-carousel ng-cloak\"><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">A man with glasses looks into the camera smiling. A woman with glasses next to him is also looking into the camera and smiling, and she is holding the camera taking a selfie. They are in a guitar store.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Dr. Monk with his wife, Anna.</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">A screenshot of a Google Meet video call with three rows of nine people in individual squares.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>The team during a call. Top row: Kylee Jaye, Shrikanth Narayanan, Auriel Wright; middle row: Candice Schumann, Courtney Heldreth, Komal Singh; bottom row: Marco Andreetto, Susanna Ricco, Kree Cole-Mclaughlin</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">Two people stand side by side, looking into the camera and smiling. They\u2019re standing in front of a green wall with a garden art installation on it.]</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Susanna Ricco and Kylee Jaye, another software engineer who worked on the project.</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">Two women standing side by side with their arms around each other, looking into the camera and smiling. They are standing in front of shrubs and trees, standing on a sidewalk next to yellow and blue circular objects on the sidewalk.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Dr. Susanna Ricco and Dr. Courtney Heldreth on the Google Mountain View campus.</p></div></figcaption></figure></li></ul><div class=\"h-c-carousel__paginate glue-pagination-previous uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-left\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div><div class=\"h-c-carousel__paginate glue-pagination-next uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-right\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div></div><div class=\"h-c-carousel__navigation\"><div class=\"glue-pagination-page-list uni-click-tracker\"></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>A social-technical approach</h3><p>\"Skin tone is something that changes the physical properties of images, and it\u2019s something that affects people\u2019s lived experiences \u2014 and both of these things can impact how a piece of technology performs,\u201d Dr. Susanna Ricco says. Dr. Ricco, a software engineer on Google Research's Perception team, leads a group that specializes in finding new ways to make sure Google's computer vision systems work well for more users, regardless of their backgrounds or how they look. To make sure that tech works across skin tones, we need to intentionally test and improve it across a diverse range. \u201cTo do that, we need a scale that doesn\u2019t leave skin tones out or over-generalize,\u201d she says.</p><p>\u201cThere\u2019s the physics side of things \u2014 how well a sensor responds to a person\u2019s skin tone,\u201d Dr. Ricco says. \u201cThen there\u2019s the social side of things: We know that skin tone correlates with life experiences, so we want to make sure we\u2019re looking at fairness from this perspective, too. Ultimately what matters is, <i>does this work for me?</i> \u2014 and not just <i>me</i>, the person who\u2019s making this technology, but <i>me</i>, as in anyone who comes across it.\u201d</p><p>\u201cDeveloping a scale for this isn\u2019t just an AI or technology problem, but a social-technical problem,\u201d Dr. Heldreth says. \u201cIt\u2019s important that we understand how skin tone inequality can show up in the technology we use and importantly, do our best to avoid reproducing the colorism that exists. Fairness is contextual and uniquely experienced by each individual, so it\u2019s important to center this problem on the people who will ultimately be affected by the choices we make. Therefore, doing this right requires us to take a human-centered approach because this is a human problem.\u201d</p><p>\u201cConnecting the technical to the human is the challenge here,\u201d Dr. Ricco says. \"The groups we test should be influenced by the ways in which individuals experience technology differently, not purely decided based on mathematical convenience.\u201d</p><p>If it sounds like an intricate process, that\u2019s because it is. \u201cOur goal is not to tackle all of this complexity at once, but instead learn deeply about what each piece of research is telling us and put together the puzzle pieces,\u201d Dr. Heldreth says.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Ten circles in a row, ranging from dark to light.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Monkscale.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The Monk Skin Tone Scale</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>The Monk Skin Tone Scale</h3><p>The team knew piecing together that puzzle, and particularly thinking about how to define a range of skin tones, would be a wider effort that extended beyond Google.</p><p>So over the last year, they partnered with Dr. Monk to learn about and further test the scale for technology use cases. Dr. Monk\u2019s research focuses on how factors such as skin tone, race and ethnicity affect inequality. He has been surveying people about the kinds of ways that skin tone has played a role in their lives for a decade. \u201cIf you talk to people of color, if you ask them, \u2018How does your appearance matter in your everyday life? How does your skin color, your hair, how do they impact your life?\u2019 you find it really does matter,\u201d he says.</p><p>Dr. Monk began this research in part to build on the most prominently used skin tone scale, the Fitzpatrick Scale. Created in 1975 and made up of six broad shades, it was meant to be a jumping off point for medically categorizing skin type. The technology industry widely adopted it and applied it to skin tones and it became the standard. It\u2019s what most AI systems use to measure skin tone.</p><p>In comparison, the MST Scale is composed of 10 shades \u2014 a number chosen so as not to be too limiting, but also not too complex.</p></div></div><div class=\"block-pull_quote\"><div class=\"uni-pull-quote h-c-page\"><section class=\"h-c-grid\"><div class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"><div class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"><q class=\"uni-pull-quote__text\">It\u2019s not just about this precise numeric value of skin tone. It\u2019s about giving people something they can see themselves in.</q> <cite class=\"uni-pull-quote__author\"><span class=\"uni-pull-quote__author-meta\"><strong class=\"h-u-font-weight-medium\">Dr. Ellis Monk</strong><br /></span></cite></div></div></section></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Together, the team and Dr. Monk surveyed thousands of adults in the United States to learn if people felt more represented by the MST Scale compared to other scales that have been used in both the machine learning and beauty industries. \u201cAcross the board, people felt better represented by the MST Scale than the Fitzpatrick Scale,\u201d Eye\u00e9 says, and this was especially true for less represented demographic groups.</p><p>\u201cWhat you\u2019re looking for is that subjective moment where people can see their skin tone on the scale,\u201d Dr. Heldreth says. \u201cTo see the results of our research demonstrate that there are other skin tone measures where more people see themselves better represented felt like we were making steps in the right direction, that we could really make a difference.\u201d</p><p>Of course, 10 points are not as comprehensive as scales that have 16, 40 or 110 shades. And for many use cases, like makeup, more is better. What was exciting about the MST Scale survey results was that the team found, even with 10 shades, participants felt the scale was equally representative as scales from the beauty industry with larger variety. \u201cThey felt that the MST Scale was just as inclusive, even with only 10 points on it,\u201d Eye\u00e9 says. A 10-point scale is also something that can be used during data annotation, whereas rating skin tone images using a 40-point scale would be an almost impossible task for raters to do reliably.</p><p>What is particularly exciting about this work is that it continues to highlight the importance of a sociotechnical approach to building more equitable tools and products. Skin tones are continuous, and can be defined and categorized in a number of different ways, the simplest being to pick equally spaced RGB values on a scale of light to dark brown. But taking such a technical approach leaves out the nuance of how different communities have been historically affected by colorism. A scale that is effective for measuring and reducing inconsistent experiences for more people needs to adequately reflect a wide range of skin-tones that represent a diversity of communities \u2013 this is where Dr. Monk\u2019s expertise and research proves particularly valuable.</p><p>Over the past two years, the team has shared their research with various other departments at Google. And work has begun on building annotation \u2014 or labeling \u2014 best practices based on the MST Scale, informed by expertise in computer vision, skin tone inequality and social cognition. Since perceptions of skin tones are subjective, it\u2019s incredibly important that the same interdisciplinary research that went into creating and validating the scale is also applied to how it is used.</p><p></p><h3>What\u2019s next</h3><p>One of the first areas in which this technology will be used is Google\u2019s image-based products. Until now, Google has largely relied on the Fitzpatrick Scale for photo AI. The MST scale is now being incorporated into products like Google Photos and Image Search, and will be expanded even more broadly in the coming months.</p><p>In addition to incorporating the MST Scale into Google products and sharing the 10 shades for anyone to use, Google and Dr. Monk are publishing their peer-reviewed research and expanding their research globally. Going through the research and peer review process has helped the team make sure their work is adding to the long history of multi-sector progress in this space and also offering new ideas in the quest for more inclusive AI.</p><p>Ultimately, we want the work to extend far beyond Google. The team is hopeful this is an industry starting point, and at the same time, they want to keep improving on it. \u201cThis is an evergreen project,\u201d Dr. Heldreth says. \u201cWe\u2019re constantly learning, and that\u2019s what makes this so exciting.\u201d The team plans to take the scale to more countries to learn how they interpret skin tone, and include those learnings in future iterations of the scale.</p><p>So the work continues. And while it\u2019s certainly a \u201cmassive scientific challenge,\u201d as Dr. Heldreth calls it, it\u2019s also a very human one because it\u2019s critical that tools we use to define skin tone ensure that more people see themselves represented and thus feel worthy of being seen. \u201cIt\u2019s not just about this precise numeric value of skin tone,\u201d Dr. Monk says. \u201cIt\u2019s about giving people something they can see themselves in.\u201d</p></div></div>",
            "pubdate": "Wed, 11 May 2022 09:32:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                11
            ],
            "email_sent": true
        },
        "How we build with and for people with disabilities": {
            "url": "https://blog.google/outreach-initiatives/accessibility/building-with-the-disability-community-2022/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p><i>Editor\u2019s note: Today is Global Accessibility Awareness Day. We\u2019re also sharing how we\u2019re making</i> <a href=\"https://blog.google/outreach-initiatives/education/global-accessibility-awareness-day--2022/\"><i>education more accessible</i></a><i>and launching a new</i><a href=\"http://blog.google/products/android/braille-display-talkback\"><i>Android accessibility feature</i></a><i>.</i></p><p>Over the past nine years, my job has focused on building accessible products and supporting Googlers with disabilities. Along the way, I\u2019ve been constantly reminded of how vast and diverse the disability community is, and how important it is to continue working alongside this community to build technology and solutions that are truly helpful.</p><p>Before delving into some of the accessibility features our teams have been building, I want to share how we\u2019re working to be more inclusive of people with disabilities to create more accessible tools overall.</p><h3>Nothing about us, without us</h3><p>In the disability community, people often say \u201cnothing about us without us.\u201d It\u2019s a sentiment that I find sums up what disability inclusion means. The types of barriers that people with disabilities face in society vary depending on who they are, where they live and what resources they have access to. No one\u2019s experience is universal. That\u2019s why it\u2019s essential to include a wide array of people with disabilities at every stage of the development process for any of our accessibility products, initiatives or programs.</p><p>We need to work to make sure our teams at Google are reflective of the people we\u2019re building for. To do so, last year we launched our <a href=\"https://careers.google.com/programs/people-with-disabilities/\">hiring site</a> geared toward people with disabilities \u2014 including our <a href=\"https://cloud.google.com/blog/topics/inside-google-cloud/google-cloud-launches-a-career-program-for-people-with-autism\">Autism Career Program</a> to further grow and strengthen our autistic community. Most recently, we helped launch the <a href=\"https://ndcc.simplifyhire.com/\">Neurodiversity Career Connector</a> along with other companies to create a job portal that connects neurodiverse candidates to companies that are committed to hiring more inclusively.</p><p>Beyond our internal communities, we also must partner with communities outside of Google so we can learn what is truly useful to different groups and parlay that understanding into the improvement of current products or the creation of new ones. Those partnerships have resulted in the creation of <a href=\"https://blog.google/outreach-initiatives/accessibility/project-relate/\">Project Relate</a>, a communication tool for people with speech impairments, the development of a <a href=\"https://blog.google/products/android/all-new-talkback/\">completely new TalkBack</a>, Android\u2019s built-in screen reader, and the <a href=\"https://blog.google/products/chromebooks/accessibility-features/\">improvement of Select-to-Speak</a>, a Chromebook tool that lets you hear selected text on your screen spoken out loud.</p><h3>Equitable experiences for everyone</h3><p>Engaging and listening to these communities \u2014 inside and outside of Google \u2014 make it possible to create tools and features like the ones we\u2019re sharing today.</p><p>The ability to add alt-text, which is a short description of an image that is read aloud by screen readers, directly to images sent through Gmail starts rolling out today. With this update, people who use screen readers will know what\u2019s being sent to them, whether it\u2019s a GIF celebrating the end of the week or a screenshot of an important graph.</p><p>Communication tools that are inclusive of everyone are especially important as teams have shifted to fully virtual or hybrid meetings. Again, everyone experiences these changes differently. We\u2019ve heard from some people who are deaf or hard of hearing, that this shift has made it easier to identify who is speaking \u2014 something that is often more difficult in person. But, in the case of people who use ASL, we\u2019ve heard that it can be difficult to be in a virtual meeting and simultaneously see their interpreter and the person speaking to them.</p><p>Multi-pin, a new feature in Google Meet, helps solve this. Now you can pin multiple video tiles at once, for example, the presenter\u2019s screen and the interpreter\u2019s screen. And like many accessibility features, the usefulness extends beyond people with disabilities. The next time someone is watching a panel and wants to pin multiple people to the screen, this feature makes that possible.</p><p>We've also been working to make video content more accessible to those who are blind or low-vision through audio descriptions that describe verbally what is on the screen visually. All of our English language YouTube Originals content from the past year \u2014 and moving forward \u2014 will now have English audio descriptions available globally. To turn on the audio description track, at the bottom right of the video player, click on \u201cSettings\u201d, select \u201cAudio track\u201d, and choose \u201cEnglish descriptive\u201d.</p><p>For many people with speech impairments, being understood by the technology that powers tools like voice typing or virtual assistants can be difficult. In 2019, we started work to change that through Project Euphonia, a research initiative that works with community organizations and people with speech impairments to create more inclusive speech recognition models. Today, we\u2019re expanding Project Euphonia\u2019s research to include four more languages: <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfroo5csa5ZZsx9ysjS8vO9BihmV6SLnKUCNNKdA8oSgt6zWQ/viewform\">French</a>, <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSeBFmyc9hkc7e6Llm0-2VRIpz5j7iu5Rwfpxg-F_8IvuQMwDg/viewform\">Hindi</a>, <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScSEbxXZsfZbIUNmm0TlkWvF3C_gPcpucTzGm36TsBSCLV3oA/viewform?resourcekey=0-quzTBS9bMQ2zL5fXh3IjDA\">Japanese</a> and <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSc9tkmL5VwSP88nGHULo6NauivSLgEBxgR3vO9knc1P4cvsUA/viewform?resourcekey=0-nNMgUv0K876UAOCbu7i2zw\">Spanish</a>. With this expansion, we can create even more helpful technology for more people \u2014 no matter where they are or what language they speak.</p><p>I\u2019ve learned so much in my time working in this space and among the things I\u2019ve learned is the absolute importance of building right alongside the very people who will most use these tools in the end. We\u2019ll continue to do that as we work to create a more inclusive and accessible world, both physically and digitally.</p></div></div>",
            "pubdate": "Thu, 19 May 2022 13:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                19
            ],
            "email_sent": true
        },
        "Building a more helpful browser with machine learning": {
            "url": "https://blog.google/products/chrome/building-a-more-helpful-browser-with-machine-learning/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>At Google we use technologies like machine learning (ML) to build more useful products \u2014 from filtering out <a href=\"https://cloud.google.com/blog/products/workspace/an-overview-of-gmails-spam-filters\">email spam</a>, to keeping maps <a href=\"https://blog.google/products/maps/how-ai-and-imagery-build-self-updating-map/\">up to date</a>, to offering more relevant <a href=\"https://blog.google/products/search/how-ai-powers-great-search-results/\">search results</a>. Chrome is no exception: We use ML to make <a href=\"https://blog.google/outreach-initiatives/accessibility/get-image-descriptions/\">web images</a> more accessible to people who are blind or have low vision, and we also generate <a href=\"https://blog.google/products/chrome/live-caption-chrome/\">real-time captions</a> for online videos, in service of people in noisy environments, and those who are hard of hearing.</p><p>This work in Chrome continues, so we wanted to share some recent and future ML improvements that offer a safer, more accessible and more personalized browsing experience. Importantly: these updates are powered by on-device ML models, which means your data stays private, and never leaves your device.</p><h3><b>More peace of mind, less annoying prompts</b></h3><p><a href=\"https://safebrowsing.google.com/\">Safe Browsing in Chrome</a> helps protect billions of devices every day, by showing warnings when people try to navigate to dangerous sites or download dangerous files (see the big red example below). Starting in March of this year, we rolled out a new ML model that identifies 2.5 times more potentially malicious sites and phishing attacks as the previous model \u2013 resulting in a safer and more secure web.</p><p>To further improve the browsing experience, we\u2019re also evolving how people interact with web notifications. On the one hand, page notifications help deliver updates from sites you care about; on the other hand, notification permission prompts can become a nuisance. To help people browse the web with minimal interruption, Chrome predicts when permission prompts are unlikely to be granted based on how the user previously interacted with similar permission prompts, and silences these undesired prompts. In the next release of Chrome, we\u2019re launching an ML model that makes these predictions entirely on-device.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Two separate images side by side. The first on the left is a smartphone showing a red screen and a warning message about phishing. The image on the right shows a Chrome browser window showing a pop-up message saying \u201cNotifications blocked\u201d.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screen_Shot_2022-06-09_at_11.39.00_AM.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>With the next release of Chrome, this is what you will see if a phishing attempt is detected (Left) and Chrome will show permission requests quietly when the user is unlikely to grant them (Right).</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3><b>Finding what's important, always in your language</b></h3><p>Earlier this year we launched <a href=\"https://blog.google/products/chrome/finding-answers-gets-better-chrome/\">Journeys</a> to help people retrace their steps online. For example: You might spend weeks planning a national park visit \u2013 researching attractions, comparing flights and shopping for gear. With ML and Journeys, Chrome brings together the pages you\u2019ve visited about a given topic, and makes it easy to pick up where you left off (vs. scr o o o l l ling through your browser history).</p><p>When you return to those hiking boots and camping guides, we\u2019re also using ML to make those websites available in your preferred language. In particular, we\u2019ve launched an updated language identification model to figure out the language of the page, and whether it needs to be translated to match your preferences. As a result, we\u2019re seeing tens of millions more successful translations every day.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"A Chrome browser showing Journeys related to travel. The user can see a cluster of recent searches they did related to a trip to Yosemite.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Chrome-Blog-Journeys_V4_YGdhgQw.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The Journeys feature of Chrome groups together your search history based on topic or intent.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3><b>A browser built just for you</b></h3><p>Maybe you like to read news articles in the morning \u2013 phone in one hand, cereal spoon in the other \u2013 so you share lots of links from Chrome. Or maybe voice search is more your thing, as you sneak in a few questions during your transit ride to work. Either way, we want to make sure Chrome is meeting you where you\u2019re at, so in the near future, we\u2019ll be using ML to adjust the toolbar in real-time \u2013 highlighting the action that\u2019s most useful in that moment (e.g., share link, voice search, etc.). Of course, you\u2019ll be able to customize it manually as well.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"A Chrome browser with a highlighted square around an icon to the right of the address bar. At the top is a share icon, and at the bottom is a microphone icon.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/VoiceShare_toolbar.max-1000x1000.png\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The toolbar in Chrome on Android will adapt based on your needs.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our goal is to build a browser that\u2019s genuinely and continuously helpful, and we\u2019re excited about the possibilities that ML provides. At the end of the day, though, your experience is what really matters, so please tweet <a href=\"https://twitter.com/googlechrome\">@googlechrome</a> to send us your feedback.</p></div></div>",
            "pubdate": "Thu, 09 Jun 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "How AI creates photorealistic images from text": {
            "url": "https://blog.google/technology/research/how-ai-creates-photorealistic-images-from-text/",
            "description": "<div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"Pictures of puppy in a nest emerging from a cracked egg. Photos overlooking a steampunk city with airships. Picture of two robots having a romantic evening at the movies.\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Final_Hero_Image_Imagen_Parti.max-1000x1000.png\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Have you ever seen a puppy in a nest emerging from a cracked egg? What about a photo that\u2019s overlooking a steampunk city with airships? Or a picture of two robots having a romantic evening at the movies? These might sound far-fetched, but a novel type of machine learning technology called text-to-image generation makes them possible. These models can generate high-quality, photorealistic images from a simple text prompt.</p><p>Within Google Research, our scientists and engineers have been exploring text-to-image generation using a variety of AI techniques. After a lot of testing we recently announced two new text-to-image models \u2014 <a href=\"https://imagen.research.google/\">Imagen</a> and <a href=\"https://parti.research.google/\">Parti</a>. Both have the ability to generate photorealistic images but use different approaches. We want to share a little more about how these models work and their potential.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>How text-to-image models work</h3><p>With text-to-image models, people provide a text description and the models produce images matching the description as closely as possible. This can be something as simple as \u201can apple\u201d or \u201ca cat sitting on a couch\u201d to more complex details, interactions and descriptive indicators like \u201ca cute sloth holding a small treasure chest. A bright golden glow is coming from the chest.\u201d</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col-l--6 h-c-grid__col--8 h-c-grid__col-l--offset-3 h-c-grid__col--offset-2\"><img alt=\"A picture of a cute sloth holding a small treasure chest. A bright golden glow is coming from the chest\" class=\"article-image--large\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Sloth_Image.max-1000x1000.png\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In the past few years, ML models have been trained on large image datasets with corresponding textual descriptions, resulting in higher quality images and a broader range of descriptions. This has sparked major breakthroughs in this area, including Open AI\u2019s <a href=\"https://openai.com/dall-e-2/\">DALL-E 2</a>.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>How Imagen and Parti work</h3><p>Imagen and Parti build on previous models. Transformer models are able to process words in relationship to one another in a sentence. They\u00a0are foundational to how we represent text in our text-to-image models. Both models also use a new <a href=\"https://openreview.net/forum?id=qw8AKxfYbI\">technique</a> that helps generate images that more closely match the text description. While Imagen and Parti use similar technology, they pursue different, but complementary strategies.</p><p>Imagen is a Diffusion model, which learns to convert a pattern of random dots to images. These images first start as low resolution and then progressively increase in resolution. Recently, Diffusion models have seen success in both <a href=\"https://iterative-refinement.github.io/palette/\">image</a> and <a href=\"https://wavegrad.github.io/\">audio</a> tasks like enhancing image resolution, recoloring black and white photos, editing regions of an image, uncropping images, and text-to-speech synthesis.</p><p>Parti\u2019s approach first <a href=\"https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html\">converts</a> a collection of images into a sequence of code entries, similar to puzzle pieces. A given text prompt is then <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\">translated</a> into these code entries and a new image is created. This approach takes advantage of existing research and infrastructure for large language models such as <a href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\">PaLM</a> and is critical for handling long, complex text prompts and producing high-quality images.</p><p>These models have many limitations. For example, neither can reliably produce specific counts of objects (e.g. \u201cten apples\u201d), nor place them correctly based on specific spatial descriptions (e.g. \u201ca red sphere to the left of a blue block with a yellow triangle on it\u201d). Also, as prompts become more complex, the models begin to falter, either missing details or introducing details that were not provided in the prompt. These behaviors are a result of several shortcomings, including lack of explicit training material, limited data representation, and lack of 3D awareness. We hope to address these gaps through broader representations and more effective integration into the text-to-image generation process.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Taking a responsible approach to Imagen and Parti</h3><p>Text-to-image models are exciting tools for inspiration and creativity. They also come with risks related to disinformation, bias and safety. We\u2019re having discussions around Responsible AI practices and the necessary steps to safely pursue this technology. As an initial step, we\u2019re using easily identifiable watermarks to ensure people can always recognize an Imagen- or Parti-generated image. We\u2019re also conducting experiments to better understand biases of the models, like how they represent people and cultures, while exploring possible mitigations. The <a href=\"https://arxiv.org/pdf/2205.11487.pdf\">Imagen</a> and <a href=\"https://parti.research.google/paper\">Parti</a> papers provide extensive discussion of these issues.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>What\u2019s next for text-to-image models at Google</h3><p>We will push on new ideas that combine the best of both models, and expand to related tasks such as adding the ability to interactively generate and edit images through text. We\u2019re also continuing to conduct in-depth comparisons and evaluations to align with our <a href=\"https://ai.google/principles/\">Responsible AI Principles</a>. Our goal is to bring user experiences based on these models to the world in a safe, responsible way that will inspire creativity.</p></div></div>",
            "pubdate": "Wed, 22 Jun 2022 17:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                22
            ],
            "email_sent": true
        },
        "Reducing gender-based harms in AI with Sunipa Dev": {
            "url": "https://blog.google/technology/ai/reducing-gender-based-harms-in-ai-with-sunipa-dev/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Natural language processing (NLP) is a form of artificial intelligence that teaches computer programs how to take in, interpret, and produce language from large data sets. For example, grammar checkers use NLP to come up with grammar suggestions that help people write grammatically correct phrases. But as <a href=\"https://ai.google/principles\">Google\u2019s AI Principles</a> note, it\u2019s sometimes necessary to have human intervention to identify risks of unfair bias.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Sunipa Dev is a research scientist at Google who focuses on Responsible AI. Some of her work focuses specifically on ways to evaluate unfair bias in NLP outcomes, reducing harms for people with queer and non-binary identities. Sunipa\u2019s <a href=\"https://aclanthology.org/2021.emnlp-main.150.pdf\">work</a> was recently featured at a <a href=\"https://facctconference.org/2022/acceptedcraft.html#colab\">workshop</a> at the ACM Fairness, Accountability, and Transparency (FAcct) <a href=\"https://facctconference.org/2022/acceptedcraft.html#colab\">conference</a> in Seoul, Korea.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In our interview, she emphasizes that her work is achievable only through forging collaborative partnerships between researchers, engineers, and AI practitioners with everyday users and communities.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>What inspired you to take on this career path?</b></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>While working on my PhD at the University of Utah, I explored research questions such as, \u201cHow do we evaluate NLP tech if they contain biases?\u201d As language models evolved, our questions about potential harms did, too. During my postdoc work at UCLA, we ran a study to evaluate challenges in various language models by surveying respondents who identified as non-binary and had some experience with AI. With a focus on gender bias, our respondents helped us understand that experiences with language technologies cannot be understood in isolation. Rather, we must consider how these technologies intersect with systemic discrimination, erasure, and marginalization. For example, the harm of misgendering by a language technology can be compounded for trans, non-binary, and gender-diverse individuals who are already fighting against society to defend their identities. And when it\u2019s in your personal space, like on your devices while emailing or texting, these small jabs can build up to larger psychological damage.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>What is your current role at Google?</b></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>I am currently a Research Scientist at the Responsible AI - Human Centered Technology team. In my current role, I am working to build a better understanding of how to avoid unfair bias in AI language models across different cultures and geographies, aligned with Google\u2019s AI Principles.</p><p>This is a challenge because language changes, and so do cultures and regional laws as we move from one place to another. This can all impact how people express themselves, what identities they choose and how they experience discrimination on a daily basis. Gender bias can manifest in entirely different ways in different parts of the world. In some of my ongoing work that focuses on a non-Western point of view, we are working with social scientists and NGOs in India while engaging with local communities. We are using the voices of many people who are living in a specific region and asking, \u201cWhat are the biases prevalent in their society?\u201d</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>What is gender bias in NLP?</b></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Written text and training data for language technologies can lack representation or misrepresent different gender identities; this can reflect social biases. As a result, some NLP technologies can reinforce gender stereotypes and slurs, erase people\u2019s gender identities, or have reduced quality of service for marginalized communities. What drives me in my work is my goal to make language technologies more inclusive and usable.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>Why does this matter for AI?</b></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Gender can be such an integral part of someone's identity, and having that wrongly assumed by an AI system can be triggering, unfair, and harmful. We need to work towards systems and societies that do not encode unfair biases and harmful stereotypes in order to break out of the cycle of perpetuating harms of stereotyping, misgendering, and erasure.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>How can people who are not researchers, engineers or AI practitioners engage in this work?</b></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>A very direct way is for people to report potential harms as bugs within products they use. People can also participate in open discussions in workshops, panels and town halls. These are all helpful ways to build inclusive AI.</p><p>I want to emphasize, however, that the onus can\u2019t only be on the user. It\u2019s also on the side of the researcher, engineer and AI practitioner. The goal is to create a continuous feedback loop between humans and machines, with real people stepping in to ensure the creation of more responsible AI. As AI practitioners, we need to work with the people we\u2019re trying to serve and have users collaborate with us to tell us what we need to do better.</p></div></div>",
            "pubdate": "Wed, 29 Jun 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                29
            ],
            "email_sent": true
        },
        "Mahima Pushkarna is making data easier to understand": {
            "url": "https://blog.google/technology/research/mahima-pushkarna-interview/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Five years ago, information designer Mahima Pushkarna joined Google to make data easier to understand. As a senior interaction designer on the <a href=\"http://pair.withgoogle.com/\">People + AI Research</a> (PAIR) team, she designed <a href=\"https://pair-code.github.io/datacardsplaybook/\">Data Cards</a> to help everyone better understand the contexts of the data they are using. The Data Cards Playbook puts <a href=\"https://ai.google/principles/\">Google\u2019s AI Principles</a> into practice by providing opportunities for feedback, relevant explanations and appeal.</p><p>Recently, Mahima\u2019s <a href=\"https://arxiv.org/abs/2204.01075\">paper</a> on Data Cards (co-written with Googlers Andrew Zaldivar and Oddur Kjartansson) was accepted to the ACM Conference on Fairness, Accountability and Transparency (<a href=\"https://facctconference.org/\">ACM FAccT</a>). Let\u2019s catch up with her and find out more about what brought her to Google.</p><p><b>How did your background lead you to the work you\u2019re doing now?</b></p><p>I've always been fascinated by conjuring up solutions to things. The kind of questions that I\u2019ve found meaningful are those that are never truly solved, or never have one correct answer. (The kind of questions that exasperate us!) Those have been the problems I am always drawn towards.</p><p>Early in my career, I realized the power in visualizing data, but spreadsheets were intimidating. I wondered how design could make communicating complexity easier. So I found myself in grad school in Boston studying information design and data visualization. I focused on how people experience data and how our relationships to each other and our contexts are mediated.</p><p>I joined Google Brain as the first visual designer in a full-time capacity, though I had no background in artificial intelligence or machine learning \u2014 this was the deep end of the pool. This opened up the space to explore human-AI interaction, and make AI more accessible to a broader class of developers. At PAIR, my work focuses on making information experiences more meaningful for developers, researchers and others who build AI technologies.</p><p><b>What\u2019s it like to have a unique background as a designer on a technical AI research team?</b></p><p>When you're an engineer and immersed in building technology, it's easy to assume everyone has a similar experience to your own \u2014 especially when you\u2019re surrounded by peers who share your expertise. The actual user experience is very personal and varies drastically across users and contexts. That particular clarity is what designers bring to the table.</p><p>I\u2019ve been able to engage my engineering and research colleagues with simple, people-centered questions right in the very beginning. How are people using an AI tool? What are they learning from it? Who else might be involved in the conversation? Do they have the proficiency we assume they have?</p><p>Pull quote: \u201cIdentifying what we don\u2019t know about data is just as important as articulating what we do know.\u201d</p><p><b>How did you begin designing Data Cards?</b></p><p>This project started when I was working on another visualization toolkit, <a href=\"http://facets.dev/\">Facets</a>, to communicate the skews and imbalances within datasets to help machine learning practitioners make informed decisions. At the time, transparency was a moving target. Andrew, Tulsee Doshi and I started to proactively think about fairness in data, and saw a huge gap in the documentation of human decisions that dot a dataset's lifecycle.</p><p>This \u201cinvisible\u201d information shapes how we use data and the outcomes of models trained on them. For example, a model trained on a dataset that captures age in just two or three buckets will have very different outcomes compared to a dataset with ten buckets. The goal of Data Cards is to make both visible and invisible information about datasets available and simple to understand, so people from a variety of backgrounds can knowledgeably make decisions.</p><p>As we cover in our <a href=\"https://arxiv.org/abs/2204.01075\">FAccT paper</a>, Andrew and Oddur and I arrived at two insights. The first is that identifying what we don\u2019t know about data is just as important as articulating what we do know. In capturing these nuances, it is possible to narrow those knowledge gaps before even collecting data. The second thing that surprised us was the sheer number of people involved in a dataset\u2019s life cycle, and how fragile knowledge is. Context is easily lost in translation both between and within teams, across documents, emails, people and time.</p><p>Data Cards stand on the shoulders of giants, like <a href=\"https://arxiv.org/abs/1803.09010\">Data Sheets</a> (Gebru, et al.) and <a href=\"https://arxiv.org/abs/1810.03993\">Model Cards</a> (Mitchell et al.). We've been immensely lucky to have had the support of many original authors on these seminal papers that have paved our path to FAccT.</p><p><b>How do you hope the paper is used across the tech industry?</b></p><p>Imagine a world in which finding verifiable information about the motivations of a dataset\u2019s creators or performance of a model is as easy as learning about the ethical beliefs of a celebrity or the rating of a movie. Our vision for Data Cards is that they become a cultural mainstay \u2014 invisible, but their absence would be missed by ML practitioners.</p><p>In this paper, we introduce frameworks that other teams can use in their work. Alongside that, we\u2019ve open-sourced the <a href=\"https://pair-code.github.io/datacardsplaybook/\">Data Cards Playbook</a>, so we're trying to lower the barrier to access in every way possible.</p></div></div>",
            "pubdate": "Thu, 30 Jun 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                30
            ],
            "email_sent": true
        },
        "An update on our work in responsible innovation": {
            "url": "https://blog.google/technology/ai/an-update-on-our-work-in-responsible-innovation/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Over the last year, we\u2019ve seen artificial intelligence (AI) systems advance our work in areas like <a href=\"https://blog.google/products/pixel/image-equity-real-tone-pixel-6-photos/\">inclusive product development</a> and support for <a href=\"https://blog.google/products/maps/how-ai-and-imagery-build-self-updating-map/\">small businesses</a> and <a href=\"https://grow.google/certificates/interview-warmup/\">job seekers</a>. We\u2019ve also seen its potential to be helpful in addressing major global needs \u2014 like <a href=\"https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/\">forecasting</a> and planning <a href=\"https://blog.google/around-the-globe/google-africa/using-ai-to-map-africas-buildings/\">humanitarian responses</a> to natural disasters, <a href=\"https://blog.google/intl/en-au/company-news/outreach-initiatives/protecting-our-reef-with-csiro/\">addressing global environmental</a> challenges, and delivering groundbreaking <a href=\"https://www.nature.com/articles/d41586-021-02025-4\">scientific research</a>.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>AI is exciting \u2014 both from a technical perspective and when considering its underlying social benefits. And yet, to fully realize AI\u2019s potential, it must be developed responsibly, thoughtfully and in a way that gives deep consideration to core ethical questions. After all, the promise of great reward inherently involves risk \u2014 and we\u2019re committed to ethically developing AI in a way that is socially beneficial.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our <a href=\"http://ai.google/principles\">AI Principles</a> guide how we integrate AI research into Google\u2019s products and services and engage with external partners. Internally, we implement the Principles, every day, through education programs, AI ethics reviews and technical tools. There are more than 200 Googlers across the company whose full-time roles are to operationalize responsible practices for developing AI.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re committed to sharing our lessons learned so others across the industry can learn, too (see our posts from <a href=\"https://www.blog.google/technology/ai/google-ai-principles-updates-six-months/\">2018,</a> <a href=\"https://www.blog.google/technology/ai/responsible-ai-principles/\">2019</a>, <a href=\"https://blog.google/technology/ai/update-work-ai-responsible-innovation/\">2020</a> and <a href=\"https://blog.google/technology/ai/update-our-progress-responsible-ai-innovation/#:~:text=Over%20the%20past%20year%2C%20responsibly,and%20protected%20wildlife%20after%20bushfires.\">2021</a>, and our in-depth annual <a href=\"https://ai.google/responsibilities/review-process/#:~:text=the%20current%20process.-,annual%20updates,-AI%20Principles%202021\">AI Principles Progress Updates</a>).</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Internal education</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>It\u2019s important to craft principles, but putting them into practice requires both training and constant dialogue.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Launched in late 2019, to date more than 32,000 employees across Google have engaged in AI Principles training. Given our growing understanding of effective hybrid and remote learning, we continue to expand and modify the courses. For example, this year we adapted our popular four-part Tech Ethics self-study course to a one-part deep dive based on Googler feedback. Similarly, we launched the <a href=\"https://blog.google/technology/ai/crossword-puzzle-big-purpose/\">Responsible Innovation Challenge</a> \u2014 taken by more than 13,000 employees \u2014 as a series of engaging online puzzles, quizzes and games to raise awareness of the AI Principles and measure employees' retention of ethical concepts, such as avoiding unfair bias.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We also piloted a new Moral Imagination workshop, a two-day, live-video immersive set of activities for product teams to walk through the ethical implications of potential AI products. To date, 248 Googlers across 23 Google product and research teams have taken the workshop, resulting in deeper, ongoing AI ethics consultations on product development.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As we develop internal training, we\u2019re committed to incorporating the input of both Googlers and outside experts. This year, when we launched a live workshop to educate our internal user experience and product teams on the concept of <a href=\"https://blog.google/inside-google/googlers/ask-techspert-how-do-machine-learning-models-explain-themselves/\">AI explainability</a>, we first piloted the workshop with outside experts at the international <a href=\"https://summit.ttclabs.net/\">Trust, Transparency and Control Labs</a> summit in May.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We believe this approach complements programs like our internal AI Principles Ethics Fellows program, a six-month fellowship that this year involved Googlers from 17 different global offices. We also just launched a version of the fellowship program tailored for senior leaders.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Putting the Principles into practice</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our approach to responsible AI innovation starts early, before teams plan a new AI application. When a team starts to build a machine learning (ML) model, dataset or product feature, they can attend office hours with experts to ask questions and engage in analyses using responsible AI <a href=\"https://www.tensorflow.org/responsible_ai?hl=en\">tools</a> that Google develops, or seek adversarial proactive fairness <a href=\"https://www.blog.google/inside-google/googlers/meet-3-women-who-test-google-products-fairness/\">(ProFair) testing.</a> Pre-launch, a team then can request an AI Principles review.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>AI Principles reviewers are in place to implement a structured assessment to identify, measure and analyze potential risk of harm. The risk rating focuses on the extent to which people and society may be impacted if solutions did not exist or were to fail. Reviewers also consider a growing body of lessons from thousands of previous AI Principles reviews conducted since 2019.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>When reviewers find medium- to high-risk issues, such as product exclusion or a potential privacy or security concern, they work with the teams to address these issues. Reviews either result in an approval, approval with conditions or recommendations, or non-approval. New AI applications that might affect multiple product areas are escalated to the Advanced Technology Review Council \u2014 a group of senior research, product and business leaders who make the final decision.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>To supplement the expertise of our internal AI Principles group members, we often incorporate trusted external advisors. For example, a team was incorporating AI to help build a <a href=\"https://dynamicworld.app/\">near real-time dataset</a> to enable reliable measurement of global land cover for environmental and social benefit. They submitted for <a href=\"https://ai.google/static/documents/case-study-dynamic-world.pdf\">AI Principles review</a> and then collaborated with the review team to design several safeguards. The review team also worked with third-party experts at the <a href=\"https://www.wri.org/about\">World Resources Institute</a> and <a href=\"https://www.bsr.org/en/about\">BSR</a>. Following the example of the European Commission\u2019s <a href=\"https://www.copernicus.eu/en\">Copernicus mission\u2019s</a> open <a href=\"https://sentinel.esa.int/documents/247904/690755/sentinel_data_legal_notice\">data and services</a> terms, the product team applied open data principles, making the ML model\u2019s <a href=\"https://doi.pangaea.de/10.1594/PANGAEA.933475\">training</a> and <a href=\"https://doi.org/10.5281/zenodo.4766508\">test data</a> used to create the dataset, as well as the dataset itself, freely available under CC-BY-4.0, and the <a href=\"https://github.com/google/dynamicworld\">model available on Github</a> under an Apache 2.0 license. We recently released a <a href=\"https://ai.google/static/documents/case-study-dynamic-world.pdf\">Codelab</a> for developers to walk through the ethics review process and apply learnings to their own projects.</p></div></div><div class=\"block-video\"><div class=\"h-c-page h-c-page--mobile-full-bleed\"><div class=\"h-c-grid\"><div class=\"h-c-grid__col h-c-grid__col-l--10 h-c-grid__col-l--offset-1\"><div class=\"article-module uni-article-video uni-article-video--body\"><div class=\"uni-article-video__embed-container hidden\"><div id=\"uni-article-yt-player-S75NcDqbPbI\"></div></div><figure><a class=\"h-c-video h-c-video--marquee uni-article-video__custom-wrapper\" tabindex=\"0\"><div class=\"uni-article-video__aspect-image\"><img alt=\"A video explaining Google's AI Principles Review process\" src=\"https://img.youtube.com/vi/S75NcDqbPbI/maxresdefault.jpg\" /><div class=\"uni-article-video__dimmer\"></div><svg class=\"uni-article-video__play-button--active\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button_no_hole\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><svg class=\"uni-article-video__play-button\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_play_button\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><div class=\"uni-article-video__duration loading\"><svg class=\"uni-article-video__duration-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#yt_video_duration\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg><span class=\"uni-article-video__duration-time\">10:25</span></div></div></a><p>Google\u2019s AI Principles Review Process: How we assess new AI research and applications for alignment with our Principles</p></figure></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Projects such as research methods for evaluating misinformation and datasets that need more diverse representation tend to receive conditions to proceed toward a launch. A recurring condition given to teams is to engage in ProFair testing with people from a diversity of backgrounds, often in partnership with our central Product Inclusion and Equity team. This year, the number of ProFair consultations increased annually by 100%. A recurring approach is to create and release detailed documentation in the form of<a href=\"https://www.youtube.com/watch?v=IYK6fkODXNU\">data cards</a> and <a href=\"https://modelcards.withgoogle.com/about\">model cards</a> for transparency and accountability. The number of AI Principles reviews with model or data card mitigations increased 68% in the last year.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As we\u2019ve stated, we\u2019ve embedded customized AI governance and review committees within certain product areas (like Cloud and Health). As a result, both the Health Ethics Committee and Cloud make decisions with specialized expertise, such as establishing policies for potentially winding down the <a href=\"https://www.google.com/covid19/mobility/\">Covid-19 Community Mobility Reports</a> and the <a href=\"https://ai.googleblog.com/2021/10/an-ml-based-framework-for-covid-19.html\">Covid-19 Forecaster</a>, respectively, if situations arise that might cause the data quality to degrade. This year, we extended this specialized approach and created a dedicated consumer hardware AI Principles review process.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>It\u2019s important to note that product teams across Google engage in everyday responsible AI practices even if not in formal reviews. <a href=\"https://blog.youtube/inside-youtube/inside-responsibility-whats-next-on-our-misinfo-efforts/\">YouTube</a> is leveraging a more targeted mix of classifiers, keywords in additional languages, and information from regional analysts. This work is a result of collaboration with our researchers who focus on new tools for AI fairness. The Photos team participated in an <a href=\"https://blog.google/technology/ai/update-our-progress-responsible-ai-innovation/#:~:text=equitable%20ai%20research%20roundtables%20(earr)%2C\">Equitable AI Research Roundtable (EARR)</a> with a group of external advisors on potential fairness considerations. And the Gboard team deployed a new, <a href=\"https://ai.googleblog.com/2021/12/a-scalable-approach-for-partially-local.html\">privacy-by-design</a> approach to federated machine learning. These examples did not stem from AI Principles reviews, but reflect the adoption of the AI Principles across Google.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Tools and research</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In early 2022, to offer easier access to our publications on responsible AI, we curated an <a href=\"https://research.google/pubs/?collection=responsible-ai\">external collection</a> of more than 200 research papers focused on the topic. We continue to launch, refine and consolidate technical resources, including proactive tools like:</p><ul><li>The <a href=\"https://skintone.google/\">Monk Skin Tone Scale</a>, developed by Harvard University Sociology Professor Dr. Ellis Monk. The scale offers a spectrum of skin tones from all around the world for use in evaluating and addressing fairness considerations in AI.</li><li>The <a href=\"https://knowyourdata.withgoogle.com/\">Know Your Data</a> tool (KYD), which helps developers with tasks such as quickly identifying issues in fairness, and which has integrated the Monk Scale to help developers examine skin tone data for unfair bias.</li><li>The <a href=\"https://pair-code.github.io/lit/\">Language Interpretability Tool</a>, or LIT, to help developers probe an ML model, now with a <a href=\"https://arxiv.org/abs/1711.11279\">new method</a> to better understand, test and debug its behaviors.</li><li><a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_overview?hl=en\">Counterfactual Logit Pairing</a>, which helps ensure that a model\u2019s prediction doesn\u2019t change when sensitive attributes or identity terms referenced in an example are removed or replaced, now added to the <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation\">TensorFlow Model Remediation Library</a> (see the <a href=\"https://arxiv.org/abs/1809.10610\">research paper</a> for more).</li><li>And to help teams measure their progress against the AI Principles, we\u2019re piloting an internal tool to help teams assess how ML models were developed in accordance with emerging smart practices, previous reviews, and our growing body of ethics, fairness, and human-rights work.</li></ul></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Many responsible AI tools developed by researchers are actively in use by product teams at Google. For example, Photos, Pixel and Image Search are leveraging the Monk Skin Tone Scale.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>External engagement</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Ensuring the responsible development and deployment of AI is an ongoing process. We believe it should be a collaborative one, too, so we remain deeply engaged with governments across Europe, the Middle East and Africa, Latin America, Asia Pacific, and the U.S. to advocate for AI regulation that supports innovation around the world for businesses of all sizes. We share our approach to responsible AI and <a href=\"https://ai.google/static/documents/google-response-to-nist-ai-risk-management-framework-rfi.pdf\">recommendations</a>, <a href=\"https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/12527-Artificial-intelligence-ethical-and-legal-requirements/F2662492_en\">comments</a> and <a href=\"https://ai.google/static/documents/google-ostp-biometrics-rfi.pdf\">responses</a> to open requests for information. We also initiated and are leading an effort with the <a href=\"https://www.iso.org/home.html\">International Standards Organization</a> (ISO/IEC PWI TS 17866) to share best practice guidance for the development of AI.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As these efforts look toward the future, Responsible AI needs to be supported across industries today. So for current Google Cloud Partners and customers seeking best practices to help with the responsible implementation and AI governance in their organization, we added responsible AI prerequisites to the Google Cloud Partner Advantage <a href=\"https://cloud.google.com/find-a-partner/?specializations=Machine%20Learning%20-%20Services\">ML Specialization</a>, including a newly-released training, \u201c<a href=\"https://www.cloudskillsboost.google/course_templates/388\">Applying AI Principles with Google Cloud</a>.\u201d</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>To help nurture the next generation of responsible AI practitioners, we launched a free <a href=\"https://blog.google/technology/ai/discover-ai-in-daily-life/\">introduction</a> to AI and machine learning for K-12 students. And we continue to develop an external Responsible Innovation Fellowship program in the U.S. for students at Historically Black Colleges and Universities (HBCUs).</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our approach to responsible innovation also means keeping an eye on emerging markets where AI is being developed. We launched a new <a href=\"https://blog.google/technology/ai/investing-in-eastern-europes-ai-future/\">AI research center in Bulgaria</a> and expanded <a href=\"https://blog.google/around-the-globe/google-africa/supporting-growth-in-africa/\">support for African entrepreneurs</a> whose businesses use AI through our <a href=\"https://startup.google.com/accelerator/africa/\">Google for Startups Accelerator: Africa</a>.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>The examples we\u2019re sharing today are a sampling of our ongoing commitment to responsible innovation. They also reflect our ability to change and keep setting a high bar for trustworthy AI standards for our company. We remain dedicated to sharing helpful information on Google\u2019s journey, as recommended practices for responsible AI continue to emerge and evolve.</p></div></div>",
            "pubdate": "Wed, 06 Jul 2022 18:00:00 +0000",
            "pubdate_parsed": [
                2022,
                7,
                6
            ],
            "email_sent": true
        },
        "Making robots more helpful with language": {
            "url": "https://blog.google/technology/ai/making-robots-more-helpful-with-language/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>Even the simplest human tasks are unbelievably complex. The way we perceive and interact with the world requires a lifetime of accumulated experience and context. For example, if a person tells you, \u201cI am running out of time,\u201d you don\u2019t immediately worry they are jogging on a street where the space-time continuum ceases to exist. You understand that they\u2019re probably coming up against a deadline. And if they hurriedly walk toward a closed door, you don\u2019t brace for a collision, because you trust this person can open the door, whether by turning a knob or pulling a handle.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>A robot doesn\u2019t innately have that understanding. And that\u2019s the inherent challenge of programming helpful robots that can interact with humans. We know it as \u201cMoravec's paradox\u201d \u2014 the idea that in robotics, it\u2019s the easiest things that are the most difficult to program a robot to do. This is because we\u2019ve had all of human evolution to master our basic motor skills, but relatively speaking, humans have only just learned algebra.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In other words, there\u2019s a genius to human beings \u2014 from understanding idioms to manipulating our physical environments \u2014 where it seems like we just \u201cget it.\u201d The same can\u2019t be said for robots.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today, robots by and large exist in industrial environments, and are painstakingly coded for narrow tasks. This makes it impossible for them to adapt to the unpredictability of the real world. That\u2019s why <a href=\"https://research.google/\">Google Research</a> and <a href=\"https://everydayrobots.com/\">Everyday Robots</a> are working together to combine the best of language models with robot learning.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Called <a href=\"https://sites.research.google/palm-saycan\">PaLM-SayCan</a>, this joint research uses <a href=\"https://arxiv.org/pdf/2204.02311.pdf\">PaLM</a> \u2014 or Pathways Language Model \u2014 in a robot learning model running on an Everyday Robots helper robot. This effort is the first implementation that uses a large-scale language model to plan for a real robot. It not only makes it possible for people to communicate with helper robots via text or speech, but also improves the robot\u2019s overall performance and ability to execute more complex and abstract tasks by tapping into the world knowledge encoded in the language model.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><video alt=\"I just worked out bring me a snack\" class=\"article-image__media\" loop=\"\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/I_just_worked_out_bring_me_a_snack_QT.mp4\" tabindex=\"0\" title=\"A helper robot responding to the task \u2018I\u2019m tired. Bring me a snack that\u2019ll give me some energy, please.\" type=\"video/mp4\">Video format not supported</video></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Using language to improve robots</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>PaLM-SayCan enables the robot to understand the way we communicate, facilitating more natural interaction. Language is a reflection of the human mind\u2019s ability to assemble tasks, put them in context and even reason through problems. Language models also contain enormous amounts of information about the world, and it turns out that can be pretty helpful to the robot. PaLM can help the robotic system process more complex, open-ended prompts and respond to them in ways that are reasonable and sensible.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>PaLM-SayCan shows that a robot\u2019s performance can be improved simply by enhancing the underlying language model. When the system was integrated with PaLM, compared to a less powerful baseline model, we saw a 14% improvement in the planning success rate, or the ability to map a viable approach to a task. We also saw a 13% improvement on the execution success rate, or ability to successfully carry out a task. This is half the number of planning mistakes made by the baseline method. The biggest improvement, at 26%, is in planning long horizon tasks, or those in which eight or more steps are involved. Here\u2019s an example: \u201cI left out a soda, an apple and water. Can you throw them away and then bring me a sponge to wipe the table?\u201d Pretty demanding, if you ask me.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Making sense of the world through language</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>With PaLM, we\u2019re seeing new capabilities emerge in the language domain such as reasoning via <a href=\"https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\">chain of thought prompting</a>. This allows us to see and improve how the model interprets the task. For example, if you show the model a handful of examples with the thought process behind how to respond to a query, it learns to reason through those prompts. This is similar to how we learn by showing our work on our algebra homework.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"PaLM-SayCan uses chain of thought prompting, which interprets the instruction in order to score the likelihood of completing the task\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Chain_of_thought_prompting.max-1000x1000.png\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>So if you ask PaLM-SayCan, \u201cBring me a snack and something to wash it down with,\u201d it uses chain of thought prompting to recognize that a bag of chips may be a good snack, and that \u201cwash it down\u201d means bring a drink. Then PaLM-SayCan can respond with a series of steps to accomplish this. While we\u2019re early in our research, this is promising for a future where robots can handle complex requests.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Grounding language through experience</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Complexity exists in both language and the environments around us. That\u2019s why grounding artificial intelligence in the real world is a critical part of what we do in Google Research. A language model may suggest something that appears reasonable and helpful, but may not be safe or realistic in a given setting. Robots, on the other hand, have been trained to know what is possible given the environment. By fusing language and robotic knowledge, we\u2019re able to improve the overall performance of a robotic system.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Here\u2019s how this works in PaLM-SayCan: PaLM suggests possible approaches to the task based on language understanding, and the robot models do the same based on the feasible skill set. The combined system then cross-references the two to help identify more helpful and achievable approaches for the robot.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"By combining language and robotic affordances, PaLM-SayCan breaks down the requested task to perform it successfully\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/palm_gif_8_12_22.gif\" tabindex=\"0\" /></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>For example, if you ask the language model, \u201cI spilled my drink, can you help?,\u201d it may suggest you try using a vacuum. This seems like a perfectly reasonable way to clean up a mess, but generally, it\u2019s probably not a good idea to use a vacuum on a liquid spill. And if the robot can\u2019t pick up a vacuum or operate it, it\u2019s not a particularly helpful way to approach the task. Together, the two may instead be able to realize \u201cbring a sponge\u201d is both possible and more helpful.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Experimenting responsibly</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We take a responsible approach to this research and follow Google\u2019s <a href=\"https://ai.google/principles/\">AI\u2019s Principles</a> in the development of our robots. Safety is our number-one priority and especially important for a learning robot: It may act clumsily while exploring, but it should always be safe. We follow all the tried and true principles of robot safety, including risk assessments, physical controls, safety protocols and emergency stops. We also always implement multiple levels of safety such as force limitations and algorithmic protections to mitigate risky scenarios. PaLM-SayCan is constrained to commands that are safe for a robot to perform and was also developed to be highly interpretable, so we can clearly examine and learn from every decision the system makes.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Making sense of our worlds</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Whether it\u2019s moving about busy offices \u2014 or understanding common sayings \u2014 we still have many mechanical and intelligence challenges to solve in robotics. So, for now, these robots are just getting better at grabbing snacks for Googlers in our micro-kitchens.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>But as we continue to uncover ways for robots to interact with our ever-changing world, we\u2019ve found that language and robotics show enormous potential for the helpful, human-centered robots of tomorrow.</p></div></div>",
            "pubdate": "Tue, 16 Aug 2022 14:00:00 +0000",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "Helping people understand AI": {
            "url": "https://blog.google/technology/ai/helping-people-understand-ai/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>If you\u2019re like me, you may have noticed that AI has become a part of daily life. I wake up each morning and ask my smart assistant about the weather. I recently applied for a new credit card and the credit limit was likely determined by a machine learning model. And while typing the previous sentence, I got a word choice suggestion that \u201cprobably\u201d might flow better than \u201clikely,\u201d a suggestion powered by AI.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As a member of <a href=\"https://ai.google/responsibilities/review-process/\">Google\u2019s Responsible Innovation team</a>, I think a lot about how AI works and how to develop it responsibly. Recently, I spoke with Patrick Gage Kelley, Head of Research Strategy on Google\u2019s Trust &amp; Safety team, to learn more about developing products that help people recognize and understand AI in their daily lives.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>How do you help people navigate a world with so much AI?</b></p><p>My goal is to ensure that people, at a basic level, know how AI works and how it impacts their lives. AI systems can be really complicated, but the goal of explaining AI isn\u2019t to get everyone to become programmers and understand all of the technical details \u2014 it\u2019s to make sure people understand the parts that matter to them.</p><p>When AI makes a decision that affects people (whether it\u2019s recommending a video or qualifying for a loan), we want to explain how that decision was made. And we don\u2019t want to just provide a complicated technical explanation, but rather, information that is meaningful, helpful, and equips people to act if needed.</p><p>We also want to find the best times to explain AI. Our goal is to help people develop AI literacy early, including in primary and secondary education. And when people use products that rely on AI (everything from online services to medical devices), we want to include a lot of chances for people to learn about the role AI plays, as well as its benefits and limitations. For example, if people are told early on what kinds of mistakes AI-powered products are likely to make, then they are better prepared to understand and remedy situations that might arise.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>Do I need to be a mathematician or programmer to have a meaningful understanding of AI?</b></p><p>No! A good metaphor here is financial literacy. While we may not need to know every detail of what goes into interest rate hikes or the intricacies of financial markets, it\u2019s important to know how they impact us \u2014 from paying off credit cards, to buying a home, or paying for student loans. In the same way, AI explainability isn\u2019t about understanding every technical aspect of a machine learning algorithm \u2013 it\u2019s about knowing how to interact with it and how it impacts our daily lives.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>How should AI practitioners \u2014 developers, designers, researchers, students, and others \u2014 think about AI explainability?</b></p><p>Lots of practitioners are doing important work on explainability. Some focus on interpretability, making it easier to identify specific factors that influence a decision. Others focus on providing \u201cin-the-moment explanations\u201d right when AI makes a decision. These can be helpful, especially when carefully designed. However, AI systems are often so complex that we can\u2019t rely on in-the-moment explanations entirely. It\u2019s just too much information to pack into a single moment. Instead, AI education and literacy should be incorporated into the entire user journey and built continuously throughout a person\u2019s life.</p><p>More generally, AI practitioners should think about AI explainability as fundamental to the design and development of the entire product experience. At Google, we use our <a href=\"https://ai.google/principles\">AI Principles</a> to guide responsible technology development. In accordance with AI Principle #4: \u201cBe accountable to people,\u201d we encourage AI practitioners to think about all the moments and ways they can help people understand how AI operates and makes decisions.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>How are you and your collaborators working to improve explanations of AI?</b></p><p>We develop resources that help AI practitioners learn creative ways to incorporate AI explainability in product design. For example, in the <a href=\"https://pair.withgoogle.com/guidebook/\">PAIR Guidebook</a> we launched a series of <a href=\"https://arxiv.org/abs/2009.00246\">ethical case studies</a> to help AI practitioners think through tricky issues and hone their skills for explaining AI. We also do fundamental research like <a href=\"https://arxiv.org/abs/2012.00874\">this paper</a> to learn more about how people perceive AI as a decision-maker, and what values they would like AI-powered products to embody.</p><p>We\u2019ve learned that many AI practitioners want concrete examples of good explanations of AI that they can build on, so we\u2019re currently developing a story-driven visual design toolkit for explanations of a fictional AI app. The toolkit will be publicly available, so teams in startups and tech companies everywhere can prioritize explainability in their work.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><img alt=\"An illustration of a sailboat navigating the coast of Maine\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Inline_Image_--_Helping_People_Understand_.max-1000x1000.jpg\" tabindex=\"0\" /></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The visual design toolkit provides story-driven examples of good explanations of AI.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p><b>I want to learn more about AI explainability. Where should I start?</b></p><p>This February, we released an <a href=\"https://applieddigitalskills.withgoogle.com/en/teach?gclid=CjwKCAjwi8iXBhBeEiwAKbUofZfj4ve-ub1b5Y5ZW2qe1NbNX4OOvdXWsxZhumVAoAOe67Bld2LGBxoCPNAQAvD_BwE\">Applied Digital Skills</a> lesson, \u201c<a href=\"https://applieddigitalskills.withgoogle.com/c/middle-and-high-school/en/discover-ai-in-daily-life/overview.html\">Discover AI in Daily Life</a>.\u201d It\u2019s a great place to start for anyone who wants to learn more about how we interact with AI everyday.</p><p>We also hope to speak about AI explainability at the upcoming <a href=\"https://www.sxsw.com/conference/\">South by Southwest Conference</a>. Our proposed session would dive deeper into these topics, including our visual design toolkit for product designers. If you\u2019re interested in learning more about AI explainability and our work, you can vote for our proposal through the SXSW PanelPicker\u00ae <a href=\"https://panelpicker.sxsw.com/vote/129081\">here</a>.</p></div></div>",
            "pubdate": "Thu, 18 Aug 2022 15:00:00 +0000",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Join us in the AI Test Kitchen": {
            "url": "https://blog.google/technology/ai/join-us-in-the-ai-test-kitchen/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>As AI technologies continue to advance, they have the potential to unlock new experiences that support more natural human-computer interactions. We see a future where you can find the information you\u2019re looking for in the same conversational way you speak to friends and family. While there\u2019s still lots of work to be done before this type of human-computer interaction is possible, recent research breakthroughs in generative language models \u2014 inspired by the natural conversations of people \u2014 are accelerating our progress. One of our most promising models is called LaMDA (Language Model for Dialogue Applications), and as we move ahead with development, we feel a great responsibility to get this right.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>That\u2019s why we introduced an app called <a href=\"https://youtu.be/nP-nMZpLM1A?t=2380\">AI Test Kitchen</a> at Google I/O earlier this year. It provides a new way for people to learn about, experience, and give feedback on emerging AI technology, like LaMDA. Starting today, you can <a href=\"https://aitestkitchen.withgoogle.com/\">register your interest</a> for the AI Test Kitchen as it begins to gradually roll out to small groups of users in the US, launching on Android today and iOS in the coming weeks.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col-l--6 h-c-grid__col--8 h-c-grid__col-l--offset-3 h-c-grid__col--offset-2\"><a class=\"article-image--link\" href=\"https://aitestkitchen.withgoogle.com/\" rel=\"external\" target=\"_blank\"><img alt=\"Linked image of AI Test Kitchen registration page\" class=\"article-image--large\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_post_image.max-1000x1000.png\" tabindex=\"0\" /></a></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>Our goal is to learn, improve and innovate responsibly on AI together.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Similar to a real test kitchen, AI Test Kitchen will serve a rotating set of experimental demos. These aren\u2019t finished products, but they\u2019re designed to give you a taste of what\u2019s becoming possible with AI in a responsible way. Our first set of demos explore the capabilities of our latest version of <a href=\"https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\">LaMDA</a>, which has undergone key safety improvements. The first demo, \u201cImagine It,\u201d lets you name a place and offers paths to explore your imagination. With the \u201cList It\u201d demo, you can share a goal or topic, and LaMDA will break it down into a list of helpful subtasks. And in the \u201cTalk About It (Dogs Edition)\u201d demo, you can have a fun, open-ended conversation about dogs <i>and only dogs</i>, which explores LaMDA\u2019s ability to stay on topic even if you try to veer off-topic.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Evaluating LaMDA\u2019s potential and its risks</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As you try each demo, you\u2019ll see LaMDA\u2019s ability to generate creative responses on the fly. This is one of the model\u2019s strengths, but it can also pose challenges since some responses can be inaccurate or inappropriate. We\u2019ve been testing LaMDA internally over the last year, which has produced significant quality improvements. More recently, we\u2019ve run dedicated rounds of adversarial testing to find additional flaws in the model. We enlisted expert red teaming members \u2014 product experts who intentionally stress test a system with an adversarial mindset \u2014 who have uncovered additional harmful, yet subtle, outputs. For example, the model can misunderstand the intent behind identity terms and sometimes fails to produce a response when they\u2019re used because it has difficulty differentiating between benign and adversarial prompts. It can also produce harmful or toxic responses based on biases in its training data, generating responses that stereotype and misrepresent people based on their gender or cultural background. These areas and more continue to be under active research.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In response to these challenges, we\u2019ve added multiple layers of protection to the AI Test Kitchen. This work has minimized the risk, but not eliminated it. We\u2019ve designed our systems to automatically detect and filter out words or phrases that violate our policies, which prohibit users from knowingly generating content that is sexually explicit; hateful or offensive; violent, dangerous, or illegal; or divulges personal information. In addition to these safety filters, <a href=\"https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\">we made improvements to LaMDA</a> around quality, safety, and groundedness \u2014 each of which are carefully measured. We have also developed techniques to keep conversations on topic, acting as guardrails for a technology that can generate endless, free-flowing dialogue. As you\u2019re using each demo, we hope you see LaMDA\u2019s potential, but also keep these challenges in mind.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>Responsible progress, together</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In accordance with our <a href=\"https://ai.google/principles/\">AI Principles</a>, we believe responsible progress doesn\u2019t happen in isolation. We\u2019re at a point where external feedback is the next, most helpful step to improve LaMDA. When you rate each LaMDA reply as nice, offensive, off topic, or untrue, we\u2019ll use this data \u2014 which is not linked to your Google account\u00a0\u2014 to improve and develop our future products. We intend for AI Test Kitchen to be safe, fun, and educational, and we look forward to innovating in a responsible and transparent way together.</p></div></div>",
            "pubdate": "Thu, 25 Aug 2022 16:00:00 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "How mapping the worlds buildings makes a difference": {
            "url": "https://blog.google/around-the-globe/google-africa/how-mapping-the-worlds-buildings-makes-a-difference/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>In Lamwo district, in northern Uganda, providing access to electricity is a challenge. In a country where only about 24% of the population has a power supply to their home from the national grid, the rate in Lamwo is even lower. This is partly due to lack of information: The government doesn\u2019t have precise data about where settlements are located, what types of buildings there are, and what the buildings\u2019 electricity needs might be. And canvassing the area isn\u2019t practical, because the roads require four-wheel-drive vehicles and are impassable in the rain.</p><p>Ernest Mwebaze leads <a href=\"https://sunbird.ai/\">Sunbird AI</a>, a Ugandan nonprofit that uses data technology for social good. They\u2019re assessing areas in Lamwo district to support planning at the Ministry of Energy in Uganda. \u201cThere are large areas to plan for,\u201d explains Ernest. \u201cEven when you\u2019re there on the ground, it\u2019s difficult to get an overall sense of where all the buildings are and what is the size of each settlement. Currently people have to walk long distances just to charge their phones.\u201d</p><p>To help with their analysis, Ernest\u2019s team have been using Google\u2019s <a href=\"https://sites.research.google/open-buildings/\">Open Buildings</a>. An open-access dataset project <a href=\"https://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html\">based on satellite imagery</a> pinpointing the locations and geometry of buildings across Africa, Open Buildings allows the team to study the electrification needs, and potential solutions, at a level of detail that was previously impossible.</p></div></div><div class=\"block-paragraph_with_image\"><div class=\"article-module h-c-page\"><div class=\"h-c-grid uni-paragraph-wrap\"><div class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"></div></div></div></div><div class=\"block-image_carousel\"><div class=\"h-c-page article-module\"><div class=\"article-module glue-pagination h-c-carousel h-c-carousel--simple h-c-carousel--dark ng-cloak\"><div class=\"h-c-carousel__wrap\"><ul class=\"glue-carousel ng-cloak\"><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">A satellite view of Accra, Ghana, with building footprints from Open Buildings v1 and v2.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Improved detail in urban areas</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">A satellite view of Tiris Zemmour, Mauritania, with building footprints from Open Buildings v1 and v2.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Model improvements have helped to reduce false detections \u2014 for example, in this desert area, rectangular rocks were being misidentified as buildings.</p></div></figcaption></figure></li><li class=\"h-c-carousel__item article-carousel__slide\"><figure class=\"h-c-grid\"><div class=\"article-carousel__slide-img h-c-grid__col h-c-grid__col--10 h-c-grid__col--offset-1\"><span class=\"h-u-visually-hidden\">A satellite view of Kitui, Kenya, with building footprints from Open Buildings v1 and v2.</span></div><figcaption class=\"article-carousel__caption h-c-grid__col h-c-grid__col--10 h-c-grid__col-l--8 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2\"><div class=\"rich-text\"><p>Using more recent satellite imagery helps us to extend the coverage in rural areas.</p></div></figcaption></figure></li></ul><div class=\"h-c-carousel__paginate glue-pagination-previous uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-left\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div><div class=\"h-c-carousel__paginate glue-pagination-next uni-click-tracker\"><div class=\"h-c-carousel__paginate-wrap\"><svg class=\"h-c-icon h-c-icon--keyboard-arrow-right\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-keyboard-arrow-right\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></div></div></div><div class=\"h-c-carousel__navigation\"><div class=\"glue-pagination-page-list uni-click-tracker\"></div></div></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our <a href=\"https://africa.googleblog.com/2022/05/google-research-enhances-its-ai-growth.html\">research center in Ghana</a> led the development of the Open Buildings project to support policy planning for the areas in the world with the biggest information gaps. We created it by <a href=\"https://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html\">applying artificial intelligence</a> methods to satellite imagery to identify the locations and outlines of buildings.</p><p>Since we <a href=\"https://blog.google/around-the-globe/google-africa/using-ai-to-map-africas-buildings/\">released the data</a>, we\u2019ve heard from many organizations \u2014 including UN agencies, nonprofits and academics \u2014 who have been using it:</p><ul><li>The UN Refugee Agency, <a href=\"https://www.unhcr.org/\">UNHCR</a>, has been using Open Buildings for survey sampling. It\u2019s common to do household surveys in regions where people have been displaced, in order to know what people need. But UNHCR needs to first have an assessment of where the households actually are, which is where the Open Buildings project has been useful.</li><li><a href=\"https://unhabitat.org/\">UN Habitat</a> is using Open Buildings to study urbanization across the African continent. Having detail on the way that cities are laid out enables them to make recommendations on urban planning.</li><li>The <a href=\"https://www.iea.org/\">International Energy Agency</a> is using Open Buildings to estimate energy needs. With data about individual buildings, they can assess the needs of communities at a new level of precision and know how much energy is needed for cooking, lighting and for operating machinery. This will help with planning sustainable energy policy.</li></ul><p>We\u2019re excited to make this information available in more countries and to assist more organizations in their essential work. As Ernest says, \u201cBy providing decision makers with better data, they can make better decisions. Geographical data is particularly important for providing an unbiased source of information for planning basic services, and we need more of it.\u201d</p></div></div>",
            "pubdate": "Tue, 11 Oct 2022 12:00:00 +0000",
            "pubdate_parsed": [
                2022,
                10,
                11
            ],
            "email_sent": true
        },
        "3 ways AI is scaling helpful technologies worldwide": {
            "url": "https://blog.google/technology/ai/ways-ai-is-scaling-helpful/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p>I was first introduced to neural networks as an undergraduate in 1990. Back then, many people in the AI community were excited about the potential of neural networks, which were impressive, but couldn\u2019t yet accomplish important, real-world tasks. I was excited, too! I did my senior thesis on using parallel computation to train neural networks, thinking we only needed 32X more compute power to get there. I was <i>way</i> off. At that time, we needed <i>1 million times</i> as much computational power.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>A short 21 years later, with exponentially more computational power, it was time to take another crack at neural networks. In 2011, I and a few others at Google started training very large neural networks using millions of randomly selected frames from videos online. The results were <a href=\"https://blog.google/technology/ai/using-large-scale-brain-simulations-for/\">remarkable</a>. Without explicit training, the system automatically learned to recognize different objects (especially cats, the Internet is full of cats). This was one transformational discovery in AI among a long string of successes that is still ongoing \u2014 at Google and elsewhere.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>I share my own history of neural networks to illustrate that, while progress in AI might feel especially fast right now, it\u2019s come from a long arc of progress. In fact, prior to 2012, computers had a really difficult time seeing, hearing, or understanding spoken or written language. Over the past 10 years we\u2019ve made especially <a href=\"https://blog.google/technology/ai/decade-deep-learning-and-whats-next/\">rapid progress in AI</a>.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today, we\u2019re excited about many recent advances in AI that Google is leading \u2014 not just on the technical side, but in responsibly deploying it in ways that help people around the world. That means deploying AI <a href=\"https://cloud.google.com/blog/products/ai-machine-learning\">in Google Cloud</a>, in our products from <a href=\"https://blog.google/intl/en-in/pixel-7-pixel-7-pro/\">Pixel phones</a> to <a href=\"https://blog.google/products/search/search-on-2022-announcements/\">Google Search</a>, and in many fields of science and other human endeavors.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re aware of the challenges and risks that AI poses as an emerging technology. We were the first major company to release and operationalize a set of <a href=\"https://ai.google/principles/\">AI Principles</a>, and following them has actually (and some might think counterintuitively) allowed us to focus on making rapid progress on technologies that can be helpful to everyone. Getting AI right needs to be a collective effort \u2014 involving not just researchers, but domain experts, developers, community members, businesses, governments and citizens.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>I\u2019m happy to make announcements in three transformative areas of AI today: first, using AI to make technology accessible in many more languages. Second, exploring how AI might bolster creativity. And third, in AI for Social Good, including climate adaptation.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>1. Supporting 1,000 languages with AI</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Language is fundamental to how people communicate and make sense of the world. So it\u2019s no surprise it\u2019s also the most natural way people engage with technology. But more than 7,000 languages are spoken around the world, and only a few are well represented online today. That means traditional approaches to training language models on text from the web fail to capture the diversity of how we communicate globally. This has historically been an obstacle in the pursuit of our mission to make the world\u2019s information universally accessible and useful.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>That\u2019s why today we\u2019re announcing the 1,000 Languages Initiative, an ambitious commitment to build an AI model that will support the 1,000 most spoken languages, bringing greater inclusion to billions of people in marginalized communities all around the world. This will be a many years undertaking \u2013 some may even call it a moonshot \u2013 but we are already making meaningful strides here and see the path clearly. Technology has been changing at a rapid clip \u2013 from the way people use it to what it\u2019s capable of. Increasingly, we see people finding and sharing information via new modalities like images, videos, and speech. And our most advanced language models are multimodal \u2013 meaning they\u2019re capable of unlocking information across these many different formats. With these seismic shifts come new opportunities.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><a class=\"article-image--link\"><img alt=\"spinning globe with languages\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/GoogleAI_GlobeAnimation.gif\" tabindex=\"0\" /></a></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>As part of our this initiative and our focus on multimodality, we\u2019ve developed a Universal Speech Model \u2014 or USM \u2014 that\u2019s trained on over 400 languages, making it the largest language coverage seen in a speech model to date. As we expand on this work, we\u2019re partnering with communities across the world to source representative speech data. We <a href=\"https://africa.googleblog.com/2022/10/voice-typing-for-african-languages.html\">recently announced</a> voice typing for 9 more African languages on Gboard by working closely with researchers and organizations in Africa to create and publish data. And in South Asia, we are actively working with local governments, NGOs, and academic institutions to eventually collect representative audio samples from across all the regions\u2019 dialects and languages.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>2. Empowering creators and artists with AI</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>AI-powered generative models have the potential to unlock creativity, helping people across cultures express themselves using video, imagery, and design in ways that they previously could not.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our researchers have been hard at work developing models that lead the field in terms of quality, generating images that human raters prefer over other models. We recently shared important breakthroughs, applying our diffusion model to video sequences and generating long coherent videos for a sequence of text prompts. We can combine these techniques to produce video \u2014 for the first time, today we\u2019re sharing AI-generated super-resolution video:</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><a class=\"article-image--link\"><video alt=\"Phenaki\" class=\"article-image__media\" loop=\"\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Google-AI-Phenaki__Imagen-15fps.mp4\" tabindex=\"0\" title=\"AI super generated video\" type=\"video/mp4\">Video format not supported</video></a></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019ll soon be bringing our text-to-image generation technologies to AI Test Kitchen, which provides a way for people to learn about, experience, and give feedback on emerging AI technology. We look forward to hearing feedback from users on these demos in AI Test Kitchen Season 2. You\u2019ll be able to build themed cities with \u201cCity Dreamer\u201d and design friendly monster characters that can move, dance, and jump with \u201cWobble\u201d \u2014 all by using text prompts.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>In addition to 2D images, text-to-3D is now a reality with DreamFusion, which produces a three-dimensional model that can be viewed from any angle and can be composited into any 3D environment. Researchers are also making significant progress in the audio generation space with AudioLM, a model that learns to generate realistic speech and piano music by listening to audio only. In the same way a language model might predict the words and sentences that follow a text prompt, AudioLM can predict which sounds should follow after a few seconds of an audio prompt.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We're collaborating with creative communities globally as we develop these tools. For example, we're working with writers using Wordcraft, which is built on our state-of-the-art dialog system LaMDA, to experiment with AI-powered text generation. You can read the first volume of these stories at the <a href=\"http://g.co/research/wordcraft\">Wordcraft Writers Workshop</a>.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>3. Addressing climate change and health challenges with AI</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>AI also has great potential to address the effects of climate change, including helping people adapt to new challenges. One of the worst is wildfires, which affect hundreds of thousands of people today, and are increasing in frequency and scale.</p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Today, I\u2019m excited to share that we\u2019ve advanced our use of satellite imagery to train AI models to identify and track wildfires in real time, helping predict how they will evolve and spread. We\u2019ve launched this wildfire tracking system in the U.S., Canada, Mexico, and are rolling out in parts of Australia, and since July we\u2019ve covered more than 30 big wildfire events in the U.S. and Canada, helping inform our users and firefighting teams with over 7 million views in Google Search and Maps.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col-l--4 h-c-grid__col--6 h-c-grid__col-l--offset-4 h-c-grid__col--offset-3\"><a class=\"article-image--link\"><img alt=\"wildfire alert on phone\" class=\"article-image--medium\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Google-AI-Wildfire.gif\" tabindex=\"0\" /></a></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>We\u2019re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\u2019ve already <a href=\"https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/\">helped communities</a> to predict when floods will hit and how deep the waters will get \u2014 in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\u2019re sharing that we\u2019re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\u2019ve used an AI technique called transfer learning to make it work in areas where there\u2019s less data available. We\u2019re also announcing the global launch of Google <a href=\"http://g.co/floodhub\">FloodHub</a>, a new platform that displays when and where floods may occur. We\u2019ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.</p></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image h-c-grid__col-l--4 h-c-grid__col--6 h-c-grid__col-l--offset-4 h-c-grid__col--offset-3\"><a class=\"article-image--link\"><img alt=\"flood alert on a phone\" class=\"article-image--medium\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google-AI-Flood.max-100x100.png\" tabindex=\"0\" /></a></div></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Finally, AI is helping provide ever more access to healthcare in under-resourced regions. For example, we\u2019re researching ways AI can help read and analyze outputs from low-cost ultrasound devices, giving parents the information they need to identify issues earlier in a pregnancy. We also plan to continue to partner with caregivers and public health agencies to expand access to diabetic retinopathy screening through our Automated Retinal Disease Assessment tool (ARDA). Through ARDA, we\u2019ve successfully screened more than 150,000 patients in countries like India, Thailand, Germany, the United States, and the United Kingdom across deployed use and prospective studies \u2014 more than half of those in 2022 alone. Further, we\u2019re exploring how AI can help your phone detect respiratory and heart rates. This work is part of Google Health\u2019s broader vision, which includes <a href=\"https://blog.google/technology/health/check-up-ai-developments-2022/\">making healthcare more accessible</a> for anyone with a smartphone.</p><p></p></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><h3>AI in the years ahead</h3></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p>Our advancements in neural network architectures, machine learning algorithms and new approaches to hardware for machine learning have helped AI solve important, real-world problems for billions of people. Much more is to come. What we\u2019re sharing today is a hopeful vision for the future \u2014 AI is letting us reimagine how technology can be helpful. We hope you\u2019ll join us as we explore these new capabilities and use this technology to improve people\u2019s lives around the world.</p></div></div>",
            "pubdate": "Wed, 02 Nov 2022 14:00:00 +0000",
            "pubdate_parsed": [
                2022,
                11,
                2
            ],
            "email_sent": true
        },
        "How we're using AI to help address the climate crisis": {
            "url": "https://blog.google/outreach-initiatives/sustainability/cop27-adaptation-efforts/",
            "description": "<div class=\"block-paragraph\"><div class=\"rich-text\"><p><br /></p><p>Communities around the world are facing the effects of climate change \u2014 from devastating floods and wildfires to challenges around food security. As global leaders meet in Egypt for <a href=\"https://cop27.eg/#/\">COP27</a>, a key area of focus will be on how we can work together to adapt to climate change <a href=\"https://blog.google/outreach-initiatives/sustainability/COP27-Google-climate-action/\">and implement sustainable solutions</a>. At Google, we\u2019re investing in technologies that can help communities prepare for and respond to climate-related disasters and threats.</p><h3>Tools to alert people and governments about immediate risks</h3><p>Natural disasters are increasing in frequency and intensity due to climate change. As part of our <a href=\"https://crisisresponse.google/\">Crisis Response</a> efforts, we're working to bring trusted information to people in critical moments to keep them safe and informed. To do so, we rely on the research and development of our AI-powered technologies and longstanding partnerships with frontline emergency workers and organizations. Here\u2019s a look at some of our crisis response efforts and new ways we\u2019re expanding these tools.</p><ul><li><b>Floods:</b> Catastrophic <a href=\"http://g.co/floods\">damage from flooding</a> affects more than 250 million people every year. In 2018, we launched our flood <a href=\"https://blog.google/technology/ai/expanding-our-ml-based-flood-forecasting/\">forecasting initiative</a> that <a href=\"https://hess.copernicus.org/articles/26/4013/2022/\">uses machine learning models</a> to provide people with detailed alerts. In 2021, we sent 115 million flood alert notifications to 23 million people over Search and Maps, helping save countless lives. Today, we\u2019re expanding our flood forecasts to river basins in 18 additional countries across Africa, Latin America and Southeast Asia. We\u2019re also announcing the global launch of the new <a href=\"http://g.co/floodhub\">FloodHub</a>, a platform that displays flood forecasts and shows when and where floods may occur to help people directly at risk and provide critical information to aid organizations and governments. This expansion in geographic coverage is possible thanks to our recent breakthroughs in AI-based flood forecasting models, and we\u2019re committed to expanding to more countries.</li></ul></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col--10 h-c-grid__col--offset-1 h-c-grid__col-l--offset-2 h-c-grid__col-l--8\"><a class=\"article-image--link\"><img alt=\"An image of a FloodHub map showing areas where riverine floods my occur\" class=\"article-image--full\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Screen_Shot_2022-10-31_at_22.08.32_cuEwOVg.max-100x100.png\" tabindex=\"0\" /></a></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The new Google FloodHub at <a href=\"https://g.co/floodhub\">g.co/floodhub</a> shows forecasts for riverine floods. Forecasts are now available in 18 additional countries: Brazil, Colombia, Sri Lanka, Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, South Africa.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><p></p><ul><li><b>Wildfires:</b> Wildfires affect hundreds of thousands of people each year, and are increasing in frequency and size. I <a href=\"https://blog.google/products/search/mapping-wildfires-with-satellite-data/\">experienced firsthand</a> the need for accurate information when wildfires occur and this inspired our crisis response work. We detect wildfire boundaries using new AI models based on satellite imagery and show their real-time location in Search and Maps. Since July, we\u2019ve covered more than 30 big wildfire events in the U.S. and Canada, helping inform people and firefighting teams with over 7 million views in Search and Maps. Today, wildfire detection is now available in the U.S., Canada, Mexico and Australia.</li></ul></div></div><div class=\"block-image_full_width\"><div class=\"h-c-page\"><div class=\"article-image__is-caption h-c-grid__col-l--6 h-c-grid__col--8 h-c-grid__col-l--offset-3 h-c-grid__col--offset-2\"><a class=\"article-image--link\"><img alt=\"Picture shows the location of the Pukatawagan fire in Manitoba, Canada.\" class=\"article-image--large\" src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/boundary_tracking_graphics_1_xGJMBgH.max-100x100.png\" tabindex=\"0\" /></a></div><figcaption class=\"article-image__caption article-image__is-caption-image h-c-grid__col--8 h-c-grid__col--offset-2 h-c-grid__col-l--6 h-c-grid__col-l--offset-3\"><div class=\"rich-text\"><p>The location of the Pukatawagan fire in Manitoba, Canada.</p></div></figcaption></div></div><div class=\"block-paragraph\"><div class=\"rich-text\"><ul><li><b>Hurricanes:</b> Access to authoritative forecasts and safety information about hurricanes can be life-saving. In the days before a hurricane in North America or a typhoon in Japan, detailed forecasts from authoritative sources appear on <a href=\"https://support.google.com/sosalerts/?hl=en\">SOS Alerts</a> in Search and Maps to show a storm\u2019s predicted trajectory. We're also using machine learning to <a href=\"https://ai.googleblog.com/2020/06/machine-learning-based-damage.html\">analyze satellite imagery</a> after disasters and identify which areas need help. When Hurricane Ian hit Florida in September, this technology was <a href=\"https://www.wired.com/story/hurricane-ian-destroyed-homes-google-algorithms-sent-money/\">deployed in partnership with Google.org grantee GiveDirectly</a> to quickly allocate aid to those most affected.</li></ul><h3>Managing current and future climate impacts</h3><p>Climate change poses a threat to our world's natural resources and food security. We\u2019re working with governments, organizations and communities to provide information and technologies to help adapt to these changes.</p><ul><li><b>Keeping cities greener and healthier:</b> Extreme temperatures and poor air quality are increasingly common in cities and can impact public health. To mitigate this, our <a href=\"https://blog.google/outreach-initiatives/sustainability/sustainability-2021/\">Project Green Light</a> uses AI to optimize traffic lights at intersections around the world with the aim to help minimize congestion and related pollution. <a href=\"https://www.google.com/earth/outreach/special-projects/air-quality/\">Project Air View</a> also brings detailed air quality maps to scientists, policymakers and communities. And we\u2019re working to expand our Environmental Insights Explorer\u2019s <a href=\"https://insights.sustainability.google/labs/treecanopy\">Tree Canopy Insights</a> tool to hundreds of cities by the end of this year so they can use <a href=\"https://www.epa.gov/heatislands/using-trees-and-vegetation-reduce-heat-islands\">trees to lower street-level temperatures</a> and improve quality of life.</li><li><b>Meeting the world\u2019s growing demand for food:</b> <a href=\"https://mineral.ai/\">Mineral</a> \u2014 a project from X, Alphabet\u2019s moonshot factory \u2014 is working to build a more sustainable and productive food system. The team is joining diverse data sets in radically new ways \u2014 from soil and weather data to drone and satellite images \u2014 and using AI to reveal insights never before possible about what\u2019s happening with crops. As part of our <a href=\"http://g.co/startups/sdg\">Startups For Sustainable Development</a> program, we\u2019re also supporting startups addressing food security. These include startups like <a href=\"https://events.withgoogle.com/startups-for-sustainable-development/oko/\">OKO,</a> which provides crop insurance to keep farmers in business in case of adverse weather events and has reached tens of thousands of farmers in Mali and Uganda.</li><li><b>Helping farmers protect their crops:</b> Pest infestations can threaten entire crops and impact the livelihoods of millions. In collaboration with InstaDeep and the Food and Agriculture Organization of the United Nations, our team at the Google AI Center in Ghana is using AI to better detect locust outbreaks so that it's possible to implement control measures. In India, Google.org Fellows worked with <a href=\"https://www.wadhwaniai.org/\">Wadhwani AI</a> to <a href=\"https://www.fastcompany.com/90640843/google-is-helping-deploy-ai-to-prevent-pests-devastating-indian-crops\">create an AI-powered app</a> that helps identify and treat infestations of pests, resulting in a 20% reduction in pesticide sprays and a 26% increase in profit margins for farmers. Google Cloud is also <a href=\"https://blog.google/products/google-cloud/helping-farmers-with-cloud-technology-up-close-and-global/\">working with agricultural technology companies</a> to use machine learning and cloud services to improve crop yields.</li><li><b>Analyzing a changing planet:</b> Using Google Cloud and Google Earth Engine, organizations and businesses can better assess and manage climate risks. For example, <a href=\"https://cloud.google.com/blog/topics/sustainability/how-the-us-forest-service-uses-google-cloud\">the U.S. Forest Service</a> uses these tools to analyze land-cover changes to better respond to new wildfire threats and monitor the impacts of invasive insects, diseases and droughts. Similarly, the Bank of Montreal is <a href=\"https://sustainabilityleaders.bmo.com/en/news-insights/sustainable-finance/mitigating-physical-impacts-climate-change-spatial-finance/\">integrating climate data</a> \u2014 like precipitation trends \u2014 into its business strategy and risk management for clients.</li></ul><p>AI already plays a critical role in addressing many urgent, climate-related challenges. It is important that we continue to invest in research and raise awareness about why we are doing this work. Google Arts and Culture has collaborated with artists on the <a href=\"http://g.co/culturemeetsclimate\">Culture meets Climate</a> collection so everyone can explore more perspectives on climate change. And at COP27 we hope to generate more awareness and engage in productive discussions about how to use AI, innovations, and <a href=\"https://www.datacommons.org/\">shared data</a> to help global communities address the changing climate.</p></div></div><div class=\"block-perspective_qa\"><div class=\"uni-related-article-tout h-c-page\"><section class=\"h-c-grid\"><a class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" href=\"https://blog.google/outreach-initiatives/sustainability/cop27-google-climate-action/\"><div class=\"uni-related-article-tout__inner-wrapper\"><p class=\"uni-related-article-tout__eyebrow h-c-eyebrow\">Related Article</p><div class=\"uni-related-article-tout__content-wrapper\"><div class=\"uni-related-article-tout__image-wrapper\"><div class=\"uni-related-article-tout__image\"></div></div><div class=\"uni-related-article-tout__content\"><h4 class=\"uni-related-article-tout__header h-has-bottom-margin\">Accelerating climate action at Google and beyond</h4><p class=\"uni-related-article-tout__body\">A new paper we published today shares updates on the work we have been doing as part of our third decade of climate action.</p><div class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"><span class=\"nowrap\">Read Article<svg class=\"icon h-c-icon\" xmlns=\"http://www.w3.org/2000/svg\"><use xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></use></svg></span></div></div></div></div></a></section></div></div>",
            "pubdate": "Wed, 02 Nov 2022 14:00:00 +0000",
            "pubdate_parsed": [
                2022,
                11,
                2
            ],
            "email_sent": true
        },
        "Partnering with iCAD to improve breast cancer screening": {
            "url": "https://blog.google/technology/ai/icad-partnership-breast-cancer-screening/",
            "description": "A visual representation of multiple mammograms being scanned by artificial intelligence for signs of breast cancer.",
            "pubdate": "Mon, 28 Nov 2022 13:00:00 +0000",
            "pubdate_parsed": [
                2022,
                11,
                28
            ],
            "email_sent": true
        },
        "Why we focus on AI (and to what end)": {
            "url": "https://blog.google/technology/ai/why-we-focus-on-ai-and-to-what-end/",
            "description": "Illustration of colorful dots and lines",
            "pubdate": "Mon, 16 Jan 2023 09:00:00 +0000",
            "pubdate_parsed": [
                2023,
                1,
                16
            ],
            "email_sent": true
        },
        "7 ways Google is using AI to help solve society's challenges": {
            "url": "https://blog.google/technology/ai/7-ways-google-is-using-ai-to-help-solve-societys-challenges/",
            "description": "Colorful illustration depicting five of the uses of AI: A map showing a river and where it will flood; a collection of wildfire symbols; a microscope; bugs crawling on leaves; and a map marker with dotted lines coming from it",
            "pubdate": "Tue, 17 Jan 2023 08:00:00 +0000",
            "pubdate_parsed": [
                2023,
                1,
                17
            ],
            "email_sent": true
        },
        "The next generation of AI for developers and Google Workspace": {
            "url": "https://blog.google/technology/ai/ai-developers-google-cloud-workspace/",
            "description": "Moving lines and dots in the 4 Google colors",
            "pubdate": "Tue, 14 Mar 2023 13:00:00 +0000",
            "pubdate_parsed": [
                2023,
                3,
                14
            ],
            "email_sent": true
        },
        "Ask a Techspert: What is generative AI?": {
            "url": "https://blog.google/inside-google/googlers/ask-a-techspert/what-is-generative-ai/",
            "description": "illustration of lines and dots",
            "pubdate": "Tue, 11 Apr 2023 13:00:00 +0000",
            "pubdate_parsed": [
                2023,
                4,
                11
            ],
            "email_sent": true
        }
    },
    "OpenAI Blog": {
        "DALLE 2": {
            "url": "https://openai.com/blog/dall-e-2/",
            "description": "DALL\u00b7E 2 is a new AI system that can create realistic images and art from a description in natural language.",
            "pubdate": "Wed, 06 Apr 2022 13:42:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                6
            ],
            "email_sent": true
        },
        "Measuring Goodharts Law": {
            "url": "https://openai.com/blog/measuring-goodharts-law/",
            "description": "Goodhart\u2019s law famously says: \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d Although originally from economics, it\u2019s something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.",
            "pubdate": "Wed, 13 Apr 2022 18:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                13
            ],
            "email_sent": true
        },
        "OpenAI Leadership Team Update": {
            "url": "https://openai.com/blog/leadership-team-update/",
            "description": "We\u2019re happy to announce several executive role changes that reflect our recent progress and will ensure continued momentum toward our next major milestones.",
            "pubdate": "Thu, 05 May 2022 20:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                5
            ],
            "email_sent": true
        },
        "DALLE 2 Research Preview Update": {
            "url": "https://openai.com/blog/dall-e-2-update/",
            "description": "Early users have created over 3 million images to date and helped us improve our safety processes. We're excited to begin adding up to 1,000 new users from our waitlist each week.",
            "pubdate": "Wed, 18 May 2022 20:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "Powering Next Generation Applications with OpenAI Codex": {
            "url": "https://openai.com/blog/codex-apps/",
            "description": "Codex is now powering 70 different applications across a variety of use cases through the OpenAI API.",
            "pubdate": "Tue, 24 May 2022 15:31:36 GMT",
            "pubdate_parsed": [
                2022,
                5,
                24
            ],
            "email_sent": true
        },
        "Best Practices for Deploying Language Models": {
            "url": "https://openai.com/blog/best-practices-for-deploying-language-models/",
            "description": "<!--kg-card-begin: markdown--><p>Cohere, OpenAI, and AI21 Labs have developed a preliminary set of best practices applicable to any organization developing or deploying large language models. Computers that can read and write are here, and they have the potential to fundamentally impact daily life. The future of human&#x2013;machine interaction is full</p>",
            "pubdate": "Thu, 02 Jun 2022 16:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                2
            ],
            "email_sent": true
        },
        "Techniques for Training Large Neural Networks": {
            "url": "https://openai.com/blog/techniques-for-training-large-neural-networks/",
            "description": "<!--kg-card-begin: markdown--><p>Large neural networks are at the core of many recent advances in AI, but training them is a difficult engineering and research challenge which requires orchestrating a cluster of GPUs to perform a single synchronized calculation. As cluster and model sizes have grown, machine learning practitioners have developed an increasing</p>",
            "pubdate": "Thu, 09 Jun 2022 16:00:56 GMT",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "AI-Written Critiques Help Humans Notice Flaws": {
            "url": "https://openai.com/blog/critiques/",
            "description": "Showing model-generated critical comments to humans helps them find flaws in summaries.",
            "pubdate": "Mon, 13 Jun 2022 18:20:37 GMT",
            "pubdate_parsed": [
                2022,
                6,
                13
            ],
            "email_sent": true
        },
        "Learning to Play Minecraft with Video PreTraining (VPT)": {
            "url": "https://openai.com/blog/vpt/",
            "description": "<!--kg-card-begin: markdown--><div class=\"js-excerpt\">\n<p>We trained a neural network to play Minecraft by Video PreTraining (VPT) on a massive unlabeled video dataset of human Minecraft play, while using only a small amount of labeled contractor data. With fine-tuning, our model can learn to craft diamond tools, a task that usually takes proficient humans over</p></div>",
            "pubdate": "Thu, 23 Jun 2022 16:00:59 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "DALLE 2 Pre-Training Mitigations": {
            "url": "https://openai.com/blog/dall-e-2-pre-training-mitigations/",
            "description": "<!--kg-card-begin: markdown--><p>In order to share the magic of <a href=\"https://openai.com/dall-e-2/\">DALL&#xb7;E 2</a> with a broad audience, we needed to reduce the risks associated with powerful image generation models. To this end, we put various <a href=\"https://github.com/openai/dalle-2-preview/blob/main/system-card.md\">guardrails</a> in place to prevent generated images from violating our <a href=\"https://labs.openai.com/policies/content-policy\">content policy</a>. This post focuses on <em>pre-training</em></p>",
            "pubdate": "Tue, 28 Jun 2022 17:00:58 GMT",
            "pubdate_parsed": [
                2022,
                6,
                28
            ],
            "email_sent": true
        },
        "DALLE 2: Extending Creativity": {
            "url": "https://openai.com/blog/dall-e-2-extending-creativity/",
            "description": "<!--kg-card-begin: markdown--><div class=\"js-excerpt\">\n<p>As part of our DALL&#xb7;E 2 research preview, more than 3,000 artists from more than 118 countries have incorporated DALL&#xb7;E into their creative workflows. The artists in our early access group have helped us discover new uses for DALL&#xb7;E and have served as</p></div>",
            "pubdate": "Thu, 14 Jul 2022 16:30:22 GMT",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "Reducing Bias and Improving Safety in DALLE 2": {
            "url": "https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/",
            "description": "<!--kg-card-begin: markdown--><p>Today, we are implementing a new technique so that DALL&#xb7;E generates images of people that more accurately reflect the diversity of the world&#x2019;s population. This technique is applied at the system level when DALL&#xb7;E is given a prompt describing a person that does not</p>",
            "pubdate": "Mon, 18 Jul 2022 16:30:23 GMT",
            "pubdate_parsed": [
                2022,
                7,
                18
            ],
            "email_sent": true
        },
        "DALLE Now Available in Beta": {
            "url": "https://openai.com/blog/dall-e-now-available-in-beta/",
            "description": "<!--kg-card-begin: markdown--><div class=\"js-excerpt\">\n<p>We&#x2019;ll invite 1 million people from our waitlist over the coming weeks. Users can create with DALL&#xb7;E using free credits that refill every month, and buy additional credits in 115-generation increments for $15.</p>\n<div class=\"btns mb-0.5\">\n<a class=\"btn btn-padded btn-dark btn-circle icon-external right\" href=\"https://labs.openai.com/waitlist\">Join DALL&#xb7;E 2 waitlist</a>\n</div>\n</div>\n<p><a href=\"https://openai.com/dall-e-2/\">DALL&#xb7;E</a>, the AI system that</p>",
            "pubdate": "Wed, 20 Jul 2022 16:29:05 GMT",
            "pubdate_parsed": [
                2022,
                7,
                20
            ],
            "email_sent": true
        },
        "New-and-Improved Content Moderation Tooling": {
            "url": "https://openai.com/blog/new-and-improved-content-moderation-tooling/",
            "description": "<!--kg-card-begin: markdown--><div class=\"js-excerpt\">\n<p>We are introducing a new-and-improved content moderation tool: The <a href=\"https://beta.openai.com/docs/api-reference/moderations\">Moderation endpoint</a> improves upon our previous content filter, and is available for free today to OpenAI API developers.</p>\n</div>\n<p>To help developers protect their applications against possible misuse, we are introducing the faster and more accurate <a href=\"https://beta.openai.com/docs/api-reference/moderations\">Moderation endpoint</a>. This endpoint provides OpenAI</p>",
            "pubdate": "Wed, 10 Aug 2022 16:00:43 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Our approach to alignment research": {
            "url": "https://openai.com/blog/our-approach-to-alignment-research/",
            "description": "<!--kg-card-begin: markdown--><div class=\"js-excerpt\">\n<p>Our approach to aligning AGI is empirical and iterative. We are improving our AI systems&#x2019; ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignment problems.</p>\n</div>\n<h2 class=\"sr-only\" id=\"introduction\">Introduction</h2>\n<p><a href=\"https://openai.com/alignment\">Our</a></p>",
            "pubdate": "Wed, 24 Aug 2022 18:00:33 GMT",
            "pubdate_parsed": [
                2022,
                8,
                24
            ],
            "email_sent": true
        },
        "Forecasting Potential Misuses of Language Models for Disinformation Campaignsand How to Reduce Risk": {
            "url": "https://openai.com/blog/forecasting-misuse/",
            "description": "<!--kg-card-begin: markdown--><div class=\"post-excerpt\">\n<p>OpenAI researchers collaborated with Georgetown University&#x2019;s Center for Security and Emerging Technology and the Stanford Internet Observatory to investigate how large language models might be misused for disinformation purposes. The collaboration included an October 2021 workshop bringing together 30 disinformation researchers, machine learning experts, and policy analysts, and</p></div>",
            "pubdate": "Wed, 11 Jan 2023 11:00:41 GMT",
            "pubdate_parsed": [
                2023,
                1,
                11
            ],
            "email_sent": true
        }
    },
    "Apple Machine Learning Blog": {
        "NeuMan: Neural Human Radiance Field from a Single Video": {
            "url": "https://machinelearning.apple.com/research/neural-human-radiance-field",
            "description": "Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical\u2026",
            "pubdate": "Tue, 02 Aug 2022 19:29:38 GMT",
            "pubdate_parsed": [
                2022,
                8,
                2
            ],
            "email_sent": true
        },
        "Integrating Categorical Features in End-To-End ASR": {
            "url": "https://machinelearning.apple.com/research/integrating-categorical-features",
            "description": "All-neural, end-to-end ASR systems gained rapid interest from the speech recognition community. Such systems convert speech input to text units using a single trainable neural network model. E2E models require large amounts of paired speech text data that is expensive to obtain. The amount of data available varies across different languages and dialects. It is critical to make use of all these data so that both low resource languages and high resource languages can be improved. When we want to deploy an ASR system for a new application domain, the amount of domain specific training data is\u2026",
            "pubdate": "Mon, 08 Aug 2022 18:21:13 GMT",
            "pubdate_parsed": [
                2022,
                8,
                8
            ],
            "email_sent": true
        },
        "Space-Efficient Representation of Entity-centric Query Language Models": {
            "url": "https://machinelearning.apple.com/research/space-efficient-representation",
            "description": "Virtual assistants make use of automatic speech recognition (ASR) to help users answer entity-centric queries. However, spoken entity recognition is a difficult problem, due to the large number of frequently-changing named entities. In addition, resources available for recognition are constrained when ASR is performed on-device. In this work, we investigate the use of probabilistic grammars as language models within the finite-state transducer (FST) framework. We introduce a deterministic approximation to probabilistic grammars that avoids the explicit expansion of non-terminals at model\u2026",
            "pubdate": "Tue, 09 Aug 2022 23:07:56 GMT",
            "pubdate_parsed": [
                2022,
                8,
                9
            ],
            "email_sent": true
        },
        "Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting": {
            "url": "https://machinelearning.apple.com/research/taxonomy-overfitting",
            "description": "The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods, which perfectly fit their training data. Certain interpolating methods, including neural networks, can fit noisy training data without catastrophically bad test performance, in defiance of standard intuitions from statistical learning theory. Aiming to explain this, a body of recent work has studied benign overfitting, a phenomenon where some interpolating methods approach Bayes optimality, even in the presence of noise. In this work we argue that while benign\u2026",
            "pubdate": "Wed, 10 Aug 2022 00:14:34 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "FORML: Learning to Reweight Data for Fairness": {
            "url": "https://machinelearning.apple.com/research/learning-to-reweight-data",
            "description": "Machine learning models are trained to minimize the mean loss for a single metric, and thus typically do not consider fairness and robustness. Neglecting such metrics in training can make these models prone to fairness violations when training data are imbalanced or test distributions differ. This work introduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training algorithm that balances fairness and robustness with accuracy by jointly learning training sample weights and neural network parameters. The approach increases model fairness by learning to balance the contributions\u2026",
            "pubdate": "Wed, 10 Aug 2022 23:08:05 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Minimax Demographic Group Fairness in Federated Learning": {
            "url": "https://machinelearning.apple.com/research/minimax-demographic-group",
            "description": "Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm -- FedMinMax -- for\u2026",
            "pubdate": "Wed, 10 Aug 2022 22:54:13 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Regularized Training of Nearest Neighbor Language Models": {
            "url": "https://machinelearning.apple.com/research/regularized-training",
            "description": "Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon kNN-LM, which uses a pre-trained language model together with an exhaustive kNN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the kNN-LM performance by instead training a LM with the knowledge that we will be using a kNN post-hoc. We achieved significant improvement using our method on language modeling tasks on WIKI-2 and WIKI-103. The main\u2026",
            "pubdate": "Mon, 15 Aug 2022 13:26:43 GMT",
            "pubdate_parsed": [
                2022,
                8,
                15
            ],
            "email_sent": true
        },
        "A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing": {
            "url": "https://machinelearning.apple.com/research/dense-material",
            "description": "A key algorithm for understanding the world is material segmentation, which assigns a label (metal, glass, etc.) to each pixel. We find that a model trained on existing data underperforms in some settings and propose to address this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor and outdoor images, which is 23x more segments than existing data. Our data covers a more diverse set of scenes, objects, viewpoints and materials, and contains a more fair distribution of skin types. We show that a model trained on our data outperforms a state-of-the-art model across\u2026",
            "pubdate": "Mon, 15 Aug 2022 14:20:22 GMT",
            "pubdate_parsed": [
                2022,
                8,
                15
            ],
            "email_sent": true
        },
        "Combining Compressions for Multiplicative Size Scaling on Natural Language Tasks": {
            "url": "https://machinelearning.apple.com/research/combining-compressions",
            "description": "Quantization, knowledge distillation, and magnitude pruning are among the most popular methods for neural network compression in NLP. Independently, these methods reduce model size and can accelerate inference, but their relative benefit and combinatorial inter- actions have not been rigorously studied. For each of the eight possible subsets of these techniques, we compare accuracy vs. model size tradeoffs across six BERT architecture sizes and eight GLUE tasks. We find that quantization and distillation consistently provide greater benefit than pruning. Surprisingly, except for the pair of\u2026",
            "pubdate": "Mon, 22 Aug 2022 17:02:48 GMT",
            "pubdate_parsed": [
                2022,
                8,
                22
            ],
            "email_sent": true
        },
        "CVNets: High Performance Library for Computer Vision": {
            "url": "https://machinelearning.apple.com/research/high-performance-library",
            "description": "We introduce CVNets, a high-performance open-source library for training deep neural networks for visual recognition tasks, including classification, detection, and segmentation. CVNets supports image and video understanding tools, including data loading, data transformations, novel data sampling methods, and implementations of several standard networks with similar or better performance than previous studies.",
            "pubdate": "Mon, 22 Aug 2022 17:09:34 GMT",
            "pubdate_parsed": [
                2022,
                8,
                22
            ],
            "email_sent": true
        }
    },
    "Towards AI Blog": {
        "This AI newsletter is all you need #9": {
            "url": "https://towardsai.net/p/newsletter/this-ai-newsletter-is-all-you-need-9",
            "description": "Last Updated on August 23, 2022 by Editorial Team Author(s): Towards AI Editorial Team Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What happened this week in\u00a0AI Images generated with Stable Diffusion This week we had our attention turned from Dalle to a new open-source model called Stable Diffusion. Stable Diffusion comes from researchers at Stability AI, a London- and Los Altos-based startup, along with RunwayML, LMU Munich, EleutherAI, and LAION. The big deal: the AI model and code will be published as open source. Yes, it is open source, which means you have access to its code compared to DALLE, which is closed\u00a0source. Stable diffusion is very similar to DALLE 2 in terms of architectural choices, though much lighter due to their use of latent diffusion. Latent diffusion is a powerful way of using the diffusion process, which consists of taking the noise and generating an image, but does that in the latent space using an encoder and a decoder to go from the latent space to the image space. This enables anyone to implement their code as it can run on a single GPU, and it also allows for much faster inference times, which means you won\u2019t have to wait for two minutes to get funny results as with DALLE or Craiyon anymore! It\u2019s also an important step for the community to implement powerful state-of-the-art text-to-image models and experiment with the code, creating a lot of opportunities to bring the research forward with diffusion models and image models. We are very excited to follow the news and work done with Stable Diffusion! If you implement their code, please let us know and share your creations on our discord\u00a0server! You can also play with the Stable Diffusion API right\u00a0now! Hottest News Engineers from Stanford have developed a chip that does AI processing within its memory (improving computing efficiency)\u201cA novel resistive random-access memory (RRAM) chip that does the AI processing within the memory itself, thereby eliminating the separation between the compute and memory units. Their \u201ccompute-in-memory\u201d (CIM) chip, called NeuRRAM, is about the size of a fingertip and does more work with limited battery power than what current chips can\u00a0do.\u201d Stable Diffusion: An Open-Source image generator AI model!Stable Diffusion comes from researchers at Stability AI, a London- and Los Altos-based startup, along with RunwayML, LMU Munich, EleutherAI, and LAION. The big deal: the AI model and code has been published as open source. Play with it\u00a0here. TikTok introduced \u201cAI greenscreen\u201d, their text-to-image generatorThe results aren\u2019t those of Dalle 2 or Midjourney yet, but it is quite cool and they are certainly getting into this race. Right now, AI greenscreen generates abstract \u201cblobs\u201d and swirling imagery focusing on creating nice backgrounds and not photorealistic images. Most interesting papers of the\u00a0week Paint2Pix: Interactive Painting based Progressive Image Synthesis and EditingA novel approach that predicts (and adapts) \u201cwhat a user wants to draw\u201d from rudimentary brushstroke inputs, by learning a mapping from the manifold of incomplete human paintings to their realistic renderings. Get the\u00a0code! UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D SceneA novel 3D scene photorealistic style transfer framework for transferring photorealistic style from an input image to a 3D scene, merging a 2D style transfer approach with voxel representations. Transframer: Arbitrary Frame Prediction with Generative Models [deepmind paper]Transframer: A general-purpose framework for image modeling and vision tasks based on probabilistic frame prediction, combining U-Net and Transformer components. Enjoy these papers and news summaries? Get a daily recap in your\u00a0inbox! Announcements At Towards AI Inc we have exciting news to announce\u200a\u2014\u200awe have acquired Confetti AI. Confetti AI was founded by Mihail Eric and Henry Zhao in 2020 and has grown to 6,000 active users with a content library of over 350 questions. Confetti AI was built on a decade of experience in artificial intelligence and hundreds of hours of discussions with experts in the field. It aims to curate the leading library of machine learning and data science interview questions, focusing on both conceptual understanding and practical applications. Learn more about the\u00a0news. The Learn AI Together Community section!Hey Meme of the\u00a0week! What an awesome way to evaluate the actual technical knowledge of a recruiter \ud83d\ude02. Of course, do not do that, it\u2019s just a meme! Meme shared once again by one of our fantastic moderators with great humor, DrDub#0108. Join the conversation and share your memes with\u00a0us! Featured Community post from the\u00a0Discord Awesome! One of our moderators just launched a super cool\u00a0product! \u201cmy product of book-&#62;IG post generator is inching towards launch.\u201d DrDub#0108DrDub\u2019s product is all about using AI to generate Instagram posts from a book. Authors can use it for free to automatically generate and share quotes and interesting ideas for their books on Instagram. How cool is\u00a0that! Watch some of the results on its Instagram page, and don\u2019t forget to share your own projects or products with the community if you\u2019d like to be featured in the newsletter as\u00a0well! AI poll of the\u00a0week! TAI Curated\u00a0section Article of the\u00a0week Which NLP Task Does NOT Benefit From Pre-trained Language\u00a0Models? There is such a vast history of massively influential pre-trained generic language models that we take then for granted. Still, they are an absolutely required basis for most NLP applications. However, the author of the article we wanted to highlight claims that there are cases in which a pre-trained general model is ineffective, backing up his claims with excellent analysis. If you are interested in publishing with us at Towards AI, please sign up here, and we will publish your blog to our network if it meets our editorial policies and standards. Lauren\u2019s Ethical Take on Stanford\u2019s NeuRRAM Stanford\u2019s progress on NeuRRAM is an incredible feat! I highly encourage reading the entire article on [&#8230;]",
            "pubdate": "Tue, 23 Aug 2022 17:08:03 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Image segmentation of rotating iPhone with scikit-image": {
            "url": "https://towardsai.net/p/l/image-segmentation-of-rotating-iphone-with-scikit-image",
            "description": "Last Updated on August 23, 2022 by Editorial Team Author(s): Dmitri Azarnyh Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Image Segmentation of Rotating iPhone With Scikit-image Modern devices are full of different sensors: front and back cameras, accelerometers, gyroscopes, magnetometers, GPS, etc. Smart use of these devices allows for a large number of interesting applications such as compass, location detection, and\u00a0games. Photo by Nikita Vinogradov on Unsplash. Try to segment this cat from the background\u00a0\ud83d\ude42 One such application is to detect the orientation of an iPhone in space which can be accomplished with data from the accelerometer and gyroscope. In the previous blog post, I wrote about the detection of iPhone orientation around one axis with data from two sensors of iPhone: gyroscope and accelerometer. The reconstruction of iPhone orientation was compared with the video. The results of the orientation reconstructed with sensors were a good qualitative match with the orientation depicted in the\u00a0video: Image by\u00a0author In this blog post, I present the way to extract the angle of iPhone inclination from the video with image segmentation. Then, I show the comparison of an error between two ways of measurement. Let me guide you\u00a0through. Video preparation Before starting with the segmentation, there are a few things to prepare. My video was recorded with an iPhone and had a variate frame rate. To synchronize video clocks with sensor clocks, a constant frame rate is preferable. So, the first step would be to transfer the video to a constant frame rate. The next step would be to split the video into images where each frame corresponds to a jpeg image. The whole dataset of images can be found\u00a0here. Deep learning approach with Mask-R-CNN When we talk about the segmentation of an image, it\u2019s a great temptation to try some cool deep learning algorithm and hope that it will work. Following this temptation, I tried pre-trained Mask-RCNN. Fortunately, a cell phone is one of the classes the original model was trained on. The first result was quite impressive, as shown in the picture\u00a0below: Image by\u00a0author However, with a closer investigation, it appears that some images of the iPhone were misidentified (see the picture\u00a0below): Image by\u00a0author Like in many real-life data science projects, the results of the deep learning model are surprisingly good but do not fully deliver what is expected. At least not for the whole dataset. One option to overcome this challenge would be to label the data and fine-tune the original model. However, given the fact that it\u2019s a red iPhone on an almost white background, there should be some easier ways. Let us have a look at\u00a0them. Classical CV approach with RGB\u00a0image To segment an image, we first try using a 3-channel RGB representation of an image that is short from red, green, and blue. Red would correspond to the first number that is very high, while the second and the third numbers are expected to be low, somewhat like (255,0,0). In this case, we can just choose all red pixels and hope that it will be the iPhone. To capture more shadows of red, I tried to filter all pixels which have a color value for the first channel (red) bigger than 130. For the second and the third channels, I filter the pixels which are lower than 60. Here is the\u00a0code. https://medium.com/media/a551e688f73eff9e4396f46a1c1e2eaf/href Such an approach produces reasonably good\u00a0results: Image by\u00a0author There is also a camera \u201chole\u201d in the iPhone, which is black and was hence not segmented. We can fill out this hole with scikit-image tools: https://medium.com/media/03839bb0955d37149e28db052f339678/href One has to set up the area of the holes to fill out and small objects to filter out. I choose 40000 as the area. Now the segmentation does include the cameras of the\u00a0iPhone: Image by\u00a0author So, the segmentation based on red color in RGB representation works well on one image. However, after running this threshold-based algorithm through all images, I found that there are some segmentation inaccuracies: Image by\u00a0author These inaccuracies could be fixed by manipulating thresholds on red, green, and blue colors. However, if we relax the parameters, the mask starts to capture brown\u00a0color: Image by\u00a0author It\u2019s still possible to play around with thresholds in RGB. However, there are at least three parameters to tune (thresholds in red, green, and blue). Much easier would be to reduce it to only one tunable parameter. For that purpose, we can consider the iPhone image in a different color representation. Classical CV approach with HED\u00a0image In the scikit-image library, several transformations are available from the RGB to different color representations. One of the challenges which I experience in working with the images of rotating iPhones is the separation from dark red and brown. This color separation is well addressed in the Haematoxylin-Eosin-DAB (HED) color space, where Hematoxylin has a deep blue-purple, Eosin is pinkish, and DAB is brown. Red iPhone is quite well detected with Eosin channel of such a color representation, which fixes the issue of mixing it up with brown, as brown is presented as DAB. In the HED representation, we have only one threshold in the Eosin channel to tune. The mask for the images which are challenging for RGB is captured correctly based on the Eosin channel and threshold of\u00a00.05. Image by\u00a0author Scikit-image also provides tools to measure the properties of the segmented region. Among other properties, it\u2019s possible to measure the orientation of the\u00a0iPhone: https://medium.com/media/e988c7639c5c193a6ba5faa9aa407afa/href The full segmentation algorithm would result\u00a0to: https://medium.com/media/2b0d1ce1a118d6ce330b219d1d965485/href A qualitative comparison would result\u00a0to: Image by\u00a0author Quantitative Comparison Now it\u2019s possible to compare the inclination angle, which is derived from segmentation, with the one derived from sensors data. It gives a good\u00a0match: Image by\u00a0author The absolute error between sensors and interpolated values of video\u00a0are: Image by\u00a0author We [&#8230;]",
            "pubdate": "Tue, 23 Aug 2022 16:08:49 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Machine Learning Systems Pt. 3: Modeling Pipelines with TensorFlow Extended": {
            "url": "https://towardsai.net/p/l/machine-learning-systems-pt-3-modeling-pipelines-with-tensorflow-extended",
            "description": "Last Updated on August 23, 2022 by Editorial Team Author(s): Ani Madurkar Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Where to go after your Modeling POC is done Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 23 Aug 2022 15:33:12 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Explore and Validate Datasets with TensorFlow Extended": {
            "url": "https://towardsai.net/p/l/explore-and-validate-datasets-with-tensorflow-extended",
            "description": "Last Updated on August 23, 2022 by Editorial Team Author(s): Nicolo Cosimo Albanese Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Introduction to TensorFlow Data Validation with a practical example Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 23 Aug 2022 15:33:09 +0000",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Fake News Detection with Model Selection and Hyperparameter Optimization in Python": {
            "url": "https://towardsai.net/p/l/fake-news-detection-with-model-selection-and-hyperparameter-optimization-in-python",
            "description": "Last Updated on August 24, 2022 by Editorial Team Author(s): Giovanni Valdata Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A practical guide on fake news detection with model selection and hyperparameter optimization Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 24 Aug 2022 16:13:21 +0000",
            "pubdate_parsed": [
                2022,
                8,
                24
            ],
            "email_sent": true
        },
        "How Can You Drive Your Career in AI Positively Impacting Our Society?": {
            "url": "https://towardsai.net/p/l/how-can-you-drive-your-career-in-ai-positively-impacting-our-society",
            "description": "Last Updated on August 24, 2022 by Editorial Team Author(s): Jair Ribeiro Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Some practical insights on pursuing an engaging and satisfying career that can significantly impact the future of humanity. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 24 Aug 2022 13:33:52 +0000",
            "pubdate_parsed": [
                2022,
                8,
                24
            ],
            "email_sent": true
        },
        "Andrew Ngs 5 Crucial Mindset-shifts for Transitioning Into the Industry From Academia": {
            "url": "https://towardsai.net/p/l/andrew-ngs-5-crucial-mindset-shifts-for-transitioning-into-the-industry-from-academia",
            "description": "Last Updated on August 25, 2022 by Editorial Team Author(s): Arunn Thevapalan Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. It doesn&#x2019;t have to be confusing or hard if you listen to the experts Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 25 Aug 2022 16:04:02 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "Deep Learning A-Z Briefly Explained": {
            "url": "https://towardsai.net/p/l/deep-learning-a-z-briefly-explained",
            "description": "Last Updated on August 25, 2022 by Editorial Team Author(s): Gencay I. Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Refreshing and for Quick recall Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 25 Aug 2022 14:18:07 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "Data Science EssentialsStatistics (part I)": {
            "url": "https://towardsai.net/p/l/data-science-essentials%e2%80%8a-%e2%80%8astatistics-part-i",
            "description": "Last Updated on August 25, 2022 by Editorial Team Author(s): Nitin Chauhan Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What is Statistics? Where is statistics applied in? Types of statistics? Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 25 Aug 2022 14:18:04 +0000",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "Understanding the Why of Data Science and Machine Learning Is More Useful Than Knowing the How": {
            "url": "https://towardsai.net/p/l/understanding-the-why-of-data-science-and-machine-learning-is-more-useful-than-knowing-the-how",
            "description": "Last Updated on August 26, 2022 by Editorial Team Author(s): Suhas Maddali Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Learning the purpose of data science can be useful, especially in order to create a business impact in the organization. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 26 Aug 2022 16:21:57 +0000",
            "pubdate_parsed": [
                2022,
                8,
                26
            ],
            "email_sent": true
        },
        "Which NLP Task Does NOT Benefit From Pre-trained Language Models?": {
            "url": "https://towardsai.net/p/nlp/which-nlp-task-does-not-benefit-from-pre-trained-language-models",
            "description": "Last Updated on August 29, 2022 by Editorial Team Author(s): Nate Bush There is such a long history of pre-trained general language representation models with a massive impact that we take for granted that they are a completely 100% necessary foundation for all NLP tasks. There were two separate step function innovations that pushed the accuracy of all NLP tasks forward: (1) statistical language models like Word2Vec and GloVe and, more recently, (2) neural language models like\u00a0BERT,\u00a0ELMo, and recently\u00a0BLOOM. Inserting pre-trained neural language models at the beginning of a modeling workflow is\u00a0almost\u00a0guaranteed to increase performance, but there is at least one situation where it does not. Sidebar: why the sesame street theme?! Named Entity Recognition (NER) Look no further than the original BERT paper titled \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d to see a detailed analysis of how pre-trained BERT embeddings improve NER performance in section 5. The BERT diagram below shows a typical machine learning workflow for exploiting any language model for general NLP tasks. Source:\u00a0https://arxiv.org/pdf/1810.04805.pdf\u00a0\u2014 Overall pre-training and fine-tuning procedures for BERT The papers also show significant improvement on Question Answering (QA) evaluated against\u00a0SQUAD, and a hodgepodge of natural language understanding (NLU) tasks called\u00a0GLUE. Entity Disambiguation (ED) The global ED task also achieved new state-of-the-art results across multiple datasets using BERT. See the related work section of this \u201cGlobal Entity Disambiguation with BERT\u201d for a rundown of various workflows for applying BERT as a preprocessing step for ED. Extractive Summarization (ES) A simple variant of BERT achieving once again state-of-the-art performance on several ES datasets can be found in \u201cFine-tune BERT for Extractive Summarization\u201d. Sentiment Analysis (SA) Once again, sentiment analysis is equally graced by the existence of BERT language models in the recent paper \u201cBERT for Sentiment Analysis: Pre-trained and Fine-Tuned Alternatives\u201d. I could keep going\u2026 but I won\u2019t. The glory of pre-trained language models is obvious. We only need to stand on the shoulders of giants who spent countless hours preparing massive corpora of data, deploying expensive GPUs to pre-train these models for us. These models aren\u2019t a silver bullet, though. The main natural language task that has failed to show consistent performance improvements from sesame street and friends is\u00a0Neural Machine Translation (NMT). NMT usually doesn\u2019t benefit from Pre-trained language models It\u2019s difficult to find papers that discuss why it doesn\u2019t work, and it\u2019s easy to imagine why. Writing papers about what doesn\u2019t work is not very popular\u2026 and unlikely to gain recognition or be frequently quoted. Ah shoot \u2014 so why am I writing this article again? I found one paper that covered this topic: \u201cWhen and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?\u201d\u00a0and it was an interesting read. They break NMT down into two categories of tasks: NMT for low-resource languages NMT for high-resource languages What they mean by\u00a0low/high\u00a0resource language is in reference to the size of the parallel corpus that can be obtained. For the world&#8217;s most popular languages, it can be easy to find open-source large parallel corpora online. The largest such repository is\u00a0OPUS, the Open Parallel Corpus, which is an amazing resource for any machine learning engineer looking to train NMT models. Source:\u00a0OPUS\u00a0&#8211; high resource parallel corpus between English (en) and Chinese (zh) The image above shows that the open parallel corpus between English and Chinese has 103 million parallel sentences or 172K parallel documents. But what if you wanted to train an NMT model to translate Farsi to Chinese? In that case, you only have 6 million parallel sentences from 517 documents to work with. Source:\u00a0OPUS\u00a0&#8211; low resource parallel corpus between Farsi (fa) and Chinese (zh) As you might expect, low-resource languages benefit from pre-trained language models and are able to achieve better performance when fine-tuning the embeddings while back-propagating errors through the NMT network.\u00a0Surprisingly, however, for high-resource languages, the effect of using pre-trained language models as a pre-processing step before NMT model training does NOT result in performance gains. It\u2019s critical to point out that\u00a0language models only make sense to use for machine translation\u00a0if they are trained on both source and target language (for example, Chinese and English in the first example). These are commonly referred to as multilingual embedding models or language agnostic embeddings. They are able to achieve the interesting result that words in multiple languages achieve similar vector representations in the embedding space. Source:\u00a0AI Googleblog But how are multilingual language models trained? Turns out they are trained over the exact same data as NMT: a massive parallel corpus between the source and target language. So, is there a fundamental shortcoming to language models that prevent them from being effective for this NLP task? No, language models use the same data as NMT models, and they are both built from the same powerhouse building block: the transformer. To review, language models and NMT are trained over the same data, using very similar fundamental architectures. When you consider the similarities, there isn\u2019t really anything new that the language models are bringing to the table so it shouldn\u2019t be surprising to you that BERT, ELMo, ERNIE, and our other sesame street friends aren\u2019t appearing in NMT papers touting huge breakthroughs in model performance. A skeptical reader will likely be able to poke holes in this explanation. There are certainly devisable use cases were training an LM on a large parallel corpus but then training BERT + NMT workflow on a much smaller corpus would intuitively result in performance gains. But I think it\u2019s unlikely that a serious deep learning engineer would attempt to build an NMT model without all available data at their disposal\u2026 outside of purely academic curiosities. I tip-toed over some hairy details, so I recommend reading\u00a0the original paper\u00a0if you\u2019re interested! I hope you enjoyed this short exploration into the intuition behind what makes NLP algorithms successful. Please like, share, and follow for more deep learning knowledge. Which NLP Task Does NOT Benefit From Pre-trained Language Models?\u00a0was originally published in Towards AI\u00a0on Medium, where people are continuing the conversation by [&#8230;]",
            "pubdate": "Mon, 29 Aug 2022 13:34:37 +0000",
            "pubdate_parsed": [
                2022,
                8,
                29
            ],
            "email_sent": true
        },
        "Improve Your Model Validation With TorchMetrics": {
            "url": "https://towardsai.net/p/l/improve-your-model-validation-with-torchmetrics",
            "description": "Last Updated on August 29, 2022 by Editorial Team Author(s): Mattia Gatti Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A guide on how to use TorchMetrics with PyTorch or PyTorch Lightning models Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 30 Aug 2022 00:14:55 +0000",
            "pubdate_parsed": [
                2022,
                8,
                30
            ],
            "email_sent": true
        },
        "Impact of Optimizers in Image Classifiers": {
            "url": "https://towardsai.net/p/l/impact-of-optimizers-in-image-classifiers",
            "description": "Last Updated on August 30, 2022 by Editorial Team Author(s): Toluwani Aremu Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Photo by Ales Krivec on\u00a0Unsplash INTRODUCTION Ever wondered why a DNN fails to perform as high as expected when it comes to accuracy, especially when there are official or unofficial reports of experts and enthusiasts getting some top performance with the same network and on that same dataset you are using? I remember having hard times trying to wrap my head around the thought that my models just failed when it was expected to perform well. What causes this? In reality, there are lots of factors with varying levels of potential to impact the performance of your architecture. However, I\u2019ll discuss just one in this article. This factor is \u201cThe choice of Optimization algorithm to\u00a0use\u201d. What is an optimizer? An optimizer is a function or algorithm that is created and used for neural network attribute modification (i.e., weights, learning rates) for the purpose of speeding up convergence while minimizing loss and maximizing accuracy. DNNs use millions of billions of parameters, and you need the right weights to ensure that your DNN learns well from the given data while generalizing and adapting well for a good performance on unseen related\u00a0data. Different optimization algorithms have been built over the years, and some of these algorithms have advantages over others, as well as their cons. Therefore, it is imperative to know the basics of these algorithms, as well as understand the problem being worked on so that we can select the best optimizer to work\u00a0with. Furthermore, I noticed that a lot of researchers use the SGD-M (Stochastic Gradient Descent with Momentum) optimizer, but in the industry, Adam is favored more. In this article, I will give brief high-level descriptions of the most popular optimizers being used in the AI world. Actually, I had to do a number of experiments to see the difference between these optimizers and answer some questions I have about the use of these optimizers, as well as give clues on which optimizer is the best and when/how to use them based on my observations. BASIC DESCRIPTION OF DIFFERENT OPTIMIZERS https://ruthwik.github.io/machinelearning/2018-01-15-gradient-descent/ In this section, I will briefly discuss the Stochastic Gradient Descent with Momentum(SGDM), Adaptive Gradient Algorithm (ADAGRAD), Root Mean Squared Propagation (RMSProp), and the Adam optimizers. SGDM: Since the Gradient Descent (GD) optimizer uses the whole training data to update the model\u2019s weights, it becomes so computationally expensive when we have millions of data points. Due to this, the Stochastic Gradient Descent (SGD) was created to solve this problem by using each datapoint to update the weights. Still, this was computationally expensive for Neural Networks (NN)each datapoint used in the NN needed both forward and back propagations. Also, with SGD, we can\u2019t increase the learning rate while it tries to reach the global minimum. This makes convergence very slow while utilizing the SGD. The SGDM was the solution to that, as it added a momentum term to the normal SGD, which improved the speed of convergence. For deeper explanations, click\u00a0here. Image by Sebastian Ruder ADAGRAD: Adaptive Gradient Algorithm (Adagrad) is an algorithm for gradient-based optimization which tries to adapt the learning rate to the parameters. The learning rate fits the parameters component by component by incorporating insights from past observations. It makes minor updates to parameters associated with frequent features and major updates to those with features that aren\u2019t occurring frequently. Adagrad also eliminates the need for tuning the learning rate manually as it automatically updates the learning rate based on the parameters. However, the learning rate shrinks fast, making the model think it is close to achieving convergence and stops somewhat short of the expected performance. To learn more, click\u00a0here. RMSProp: Proposed by Geoffrey Hinton (even though it remains unpublished), the RMSProp is an extension of the GD and the AdaGrad version of gradient descent that uses a decaying average of partial gradients in the adaptation of the step size for each parameter. It was discovered that the magnitude of gradients can be different for different parameters and could change during the training. Therefore, Adagrad&#039;s automatic choice of learning rate could be the nonoptimized choice. Hinton solved this by updating the learned weights using a moving average of the squared gradients. To learn more, click\u00a0here. Adam: This optimizer was proposed by Diederik Kingma and Jimmy Ba in 2015 and could arguably be regarded as the most popular optimizer ever created. It combines the advantages and benefits of SGDM and RMSProp in the sense that it uses momentum From SGDM and scaling from RMSProp. It is computationally efficient, unlike both GD and SGD, and requires only a little memory. It was designed to be used on problems with very noisy/sparse gradients. To learn more, click here or\u00a0here. EXPERIMENTS Photo by Kevin Ku on\u00a0Unsplash Due to the size of my computing resource, I decided to focus on using LeNet and AlexNet on the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50000 training images and 10000 test images. I trained these models for 50 epochs using the SGD, SGDM, Adagrad, RMSProp, and Adam optimizers. For the SGDM, I used a momentum of 0.9. The global learning rate for my first set of experiments was 0.001\u00a0(1e-3). Note: I am not seeking very good results. I am instead trying to see the impact of each optimizer on the model\u2019s performance. I start by calling the important libraries: https://medium.com/media/425968addc0d7bb7e1dbe8466f7c788b/href Then, I loaded and transformed the CIFAR-10\u00a0dataset: https://medium.com/media/2aed5c7db98e0cf2058155c6832500ea/href The LeNet and AlexNet\u00a0models: https://medium.com/media/f92213170ff3ae0d7646ce4ed9487925/href To get the full code, check out this repository (give it a star if you don\u2019t\u00a0mind). The results are as\u00a0follows. On the LeNet model, the test accuracy of SGDM was the highest at almost 70%, while [&#8230;]",
            "pubdate": "Tue, 30 Aug 2022 12:47:29 +0000",
            "pubdate_parsed": [
                2022,
                8,
                30
            ],
            "email_sent": true
        },
        "Easy Object Detection with Transformers: Simple Implementation of Pix2Seq model in PyTorch": {
            "url": "https://towardsai.net/p/l/easy-object-detection-with-transformers-simple-implementation-of-pix2seq-model-in-pytorch",
            "description": "Last Updated on August 30, 2022 by Editorial Team Author(s): Moein Shariatnia Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Easy Object Detection with Transformers: Simple Implementation of Pix2Seq Model in\u00a0PyTorch My Simple Implementation of Pix2Seq &#124; Image by\u00a0author Introduction Object detection does not have to be a difficult task! I clearly remember the first time I implemented YOLO from scratch, and it was a pain to understand how it works under the hood. For beginners in computer vision applications, I believe that object detection is the hardest one to understand among classification, segmentation, etc. Once I first heard about the paper \u201cPix2seq: A Language Modeling Framework for Object Detection\u201d from ICLR 2022, I got pretty excited, and I was sure my next blog post would be about it; so, here I am writing this post and hoping that you\u2019ll like it and find the pix2seq model easy to understand and implement. At the end of this tutorial, you\u2019ll learn to implement a simple model for object detection which produces the following results: Our final model\u2019s predictions on validation set &#124; Image by\u00a0Author I have made all of my code available as Google Colab Notebook and a Kaggle Notebook. I\u2019ve also put the whole project and codes on my\u00a0GitHub. What\u2019s interesting about this\u00a0paper The idea is pretty simple: Reframe the object detection problem as a task of text (token) generation! We want the model to \u201ctell us\u201d what objects exist in the image and also the (x, y) coordinates of their bounding boxes (bboxes), all in a specific format in the generated sequence, just like text generation! How pix2seq works: it generates a sequence of tokens telling where each object is (BOS=beginning of sentence, EOS=end of sentence) &#124; Image by\u00a0author As you see, the object detection task is transformed into an image-captioning-ish task: describe the image in the text (sequence) but this time tell us exactly where the objects\u00a0are. Pix2Seq: Simple Implementation Needed Modules The closest task to what Pix2Seq does is image-captioning. So, we are going to need an image encoder to convert an image into vectors of hidden representation and then a decoder to take the image representations and those of the previously generated tokens and predict the next token. We also need a tokenizer to convert object classes and coordinates into tokens that form their special vocabulary, just like the words in a natural language. My Simple Implementation of\u00a0Pix2Seq My Simple Implementation of Pix2Seq &#124; Image by\u00a0author You can see the high-level pipeline of this project in the picture above. As you see, we need a dataset of images and their bboxes for which we will use Pascal VOC 2012 dataset. Next, we will write our own tokenizer from scratch to convert the bbox classes and coordinates into a sequence of tokens. Then, we will use DeiT (from this paper) as our image encoder and feed the image embeddings to a vanilla Transformer Decoder (from this paper). The decoder\u2019s task is to predict the next token given the previous ones. The outputs of the decoder are given to the language modeling loss function. The codes of this tutorial are available in the following links:&#8211; Google Colab Notebook&#8211; Kaggle Notebook&#8211; My GitHub\u00a0repo Dataset As I mentioned earlier, we will use VOC 2012 dataset with images and their corresponding objects from 20 classes. The paper uses the COCO dataset, which is an order of magnitude larger than VOC, and they also pre-train the models on a much larger dataset before training on COCO. But, to stay simple, I\u2019m going to use this rather small VOC\u00a0dataset. https://medium.com/media/162ec2395c7e4072b199f4f644e15ebd/href We need a PyTorch dataset class that gives us an image and its bbox coordinates and classes in the form of a sequence. https://medium.com/media/2ec93cbbe19051b30e8609a0d49051a0/href As you see, most of the code here is what you expect from a simple dataset for classification, but there are small differences too. We need a Tokenizer to convert our labels and bbox coordinates (x and y) to a sequence so that we can perform train our model for the language modeling task (predicting the next tokens conditioned on the previously seen\u00a0tokens). Tokenizer How are we going to convert this information into a sequence? Well, it\u2019s not that difficult. To represent an object in an image, we need 5 numbers: 4 coordinate numbers and 1 to indicate which class it belongs\u00a0to. You actually need to know the coordinates of 2 points of a bounding box to be able to draw it in an image; in pascal format, we use the top left point and the bottom right point of the bbox as those 2 critical points, and each point is represented by its x and y values \u2192 so, we will need 4 numbers overall to draw a bounding box. You can see alternative formats to represent a bounding box down below. Also, look at where the start of x and y axis is (the 0, 0\u00a0point). Different formats to represent a bounding box with its coordinates &#124; Image from Albumentations docs As you see in the dataset\u2019s code, we give the bbox coordinates and labels to our tokenizer and get a simple list of tokens out. The tokenizer needs to do the following tasks: mark the start and end of the sequence w/e special tokens (BOS and EOS\u00a0tokens). quantize the continuous value of coordinates (we can have x=34.7 as the coordinate of a point, but we need discrete values like 34 as our tokens because we are finally doing classification on a finite set of\u00a0tokens) encode the label of the objects into their corresponding tokens randomize the order of objects in the final sequence (more on this\u00a0below) If you are familiar with NLP applications, these steps might sound familiar to you [&#8230;]",
            "pubdate": "Tue, 30 Aug 2022 12:14:44 +0000",
            "pubdate_parsed": [
                2022,
                8,
                30
            ],
            "email_sent": true
        },
        "List of Important Libraries for Machine Learning and Data Science in Python": {
            "url": "https://towardsai.net/p/l/list-of-important-libraries-for-machine-learning-and-data-science-in-python",
            "description": "Last Updated on August 30, 2022 by Editorial Team Author(s): Suhas Maddali Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Understanding the use of various libraries in python and machine learning is handy for professionals in the field of data science. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 31 Aug 2022 00:13:46 +0000",
            "pubdate_parsed": [
                2022,
                8,
                31
            ],
            "email_sent": true
        },
        "What is Panoptic Scene Graph Generation (PSG)?": {
            "url": "https://towardsai.net/p/l/what-is-panoptic-scene-graph-generation-psg",
            "description": "Last Updated on September 1, 2022 by Editorial Team Author(s): Louis Bouchard Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. PSG&#x200a;&#x2014;&#x200a;A New Challenging Task for AI Explained Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 02 Sep 2022 00:13:45 +0000",
            "pubdate_parsed": [
                2022,
                9,
                2
            ],
            "email_sent": true
        },
        "Recurrent Neural Networks: A Very Special Kind of Mnemonist": {
            "url": "https://towardsai.net/p/l/recurrent-neural-networks-a-very-special-kind-of-mnemonist",
            "description": "Last Updated on September 7, 2022 by Editorial Team Author(s): Diego Manfre Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Preserving information is another way of remembering Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 08 Sep 2022 00:13:50 +0000",
            "pubdate_parsed": [
                2022,
                9,
                8
            ],
            "email_sent": true
        },
        "Difference Between Normalization and Standardization": {
            "url": "https://towardsai.net/p/l/difference-between-normalization-and-standardization",
            "description": "Last Updated on September 10, 2022 by Editorial Team Author(s): Chetan Ambi Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Understand the differences between normalization and standardization, different methods, and most importantly, when should you consider&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 11 Sep 2022 00:08:46 +0000",
            "pubdate_parsed": [
                2022,
                9,
                11
            ],
            "email_sent": true
        },
        "From Classification to Ordinal Regression": {
            "url": "https://towardsai.net/p/l/from-classification-to-ordinal-regression",
            "description": "Last Updated on September 13, 2022 by Editorial Team Author(s): Topaz Gilad Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Unlock the Potential of Your\u00a0Labels AI-generated (Midjourney): \u201clion-elephant hybrid, illustration, children\u2019s book\u201d and \u201celephant with lion\u00a0face\u201d \u201cIs a lion closer to be a giraffe or an elephant?\u201d It is a question no one asked. Ever. Classifying lions, elephants and giraffes is a straightforward classification task. As such, it can be mostly addressed with a cross-entropy loss. Should the task of classifying someone as a child/adult/elderly be addressed in the same\u00a0way? In this blog post, we will overview best practice approaches and papers for:(1) Addressing ordered classification.(2) Coarse classification labels into regression continuous predictions.(3) How to choose.(4) How to evaluate.The papers reviewed in this post focus on deep learning models, but the main concepts apply and may be adapted for other ML architectures as\u00a0well. Discrete Labels in a Continuous World \u201cThe world is continuous, but the mind is discrete.\u201d&#8211; David Mumford, ICM\u00a02002 We often define categories when breaking down a real-world problem into an ML-based solution. However, real target values may be continuous or at least ordered. This is something to consider and even leverage in the design of your ML model. Are you facing what seems as a classification problem? Take a moment to understand the hidden relations between your \u201cclasses\u201d. Let\u2019s list some classification examples:&#8211; Online ratings: 1\u20135 stars.&#8211; Medical diagnosis index\u200a\u2014\u200astage 1/2/3.&#8211; Face pose estimation [1]\u200a\u2014\u200a45/90/180 degrees.&#8211; Age estimation and so\u00a0on. Left to right: Age prediction: https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/. image credit: https://en.wikipedia.org/wiki/What%27s_My_Age_Again%3F Predict rating (ilmakiage.com), medical severity index of dVRS, Zhu et al. https://www.ahajournals.org/doi/10.1161/strokeaha.110.591586 Know Your\u00a0Data Approaching the design of an ML solution, some of the first steps are\u00a0:(a) Understand what data you currently have / what data you are likely to obtain in a reasonable time. In some cases, you will have continuous measurements associated with your data. For example, blood measurements or information on exact age. More often than not, those will not be available. Manual annotation of fine-grained labels is an extremely difficult and time-consuming task. Therefore, in many cases, all you will have are coarse categorical labels. Especially where an expert human opinion is required. (b) Explore the domain. Ask your data domain expert to clarify the relations between the target classes. Every bit of prior knowledge or assumptions they assume. Question the format of the output they ask for. For example: Can a smooth real number between 0 to 1 be a better product use than a failed/passed/excellent student score predictor? If your classes are indeed independent, this is not the blog post for you!However, if they are dependent, ask yourself:(1) Do the labels have an order?(2) Do you only care about the order, or are some labels closer to their nearest labels than others? Are the \u201csteps\u201d of distances between labels\u00a0uniform? Regression to the\u00a0Rescue Logistic RegressionMost are familiar with logistic regression to discriminate between classes. While the target labels are discrete, the estimated class confidence is continuous. Let\u2019s consider the following case: The consequences of wrongly classifying class A as class B is not as severe as a mistake between A and C. One approach can be to use a weighted loss with the risk coefficient of each type of mistake\u00a0[2]. Ordinal RegressionAnother approach can be to encode the one-hot vector labels differently, as suggested in the NNRank paper [3]. For a hands-on example, I recommend Gruber\u2019s post and Kotwani\u2019s. Note that if modeling as an aggregation of an ensemble of binary classifiers, inconsistencies in class predictions may occur. Cao et al. 2020 CORAL suggest a solution to achieve consistency of rank\u00a0[4]. Figure from Cao et al 2020 CORAL [4]: inconsistent rank when aggregating binary classifiers (left) compared to the desired outcome\u00a0(right) Linear Regression\u200a\u2014\u200aSeeing Beyond the Available Labels Netflix launched lately a double thumbs up. By that, they expanded the 3 (dislike/indifferent/like) into 4 categories. This is a better distinction inside what used to be the \u201cliked\u201d category: liked vs. liked a lot. Imagine you have many coarse \u201cliked\u201d votes from the past and only a few new \u201cdouble thumbs\u201d votes. Estimating a score of user satisfaction (instead of a class) gives you better adaptivity to the new user\u00a0inputs. Qin et al BioeNet 2020 showcases a strategy to leverage coarse labels into a fine-grained linear regression [5]. One of the important things to consider when taking that path is to evaluate your inner-class order. We will discuss next\u00a0how. Figures from Qin et al. BioeNet 2020: Learning fine-grained estimation from coarse labels\u00a0[5] Another possible benefit of transforming your discrete integer target labels into real numbers is the positive effect of soft labels. As shown by the Google Brain team (2020), using soft labels not only reduced over-confidence but also improved the calibration even without temperature scaling\u00a0[6\u20138]. Make Sure You Are in the Right Direction Shape and Location of ClustersI am a big believer in the analysis of your DNN embedded feature space. Create a T-SNE of your test set feature space. Follow the illustrations in the figure below. With a cross-entropy loss, you expect each cluster to condense (minimize inner-class variance) and move away from one another (maximize intra-class variance). However, in ordinal regression, you expect to see the clusters in the right order of proximity in the feature space. For linear regression with an MSE loss, for example, you will not only expect to see the right order but also a continuous order between classes with less margin from one cluster to the next. If you still do see a noticeable margin, this may also indicate your test set is lacking border examples. Illustration by the\u00a0author Relations Between Samples from the Same CategoryIf you can get your hands on a few finer-grained labeled samples, you can use them [&#8230;]",
            "pubdate": "Wed, 14 Sep 2022 00:18:36 +0000",
            "pubdate_parsed": [
                2022,
                9,
                14
            ],
            "email_sent": true
        },
        "Is Google AI Sentient? Heres What I Really Think About LaMDA": {
            "url": "https://towardsai.net/p/l/is-google-ai-sentient-heres-what-i-really-think-about-lamda",
            "description": "Last Updated on September 14, 2022 by Editorial Team Author(s): Edoardo Bianchi Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Let&#x2019;s try to figure out if an AI can feel emotions Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 14 Sep 2022 12:18:52 +0000",
            "pubdate_parsed": [
                2022,
                9,
                14
            ],
            "email_sent": true
        },
        "Univariate Linear Regression From Scratch": {
            "url": "https://towardsai.net/p/l/univariate-linear-regression-from-scratch",
            "description": "Last Updated on September 19, 2022 by Editorial Team Author(s): Erkan Hatipo\u011flu Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. With Code in\u00a0Python Univariate Linear Regression\u200a\u2014\u200aImage by\u00a0Author Introduction Generally, one of the first subjects of a Machine Learning Course is Linear Regression, which is not very complex and easy to understand. Moreover, it includes many Machine Learning notions that can be used later in more complex concepts. So it is reasonable to start with Linear Regression to a new Machine Learning basics\u00a0series. In this blog post, we will learn about Univariate Linear Regression which means Linear Regression with just one variable. At the end of this article, we will understand fundamental Machine Learning concepts such as the hypothesis, cost function, and gradient descent. In addition, we will write the necessary code for those functions and train/test a linear model for a Kaggle dataset using Python\u00a0[1]. Before diving into Univariate Linear Regression, let&#039;s briefly talk about some basic concepts: Basic Concepts Machine Learning is the subfield of Artificial Intelligence that uses data to program computers for some tasks. A typical Machine Learning application uses a dataset and a learning algorithm to make a model that can then predict the outputs for new inputs to the application. Machine Learning applications are generally divided into three categories: Supervised Learning Unsupervised Learning Reinforcement Learning In supervised learning [2], the data have identical features that can be mapped to some label. Supervised learning has two\u00a0types. It is called classification if the number of labels is limited and can be transformed into some categories. Identifying animals or handwritten digits in images are examples of classification. If the set of labels is not restricted to some small number of elements but can be assumed infinitely many, then it is called regression. Predicting the price of houses in a region or gas usage per kilometer of cars are regression examples. In unsupervised learning [3], the dataset has no labeled data. After the teaching process, the model discovers the patterns in the data. Categorizing the customers of an e-commerce website is an example of unsupervised learning. Reinforcement learning [4] uses intelligent agents who act according to reward and punishment mechanisms [5]. It is mainly used in computer\u00a0gaming. Linear Regression As we have said above, regression maps inputs to infinitely many outputs. In addition, Linear Regression suggests a linear relationship between the inputs and outcomes [6]; it is challenging to visualize this if we have more than one input variable. If we have one input variable, we need to draw a line in the XY plane; if we have two input variables, we need to construct a plane in the three-dimensional coordinate system. More than two input variables are extremely tough to imagine. As a result, we will demonstrate Linear regression using univariate data. Below are the housing prices from Portland, Oregon dataset [7], which we will use as an\u00a0example. Housing Prices from Portland, Oregon\u200a\u2014\u200aImage by\u00a0Author In the graph above, the y-axis is the price (K$) of some houses in Portland, and the x-axis is the house&#039;s size (feet\u00b2). We want to draw a straight line on this graph (such as the image below) so that when we know the size of a new home not in this dataset, we can predict its price by using that\u00a0line. Housing Prices from Portland, Oregon with Hypothesis\u2014 Image by\u00a0Author For example, we can predict that the price of a 3000 feet\u00b2 house is approximately 500 K$, as seen in the graph\u00a0below. Calculating New Inputs for Housing Prices from Portland, Oregon\u200a\u2014\u200aImage by\u00a0Author So without further ado, let&#039;s dive into more profound concepts to understand how to draw that\u00a0line. Note that this article uses the Linear Regression dataset [1] and Univariate Linear Regression from Scratch notebook [8] from Kaggle. As described in the content part of the dataset, it has a training dataset of 700 and a test dataset of 300 elements. The data consists of pairs of (x,y) where x\u2019s are numbers between 0 and 100 and y\u2019s have been generated using the Excel function NORMINV(RAND(), x, 3). It is also given that the best estimate for y should be\u00a0x. First, we must write three functions to find a univariate linear regression solution on a dataset. These functions are for the hypothesis, cost, and gradient descent calculations in order. Foremost, we will explain them one by\u00a0one. The Hypothesis We aim to find a model we can use to predict new inputs. A hypothesis is a potential model for a machine learning system\u00a0[9]. As seen in the figure below, we can find the hypothesis for a dataset by using a learning algorithm [10]. Later we will talk about how well this hypothesis fits our data but for now, let&#039;s just define the hypothesis formula. The Hypothesis\u200a\u2014\u200aImage by\u00a0Author For a univariate linear regression task, the hypothesis is of the\u00a0form: The Hypothesis Formula\u200a\u2014\u200aImage by\u00a0Author The intersection of the y-axis with the hypothesis line is called intercept. This is \u03b8\u2080 in our formula, and the slope of the hypothesis line is\u00a0\u03b8\u2081. If we know x, \u03b8\u2080, and \u03b8\u2081 values, we can predict the target value for a machine learning system. For example, in the housing prices example above, \u03b8\u2080 = 0.00008, and \u03b8\u2081 = 0.16538 for the red line. So for a house of 3000 feet\u00b2 (x=3000), the expected price\u00a0is: As can be seen, we need three parameters to write the hypothesis function. These are x (input), \u03b8\u2080, and \u03b8\u2081. So, let&#039;s write the Python function for the hypothesis: https://medium.com/media/3c752accc7341b367a1a05bcd7148652/href The process is pretty straightforward. We just need to return (\u03b8\u2080+ \u03b8\u2081*\u00a0x). The Cost\u00a0Function When we have a hypothesis (i.e., we know the values of \u03b8\u2080 and \u03b8\u2081), we must decide how well this hypothesis fits our data. Turning back to the [&#8230;]",
            "pubdate": "Mon, 19 Sep 2022 12:13:16 +0000",
            "pubdate_parsed": [
                2022,
                9,
                19
            ],
            "email_sent": true
        },
        "Important Techniques to Handle Imbalanced Data in Machine Learning Python": {
            "url": "https://towardsai.net/p/l/important-techniques-to-handle-imbalanced-data-in-machine-learning-python",
            "description": "Last Updated on September 19, 2022 by Editorial Team Author(s): Muttineni Sai Rohith Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. How to Handle Imbalanced Data in ML Classification using\u00a0Python In this article, we will discuss what is Imbalanced Data, the Metrics we should use to evaluate the model with Imbalanced Data, and the Techniques used to Handle Imbalanced Data. While doing binary classification, almost every data scientist might have encountered the problem of handling Imbalanced Data. Generally Imbalanced data occurs when the datasets are distributed unequally i.e. when the frequency of data points or the number of rows in one class is much more than in other classes, then the data is imbalanced. For example, suppose we have a covid Dataset, and our target class is whether a person is having covid or not, if the positive ratio is 10% in our class and the negative ratio is 90%, then we can say that our Data is imbalanced. Image By\u00a0Author Problem with Imbalanced Data Most machine learning algorithms are designed in a way to improve accuracy and reduces errors. In this process, they don\u2019t consider the distribution of classes. Also, standard machine learning algorithms like Decision trees and Logistic Regression have a bias toward Majority classes and tend to ignore minority classes. So in these cases, even though the model has 95% accuracy, it cannot be said as a perfect model as the frequency of the number of classes in testing data may be 95%, and 5% wrongly predicted data must be from the minority\u00a0class. Accuracy pitfall Before diving into the handling of Imbalanced Datasets, Let\u2019s understand the metrics we should use while evaluating the models. Generally, accuracy_score is calculated as the ratio of the number of correct predictions to the total number of predictions. Accuracy = Number of Correct Predictions / Total Number of Predictions. So we can see that accuracy_score will not consider the distribution of classes. It only focuses on the Number of Correct Predictions. So Even though we get 95+ accuracy, as shown in the above example, we can\u2019t guarantee the performance of the model and its prediction of the minority\u00a0class. So for classification techniques, instead of accuracy_score, it is recommended to use Confusion Matrix, precision_score, recall_score, and Area under the ROC Curve(AUC) as Evaluation Metrics. Handling Imbalanced Data A technique that is widely used while handling imbalanced data is Sampling. There are two types of Sampling\u00a0\u2014 Under Sampling Over Sampling In Under Sampling, samples are removed from the majority class, whereas, in Over Sampling, samples are added to the minority\u00a0class. To demonstrate the usage of the above techniques, initially, we will consider an example without Handling Imbalanced Data. Dataset used can be found\u00a0here. Importing Libraries # import necessary modules import pandas as pdimport matplotlib.pyplot as pltimport numpy as npfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score Loading Data df = pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\") Preparing Data # normalise the amount column df[&#039;normAmount&#039;] = StandardScaler().fit_transform(np.array( df[&#039;Amount&#039;]).reshape(-1, 1)) # drop Time and Amount columns as they are not relevant for prediction purposedata = df.drop([&#039;Time&#039;, &#039;Amount&#039;], axis = 1) # as you can see there are 492 fraud transactions.data[&#039;Class&#039;].value_counts() Output X = data.drop([&#039;Class&#039;], axis = 1)y = data[\"Class\"] Splitting train-test-data from sklearn.model_selection import train_test_split # split into 70:30 ratioX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) # describes info about train and test set print(\"Number transactions X_train dataset: \", X_train.shape)print(\"Number transactions y_train dataset: \", y_train.shape)print(\"Number transactions X_test dataset: \", X_test.shape)print(\"Number transactions y_test dataset: \", y_test.shape) Output Classification # logistic regression objectlr = LogisticRegression() # train the model on train setlr.fit(X_train, y_train.ravel())predictions = lr.predict(X_test) # print classification reportprint(\"Accuracy score is: \",accuracy_score(y_test, predictions))print(\"Recall score is: \",recall_score(y_test, predictions))print(\"Precision score is: \",precision_score(y_test, predictions))print(\"Confusion Matrix: \\n\",confusion_matrix(y_test, predictions)) Output As we can see, even though the Accuracy score is 99.9%, we can see that the Recall score is 61.9% which is relatively low, and the Precision score is\u00a088.3%. This is because the dataset is imbalanced, and now we will try to improve these scores using the techniques mentioned above. Handling Imbalanced Data using Under\u00a0Sampling Under Sampling involves the removal of records from the majority class to balance out with the minority\u00a0class. The simplest technique involved in under-sampling is Random under-sampling. This technique involves the removal of random records from the majority class. But there will be a loss of important information if we randomly remove the rows. So various techniques are implemented for undersampling the data. One such import technique is NearMiss Undersampling. NearMiss Undersampling In this technique, data points are selected based on the distance between the majority and minority classes. It has 3 different versions, and each version considers the different data points from the majority\u00a0class. Version 1\u200a\u2014\u200aIt selects data points of the majority class whose average distances to the K closest instances of minority class is the\u00a0smallest Version 2\u200a\u2014\u200aIt selects data points of the majority class whose average distances to the K farthest instances of minority class is the\u00a0smallest Version 3\u200a\u2014\u200aIt works in 2 steps. Firstly, for each minority class instance, their M nearest neighbors will be stored. Then finally, the majority class instances are selected for which the average distance to the N nearest neighbors is the\u00a0largest. In short, Version 3 is the more accurate version as it will remove the tomek links and makes the classification process easy as it forms a decision boundary. NearMiss Undersampling # apply near missfrom imblearn.under_sampling import NearMiss nr = NearMiss() X_train_miss, y_train_miss = nr.fit_resample(X_train, y_train.ravel()) print(&#039;After Undersampling, the shape of train_X: {}&#039;.format(X_train_miss.shape)) print(&#039;After Undersampling, the shape of train_y: {} \\n&#039;.format(y_train_miss.shape)) print(\"After Undersampling, counts of label &#039;1&#039;: {}\".format(sum(y_train_miss == 1))) print(\"After Undersampling, counts of label &#039;0&#039;: {}\".format(sum(y_train_miss == 0))) Output We have undersampled the majority class\u200a\u2014\u200a0and balanced it out [&#8230;]",
            "pubdate": "Mon, 19 Sep 2022 07:18:46 +0000",
            "pubdate_parsed": [
                2022,
                9,
                19
            ],
            "email_sent": true
        },
        "Latent Diffusion Explained Simply (with Pokmon)": {
            "url": "https://towardsai.net/p/l/latent-diffusion-explained-simply-with-pokemon",
            "description": "Last Updated on September 19, 2022 by Editorial Team Author(s): Leonardo Castorina Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. From Text to Image, Image to Image and Inpainting&#x200a;&#x2014;&#x200a;the Latent Diffusion revolution Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 19 Sep 2022 07:03:45 +0000",
            "pubdate_parsed": [
                2022,
                9,
                19
            ],
            "email_sent": true
        },
        "This Is How You Can Explain SVD to a 7-Year-Old": {
            "url": "https://towardsai.net/p/l/this-is-how-you-can-explain-svd-to-a-7-year-old",
            "description": "Last Updated on September 20, 2022 by Editorial Team Author(s): Paul Iusztin Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Understand the intuition behind the Singular Value Decomposition (SVD) algorithm. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 20 Sep 2022 12:08:45 +0000",
            "pubdate_parsed": [
                2022,
                9,
                20
            ],
            "email_sent": true
        },
        "The Parameters of Your Model Are Correlated. Now, What?": {
            "url": "https://towardsai.net/p/l/the-parameters-of-your-model-are-correlated-now-what",
            "description": "Last Updated on September 20, 2022 by Editorial Team Author(s): Kevin Berlemont, PhD Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Image generated using the title as the prompt on Stable Diffusion You just got yourself a nice set of data, for example, real estate prices with the characteristics of the homes. A question you could ask yourself is: How do I predict the price of a house based on its characteristics? The first model that comes to mind would be a linear model taking the characteristics as inputs. Let\u2019s say that we have access to the following characteristics: Square footage of the\u00a0house Number of\u00a0bedrooms Localization Year of construction The problem of predicting housing prices with such datasets has been intensively studied (https://www.kaggle.com/c/house-prices-advanced-regression-techniques), and it is possible to obtain an accurate model. I am using this framework as a toy model to highlight how easily parameters can be correlated in a model. The size of the house and the number of bedrooms are correlated, but none of them can really be used to replace the other in the estimation, thus, the estimation of the model parameters could be rendered difficult. In particular, sampling methods such as Markov Chains Monte-Carlo can be inefficient when the target distribution of the parameters is unknown. If this is a problem that might not arise when estimating housing prices, it is thus necessary to have it in mind when fitting more complicated models. In this post, I will describe the most used sampling method, Markov Chains Monte-Carlo(MCMC), and highlight potential reasons for the inefficacy of this method when parameters are correlated. In the second part, I will describe a new sampling algorithm that is unaffected by the correlation between parameters, differential evolution MCMC (DE-MCMC). In addition, I will provide some Python code samples for both of the\u00a0methods. Markov Chain Monte-Carlo MCMC is a sampling method that has been extensively used in Bayesian statistics, especially when estimating posterior priors. In one sentence, MCMC can be considered a random walk process used to estimate a target distribution (the distribution of the parameters). The Metropolis-Hastings is the most used algorithm within the MCMC class\u00a0[1]. Let\u2019s P = (p1, p2,\u2026) the vector of parameters of our model. Then, the algorithm picks an initial value P1 for the parameters and samples a candidate value P* using a proposal distribution P =* K(P1). One typical proposal distribution is usually a Gaussian distribution centered around P1. Afterward, the candidate P* is accepted with the probability: Let\u2019s decompose this formula. M(P*) represents the target distribution. For example, in the case of a model fitting, the target distribution could be the exponential of the error function. Using the exponential function enables the switch from an error function to a probability distribution. q(Pt&#124;P*) represents the density of the proposal distribution evaluated at Pt with parameters P*. In other words, it represents how probable it is to have selected Pt if we were to sample from P*. This term is necessary for the stationarity of the algorithm. If the proposal is accepted, then the new starting point of the chain is P*. Otherwise, the chain does not move. This process continues until a specified number of samples has been reached. This process can be interpreted as converging to the process of sampling the target distribution and, thus, estimating the parameters P. Below is a Python code sample using the package mc3 (modified from their tutorial) to simulate an MCMC algorithm to fit the parameters of a quadratic model: https://medium.com/media/1e4fd0857b6008fd9eb520cdd89d8393/href Now, if we go back to the case where some of our parameters are correlated. If the proposal distribution does not explicitly take into account inter-parameter correlations, then a lot of candidate values are not going to lie within the target distribution. For example, in the figure below, the gray cloud represents a fictional distribution of two parameters. The circle represents the proposal distribution. A big part of the circle lies outside of the gray distribution. Time step of MCMC. The circle represents the candidate distribution. (Image by\u00a0me) This will lead to substantial rejection rates, and the MCMC is going to be very time-consuming. Thus, it might not always be appropriate to use this Bayesian technic to fit your model. The good news is that there is a class of algorithms that can handle high-dimensional, highly correlated problems: population-based MCMC. Diffusion Evolution Markov Chains Monte-Carlo Differential evolution was brought to MCMC algorithms in 2006 [2]. The idea behind the method is the following. Rather than adding noise to the current candidate value in order to produce the next step, it uses multiple interacting chains to produce the candidate value. When chains have different states, they will be used to generate new proposals for other chains. Or in other words, a candidate value of one chain is based on some weighted combination of the values of the other\u00a0chains. The combination of the values is performed the following way. P_k represents the current state of the chain k. Then the candidate value, P*, is generated using the differences between the states of two chains, picked at random, P_m and P_n. In summary, the process is the following: Where the factor \u0263 represents the jump rate. The acceptance rate of the new candidate is similar to the MCMC algorithm, using, for example, the Metropolis-Hastings rule. A rule of thumb for using the jump rate\u00a0is: with d the number of dimensions of the parameters. The DE-MCMC algorithm is represented in the figure\u00a0below: Example of DE-MCMC step (Image by\u00a0me) Why is DE-MCMC efficient at sampling across correlated parameters? The different MCMC chains are being used to inform the states in one chain. Thus, the correlations between the chains (hence correlations between samples) are being directly used [&#8230;]",
            "pubdate": "Wed, 21 Sep 2022 00:14:00 +0000",
            "pubdate_parsed": [
                2022,
                9,
                21
            ],
            "email_sent": true
        },
        "Power, Limitations, and Use Cases of Gpt-3 From My Tests and Prototype Apps You Can Replicate Right": {
            "url": "https://towardsai.net/p/l/power-limitations-and-use-cases-of-gpt-3-from-my-tests-and-prototype-apps-you-can-replicate-right",
            "description": "Last Updated on September 21, 2022 by Editorial Team Author(s): LucianoSphere Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Exemplified with smart chatbots that can even listen and talk naturally command programs, or help students as full-time online tutors. And&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 22 Sep 2022 00:08:10 +0000",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "PySpark For Beginners": {
            "url": "https://towardsai.net/p/l/pyspark-for-beginners",
            "description": "Last Updated on September 22, 2022 by Editorial Team Author(s): Muttineni Sai Rohith Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. PySpark is a Python API for Apache Spark. Using PySpark, we can run applications parallelly on the distributed cluster (multiple nodes). Source: Databricks So we will start the theory part first as to why we need the Pyspark and the background of Apache Spark, features and cluster manager types, and Pyspark modules and packages. Apache Spark is an analytical processing engine for large-scale, powerful distributed data processing and machine learning applications. Generally, Spark is written in Scala, but for industrial adaption, Python API\u200a\u2014\u200aPySpark is released to use spark with Python. In real-time, PySpark is used a lot in the machine learning &#38; Data scientists community; Spark runs operations on billions and trillions of data on distributed clusters 100 times faster than the traditional python applications. Pyspark Features and Advantages Pyspark Architecture Apache Spark works in a master-slave architecture where the master is called \u201cDriver\u201d and slaves are called \u201cWorkers\u201d. Spark Driver creates a spark context that serves as an entry point to the application, and all operations run on the worker nodes, and resources are managed by the cluster\u00a0manager. Source: Spark.Apache.org Cluster Manager\u00a0Types The system currently supports several cluster managers besides, we can also run spark locally on our desktop/system: Standalone\u200a\u2014\u200aa simple cluster manager included with Spark that makes it easy to set up a\u00a0cluster. Apache Mesos\u200a\u2014\u200aa general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated) Hadoop YARN\u200a\u2014\u200athe resource manager in Hadoop 2 and 3. Mostly used cluster\u00a0Manager Kubernetes\u200a\u2014\u200aan open-source system for automating deployment, scaling, and management of containerized applications. Pyspark conf, Pyspark context, and Pyspark\u00a0session: Pyspark conf: The SparkConf offers configuration for any Spark application. To start any Spark application on a local Cluster or a dataset, we need to set some configuration and parameters, and it can be done using SparkConf. Features of Pyspark\u00a0conf: set(key, value)\u200a\u2014\u200aSet a configuration property. setMaster(value)\u200a\u2014\u200aSet master URL to connect\u00a0to. setAppName(value)\u200a\u2014\u200aSet application name. get(key,defaultValue=None)\u200a\u2014\u200aGet the configured value for some key, or return a default otherwise. setSparkHome(value)\u200a\u2014\u200aSet path where Spark is installed on worker\u00a0nodes. Pyspark context: SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker\u00a0nodes. The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs and know what resource manager to communicate to. It is the heart of the PySpark application. We can create only one SparkContext per JVM. In order to create another first, you need to stop the existing one by using stop() method. SparkContext is available in default as \u2018sc\u2019. So creating the other variable instead of sc will give an\u00a0error. Pyspark session: Since Spark 2.0 SparkSession has become an entry point to PySpark to work with RDD, and DataFrame. Prior to 2.0, SparkContext used to be an entry point. SparkSession is a combined class for all different contexts we used to have prior to 2.0 release (SQLContext and HiveContext e.t.c). Since 2.0 SparkSession can be used in replace with SQLContext, HiveContext, and other contexts defined prior to\u00a02.0. Though SparkContext used to be an entry point prior to 2.0, It is not completely replaced with SparkSession; many features of SparkContext are still available and used in Spark 2.0 and later. SparkSession internally creates SparkConfig and SparkContext with the configuration provided with\u00a0it. We can create as many SparkSession as you want in a PySpark application using either SparkSession.builder() or SparkSession.newSession(). Many Spark session objects are required when you want to keep PySpark tables (relational entities) logically separated. Creating a SparkSession from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"Practice\").getOrCreate() spark Pyspark Modules and\u00a0Packages Pyspark Modules and\u00a0Packages PySpark RDD\u200a\u2014\u200aResilient Distributed Dataset: \u201cResilient Distributed Datasets (RDD) is a distributed memory abstraction that helps a programmer to perform in-memory computations on a large cluster.\u201d One of the important advantages of RDD is fault tolerance, which means if any failure occurs it recovers automatically. RDD becomes immutable when it is created i.e., it cannot be changed once it is\u00a0created. RDD divides data into smaller parts based on a key. The benefit of dividing data into smaller chunks is that if one executor node fails, another node will still process the data. So it is able to recover quickly from any issues as the same data chunks are replicated across multiple executor nodes. RDD provides the functionality to perform functional calculations against the dataset very quickly by binding the multiple\u00a0nodes. Pyspark For Beginners&#124; Part-4: Pyspark RDD Pyspark DataFrame: DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing\u00a0RDDs. Due to parallel execution on all cores on multiple machines, PySpark runs operations faster than pandas. In other words, pandas DataFrames run operations on a single node, whereas PySpark runs on multiple machines. Pyspark For Begineers&#124; Part-2: Pyspark DataFrame Pyspark SQL: PySpark SQL is a module in Spark which integrates relational processing with Spark\u2019s functional programming API. We can extract the data by using an SQL query language. We can use the queries same as the SQL language. In other words, Spark SQL brings native RAW SQL queries on Spark, meaning you can run traditional ANSI SQL\u2019s on Spark Dataframe, in the above section of this PySpark tutorial, you will learn in detail about using SQL select, where, group by, join, union\u00a0e.t.c PySpark [&#8230;]",
            "pubdate": "Thu, 22 Sep 2022 13:33:19 +0000",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "This Is How You Can Build a Churn Prediction Model Using Spark": {
            "url": "https://towardsai.net/p/l/this-is-how-you-can-build-a-churn-prediction-model-using-spark",
            "description": "Last Updated on September 22, 2022 by Editorial Team Author(s): Paul Iusztin Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. An end-to-end tutorial on how to build a churn prediction pipeline using only Spark. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 22 Sep 2022 12:13:31 +0000",
            "pubdate_parsed": [
                2022,
                9,
                22
            ],
            "email_sent": true
        },
        "Introduction to Confusion Matrix": {
            "url": "https://towardsai.net/p/l/introduction-to-confusion-matrix",
            "description": "Last Updated on September 23, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What is Confusion Matrix, and how to plot it in\u00a0Python? Image by\u00a0Author The Confusion Matrix is the visual representation of the Actual VS Predicted values. It is a performance evaluation tool for classification algorithms, also known as the error\u00a0matrix. A two-dimensional table layout of how many predicted classes or categories were correctly predicted and how many were not allows visualization of the performance of an algorithm, typically in supervised learning. In predictive analytics, a Confusion Matrix for binary classification is a table with two rows and two columns that reports the number of true positives, false negatives, false positives, and true negatives. This allows for more detailed analysis than simply observing accuracy. Image by\u00a0Author Why Confusion Matrix over Accuracy? The accuracy metric can be misleading if used for the Imbalance dataset when the numbers of observations in different classes vary greatly. Whereas the Confusion Matrix provides a detailed comparison between Positives and Negatives. Confusion Matrix consists of four important metrics True Positive(TP), True Negative(TN), False Positive(FP), False Negative(FN). Let\u2019s Understand them with an analogy where the algorithm has to categorize if a Person is Healthy or\u00a0Sick. Confusion Matrix for Binary Classification &#124; Image by\u00a0Author (1) True Positive\u00a0(TP) The Algorithm predicted a \u201cPerson is Sick\u201d who is Sick. This concludes that the algorithm has correctly classified the positive. It is the number of correct predictions when the actual class is positive. (2) True Negative\u00a0(TN) The Algorithm predicted a \u201cPerson is Healthy\u201d who is Healthy. This concludes that the algorithm has correctly classified the negative. It is the number of correct predictions when the actual class is negative. (3) False Positive\u00a0(FP) The Algorithm predicted a \u201cPerson is Sick\u201d who is Healthy. Here algorithm gave a false alarm by misclassifying it as Positive instead of Negative. It is the number of incorrect predictions when the actual class is positive, also referred to as Type I\u00a0Error. (4) False Negative\u00a0(FN) The Algorithm predicted a \u201cPerson is Healthy\u201d who is Sick. Here algorithm missed a Sick Person by categorizing it healthy. It is the number of incorrect predictions when the actual class is negative, also referred to as Type II\u00a0Error. from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix X, y = load_breast_cancer(return_X_y=True)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)lr= LogisticRegression() lr.fit(X_train,y_train) y_pred=lr.predict(X_test)confusion_matrix(y_test, y_pred) Output:array([[ 63, 4], [ 3, 118]]) The confusion_matrix API in sklearn provides an array as an output that has TN, FP, FN, and TP, respectively, and the same can be plotted using ConfusionMatrixDisplay API or Heatmap API of any visualization library. Below is the python method for evaluating and plotting the Confusion matrix. It will give an array of tn, fp, fn, and tp as a return type and print the confusion matrix created by in seaborn\u00a0theme. https://medium.com/media/e43191c1d0474d2d6784aa0364186ed1/href from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix X, y = load_breast_cancer(return_X_y=True)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)lr= LogisticRegression() lr.fit(X_train,y_train) y_pred=lr.predict(X_test)conf_mat, ax = confusion_matrix(y_test, y_pred) Below is the output for the\u00a0code Confusion Matrix &#124; Image by\u00a0Author The goal is to keep as many TP and TN values as possible. In this blog, we understood what confusion Matrix is and How we can plot it in Python. Interpretation of True Positive(TP), True Negative(TN), False Positive(FP), and False Negative(FN) are the building metrics of the Confusion Matrix. However, multiple metrics can be derived from the Confusion Matrix like Accuracy, Precision, Recall, ROC, and many more. Please refer to Deep dive into Confusion Matrix for\u00a0details. References: [1] sklearn Confusion Matrix API. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix [2] sklearn ConfusionMatrixDisplay API. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay [3] seaborn Heatmap API. https://seaborn.pydata.org/generated/seaborn.heatmap.html Introduction to Confusion Matrix was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 23 Sep 2022 12:14:21 +0000",
            "pubdate_parsed": [
                2022,
                9,
                23
            ],
            "email_sent": true
        },
        "Detect business insights from customer support conversations using AI": {
            "url": "https://towardsai.net/p/l/detect-business-insights-from-customer-support-conversations-using-ai",
            "description": "Last Updated on September 25, 2022 by Editorial Team Author(s): Shubham Saboo Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Learn how to analyze customer conversations with just 15 lines of Python\u00a0code! Background A positive customer experience can make the difference between losing and retaining customers. Businesses need to constantly improve their products and services and identify trends and issues at an early stage to keep their customers satisfied! It has become more crucial than ever to analyze customer support data and derive valuable insights to ensure that necessary modifications and enhancements are undertaken early on the basis of the customerpreferences, and the business perform effectively to support the\u00a0same. But that\u2019s the biggest challenge. Unfortunately, most businesses struggle to capture and make sense of the data derived here. While it would be cumbersome to listen to all the calls or skim through all the emails to figure out the insights, even sample picking wouldn\u2019t ensure that the problem areas are correctly identified. How to make sense of this huge volume of\u00a0data? Here\u2019s where One AI studio changes the\u00a0game! One AI is a language AI service where various pre-trained NLP models are packaged and made available through API, enabling language comprehension in context and transforming texts from any source into structured data. One AI studio is capable of performing a wide array of tasks including but not limited\u00a0to: Transcribing audio\u00a0files Generating Highlights of the\u00a0input Topic extraction Emotions as well as Sentiments detection Identification of\u00a0Keywords Identifying Action\u00a0Items Clustering the data basis skills as parameters like Keywords or Sentiments, etc. One AI Studio Interface Let\u2019s look at how you can build a streamlit application to analyze customer support data using One AI and Python. All you need to have is the following: Basic knowledge of\u00a0Python Streamlit One AI\u00a0API Application Walkthrough We will use the Streamlit framework to build a beautiful frontend in python itself. Here is a step-by-step walkthrough to building a Python application for call center analytics: Import the necessary libraries and get the API key from the\u00a0user. 2. Get the conversation or email trail as input from the user and create functionality to select between different intelligence features. 3. Format the input data to be processed by the language model by converting it into\u00a0JSON. 4. Set the headers, API endpoint address, and the payload to be sent to the API. Use the request library to hit the API endpoint and get the output returned in JSON\u00a0format. 5. Process the JSON file and display the output to the end\u00a0users! \ud83c\udf1f Here is the GitHub repository to get the source\u00a0code: GitHub &#8211; Shubhamsaboo/customer-center-analytics-nlp: Use OneAI API to analyze the deep customer insights with NLP and LLMs This is what our final application looks like\u00a0\ud83d\udc47 Streamlit Application to analyze customer conversations AI-powered Analytics in Action\u00a0\ud83d\udd79 Now let\u2019s look at how we can use the above application in real-world scenarios: Step 1: First things first, you have to put your API Key for authentication. Copy your One AI API Key and paste it onto the\u00a0sidebar. Getting the API key (Screen-1) Getting the API key (Screen-2) Paste the API key in the application sidebar. Step 2: Let\u2019s get the ball rolling. Enter the transcript of an audio call in the input box and select an intelligence feature. Sample conversation Ensure that the input is given in the below\u00a0format. Customer: Hi, I am Dragos from Leverkin Management. I am having a lot of trouble with using your product, it is very complicated. I require a training session. Agent: Hi, I\u2019m sorry to hear that. I will surely schedule a session for you. Is Tuesday 5 p.m. a good time? Customer: Yes, that will work thanks. \u201cSummary\u201d as an intelligence feature: 2. \u201cNamed Entity Recognition\u201d as an intelligence feature: 3. \u201cEmotion Detection\u201d as an intelligence feature: 4. \u201cSentiments Analysis\u201d as an intelligence feature: 5. \u201cTopics Detection\u201d as an intelligence feature: Try it out yourself \ud83d\udc49 Streamlit Application Conclusion AI will revolutionize the way customer center data is analyzed. It would play a pivotal role in how useful insights can be derived from customer experience efficiently and with greater accuracy. By identifying customer needs and preferences, call center agents will be able to provide a more personalized and satisfying customer service experience. If you would like to learn more or want me to write more on this subject, feel free to reach\u00a0out. My social links: LinkedIn&#124; Twitter &#124;\u00a0Github If you liked this post or found it helpful, please take a minute to press the clap button, it increases the post&#039;s visibility for other medium\u00a0users. Detect business insights from customer support conversations using AI was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 26 Sep 2022 00:03:43 +0000",
            "pubdate_parsed": [
                2022,
                9,
                26
            ],
            "email_sent": true
        },
        "Deep dive into Confusion Matrix": {
            "url": "https://towardsai.net/p/l/deep-dive-into-confusion-matrix",
            "description": "Last Updated on September 26, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Model Evaluation Deep Dive Into Confusion Matrix Precision (TPR), Recall (PPV), TNR, FPR, FNR, NPV, F1 Score, Accuracy, Balanced Accuracy, LR+,\u00a0LR- Image by\u00a0Author In the field of Data Science, model evaluation is the key component of the Training Lifecycle. There are many metrics to evaluate the classification model, but the Accuracy metric is often used. However, Accuracy might not give the correct depiction of the model due to class imbalance, and in such case, the Confusion Matrix is to be used for evaluation. Confusion Matrix is pivotal to know, as many metrics are derived from it, be it precision, recall, F1-score, or Accuracy. Confusion Matrix &#124; Image by\u00a0Author Let\u2019s understand the metrics derived from the Confusion Matrix True Positive (TP) is the number of correct predictions when the actual class is positive. True Negative (TN) is the number of correct predictions when the actual class is negative. False Positive (FP) is the number of incorrect predictions when the actual class is positive, also referred to as Type I\u00a0Error. False Negative (FN) is the number of incorrect predictions when the actual class is negative, also referred to as Type II\u00a0Error. from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegressionfrom . import confusion_matrix X, y = load_breast_cancer(return_X_y=True)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)lr= LogisticRegression()lr.fit(X_train,y_train) y_pred=lr.predict(X_test) conf_mat = confusion_matrix(y_test, y_pred, plot=False)TP = conf_mat[0,0]TN = conf_mat[1,1]FP = conf_mat[1,0]FN = conf_mat[0,1] print(\"TP: \", TP)print(\"TN: \", TN)print(\"FP: \", FP)print(\"FN: \", FN) Output:TP: 63TN: 118FP: 3FN: 4 True Positive Rate (TPR), Sensitivity, Recall: It is the probability of a person testing positive who has a disease. In other words, Recall is the proportion of examples of a particular class predicted by the model as belonging to that\u00a0class. Image by\u00a0Author from sklearn.metrics import recall_scorerecall_score(y_test, y_pred) Output:0.9752066115702479 True Positive Rate (TPR), Specificity: It is the probability of a person testing negative who does not have a\u00a0disease. Image by\u00a0Author False Positive Rate (FPR), fall-out: It is the probability of a person testing positive who does not have a\u00a0disease. Image by\u00a0Author False Negative Rate (FNR), miss rate: It is the probability of a person testing negative who does have a\u00a0disease. Image by\u00a0Author TNR = TN/(TN+FP)print(\"Specificity: \", TNR) FPR = FP/(TN+FP)print(\"FPR: \", FPR)FNR = FN/(TP+FN)print(\"FNR: \", FNR) Output:Specificity: 0.9752066115702479FPR: 0.024793388429752067FNR: 0.05970149253731343 Positive Predictive Value (PPV), Precision: It is the probability of a person having a disease who is tested positive. In other words, Precision is the proportion of correct predictions among all predictions. Image by\u00a0Author from sklearn.metrics import precision_scoreprecision_score(y_test, y_pred) Output:0.9672131147540983 Negative Predictive Value (NPV): It is the probability of a person not having a disease who is tested negative. Image by\u00a0Author Positive likelihood ratio\u00a0(LR+): Image by\u00a0Author Negative likelihood ratio\u00a0(LR-): Image by\u00a0Author TNR = TP/(TP+FN)NPV = TN/(TN+FN)print(\"NPV: \", NPV) LRp = TPR/FPRprint(\"LR+: \", LRp)LRn = FNR/TNRprint(\"LR-: \", LRn) Output:NPV: 0.9672131147540983LR+: 37.92537313432836LR-: 0.06349206349206349 Accuracy: Accuracy is the proportion of examples that were correctly classified. To be more precise, It is the ratio of correct prediction over the total number of\u00a0cases. Image by\u00a0Author from sklearn.metrics import accuracy_scoreaccuracy_score(y_test, y_pred) Output:0.9627659574468085 Balanced Accuracy: It is the arithmetic mean of TPR and TNR. Balanced Accuracy finds its usage where data imbalance exists. Image by\u00a0Author from sklearn.metrics import balanced_accuracy_scorebalanced_accuracy_score(y_test, y_pred) Output:0.9577525595164673 F1 Score: It is the harmonic mean of precision and recall, so it\u2019s an overall measure of the quality of a classifier\u2019s predictions. It is usually the metric of choice for most people because it captures both precision and recall. It finds its way during Data Imbalance. Image by\u00a0Author from sklearn.metrics import f1_scoref1_score(y_test, y_pred) Output:0.9711934156378601 What is the difference between F1 and Balanced Accuracy? F1 does not consider True Negative for evaluating the model, while Balanced Accuracy considers all four TP, TN, FP, and\u00a0FN. F1 is the composite metric where precision and recall are considered There are other composite metrics like precision-recall curve and ROC, and AUC, which are important to assess any classification model. To read more about these curves, please visit Precision-Recall and ROC\u00a0Curve. The below code is similar to the classification report of sklearn instead, it will give all metrics out of the confusion matrix for binary classification. https://medium.com/media/3927f5262ad3055b2ce9c8e909b10881/href report = binary_classification_report(y_test, y_pred)report Output:{&#039;TP&#039;: 118, &#039;TN&#039;: 63, &#039;FP&#039;: 4, &#039;FN&#039;: 3, &#039;TPR&#039;: 0.9752066115702479, &#039;Recall&#039;: 0.9752066115702479, &#039;Sensitivity&#039;: 0.9752066115702479, &#039;TNR&#039;: 0.9402985074626866, &#039;Specificity&#039;: 0.9402985074626866, &#039;FPR&#039;: 0.05970149253731343, &#039;FNR&#039;: 0.024793388429752067, &#039;PPV&#039;: 0.9672131147540983, &#039;Precision&#039;: 0.9672131147540983, &#039;Accuracy&#039;: 0.9627659574468085, &#039;Balaced Accuracy&#039;: 0.9577525595164673, &#039;F1 Score&#039;: 0.9711934156378601} Note: all the above codes mentioned in the blog are for binary classification, In this blog, we understood the confusion matrix for binary classification. However, if you are interested in multiclass, please refer to Multi-class Model Evaluation with Confusion Matrix and Classification Report and if you are wondering about the \u201cfrom\u00a0. import confusion_matrix\u201d, please refer to the Introduction to Confusion Matrix for the Python\u00a0method. References: [1] sklearn metrics API. https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics Deep dive into Confusion Matrix was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 26 Sep 2022 12:04:45 +0000",
            "pubdate_parsed": [
                2022,
                9,
                26
            ],
            "email_sent": true
        },
        "Big Data Is Not the Way to Go": {
            "url": "https://towardsai.net/p/l/big-data-is-not-the-way-to-go",
            "description": "Last Updated on September 26, 2022 by Editorial Team Author(s): Andre Ye Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. The significance of the data distribution Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 26 Sep 2022 11:53:46 +0000",
            "pubdate_parsed": [
                2022,
                9,
                26
            ],
            "email_sent": true
        },
        "How Artificial Intelligence Could Help the World Reduce Carbon Emissions": {
            "url": "https://towardsai.net/p/l/how-artificial-intelligence-could-help-the-world-reduce-carbon-emissions",
            "description": "Last Updated on September 29, 2022 by Editorial Team Author(s): Jair Ribeiro Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Businesses and governments are urged to focus on AI to prevent the worst impacts of climate change. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 29 Sep 2022 13:13:58 +0000",
            "pubdate_parsed": [
                2022,
                9,
                29
            ],
            "email_sent": true
        },
        "UNet++ Clearly ExplainedA Better Image Segmentation Architecture": {
            "url": "https://towardsai.net/p/l/unet-clearly-explained%e2%80%8a-%e2%80%8aa-better-image-segmentation-architecture",
            "description": "Last Updated on September 29, 2022 by Editorial Team Author(s): Leo Wang Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. UNet++ Clearly Explained\u200a\u2014\u200aA Better Image Segmentation Architecture Photo by Pietro Jeng on\u00a0Unsplash Table of\u00a0Contents \u2218 \u2b50\ufe0f U-Net Recap \u2218 \u2b50\ufe0f UNet++ Innovations \u2218 \u2b50\ufe0f Loss function \u2218 \u2b50\ufe0f Performance\u00b7 Citations In this article, we are going to introduce you to UNet++, essentially an upgraded version of U-Net. This article is designed to help you understand it intuitively and thoroughly with minimum time possible. It is recommended that you at least have a very rough idea of what U-Net is, but we are going to do a recap\u00a0anyways! \u2b50\ufe0f U-Net\u00a0Recap Introduced in 2015, U-Net aimed to conduct image segmentation tasks specifically in the field of medical imaging. Its name is derived from its \u201cU-Shaped\u201d architecture. The architecture consists of a contracting path (aka. downsampling path, encoder), where the width and heights of the feature maps are shrunk while the channel expands by a factor of 2 until it reaches 1024 (typically the maximum recommended level for CNNs), a bottleneck as a \u201cturning point\u201d, and an expanding path (aka. upsampling path, decoder), where widths and heights of the feature maps are expanded to the mask\u2019s dimension. Fig. 1: U-Net Architecture\u00b2 \u2b50\ufe0f UNet++ Innovations \u201cUpgraded\u201d from U-Net, UNet++ essentially added dense convolutional blocks (Fig 1 in blue and Fig. 3) and a deep supervision design (Fig. 2 in red) that nests on the top level of the network. The newly proposed model looks like\u00a0this: Fig. 2\u00b9 The first design change is added dense convolutional block, Fig. 2 intuitively and concisely shows how it\u00a0works. Fig. 3\u00b9 In U-Net, the feature maps generated from the encoder are automatically passed to the decoder at the same level (shown in BLACK in Fig. 3). However, in UNet++ that is changed (as shown in Fig 3 in BLUE and GREEN). To understand it, here\u2019s the explanation: In the formula shown at the top of Fig 3, H is DenseNet\u2019s composite function that combines Batch Normalization, ReLU activation, and a 3&#215;3 convolution. The elements inside [] are concatenated together as inputs to the H composite function. U is U-Net\u2019s composite function; by default, when you use U-Net\u2019s own backbone, you should expect two 3&#215;3 convolutions with ReLU activations (shown in Fig. 1 as how each level is structured). It should be noted that the introduced dense blocks in the middle take into not only the information from the previous \u201cnodes,\u201d in the same level but also the \u201cnodes\u201d in the level below it (shown in Fig. 2). This is a really densely connected network! Therefore, the newly introduced dense connections would help reduce the \u201csemantic gap between the feature maps of the encoder and decoder\u201d(Fig. 1), so the model could have an easier learning task because those feature maps would be more \u201csemantically similar.\u201d The second change in UNet++ is an added deep supervision design (Fig. 4 in\u00a0red). Fig. 4\u00b9 Deep Supervision is not as hard as it seems. Essentially, it helps the model to operate in two modes:1) Accurate mode (the outputs from all branches in level 0 are averaged to produce the final result)2) Fast mode (not all branches are selected for\u00a0outputs) Figure 5 shows how different choices in FAST MODE result in different models Fig. 5\u00b9 \u2b50\ufe0f Loss\u00a0function In the paper, the author proposed a combined loss function of Binary Cross Entropy and Dice loss, as shown in Formula\u00a01. Formula 1\u00b9 The author used 0.5 weights for BCE loss and 1.0 weights for Dice loss. Note: Dice coefficient is equivalent to F1 score. During implementation, it is suggested to use 1 minus the dice coefficient when using the dice coefficient as a loss. Therefore, this practice shown in the paper maybe is subject to improvement. Moreover, Dice loss is often hard to converge as its non-convex nature. Therefore, one recent solution is provided by wrapping it in a log and cosh function to \u201csmooth out the curve\u201d (https://arxiv.org/pdf/2006.14822.pdf) Also, combining BCE loss with Dice loss often leads to better\u00a0results. \u2b50\ufe0f Performance The author trained the model on four different datasets, and all yielded better performances than U-Net and Wide U-Net models. Table 1 shows the result. DS means deep supervision. The result is shown in the IoU score (area of overlap/area of the union), which illustrates how accurate the model\u00a0is. Table 1\u00b9 The result demonstrates that UNet++ indeed improved compared to its predecessor U-Net. Next, we are going to show you how UNet 3+ works. It\u2019s an upgraded version of\u00a0UNet++! UNet 3+ Fully Explained\u200a\u2014\u200aNext UNet Generation Thank you!\u00a0\u2764\ufe0f Citations [1] Z. Zhou, M. Siddiquee, N. Tajbakhsh, and J. Liang, UNet++: A Nested U-Net Architecture for Medical Image Segmentation (2015), 2015 Computer Vision and Pattern Recognition[2]: O. Ronneberger, P. Fischer, and T. Brox, U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), 2015 Computer Vision and Pattern Recognition UNet++ Clearly Explained\u200a\u2014\u200aA Better Image Segmentation Architecture was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 29 Sep 2022 12:38:22 +0000",
            "pubdate_parsed": [
                2022,
                9,
                29
            ],
            "email_sent": true
        },
        "This is how Im using A.I. To Draw Elon Musks Latest Tweets.": {
            "url": "https://towardsai.net/p/l/this-is-how-im-using-a-i-to-draw-elon-musks-latest-tweets",
            "description": "Last Updated on September 30, 2022 by Editorial Team Author(s): Jair Ribeiro Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. I can create unique images of his tweets using Python and Stable Diffusion. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 30 Sep 2022 12:28:39 +0000",
            "pubdate_parsed": [
                2022,
                9,
                30
            ],
            "email_sent": true
        },
        "Zero-Shot, One-Shot, Few-Shot Learning": {
            "url": "https://towardsai.net/p/l/zero-shot-one-shot-few-shot-learning",
            "description": "Last Updated on September 30, 2022 by Editorial Team Author(s): Jesus Rodriguez Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Understanding the different types of N-Short learning disciplines. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 30 Sep 2022 12:28:36 +0000",
            "pubdate_parsed": [
                2022,
                9,
                30
            ],
            "email_sent": true
        },
        "An Introduction to Federated Learning": {
            "url": "https://towardsai.net/p/l/an-introduction-to-federated-learning",
            "description": "Last Updated on September 30, 2022 by Editorial Team Author(s): Chinmay Bhalerao Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Data privacy and security with Federated learning People in the AI-ML-DL field often asked about privacy concerns of data and data security which is fairly logical because after training models on a wide variety of datasets, what should be the strategy to deal with data and its\u00a0privacy? Photo by Marija Zaric on\u00a0Unsplash Two significant obstacles still exist for modern AI. One is that data typically exists as isolated islands in different businesses. The other is the improvement of data security and privacy. In the current most hyped learning and training methods, we bring our datasets towards a fixed and centralized model on which insights extraction happens. transferring data from system to system, database to database, is quite challenging, and data leakage and data stealing can be\u00a0done. Federated Learning is the emerging model learning method that has a solution for all the above-mentioned problems. Let&#039;s see Federated Learning in detail. A very brute force or dictionary meaning of the federated word is \u201cto unite under a central government or organization while keeping some local control\u201d How this meaning is related to actual federated learning, we will see in the next sections of the\u00a0blog. What if I can bring my local model towards data and not data towards the model? Let&#039;s understand this by a simple example by google.ai federated learning blog. Any mobile app that interacts with users can be used to train a machine learning model, which tries to learn from user interactions. On numerous mobile devices, an ML model will be trained simultaneously. This trained model provides updates, which are then transmitted to a centralized server or model. The inputs provided by the induvial model will be used to update the centralized model. Once more, a centrally updated model will be sent to your\u00a0devices. The continuous model is updating locally and sending the updated model to the server for new updates [Source\u00a0link] Our device downloads the current model, improves it by learning from data on your phone and then summarizes the changes as a small, focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the\u00a0cloud. Explaining this, you don\u2019t have centralized data. You have data distributed across different locations and devices, and now you want to train a machine learning\u00a0model. According to me, The biggest concern of data privacy and data security is getting channelized here. The data is at the user\u2019s location, and the updated models are sent to the centralized system. The advantage of federated learning is you are not bringing data towards the model, but you are bringing the model toward the data. Training an algorithm at different local edges or servers and using it as a data sample from the population. Companies can benefit from accurate machine learning models, but typical centralized machine learning systems have limitations, such as not continuously learning on edge devices and aggregating private data on central servers. Federated learning helps to mitigate these\u00a0issues. In conventional machine learning, a central ML model is created using all training data that is accessible in a centralized setting. When predictions can be served by a central server, this operates without any problems. A pleasant user experience may be compromised by the communication delay between a user device and a central server in mobile computing since users expect quick responses. The model might be installed in the end-user device to address this, but since models are trained on entire data sets, ongoing learning becomes difficult. Federated learning in the healthcare industry: Large, diverse, and high-quality datasets provide experience for AI algorithms. Such statistics, however, have historically been difficult to find, particularly in the healthcare industry. A centralized-server approach to federated learning. [Source\u00a0link] Medical organizations have been forced to rely on their own data sources, which can be skewed by factors like patient demographics, the tools employed, or clinical specialties. Or, to obtain all the necessary data, they had to combine data from other institutions. According to BrainTorrent: A Peer-to-Peer Environment for Decentralized Federated Learning paper, A frequent difficulty in training deep neural networks on medical imaging is gaining access to enough labeled data. As data annotation is costly and time-consuming, it is challenging for a single medical center to obtain sufficient sample quantities to create its own customized models. To avoid this, data from all centers might be gathered and used to train a centralized framework that is accessible to anyone. However, this tactic is frequently used. due to the private nature of medical data, it&#039;s impractical. Federated learning (FL) has recently been developed to enable the cooperative learning of a shared prediction model across centers without the requirement for data exchange. In FL, users train models locally on site-specific datasets for a few epochs before sharing their model weights with a centralized server, which manages the entire training procedure. It\u2019s vital that the privacy of patients is not jeopardized by model\u00a0sharing. Federated learning in IoT\u00a0: The Internet of Things (IoT) is developing, opening up new options for real-time data collection and machine learning model deployment. However, a single IoT device might not have the computational power to develop and implement a full learning model. In addition to raising concerns about data security and privacy, sending continuous real-time data to a central server with powerful computational capabilities incurs large transmission costs. According to the paper, a promising approach to training machine learning models on edge servers and devices with constrained resources is federated [&#8230;]",
            "pubdate": "Fri, 30 Sep 2022 12:13:58 +0000",
            "pubdate_parsed": [
                2022,
                9,
                30
            ],
            "email_sent": true
        },
        "Multi-class Model Evaluation with Confusion Matrix and Classification Report": {
            "url": "https://towardsai.net/p/l/multi-class-model-evaluation-with-confusion-matrix-and-classification-report",
            "description": "Last Updated on September 30, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Model Evaluation Precision, Recall, F1, Micro, Macro, Weighted, and Classification Report Image by\u00a0Author If you are familiar with the Confusion Matrix, you might know that it is mainly explained for binary classification, which has only two outputs. TP, TN, FP, FN, and other derived metrics like precision and recall are convenient to understand. However, it is not the same case when we have more than two target\u00a0classes. In this blog, Focus will be on the problem with more than two classes or, in other words, Multi-class classification. Unlike binary classification, there is no negative class. It is a perception that TP, TN, and other metrics are difficult to derive out of the confusion matrix for multi-class but actually, it is quite\u00a0easy. In multi-class classification, all the metrics be it TP, precision, or any other metric, are calculated the same as in binary, except it needs to be calculated for each class. We can pretty much derive any metric for a class if we compute TP, TN, FP, and FN for a respective class. Multi-class Confusion Matrix &#124; Image by\u00a0Author TP, FP, and FN can be deduced from the matrix if we look for a particular class from both dimensions, and the rest of the numbers will contribute to TN. Other metrics can also be derived in the same fashion. Please visit Introduction to Confusion Matrix and Deep dive into Confusion Matrix to read about What Confusion Matrix is and how precision, recall, and many other metrics are derived from\u00a0it. Let us understand how to calculate metrics for multi-class; for simplicity, we will consider the problem with 3 classes (airplane, car,\u00a0train). Confusion Matrix &#124; Image by\u00a0Author ## Calculation of class \u201cAirplane\u201d: TP = 9FN = 1+5 = 6FP = 6+3 = 9TN = 7+4+2+8 = 21Precision = TP/(TP+FP) = 9/(9+9) = 0.5Recall = TP/(TP+FN) = 9/(9+6) = 0.6F1 = 2*(0.5*0.6)/(0.5+0.6) = 5.55 Similarly, we can calculate for the other classes. However, this time we will use sklearn metrics API to produce precision, recall, and f1\u00a0score. from sklearn.metrics import confusion_matrixfrom sklearn.metrics import precision_scorey_true = [0]*15 + [1]*17 + [2]*13y_pred = [0]*9 + [1]*1 + [2]*5 + [0]*6 + [1]*7 + [2]*4 + [0]*3 + [1]*2 + [2]*8 confusion_matrix(y_true, y_pred, labels=[0,1,2]) Output:array([[9, 1, 5], [6, 7, 4], [3, 2, 8]]) The above example is to calculate the confusion matrix, which returns ndarray, and if labels are not hot-encoded, we have to provide a set of labels against the \u2018labels\u2019 argument. Precision: It is referred to the proportion of correct predictions among all predictions for a particular class. from sklearn.metrics import precision_scoreprecision_score(y_true, y_pred, labels=[0,1,2], average=None) Output:array([0.5 , 0.7 , 0.47058824]) Recall: It is referred to the proportion of examples of a specific class that have been predicted by the model as belonging to that\u00a0class. from sklearn.metrics import recall_scorerecall_score(y_true, y_pred, labels=[0,1,2], average=None) Output:array([0.6 , 0.41176471, 0.61538462]) F1 Score: The Harmonic mean of precision and\u00a0recall. from sklearn.metrics import f1_scoref1_score(y_true, y_pred, labels=[0,1,2], average=None) Output:array([0.54545455, 0.51851852, 0.53333333]) The \u2018average\u2019 argument in the above evaluation methods needs to be None which return an array of metric respective to individual class. In multi-class, we have observed that precision has been calculated for individual classes, while in binary class problems, we had a single value. If we want to evaluate multi-class with one global metric, we have micro, macro, and weighted precision. Any metric from the confusion matrix can be combined with micro, macro, and weighted to make it a global\u00a0metric. Micro Precision: It is calculated by considering the total TP, TN, FN, and TN irrespective of class to calculate Precision. Global TP = TP(airplane) + TP(car) + TP(train) = 9+7+8 =\u00a024 Global FP = FP(A) + FP(C) + FP(T) = (6+3) + (1+2) + (5+4) =\u00a021 Micro Precision = 24/(24+21) =\u00a00.533 from sklearn.metrics import precision_scoreprecision_score(y_true, y_pred, labels=[0,1,2], average=&#039;micro&#039;) Output:0.5333333333333333 Macro Precision: It is referred to as the unweighted mean of the measure for each\u00a0class. Classification Report &#124; Image by\u00a0Author Macro Precision = (0.50 + 0.70 + 0.47)/3 =\u00a00.556 from sklearn.metrics import precision_scoreprecision_score(y_true, y_pred, labels=[0,1,2], average=&#039;macro&#039;) Output:0.5568627450980391 Weighted Precision: Unlike macro, it is the weighted mean of the measure. Weights are the total number of samples per class. In our example, we have 15 airplanes, 17 cars, and 13 trains which aggregated to 45 in\u00a0total. Weighted Precision = (15*0.50 + 17*0.70 + 13*0.47)/45 =\u00a00.566 from sklearn.metrics import precision_scoreprecision_score(y_true, y_pred, labels=[0,1,2], average=&#039;weighted&#039;) Output:0.5670588235294117 What is Classification Report? It is a python method under sklearn metrics API, useful when we need class-wise metrics alongside global metrics. It provides precision, recall, and F1 score at individual and global levels. Here support is the count of samples. Classification Report in sklearn calculates all necessary metrics for evaluation. from sklearn.metrics import classification_reportreport = classification_report(y_true, y_pred, labels=[0,1,2], target_names=[\"Airplane\", \"Car\", \"Train\"])print(report) Output: precision recall f1-score support Airplane 0.50 0.60 0.55 15 Car 0.70 0.41 0.52 17 Train 0.47 0.62 0.53 13 accuracy 0.53 45 macro avg 0.56 0.54 0.53 45weighted avg 0.57 0.53 0.53 45 Below is the code for plotting confusion Matrix and Detailed Classification Report https://medium.com/media/e7e4efd4e67053f5459f628e0faaca39/hrefhttps://medium.com/media/8b597fd6cedb02b224f8f1c75852fc21/href import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom . import confusion_matrix # Load Datasetdata = load_iris()X = data.datay = data.targetlabels = list(data.target_names) # Adding Noiserandom_state = np.random.RandomState(0)n_samples, n_features = X.shapeX = np.concatenate([X, random_state.randn(n_samples, 200* n_features)], axis=1)X_train, X_test, y_train, y_test = train_test_split( X[y &#60; 3], y[y &#60; 3], test_size=0.5, random_state=random_state) lr = LogisticRegression()lr.fit(X_train, y_train)y_pred = lr.predict(X_test)y_pred_prob = lr.predict_proba(X_test) confusion_matrix(y_test, y_pred, labels) If you are wondering about the \u201cfrom\u00a0. import confusion_matrix\u201d, please refer to the Introduction to Confusion Matrix for the Python\u00a0method. Confusion Matrix &#124; Image by\u00a0Author multi_classification_report(y_test, y_pred, labels=labels, encoded_labels=True, as_frame=True) Detailed Classification Report &#124; Image by\u00a0Author summarized_classification_report(y_test, y_pred, as_frame=True) [&#8230;]",
            "pubdate": "Fri, 30 Sep 2022 12:13:53 +0000",
            "pubdate_parsed": [
                2022,
                9,
                30
            ],
            "email_sent": true
        },
        "Adaptive Learning for Time Series Forecasting": {
            "url": "https://towardsai.net/p/l/adaptive-learning-for-time-series-forecasting",
            "description": "Last Updated on October 1, 2022 by Editorial Team Author(s): Reza Yazdanfar Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. There is no need to say the importance of time series forecasting applications in various industries from Energy to Healthcare, etc&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 01 Oct 2022 12:03:55 +0000",
            "pubdate_parsed": [
                2022,
                10,
                1
            ],
            "email_sent": true
        },
        "How To Set Up and Run Cuda Operations In PyTorch": {
            "url": "https://towardsai.net/p/l/how-to-set-up-and-run-cuda-operations-in-pytorch",
            "description": "Last Updated on October 4, 2022 by Editorial Team Author(s): Muttineni Sai Rohith Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Introduction The advent of deep learning in recent years created a demand for computing resources and acceleration of workloads. Various operations involved in deep learning, such as matrix multiplications, tiling of the images, and processing chunks of voice samples, can be parallelized for better performance and accelerating the development of Machine learning models. Thus, many deep learning libraries like TensorFlow and Pytorch provide users with a set of functions or APIs to take advantage of their GPUs. CUDA Is one such programming model and computing platform which enables us to perform complex operations faster by parallelizing the tasks across\u00a0GPUs. This article will discuss what CUDA is and how to set up the CUDA environment and run various CUDA operations available in\u00a0Pytorch. Photo by Lucas Kepner on\u00a0Unsplash What is\u00a0CUDA CUDA (Compute Unified Device Architecture) is a programming model and parallel computing platform developed by Nvidia. Using CUDA, one can maximize the utilization of Nvidia-provided GPUs, thereby improving the computation power and performing operations away faster by parallelizing the tasks. PyTorch provides a torch.cuda library to set up and run the CUDA operations. Using Pytorch CUDA, we can create tensors and allocate them to the device. Once allocated, we can perform operations on it, and the results are also assigned to the\u00a0device. Installation Pytorch provides a user-friendly interface on their official website where we can select our operating system, desired programming language, and other requirements, as shown in the below\u00a0figure. Refer to this official Pytorch link\u200a\u2014\u200aStart Locally &#124; PyTorch and select the requirements according to our system specifications. Pytorch provides CUDA libraries for Windows and Linux Operating systems. For windows, make sure to use CUDA 11.6 because CUDA 10.2 and ROCm are no longer supported for windows. For Python programming language, we can select one in conda, pip, and source packages, whereas LibTorch is used for C++ and Java languages. Running CUDA operations in\u00a0PyTorch Once installed successfully, we can use the torch.cuda interface to run CUDA operations in\u00a0Pytorch. To make sure whether the installation is successful, use the torch.version.cuda command as shown\u00a0below: # Importing Pytorch import torch # To print Cuda version print(\u201cPytorch CUDA Version is \u201c, torch.version.cuda) If the installation is successful, the above code will show the following output\u00a0\u2013 # Output Pytorch CUDA Version is 11.6 Before using the CUDA, we have to make sure whether CUDA is supported by our\u00a0System. Use torch.cuda.is_available() command as shown below\u00a0\u2013 # Importing Pytorch import torch # To check whether CUDA is supported print(\u201cWhether CUDA is supported by our system:\u201d, torch.cuda.is_available()) The above command will return a Boolean Value as below\u00a0\u2013 # Output Whether CUDA is supported by our system: True Pytorch CUDA also provides the following functions to know about the device id and name of the device when given device ID, as shown below\u00a0\u2013 # Importing Pytorch import torch # To know the CUDA device ID and name of the device Cuda_id = torch.cuda.current_device() print(\u201cCUDA Device ID: \u201d, torch.cuda.current_device()) print(\u201cName of the current CUDA Device: \u201d, torch.cuda.get_device_name(cuda_id)) The above code will show the following output\u00a0\u2013 # Output CUDA Device ID: 0 Name of the current CUDA Device: NVIDIA GeForce FTX 1650 We can also change the default CUDA device by specifying the ID as shown below\u00a0\u2013 # Importing Pytorch import torch # To change the Default CUDA device torch.cuda.set_device(1) Note: While using CUDA, make sure to develop device-agnostic code because some systems might not have GPUs and will have to run on CPUs, and vice versa. That can be done by adding the following line to our\u00a0code- device = \u2018cuda\u2019 if torch.cuda.is_available() else \u2018cpu\u2019 Operating Tensors with\u00a0CUDA Generally, a Pytorch tensor is the same as a NumPy array. It is an n-dimensional array used for numerical computation. The only difference between tensor and NumPy array is tensor can run both on CPUs and\u00a0GPUs. Pytorch CUDA provides the following functions to handle tensors\u00a0\u2013 \u00b7 tensor.device\u200a\u2014\u200areturns the device name of the tensor. By default, it is\u00a0\u201cCPU\u201d. \u00b7 tensor.to(device_name)\u200a\u2014\u200areturns a new instance of the tensor on the device mentioned. \u201cCPU\u201d for CPU and \u201dcuda\u201d for CUDA enabled\u00a0GPU. \u00b7 tensor.cpu()\u200a\u2014\u200ato transfer the tensor from the current device to\u00a0CPU. Let\u2019s understand the usage of the above functions by creating a tensor and performing some basic operations. We will create a sample tensor and perform a tensor operation(Squaring) on the CPU, and then we will transfer the tensor to GPU and perform the same operation again and understand the performance. import torch # Creating a sample tensor x = torch.randint(1, 1000, (100, 100)) # Checking the device name: will return \u2018CPU\u2019 by default print(\u201cDevice Name: \u201d , x.device) # Applying tensor operation res_cpu = x ** 2 # Transferring tensor to GPU x = x.to(torch.device(\u2018cuda\u2019)) # Checking the device name: will return \u2018cuda:0\u2019 print(\u201cDevice Name after transferring: \u201d, x.device) # Applying same tensor operation res_gpu = x ** 2 # Transferring tensor from GPU to CPU x.cpu() Running Machine Learning models with\u00a0CUDA CUDA provides the following function to transfer the machine learning model to the following device \u00b7 model.to(device_name)\u200a\u2014\u200areturns a new instance of the Machine learning model on the device_name specified. \u201cCPU\u201d for CPU and \u201dcuda\u201d for CUDA-enabled GPU. To demonstrate the above function, we will import the pre-trained \u201cResnet-18\u201d model from torchvision.models # Importing Pytorch Import torch import torchvision.models as models # Making the code device-agnostic device = \u2018cuda\u2019 if torch.cuda.is_available() else \u2018cpu\u2019 # Instantiating a pre-trained model model = models.resnet18(pretrained=True) # Transferring the model to a CUDA-enabled GPU model = model.to(device) Once the model is transferred, we can continue the rest of the machine learning workflow on CUDA-enabled GPU. Conclusion After reading this article, one can understand how to [&#8230;]",
            "pubdate": "Wed, 05 Oct 2022 03:53:44 +0000",
            "pubdate_parsed": [
                2022,
                10,
                5
            ],
            "email_sent": true
        },
        "ROC and AUC for Model Evaluation": {
            "url": "https://towardsai.net/p/l/roc-and-auc-for-model-evaluation",
            "description": "Last Updated on October 7, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Model Evaluation Image by\u00a0Author ROC or Receiver Operating Characteristic Curve is the most frequently used tool for evaluating the binary or multi-class classification model. Unlike other metrics, it is calculated on prediction scores like Precision-Recall Curve instead of prediction class. In my previous post, the importance of the precision-recall curve is highlighted as how to plot for multi-class classification. To understand ROC Curve, let\u2019s quickly refresh our memory on the possible outcomes in a binary classification problem by referring to the Confusion Matrix. Confusion Matrix &#124; Image by\u00a0Author ROC Curve is a plot of True Positive Rate(TPR) plotted against False Positive Rate(FPR) at various threshold values. It helps to visualize how threshold affects classifier performance. True Positive Rate (TPR) is referred to the proportion of examples of a particular class that has been predicted by the model as belonging to that class. It is also referred to as Recall or Sensitivity. Image by\u00a0Author where TP and FN are True Positive and False Negative, respectively. False Positive Rate (FPR): It is the probability of a person testing positive who does not have a disease. It is also referred to as the fall-out\u00a0rate. Image by\u00a0Author where FP is the number of False Positives and TN is the number of True Negatives. Image by\u00a0Author ROC Curve can also be defined as a Sensitivity vs. 1-Specificity plot. ROC Curve &#124; Image by\u00a0Author Let\u2019s have a look at ROC Curve for binary classification. from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import roc_curvefrom sklearn.metrics import RocCurveDisplayX, y = make_classification(n_samples=500, n_classes=2, random_state=1)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=2)lr = LogisticRegression()lr.fit(X_train, y_train)y_pred = lr.predict(X_test)y_pred_prob = lr.predict_proba(X_test)y_pred_prob = y_pred_prob[:,1]fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)plt = RocCurveDisplay(fpr=fpr, tpr=tpr)plt.plot() ROC Curve &#124; Image by\u00a0Author AUC Score AUC Stands for \u2018Area under the curve,\u2019 and it is calculated by the trapezoidal rule of area calculation under any plot. It summarizes the ROC Curve into a single metric for binary classification and each class in a multi-class model. However, to summarize the multi-class into single metric micro, macro, and weighted AUC can be\u00a0used. Higher the AUC, the better the classifier. Its value fluctuated between 0(worst model) and 1(ideal\u00a0model). ROC AUC &#124; Image by\u00a0Author from sklearn.metrics import roc_auc_scoreauc = roc_auc_score(y_test, y_pred_prob)print(auc) Output:0.9727017262143718 Let\u2019s have a look at how to plot ROC Curve and calculate AUC Score on a random classification dataset using the sklearn\u00a0library. plt = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc)plt.plot() ROC Curve with AUC &#124; Image by\u00a0Author Why ROC\u00a0Curve? ROC is calculated on a precision score, while many metrics like accuracy and precision look at the prediction class. It is a trade-off between Sensitivity(TPR) and 1-Specificity(FPR), allowing one to choose a threshold that keeps a balance between TPR and FPR, which suits the particular problem. Plotting ROC Curve is a cakewalk for binary problems. However, it daunts professionals to calculate multi-class classification. Below is the method to plot ROC and AUC for multi-class. Below is the python code to create and plot ROC and AUC for multi-class classification problems. https://medium.com/media/90775345066e0ada14362f17b686d5c4/href from sklearn.datasets import make_classificationfrom sklearn.preprocessing import label_binarizefrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.multiclass import OneVsRestClassifier # Load DatasetX, y = make_classification(n_samples=500, n_classes=3, random_state=1, n_informative=3)y = label_binarize(y, classes=[0,1,2])X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)lr = LogisticRegression()ovr = OneVsRestClassifier(lr)ovr.fit(X_train, y_train)y_pred = ovr.predict(X_test)y_pred_prob = ovr.predict_proba(X_test)fpr, tpr, threshold, auc, labels = roc_auc_curve(y_test, y_pred_prob, labels=[0,1,2])roc_auc_curve_plot(fpr, tpr, threshold, auc, labels) ROC Curve for multi-class &#124; Image by\u00a0Author References: [1] ROC Curve. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html [2] AUC. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html [3] Precision-Recall Curve. https://pub.towardsai.net/precision-recall-curve-26f9e7984add ROC and AUC for Model Evaluation was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 07 Oct 2022 12:13:26 +0000",
            "pubdate_parsed": [
                2022,
                10,
                7
            ],
            "email_sent": true
        },
        "Exploring the 500 Richest People in the World Data Set": {
            "url": "https://towardsai.net/p/l/exploring-the-500-richest-people-in-the-world-data-set",
            "description": "Last Updated on October 8, 2022 by Editorial Team Author(s): Sadrach Pierre, Ph.D. Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Generating Statistical Insights with Pie Charts, Box-Plots, and Histograms Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 08 Oct 2022 13:18:50 +0000",
            "pubdate_parsed": [
                2022,
                10,
                8
            ],
            "email_sent": true
        },
        "Python and Multi-CPU-Arch": {
            "url": "https://towardsai.net/p/l/python-and-multi-cpu-arch",
            "description": "Last Updated on October 8, 2022 by Editorial Team Author(s): Murli Sivashanmugam Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Fixing python toolchain breakages on mac M1/M2\u00a0reliably Photo by Christopher Gower on\u00a0Unsplash Introduction One of the design goals of python is to be platform agnostic and to run scripts without modification across environments. Python does a pretty good job of ensuring this compatibility as long as applications and libraries are written using python scripts. As python\u2019s popularity and adaptation increased, more and more libraries started using python extensions to include natively complied code to boost performance and efficiency. Popular python libraries like pandas and NumPy can handle large amounts of data efficiently thanks to their native extension. As more and more python libraries started using natively complied code, platform compatibility issues started showing up in python environments. If one is using Mx(M1/M2) based MacBooks, it&#039;s very likely to have witnessed platform compatibility issues in a python environment. This article talks about how python library manager like pip and conda manages platform-dependent binaries, why do they break, and how to get over them on Mac M1/M2 setups in a simpler, more reliable, and replicable way. Python binaries, like any other binary, are compiled separately for different CPU architectures and platforms(Windows/Mac/Linux). When python installs a Python library in a system, it collects and processes metadata that is specific to the local CPU and platform. If the python library includes any native extensions, it needs to be compiled into CPU and platform-specific binary to run locally. A brief understanding of how python package managers like \u2018PIP\u2019 and \u2018conda\u2019 manage CPU and platform dependencies will help understand its shortcomings and how to get over\u00a0them. PIP Multi-Arch Support PIP uses the \u2018wheel\u2019 packaging standard for distributing python packages. \u2018Wheel\u2019 is a packaging system for packaging pure python scripts and native extensions both in source code and compiled binary format. To get a better perspective on how PIP maintains different distribution formats, visit a \u2018pip\u2019 web page of a library of your choice and click on \u201cDownload Files\u201d. On this page \u201cSource Distribution\u201d section lists the packages available in source format, and the \u201cBuilt Distributions\u201d section lists all the packages available in pre-built binary formats. For example, this page link shows the source and pre-built distributions for the \u2018pandas\u2019 library. These distribution packages follow a naming convention as specified below. {dist}-{version}(-{build})?-{python}-{abi}-{platform}.whl Each section in {brackets} is a tag or a component of the wheel name that carries some meaning about what the wheel contains and where the wheel will or will not work.\u00a0Example: pandas-1.5.0-cp311-cp311-macosx_11_0_arm64.whl pandas\u200a\u2014\u200ais the library name1.5.0\u200a\u2014\u200ais the library versioncp11\u200a\u2014\u200aMinimum python version requiredcp11\u200a\u2014\u200aMinimum application binary interface(ABI) requiredmacosx_11_0_arm64\u200a\u2014\u200aPlatform tag which is again subdivided into the following:&#8211; macosx\u200a\u2014\u200aOperating system&#8211; 11_0\u200a\u2014\u200aMinimum required MacOS SDK version&#8211; arm64\u200a\u2014\u200aCpu architecture PIP naming convention also supports \u2018wildcard\u2019 to optimize package bundles. For example, \u2018chardet-3.0.4-py2.py3-none-any.whl\u2019 supports both python2 and python3 and has no dependency on ABI, and can install on any platform and CPU architecture. Many python libraries use these wildcard options to optimize the number of package bundles. For more information on Python \u2018wheel\u2019 and PIP, please refer to What Are Python Wheels and Why Should You\u00a0Care? Why does PIP install\u00a0fail? Most of the time, PIP installation fails for two primary reasons. First, if a pre-built library is not available in the repository, PIP will compile the native extension source code on the host system. To do that, it would expect the build tools and other dependent libraries to be available on the host. Sometimes this becomes a challenge to locally install the build dependencies, especially when the dependency tree grows\u00a0deep. Second, due to \u2018wildcard\u2019 in wheel package names. MacBook introduced arm-based \u2018M1/M2\u2019 CPU architecture recently. Some of the older wheel packages for macOS were listed as \u2018any\u2019 for CPU architecture because x86 was the only supported architecture by then. If PIP resolves package dependency to one of these older versions, PIP will install this package on newer CPU architecture, assuming it would run. An example of this issue is with the package \u2018azure-eventhub\u2019. This library is dependent on another library called \u2018uamqp\u2019. This library lists a universal/wildcard package for macOS that is incompatible with the M1/M2 arm64 processor. If you install \u2018azure-eventhub\u2019 on M1/M2 one would see that the package would install successfully but it will throw a runtime exception while importing this\u00a0package. Conda Multi-Arch Support Conda takes one step further in ensuring platform portability. Conda packages not only python libraries but also the dependent libraries, binaries, compilers, and python interpreters themselves for the different operating systems and CPU architecture. This way it ensures the entire toolchain is portable across environments. Since all the dependent binaries are also packaged with python libraries, it does not expect any dependencies on the local system except for the standard C libraries. So, if conda provides better portability and fixes the shortcomings of PIP, what could go wrong? The issue is not all python packages are available in Conda. It&#039;s common to use pip within a conda environment to install python packages that are not available in conda; hence, one is exposed to the shortcomings of PIP. Again (not to nitpick) \u2018azure-eventhub\u2019 package is an example of the\u00a0same. If one runs into such a platform compatibility issue and when searching for solutions in forums, one would come across different options like installing a specific version of python or library, installing the library via other packaging systems like \u2018brew\u2019 or installing alternate packages, etc. Many of such fixes are not reliable for production and may not be able to replicate across other systems. Curated below are three options that are simpler, reliable, and replicable ways to get over python platform compatibility issues. They\u00a0are: Pip Install from\u00a0Source Conda &#38;\u00a0Rosetta Docker Multi-Arch [&#8230;]",
            "pubdate": "Sun, 09 Oct 2022 00:03:43 +0000",
            "pubdate_parsed": [
                2022,
                10,
                9
            ],
            "email_sent": true
        },
        "F1 to F-beta": {
            "url": "https://towardsai.net/p/l/f1-to-f-beta",
            "description": "Last Updated on October 10, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Model Evaluation Image by\u00a0Author F1 Score The F-1 score is a popular binary classification metric representing a balance between precision and recall. It is the Harmonic mean of precision and recall. The following equation can represent the F-1\u00a0score. Image by\u00a0Author where Precision can be defined as the probability of positive predictions that are actual members of the positive\u00a0class. Image by\u00a0Author The recall is defined as the probability of the positive predictions among the actual positive. Image by\u00a0Author where TP is True Positive, FP is False Positive, and FN is the False Negative. Let\u2019s explore the F1 score for the binary classification problems with a dummy dataset in\u00a0sklearn. from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import f1_score X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=2)lr = LogisticRegression()lr.fit(X_train, y_train)y_pred = lr.predict(X_test)y_pred_prob = lr.predict_proba(X_test)y_pred_prob = y_pred_prob[:,1]f1_score(y_test, y_pred) Output:0.8585858585858585 While many Machine Learning and Deep Learning practitioners frequently use the F1 score for model evaluation, few are familiar with the F-measure, which is the general form of the F1\u00a0Score. F-beta Score The F-beta score calculation follows the same form as the F1 score. Unlike in F1 Score, which is the harmonic mean, it is the weighted harmonic mean of the precision and recall, reaching its optimal value at 1 and worst value at\u00a00. Image by\u00a0Author The beta parameter determines the weight of recall in the combined score. beta &#60; 1 lends more weight to precision while beta &#62; 1 favors\u00a0recall. Let\u2019s have a look at the F-beta score and how the value fluctuates with\u00a0beta. from sklearn.metrics import fbeta_score print(fbeta_score(y_test, y_pred, beta=0.5))print(fbeta_score(y_test, y_pred, beta=1))print(fbeta_score(y_test, y_pred, beta=2)) Output:0.8534136546184740.85858585858585850.8638211382113821 Here, we have noticed that F-beta changes with beta movement, and now let\u2019s have a look at the same relative to precision and recall curve at various thresholds. import matplotlib.pyplot as pltfrom sklearn.metrics import recall_scorefrom sklearn.metrics import precision_scorefrom sklearn.metrics import precision_recall_curve _, _, threshold = precision_recall_curve(y_test, y_pred_prob) f1score = list()f05score = list()f2score = list()precision = list()recall = list()for th in threshold: y_test_pred = list() for prob in y_pred_prob: if prob &#62; th: y_test_pred.append(1) else: y_test_pred.append(0) f1score.append(f1_score(y_test, y_test_pred)) precision.append(precision_score(y_test, y_test_pred)) recall.append(recall_score(y_test, y_test_pred)) f05score.append(fbeta_score(y_test, y_test_pred, beta=0.5)) f2score.append(fbeta_score(y_test, y_test_pred, beta=2)) _, ax = plt.subplots(figsize=(8, 6))ax.set_xlabel(&#039;Threshold&#039;)plt.plot(threshold, precision, label=&#039;precision&#039;)plt.plot(threshold, recall, label=&#039;recall&#039;)plt.plot(threshold, f05score, label=&#039;F0.5&#039;)plt.plot(threshold, f1score, label=&#039;F1&#039;)plt.plot(threshold, f2score, label=&#039;F2&#039;)plt.legend(loc=&#039;lower left&#039;) Precision, Recall, F1 vs Threshold &#124; Image by\u00a0Author It is evident in the above graph that as we increase our beta value from 0, the curve starts moving towards the recall curve, which means with an increase in the beta value gives more importance to recall, and the below code to plot the F-measure at various beta and threshold values. betas = [0.1, 0.3, 0.5, 0.7, 1, 2, 5]_, ax = plt.subplots(figsize=(8, 6))ax.set_xlabel(&#039;Threshold&#039;)ax.set_ylabel(&#039;Fbeta&#039;)for beta in betas: fbetascore = list() for i, th in enumerate(threshold): y_test_pred = list() for prob in y_pred_prob: if prob &#62; th: y_test_pred.append(1) else: y_test_pred.append(0) fbetascore.append(fbeta_score(y_test, y_test_pred, beta=beta)) plt.plot(threshold, fbetascore, label=f&#039;F{beta}&#039;)plt.legend(loc=&#039;lower left&#039;) Fbeta vs Threshold &#124; Image by\u00a0Author References: [1] F1 Score. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score [2] Fbeta Score. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html F1 to F-beta was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 10 Oct 2022 12:18:53 +0000",
            "pubdate_parsed": [
                2022,
                10,
                10
            ],
            "email_sent": true
        },
        "Two New Papers By Deepmind Exemplify How Artificial Intelligence Can Help Human Intelligence": {
            "url": "https://towardsai.net/p/l/two-new-papers-by-deepmind-exemplify-how-artificial-intelligence-can-help-human-intelligence",
            "description": "Last Updated on October 10, 2022 by Editorial Team Author(s): LucianoSphere Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. In these new works, AI assists the advancement of pure mathematics by helping to discover new relationships between mathematical objects&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 10 Oct 2022 11:43:56 +0000",
            "pubdate_parsed": [
                2022,
                10,
                10
            ],
            "email_sent": true
        },
        "DeepMinds AlphaTensor: Deepminds Alphatensor: The AI That Is Reinventing Math": {
            "url": "https://towardsai.net/p/l/deepminds-alphatensor-deepminds-alphatensor-the-ai-that-is-reinventing-math",
            "description": "Last Updated on October 10, 2022 by Editorial Team Author(s): Salvatore Raieli Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. How the DeepMind\u2019s latest model could revolutionize math Image generated with OpenAI Dall-E\u00a02 Without realizing it, any of our activities, in one way or another, involve matrix multiplications. The whole of computing relies on them; being able to improve efficiency is fundamental. DeepMind (a year after revolutionizing biology with AlphaFold2) presented an article in which, using reinforcement learning, it manages to increase the efficiency of matrix multiplication. In this article, we discuss how and why it is important. How we still struggle with\u00a0matrices image by Roman Mager at unsplash.com Algorithms have been fundamental since the beginning of history. Both the Greeks and the Egyptians invented algorithms that enabled them to succeed in great works. Algorithms are also the basis of modern civilization, and without realizing it, they underpin almost every field of knowledge and its application. On the other hand, discovering new algorithms is by no means easy (we have all suffered in studying algorithms and data structures, but discovering new ones is even more difficult). One of the most important algorithms today is the multiplication of two matrices. Why? Because practically all types of data can be represented as matrices. In fact, images can be represented as matrices, can be used to solve linear equations, used in graph video games, weather simulations, etc. Furthermore, most artificial intelligence algorithms can be reduced to multiplications of matrices (which are then efficiently processed by a\u00a0GPU). Matrix multiplication. image source: DeepMind\u00a0blogpost Matrix multiplication seems like a very simple concept, but given its importance, being able to improve its efficiency even a little would save an enormous amount of computation. For centuries, mathematicians believed that known matrix multiplication was an efficient method. In 1969, the community was shocked by the fact that the efficiency was actually suboptimal, as was demonstrated by Volker Strassen. matrix multiplication: comparison between the two algorithms, Strassen\u2019s algorithm is more efficient because uses one scalar multiplication less. image source: DeepMind\u00a0blogpost Now, one less scalar multiplication may not seem like a big deal. But if we multiply 1 billion matrices, we have saved 1 billion scalar multiplications. The problem was that Strassen\u2019s method only fit the multiplication of two 2&#215;2 matrices. How DeepMind Solves the\u00a0problem DeepMind on Twitter: \"ICYMI: On the cover of @Nature &#8211; #AlphaTensor, an AI system for discovering novel, efficient, and exact algorithms for matrix multiplication.Learn more \u2b07\ufe0fhttps://t.co/E18DezAevbhttps://t.co/SvHgsaitFt https://t.co/ia9OQYuwZg pic.twitter.com/2eQsBCC9H5 / Twitter\" ICYMI: On the cover of @Nature &#8211; #AlphaTensor, an AI system for discovering novel, efficient, and exact algorithms for matrix multiplication.Learn more \u2b07\ufe0fhttps://t.co/E18DezAevbhttps://t.co/SvHgsaitFt https://t.co/ia9OQYuwZg pic.twitter.com/2eQsBCC9H5 Strassen\u2019s work showed that matrix-multiplication algorithms can be discovered by finding new ways to decompose a 3D array of numbers called a matrix multiplication tensor into a sum of elementary building blocks.\u200a\u2014\u200aNature comments on the\u00a0article The researchers at DeepMind have turned the problem of matrix multiplication into a kind of single-player game (after all, they are particularly experienced in the field after AlphaZero and AlphaGo). In fact, in this case, the board is a three-dimensional tensor (a tensor is practically a matrix, and a 3D tensor is a 3D matrix), and the player moves around trying to arrive at the optimal solution (modify the tensor and zero out its entries). If the player succeeds, the result of his moves is the correct matrix multiplication algorithm (efficiency is given by the number of steps taken to zero out the tensor). So the aim is to minimize the number of moves (steps) to zero out the tensor. Pretty clever,\u00a0right? Image source:\u00a0here The researchers used reinforcement learning in order to \u2018play\u2019. As described in the article, one can consider this system as an adapted version of AlphaZero (where the goal of the agent was to win at Go, chess, and other games). For this reason, the model was called AlphaZero. The problem described in these terms sounds simple, but as described by the DeepMind researchers, in reality, there are so many potential combinations: This game is incredibly challenging\u200a\u2014\u200athe number of possible algorithms to consider is much greater than the number of atoms in the universe, even for small cases of matrix multiplication. Compared to the game of Go, which remained a challenge for AI for decades, the number of possible moves at each step of our game is 30 orders of magnitude larger (above 1033 for one of the settings we consider).\u200a\u2014\u200aDeepMind\u00a0Blogpost image from Felix Mittermeier at usplash.com Now, to succeed, the authors used a new type of architecture incorporating problem-specific inductive biases; they also used synthetic data and some information about the problem (symmetries). To be more specific, the researchers used a transformer-based architecture (using cross-attention, causal self-attention, etc., here and here is a detailed image of the structure). The model was then trained using reinforcement learning (the input is, in fact, the current state and the 3D tensor, and the previous\u00a0actions. Model structure of AlphaTensor. Image source: original\u00a0paper At the beginning of the training, the model has no knowledge of existing algorithms to multiply matrices, but during the training, it gets better. Interestingly, AlphaTensor first rediscovery algorithms that are already known and then find unknown algorithms (practically surpassing human intuition) This resulted in the discovery of algorithms that multiply large matrices 10\u201320% faster than those commonly used on that piece of hardware.\u200a\u2014\u200asource Speed-ups of the AlphaTensor-discovered algorithms tailored for a GPU. image source:\u00a0here Another interesting result is that practically the space of matrix multiplication algorithms is richer than previously thought. Now, this sounds like mathematical jargon, but it actually means that the authors were able to adapt AlphaTensor to look for more efficient algorithms depending on the case needed: i.e. whether a matrix multiplication algorithm was [&#8230;]",
            "pubdate": "Mon, 10 Oct 2022 11:43:54 +0000",
            "pubdate_parsed": [
                2022,
                10,
                10
            ],
            "email_sent": true
        },
        "AI Voice Assistants Could Now Become Amazing Language Teachers": {
            "url": "https://towardsai.net/p/l/ai-voice-assistants-could-now-become-amazing-language-teachers",
            "description": "Last Updated on October 13, 2022 by Editorial Team Author(s): Rafe Brena, PhD Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Whisper + PaLM is just another level Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 13 Oct 2022 12:08:45 +0000",
            "pubdate_parsed": [
                2022,
                10,
                13
            ],
            "email_sent": true
        },
        "Expand Your Skills with Open-Source Graph Database NebulaGraph": {
            "url": "https://towardsai.net/p/l/expand-your-skills-with-open-source-graph-database-nebulagraph",
            "description": "Last Updated on October 13, 2022 by Editorial Team Author(s): Cornellius Yudha Wijaya Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Learn widely used Graph Database for your skillset Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 14 Oct 2022 00:13:14 +0000",
            "pubdate_parsed": [
                2022,
                10,
                14
            ],
            "email_sent": true
        },
        "Why would a Traditional Data Scientist Learn ANN Technology?": {
            "url": "https://towardsai.net/p/l/why-would-a-traditional-data-scientist-learn-ann-technology",
            "description": "Last Updated on October 14, 2022 by Editorial Team Author(s): Poornachandra Sarang Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Bringing out the importance of ANN over GOFAI Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 14 Oct 2022 13:08:13 +0000",
            "pubdate_parsed": [
                2022,
                10,
                14
            ],
            "email_sent": true
        },
        "Googles Audiolm: Generating Music by Hearing a Songs Snippet": {
            "url": "https://towardsai.net/p/l/googles-audiolm-generating-music-by-hearing-a-songs-snippet",
            "description": "Last Updated on October 14, 2022 by Editorial Team Author(s): Salvatore Raieli Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Whether music or speech, Google&#039;s new model can continue playing what is\u00a0hearing. image by Marius Masalar at unsplash.com AudioLM is Google\u2019s new model, capable of generating music in the same style as the prompt. The model is also capable of generating complex sounds such as piano music or people talking. the result is stunning. In fact, it seems to be indistinguishable from the original. Why is generating music difficult? image by Dolo Iglesias at unsplash.com Generating music is not an easy task. In fact, generating audio signals (music, ambient sounds, people&#039;s speech) requires multiple scales of abstraction. For example, music has a structure that has to be analyzed over a long period of time and is also composed of numerous interacting signals. Even personal speech itself can be analyzed at different levels, be it the simple acoustic signal or phonetics, but also in terms of prosody, syntax, grammar, or semantics. Several attempts have been made previously. The first attempts to generate music focused on generating MIDI files (an interesting project where they generated MIDI music for piano was created in 2018 using a transformer). In addition, some studies focused on tasks such as text-to-speech, where speech is generated from a transcript. The problem is that everything that is not in the transcript is not translated into the audio file. Several studies explain how in human communication, pauses and inflections, and other signals are extremely important. For example, those using Alexa or other speakers have noticed that the voice does not sound natural. Especially in the early days, no matter how correct the pronunciation was, it sounded unnatural and gave an uncanny\u00a0effect. AudioLM, the new Google\u00a0model image by Priscilla Du Preez at unsplash.com A few days ago, Google announced the release of a new model: \u201cAudioLM: a Language Modeling Approach to Audio Generation\u201d. The new model is capable of generating audio (such as realistic music and speech) just by listening to\u00a0audio. Google AI on Twitter: \"Learn about AudioLM, an audio generation framework that demonstrates long-term consistency (e.g., syntax in speech &#38; melody in music) and high fidelity, with applications for speech synthesis and computer-assisted music. \u2193 https://t.co/onTH6HdCcX / Twitter\" Learn about AudioLM, an audio generation framework that demonstrates long-term consistency (e.g., syntax in speech &#38; melody in music) and high fidelity, with applications for speech synthesis and computer-assisted music. \u2193 https://t.co/onTH6HdCcX As they blogged, there has been a great improvement in the field of Natural Language Processing (NLP) in recent years. In fact, language models have proven to be extremely effective in a number of tasks. Many of these systems are based on the use of transformers, and those who have used them know that one of the initial pre-processing steps is to tokenize (break up the text into smaller units that are assigned a numerical value). The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.\u200a\u2014\u200aGoogle AI\u00a0blogpost AudioLM does not need transcription or labeling. The authors collected a database of sounds and fed it directly to the model. The model compresses the sound files into a series of snippets (sort of tokens). These tokens are then used as if they were an NLP model (the model, in this way, uses the same approach to learn patterns and relationships between the various audio snippets). In the same way as a text-generating model, AudioLM generates sounds from a\u00a0prompt. The result is very interesting, the sound is much more natural. AudioLM seems to be able to find and recreate certain patterns present in human music (like subtle vibrations contained in each note when piano keys are struck). In the link below, Google has provided a number of examples if you are curious to\u00a0listen: AudioLM AudioLM has been trained on a vast library of sounds that include not only music but also human voices. For this reason, the model can generate sentences produced by a human being. The model is able to pick up the accent of the speaker and add pauses and exclamations. Although many of the sentences generated by the model do not make sense, the result is impressive. Indeed, treating sequences of sounds as if they were sequences of words may seem like a clever approach, nonetheless, some difficulties remain: First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences\u200a\u2014\u200awhile a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.\u200a\u2014\u200aGoogle AI\u00a0blogpost In more detail, the audio tokenization approach was already tried by OpenAI Jukebox, only that the model generated many more artifacts, and the sound did not sound as\u00a0natural. Overview of the tokenizers used in AudioLM. image from the original paper\u00a0(here) As described by the authors, the model consists of three\u00a0parts: a tokenizer model, which maps a sequence of sounds into a discrete sequence of tokens. This step also reduces the size of the sequence (the sampling rate is reduced by about 300\u00a0times). a decoder-only transformer (a classical language model) that maximizes the likelihood of predicting the next tokens in the sequence. The model contains 12 layers with 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of\u00a04096 a detokenizer model that transforms predicted tokens into audio\u00a0tokens. The model was trained on 60,000 hours of English speech and 40,000 hours of music for the piano experiments. [&#8230;]",
            "pubdate": "Sat, 15 Oct 2022 00:08:46 +0000",
            "pubdate_parsed": [
                2022,
                10,
                15
            ],
            "email_sent": true
        },
        "Airflow is on the Cloud | ELT Pipeline Orchestration With Airflow & AWS": {
            "url": "https://towardsai.net/p/l/airflow-is-on-the-cloud-elt-pipeline-orchestration-with-airflow-aws",
            "description": "Last Updated on October 15, 2022 by Editorial Team Author(s): Kaan Boke Ph.D. Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. You will see the ELT pipeline with Airflow orchestration. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 15 Oct 2022 12:13:09 +0000",
            "pubdate_parsed": [
                2022,
                10,
                15
            ],
            "email_sent": true
        },
        "4 Questions To Ask Before You Hire a Data Engineer": {
            "url": "https://towardsai.net/p/l/4-questions-to-ask-before-you-hire-a-data-engineer",
            "description": "Last Updated on October 16, 2022 by Editorial Team Author(s): Rijul Singh Malik Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A blog around how to hire and work with a data engineer. Photo by Towfiqu barbhuiya on\u00a0Unsplash What is a data engineer? If you\u2019re in charge of managing the data team, then you\u2019ll need to employ a data engineer that will help you run the show. But if you\u2019re a business owner who is into data engineering and is wondering how you could become a data engineer, you\u2019ll need to ask yourself a few questions. Data engineers are in high demand these days because data is becoming a big thing in business, and companies are struggling to find enough people who can actually manage it all. From big data to cloud-based data solutions, more and more businesses are going digital, and a data engineer is a person who takes care of the data. So what does a data engineer do? What kind of work do they\u00a0perform? Why do you need a data engineer? Most businesses have a lot of data, but few actually know what to do with it. Data science is a hot field, with students going to school to learn the skills needed to filter, sort, and search through information. Data engineers play a major role in the data science field, as they tend to be the ones who organize the data, make sense of it, and distribute it to the right people. Not only do they organize the data, but they also make sure it is secure and accessible to everyone who needs it. Data engineers are more than just data analysts; they are more web developers, web technology experts, and database administrators who all work together to make sure all of the data is technically sound. Data engineers are in high demand and are vital members of any team. They are responsible for building, maintaining, and scaling data-related systems and tools. They design and build the data platform on which other teams build their products and services. Data engineers are software developers, but their focus is on building data products that are accessible, reliable, and scalable rather than on writing code. However, there are many different types of data engineers. Through my experience and research, I\u2019ve discovered that data engineers can be broken up into three different categories: data scientists, data analysts, and data engineers. The difference between each of these categories is quite clear, but it\u2019s not always clear which category an engineer falls under. In this blog, I\u2019m going to run down the differences between each type of data engineer so that you can figure out which type of engineer you need for your\u00a0team. How to hire the right data engineer for your\u00a0product? It\u2019s really difficult to find the right data engineer for your product, especially if you have a tight budget and time constraints. It\u2019s not just about finding any data engineer that can do the job, but it\u2019s about finding the right blend of skills, experience, and cost. It\u2019s not an easy task. It\u2019s like asking yourself: \u201cWhat makes a great product engineer? What makes a great software engineer?\u201d The answer is the same for both engineers. The top data engineers are the ones who are good at solving problems, have a good grasp of the tools and technologies needed, and have the ability to work on the\u00a0product. We have all read countless articles on how to hire the right data scientist. But data engineers are a different breed of animal. They are the ones that turn the data scientists\u2019 models into a live system. They are the ones that get to the core of your business, and they are the ones who can really contribute to the growth of your product. As a business, it is important to understand how to hire the right data engineer. You need to know the right questions to ask at an interview, the kind of skills you need to look for and the kind of personality you need to match your company\u2019s culture. We have done all the hard work for you and listed down 4 questions that you need to ask your next data engineer. How to work with a data engineer? Working with a data engineer is something many businesses need to do every day. You can find yourself in a variety of situations as a business owner or as a manager. Sometimes you will be trying to get the most out of your data to make smart decisions. Sometimes you will be trying to get your data into a shape that it can be used by your in-house team or by the public. Or maybe you are looking to hire a new data engineer. Whatever the case, there are some questions that you need to ask yourself. These four questions will help you get the most out of your\u00a0data. There are many types of data engineers. Some of them are generalists, and others are specialists. But regardless of your requirements, recruitment is a big challenge. For example, if you are looking for a junior data engineer, you will have to find a candidate who has both the skills and enthusiasm to learn new skills. If you are looking for a data engineer with experience, you will need to find a candidate who is experienced and willing to relocate. When hiring a data engineer, pay attention to the following things: What type of data engineer do you need? What skills do they have? What is their level of experience? What salary level are you ready to offer? How do candidates respond to the questions you ask? Tell candidates about the job, the culture of your company, [&#8230;]",
            "pubdate": "Mon, 17 Oct 2022 00:14:04 +0000",
            "pubdate_parsed": [
                2022,
                10,
                17
            ],
            "email_sent": true
        },
        "Julia Tuple and Dictionary": {
            "url": "https://towardsai.net/p/l/julia-tuple-and-dictionary",
            "description": "Last Updated on October 17, 2022 by Editorial Team Author(s): Vivek Chaudhary Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. The objective of this article is to understand Julia Data Structures Tuple and Dictionary and the various operations associated with\u00a0them. TUPLES: are an immutable collection of distinct values of the same or different datatypes separated by\u00a0commas. Syntax: (item1, item2,\u00a0item3\u2026) Dictionary: is a collection of key-value pairs, where each value in the dictionary can be accessed with its key. These key-value pairs need not be of the same data\u00a0type. Syntax: Dict(key1 =&#62; value1, key2 =&#62; value2,\u00a0\u2026) Tuples and methods() #create an empty tuplet1=()println(\u201cIf a Tuple is empty returns \u201c, isempty(t1))t1=(\u201cjulia\u201d,\u201dpython\u201d,\u201dsql\u201d)println(\u201cIf a Tuple is non-empty returns \u201c,isempty(t1)) Output:If a Tuple is empty returns trueIf a Tuple is non-empty returns false Indexing, Slicing, Iteration and Assignment #declare a tuple beveragebeverage= (\u201ccoffee\u201d,\u201dtea\u201d,\u201dgreentea\u201d,\u201dgrrencoffee\u201d,\u201dginger tea\u201d) #Indexingprintln(beverage[1])Output: coffee println(beverage[0]) #index in julia starts from 1BoundsError: attempt to access NTuple{5, String} at index [0]Stacktrace: [1] getindex(t::Tuple, i::Int64) @ Base .\\tuple.jl:29 [2] top-level scope @ In[7]:2 [3] eval @ .\\boot.jl:373 [inlined] [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String) @ Base .\\loading.jl:1196 #Slicingprintln(beverage[2:4]) Output:(\"tea\", \"greentea\", \"grrencoffee\") #Iterationprintln(\"iterating over tuple :\")for i in beverage println(i)end Output:iterating over tuple :coffeeteagreenteagrrencoffeeginger tea #Tuple manipulation:beverage[3]=\"chamomile tea\" Output:MethodError: no method matching setindex!(::NTuple{4, String}, ::String, ::Int64)Stacktrace: [1] top-level scope @ In[7]:2 [2] eval @ .\\boot.jl:373 [inlined] [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String) @ Base .\\loading.jl:1196 #Note: Tuples are immutable, and they don\u2019t support item assignment. Tuple methods() #reverse a tuplerev= reverse(beverage)println(rev) Output:(\"ginger tea\", \"grrencoffee\", \"greentea\", \"tea\", \"coffee\") #length of tupleprintln(\u201clength of tuple \u201c,length(beverage)) Output: length of tuple 5 #check if empty or notprintln(isempty(beverage)) Output: false Named Tuples: are just like Tuples except that each element additionally has a name! They have a special syntax using \u201c=\u201d inside a tuple.Syntax:(name1 = item1, name2 = item2,\u00a0\u2026) lang= (l1=\u201djulia\u201d,l2=\u201dpython\u201d,l3=\u201dSQL\u201d)println(lang) Output: (l1 = \"julia\", l2 = \"python\", l3 = \"SQL\") println(lang[2]) Output: python println(lang.l1) Output: julia Dictionary and methods() #create a dictionaryempIDs= Dict(\u201cvivek\u201d=&#62;121,\u201daniket\u201d=&#62;134,\u201djagdish\u201d=&#62;101)println(typeof(empIDs))println(empIDs) Output:Dict{String, Int64}Dict(\"jagdish\" =&#62; 101, \"vivek\" =&#62; 121, \"aniket\" =&#62; 134) #create dict from tupleeds=[(\"vivek\",2000),(\"jagdish\",5000),(\"aniket\",3500)]println(typeof(eds))println(eds) Output:Vector{Tuple{String, Int64}}[(\"vivek\", 2000), (\"jagdish\", 5000), (\"aniket\", 3500)] #conver the tuple into dictionaryedict=Dict(eds)println(typeof(edict))println(edict) Output:Dict{String, Int64}Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2000, \"aniket\" =&#62; 3500) Access Dictionary items println(edict)Output:Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2000, \"aniket\" =&#62; 3500) key=keys(edict)println(\u201ckeys are :\u201d,key)Output: keys are :[\"jagdish\", \"vivek\", \"aniket\"] val=values(edict)println(\u201cvalues are :\u201d, val)Output: values are :[5000, 2000, 3500] #Iterate over dictionary using for loop for (key,val) in edict println(key,\"=&#62;\",val)end Output:jagdish=&#62;5000vivek=&#62;2000aniket=&#62;3500 Dictionary methods() get() and\u00a0getkey() #get() method: Return the value stored for the given key#syntax: get(collection, key, default) println(edict)Output:Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2000, \"aniket\" =&#62; 3500) println(get(edict,\"vivek\",-1))Output: 2000 #default value is mandatory otherwise it throws errorprintln(\"Key not found \",get(edict,\"viv\",0))Output: Key not found 0 #wihtout default valueprintln(\"Key not found \",get(edict,\"viv\")) Output:MethodError: no method matching get(::Dict{String, Int64}, ::String)Closest candidates are: get(::Dict{K, V}, ::Any, ::Any) where {K, V} at C:\\Users\\lenovo\\AppData\\Local\\Programs\\Julia-1.7.1\\share\\julia\\base\\dict.jl:506 get(::IJulia.IJuliaStdio, ::Any, ::Any) at C:\\Users\\lenovo\\.julia\\packages\\IJulia\\e8kqU\\src\\stdio.jl:21 get(::Test.GenericDict, ::Any, ::Any) at C:\\Users\\lenovo\\AppData\\Local\\Programs\\Julia-1.7.1\\share\\julia\\stdlib\\v1.7\\Test\\src\\Test.jl:1800 ...Stacktrace: [1] top-level scope @ In[57]:7 [2] eval @ .\\boot.jl:373 [inlined] [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String) @ Base .\\loading.jl:1196 #getkey()Return the key matching argument key if one exists in collection, otherwise return default. println(getkey(edict,\"aniket\",-1))Output: aniket println(getkey(edict,\"viv\",-1))Output: -1 delete() items println(edict)Output:Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2000, \"aniket\" =&#62; 3500) delete!(edict,\u201dvivek\u201d)println(edict)Output:Dict(\"jagdish\" =&#62; 5000, \"aniket\" =&#62; 3500) #delete a non-existing itemdelete!(edict,\"viv\")Output: Dict{String, Int64} with 2 entries: \"jagdish\" =&#62; 5000 \"aniket\" =&#62; 3500 merge() dictionaries #merge dictionaries using merge()d1 = Dict(\u201ca\u201d=&#62;1, \u201cb\u201d=&#62;9, \u201cc\u201d=&#62;3)d2 = Dict(\u201ce\u201d=&#62;7, \u201cf\u201d=&#62;2, \u201ch\u201d=&#62;5)println(merge(d1,d2))Output:Dict(\"f\" =&#62; 2, \"c\" =&#62; 3, \"e\" =&#62; 7, \"b\" =&#62; 9, \"a\" =&#62; 1, \"h\" =&#62; 5) #common keys presentd1 = Dict(\"a\"=&#62;1, \"b\"=&#62;9, \"c\"=&#62;3)d2 = Dict(\"e\"=&#62;7, \"b\"=&#62;2, \"h\"=&#62;5)println(merge(d1,d2))Output:Dict(\"c\" =&#62; 3, \"e\" =&#62; 7, \"b\" =&#62; 2, \"a\" =&#62; 1, \"h\" =&#62; 5) #merge dictionaries using mergewith()d1 = Dict(\"a\"=&#62;1, \"b\"=&#62;9, \"c\"=&#62;3)d2 = Dict(\"e\"=&#62;7, \"b\"=&#62;2, \"h\"=&#62;5)println(mergewith!(+,d1,d2))Output:Dict(\"c\" =&#62; 3, \"e\" =&#62; 7, \"b\" =&#62; 11, \"a\" =&#62; 1, \"h\" =&#62; 5) add and change dict\u00a0items #add new item to a dictprintln(edict)Output: Dict(\"jagdish\" =&#62; 5000, \"aniket\" =&#62; 3500) edict[\u201cvivek\u201d]=2700println(edict)Output: Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2700, \"aniket\" =&#62; 3500) #change dict vaue for a keyprintln(edict)Output: Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 2700, \"aniket\" =&#62; 3500) edict[\"vivek\"]=4000println(edict)Output: Dict(\"jagdish\" =&#62; 5000, \"vivek\" =&#62; 4000, \"aniket\" =&#62; 3500) To Summarize: Julia tuple and tuple\u00a0methods. Dictionary and\u00a0methods. Thanks for reading my blog and supporting the content. Appreciation always helps to keep up the spirit. I will try my best to keep coming up with quality content. Connect with me to get updates about upcoming new\u00a0content. Keep Supporting. Julia Tuple and Dictionary was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 17 Oct 2022 12:13:24 +0000",
            "pubdate_parsed": [
                2022,
                10,
                17
            ],
            "email_sent": true
        },
        "Top 10 SQL Queries a Data Scientist Should Know": {
            "url": "https://towardsai.net/p/l/top-10-sql-queries-a-data-scientist-should-know-2",
            "description": "Last Updated on October 17, 2022 by Editorial Team Author(s): Saurabh Saxena Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Photo by Campaign Creators on\u00a0Unsplash Structured Query Language or SQL is a query-based universal language to read, write and manage databases. In any Machine Learning pipeline, whether it is data, metadata, or logs, SQL is widely used in all operations. When you are working with a database, the four basic operations any must know is CRUD (Create, Read, Update, and Delete). However, we will target read, update, and delete operations in this\u00a0blog. Let\u2019s Start from the\u00a0basics. 1) SELECT If you want to know the first_name, last_name, and email of the customers, you can specify the list of the desired columns along with the SELECT and FROM keywords. SELECT first_name, last_name, email FROM customer Image by\u00a0Author Whereas, to query all the columns in the table, then asterisks(*) is the one that goes with SELECT\u00a0. SELECT * FROM customer Image by\u00a0Author 2) DISTINCT DISTINCT restricts the duplication of rows in the query. Example: Query to print the creation date of the customers. SELECT DISTINCT create_dateFROM customer Image by\u00a0Author 3) WHERE WHERE is used in a query to filter the result. Example: Query all the addresses in the Texas district. SELECT * FROM addressWHERE district=&#039;Texas&#039; Image by\u00a0Author 4) GROUP BY and\u00a0HAVING GROUP BY clause clusters the rows with the same values. Example: What is the total payment made by each customer? SELECT customer_id, SUM(amount) AS total_amount FROM paymentGROUP BY customer_id Image by\u00a0Author In SQL, aggregation functions such as SUM, AVG, andCOUNT can not be used in the WHERE clause. We need to use the HAVING\u00a0clause. SELECT customer_id, SUM(amount) AS total_amount FROM paymentGROUP BY customer_idHAVING SUM(amount) &#60;=50 Image by\u00a0Author 5) ORDER BY and\u00a0LIMIT ORDER BY sorts the results in ascending or descending order based on certain columns, ASC and DESC keywords decide the order of the sort. Example: Find the total payment made by each customer and sort in ascending order. SELECT customer_id, SUM(amount) AS total_amount FROM paymentGROUP BY customer_idORDER BY total_amount ASC Image by\u00a0Author LIMIT restricts the number of rows to a specified number in the result. Example: Find the top 3 highest paying customers. SELECT customer_id, SUM(amount) AS total_amount FROM paymentGROUP BY customer_idORDER BY total_amount DESCLIMIT 3 Image by\u00a0Author Note: Default Order By clause sorts the result in ASCENDING order. Few SQL providers support TOP n keywords to limit the number of rows in the\u00a0result. 6) CASE The CASE expression goes through conditions and returns a value when the first condition is met. Example: A customer is a premium customer if the amount exceeds\u00a050. Image by\u00a0Author SELECT customer_id, SUM(amount) AS total_amount, CASE WHEN SUM(amount)&#62;=50 THEN &#039;Premium Customer&#039; ELSE &#039;Standard Customer&#039; END AS customer_statusFROM paymentGROUP BY customer_idORDER BY total_amount ASC Image by\u00a0Author 7) JOINS INNER JOIN that will result in the common rows between two\u00a0tables. LEFT JOINreturns all rows from the left table and the matching rows from the right table. If no matching rows are found in the right table, NULL is used. RIGHT JOIN is a vice-versa of left\u00a0join. FULL JOIN is a combination of left and right join. If no matching rows are found in the left or right table, NULL is\u00a0used. Image by\u00a0Author SELECT c.first_name, c.last_name, c.customer_id, SUM(p.amount)FROM customer cINNER JOIN payment p ON c.customer_id = p.customer_idGROUP BY c.first_name, c.last_name, c.customer_id Image by\u00a0Author SELECT c.first_name, c.last_name, c.customer_id, SUM(p.amount)FROM customer cLEFT JOIN payment p ON c.customer_id = p.customer_idGROUP BY c.first_name, c.last_name, c.customer_id Image by\u00a0Author 8) Subqueries A subquery is a SQL query nested inside a larger query.\u00a0Example: SELECT payment_id, amount, (SELECT SUM(amount) FROM payment) AS total_amountFROM payment Image by\u00a0Author 9) Windows Function with\u00a0Rank Window functions apply to aggregate and ranking functions over a particular window (set of rows). OVER clause is used with window functions to define that window. It can be combined with PARTITION BY and ORDER BY\u00a0. SELECT customer_id, SUM(amount) AS total_amount, RANK() OVER (ORDER BY SUM(amount) DESC)FROM paymentGROUP BY customer_id Image by\u00a0Author 10) INSERT, UPDATE, DELETE and\u00a0TRUNCATE As the name suggests INSERT is used to push one or more records into the table, while UPDATEis used to modify values in the table based on certain conditions provided by WHERE\u00a0clause. DELETE and TRUNCATE both are used to remove records from the table where DELETE removed the record based on WHERE condition and TRUNCATE removed all the records in any\u00a0table. Note: We have used PostgreSQL, which is an open-source Relational DBMS, pgAdmin as a Client. Queries ran on \u2018dvdrental\u2019 database, and I have mentioned all the resources about the database and PostgreSQL in the reference section. References: [1] PostgreSQL Official Page. https://www.postgresql.org/ [2] pgAdmin Official Page. https://www.pgadmin.org/ [3] PostgreSQL docker Image. https://hub.docker.com/_/postgres [4] DVD Rental DB. https://www.postgresqltutorial.com/postgresql-getting-started/load-postgresql-sample-database/ Top 10 SQL Queries a Data Scientist Should Know was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 17 Oct 2022 12:13:22 +0000",
            "pubdate_parsed": [
                2022,
                10,
                17
            ],
            "email_sent": true
        },
        "Getting Started with Applied AI and NLP": {
            "url": "https://towardsai.net/p/l/getting-started-with-applied-ai-and-nlp",
            "description": "Last Updated on October 17, 2022 by Editorial Team Author(s): Daniel Tannor Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Here are 4 Applied AI Project Ideas that You Can Code Right\u00a0Now No need for complex self-made Machine Learning Projects\u200a\u2014\u200ait\u2019s time to use APIs instead. All you need is the coding\u00a0basics. Some Background on Natural Language Processing and Applied\u00a0AI Applied AI is the branch of artificial intelligence that brings it out of the lab and into the real world, enabling computers and computer-controlled robots to execute real tasks.\u200a\u2014\u200acognizant.com Just like the above quote states\u200a\u2014\u200awe\u2019re going to use Applied AI so that we can execute real, everyday tasks. We\u2019re also going to leverage Natural Language Processing\u200a\u2014\u200agetting the machine to process and understand language so that our apps\u00a0can: Figure out which movie scenes are happy so that we can create happy movie scene compilations Summarize complete Zoom meetings into short summaries. and more. The Projects The projects are divided up into two categories: Emotion Based and Summarization Based, which I\u2019ll explain in the next paragraph. The summarization-based projects are meant to be huge time savers and productivity tools\u200a\u2014\u200ahelping people be more effective in reviewing meetings and conversations. The other projects are meant for entertainment and also improving one\u2019s social skills\u200a\u2014\u200awhether it\u2019s interviewing, dating, or meeting new people in social settings. Different Natural Language Processing Projects to\u00a0Code Emotion Based Emotion-based projects are projects that are based on detecting emotions. When our app manages to detect that an actress in a movie scene was happy, based on the words she said, that makes our project emotion-based. The emotion-based projects: Generate Happy Movie Scene Compilations Do you know those Youtube movie compilations where you get a bunch of scenes that are aligned around an\u00a0idea? Here\u2019s an example of a compilation of The Most Beautiful Shots in Movie\u00a0History We can use NLP in order to get the happy scenes out of movies and create our own movie compilation- Jim Carrey\u2019s smile comes to\u00a0mind: Jim Carrey in a happy movie scene in \u2018The\u00a0Mask\u2019 What we\u2019d do is have the NLP engine go over movie scripts and return the time stamps of scenes with the happiness emotion. We\u2019d then attach those scenes together to generate one long clip we can share on YouTube or social\u00a0media. You can do this using One AI or other free-to-use NLP APIs you insert in your code. I\u2019ll follow up on this in another\u00a0blog. Social Skills\u00a0Trainer Often times people don\u2019t know how a conversation went. What if we could build an app to provide feedback and improve people\u2019s social\u00a0skills? Below is what the app might look like\u00a0\u2014 Adam is asking Sarah out, and the conversation starts out pretty awkward. However, you can see that Adam\u2019s last remark gets him points and creates a positive interaction with\u00a0Sarah. Adam can now understand which sentences gave him points and which sentences lost him points, and he can improve his interactions with people over\u00a0time. See how to build this\u00a0here. Summarization Based The summarization-based projects are projects that are based on the NLP engine summarizing large amounts of text in order to provide accurate summaries of\u00a0content. This content can also originate in video or audio, we would then transcribe that content so that the AI runs on the text\u00a0output. The Summarization projects: Auto-Summarize Zoom\u00a0Meetings Create your own app to auto-summarize Zoom meetings. You can do this per meeting or even build an app to solve this problem at\u00a0scale. Aren\u2019t we all tired of long boring meetings? Think of how much time you\u2019d save for yourself and your colleagues if you could provide automatic, accurate summarizations of meetings on a weekly\u00a0basis. Falling asleep on a Zoom\u00a0meeting Some use cases\u00a0include: Summarize your own individual meetings Summarize company meetings on a weekly basis for\u00a0everyone Create a web app for people to summarize their\u00a0meetings See how to build an app like this\u00a0here. Auto-Summarize Slack Conversations Slack can quickly become one giant mess of information\u200a\u2014\u200ameeting notes, product requirements, or conversations with the\u00a0boss. Multiple channels and an overload of info on\u00a0Slack Can\u2019t find that last feature requirement from last week? What about the notes from your meeting with your 1:1 with your\u00a0boss? What if we could build an app to summarize the important points from the past week? These points would include important conversation summaries and action\u00a0items. Summary We\u2019ve seen a bunch of different NLP projects you can build today. Some of these ideas, or similar ones, could definitely be the basis of a startup\u200a\u2014\u200aas they solve real-world pain points that we all experience in our day to\u00a0day. We\u2019ve learned about applied AI and NLP to build apps that can help us with day-to-day tasks. In order to get started, all you need to do is make a few NLP API calls within your code, and you\u2019re good to\u00a0go. Please feel free to reach out and share project ideas you have in mind or what you\u2019ve started to\u00a0build. Getting Started with Applied AI and NLP was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 18 Oct 2022 00:13:40 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Detailed Dashboard Design Guidelines Used by Professionals": {
            "url": "https://towardsai.net/p/l/detailed-dashboard-design-guidelines-used-by-professionals",
            "description": "Last Updated on October 18, 2022 by Editorial Team Author(s): Saleha Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. As I said in my last article, data analysts give data meaning by taking raw data and turning it into data-driven visualizations that help&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 18 Oct 2022 13:18:31 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Airflow Production TipsProper Task (Not DAG) Catchup": {
            "url": "https://towardsai.net/p/l/airflow-production-tips%e2%80%8a-%e2%80%8aproper-task-not-dag-catchup",
            "description": "Last Updated on October 18, 2022 by Editorial Team Author(s): Guilherme Banhudo Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Airflow Production Tips\u200a\u2014\u200aProper Task (Not DAG)\u00a0Catchup Photo by Jackson Simmer on\u00a0Unsplash Apache Airflow has become the de facto standard for Data Orchestration. However, throughout the years and versions, it accumulated a set of nuances and bugs which can hinder its production usage. This series of articles aims at walking Apache Airflow users through the process of overcoming these issues, the same issues I have\u00a0faced. Note: As usual, all of the code is available in my GitHub repository, here. Proper Task (Not DAG)\u00a0catchup TLDR: Airflow\u2019s ability to get the previous successful TaskInstance\u2019s date does not work as intended, returning the previous successful DAG\u2019s run date instead, which prevents you from accurately and properly picking up missing/failed information. In this post, you will find out how to bypass it. See the original bug\u00a0report. TLDR #2: Click here to skip directly to the\u00a0solution Problem Statement One of the most interesting and helpful features of Apache Airflow is its ability to catch up with the past should a task fail. No, I am not referring to the catchup parameter you can define in your DAG, but rather giving Tasks the ability to encompass the previously failed TaskInstances (and their execution date) using either\u00a0the JINJA template, or directly accessing the previous TaskInstance whose status was a\u00a0success: https://medium.com/media/6712aa06940cf80b6b5899f0941a7cdb/href This is particularly useful when you want to make sure your data stays up to date in Ingestions or your ETL consumes all information for both the current and failed past TaskInstances, for instance: https://medium.com/media/0953ef183f32473dc50b371943c359e7/href On a more practical example, considering the image below, you would expect that if the first run of Task transform_data failed whilst using the above configuration, the failed/missing data would be captured on the next run of transform_data, in the next hour,\u00a0correct? Simple ETL-like DAG consisting of three linearly dependent tasks Expected Event\u00a0Loop: Loop 1\u2013 Run task extract_from_db, capture one hour of data, status: success\u2013 Run task tranform_data, transform one hour of data, status: fail\u2013 Run task load_target_db, load one hour of data, status:\u00a0fail Loop 2\u2013 Run task extract_from_db, capture one hour of data, status: success\u2013 Run task tranform_data, transform two hours of data (current and previously failed), status: success\u2013 Run task load_target_db, load two hours of data (current and previously failed), status:\u00a0success Unfortunately, that is incorrect. Whilst it is the obvious and expected behavior, Airflow\u2019s legacy code prevents such from happening. See the original bug\u00a0report. Below is the corresponding rendered template for the first and second runs of Task transform_data, respectively: https://medium.com/media/a5d909553366e31daf47859ce48ff7ce/hrefhttps://medium.com/media/47b352bc376b8b25f6ad021d6c5f4d5a/href Note that the second task does not compensate for the previous failure. Instead, it just passes its own previous execution date param, 20221013T010000 instead of including the previous successful run, the full catch-up 20221013T000000. Why does this\u00a0happen? Apache Airflow\u2019s default behavior, when the previous execution date was successful, is to look at the previous DAG\u2019s successful execution date, not of the TaskInstance\u2019s, which effectively makes your DAGs incapable of automatically catching up unless the entire DAG\u00a0fails. So did we expect to happen instead? Well, we expect the rendered template in the second run to have\u00a0been: https://medium.com/media/580e070ba983bf9d439e375d50e10f34/href You can see the original bug report on the problem, dating back to 2021 and even longer on StackOverflow. The Solution Like many other Python frameworks, Apache Airflow uses an ORM (Object Relational Mapper) to abstract access to its backend database. Specifically, the usual SQL Alchemy project has been leveraged to accelerate Apache Airflow. This allows us to access the metadata database directly and manipulate it according to our requirements. The solution to our problem is then divided into four simple\u00a0steps: Retrieve the TaskInstance objects based on a specified State Retrieve the last successful TaskInstance instance for the provided\u00a0Task Retrieve the DAGRun associated with the last successful TaskInstance instance for the provided\u00a0Task Arm our DAG with the ability to use this information! Lastly, a practical section example has been added to illustrate the\u00a0goal! Step 1: Retrieve the TaskInstance objects based on a specified State The first step corresponds to having the ability to query the ORM to retrieve the database to retrieve the last TaskInstance run corresponding to a specific\u00a0state: Note: The function is a generalization used to allow you to pick up any specific state you may\u00a0require https://medium.com/media/69163909fc202dcb6b4cd6e9b8d111d2/href The function is self-explanatory, querying the TaskInstance ORM object and filtering it via two parameters: the desired lookup state and TaskInstance instance. Step 2: Retrieve the last successful TaskInstance instance for the provided\u00a0Task Leveraging the previously defined generalization, we can now query the ORM to retrieve only the last run whose state was\u00a0Success. https://medium.com/media/447f8d9eef5ff7a199bf819156028272/href Step 3: Retrieve the DAGRun associated with the last successful TaskInstance instance for the provided\u00a0Task. Having tracked down the last successful TaskInstance instance, we can now retrieve the DAGRun object associated with said TaskInstance instance. The DAGRun model contains information regarding the ran TaskInstance instance alongside a vast array of useful information: https://medium.com/media/9ec125df8a1323739a9e033e470c3e12/href Armed with this knowledge, we can now retrieve the DAGRun instance associated with our retrieved last successful TaskInstance instance in a similar process as\u00a0before: https://medium.com/media/f41229e4fba53554130c090faa31156d/href Step 4: Arm our DAG with the ability to use this information! Finally, we have to make sure Apache Airflow is aware of our new functions and can inject them into the JINJA\u00a0engine. We can do so by importing the function and passing the user-defined function to the DAG directly on its constructor, in this case, via context-manager: https://medium.com/media/ed392fc52b84de16a6fbffe107dacd8f/href You can now freely call the function directly in JINJA and pass it as an argument to your Extraction, ETL, or any other processes you may\u00a0have! https://medium.com/media/44892e950bd60b06ba275a11c320dafe/href Let me know in the comments if you find these resources useful and as usual, you can find this code in [&#8230;]",
            "pubdate": "Tue, 18 Oct 2022 13:18:29 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "How Should We Detect and Treat the Outliers?": {
            "url": "https://towardsai.net/p/l/how-should-we-detect-and-treat-the-outliers",
            "description": "Last Updated on October 18, 2022 by Editorial Team Author(s): Gowtham S R Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What are outliers? How do we need to detect outliers? How do we need to treat the outliers? Photo by\u00a0author Table of Contents: \u00b7 What are Outliers?\u00b7 Outlier Detection and Removal Techniques:\u00b7 Z Score-based method\u00b7 IQR technique\u00b7 Percentile Method What are Outliers? An outlier is that datapoint or observation which behaves very differently from the rest of the\u00a0data. If we are finding the average net worth of a group of people, and if we find Elon Musk in that group, then the complete analysis will go wrong because of just one outlier. This is a reason why outliers should be treated properly before building a machine learning\u00a0model. Simple ways to write Complex Patterns in Python in just 4mins. If we are building a linear regression model, which has an independent feature, \u2018Num of hours studied\u2019, and the dependent feature, \u2018marks scored\u2019, and if the data is distributed as shown below, then the model will perform\u00a0well. If we have 3 students who scored good marks even after studying for fewer hours, then the regression line shifts in order to fit the outlier points as shown below, resulting in giving bad results to the actual\u00a0data. Machine learning algorithms in which the calculation of weight is involved, like linear regression, logistic regression, Ada boost, and deep learning models, will get impacted by the outliers. Tree-based algorithms like Decision trees, Random Forest will get less impacted by outliers. How do I Verify the Assumptions of Linear Regression? In anomaly detection algorithms like insurance fraud detection or credit card fraud detection, we need to catch the outliers, in this kind of situation, the purpose is to catch the outliers. So we need to treat the outliers carefully, Trimming: Remove the outliers from the dataset before training a machine learning model. E.g., Remove the students from the dataset in the above\u00a0example. Capping: Keep a maximum or minimum threshold and give values to the data points accordingly. E.g., if we are working on the age feature, we can keep the threshold of 85 and assign the value of 85 to all the people with age greater than\u00a085. Discretization: This is the method in which numerical features are converted to discrete using bins. E.g., if the age 80\u201390 is considered as a single bin, then all the ages between 80 and 90 will be treated\u00a0equally. Outlier Detection and Removal Techniques: 1. Z Score-based method The main assumption in this technique is that the data should be normally distributed or close to normal distribution. If the data is normally distributed, the Empirical Rule says that 68.2% of the data points lie in between the 1st standard deviation, 95.4% of the data points lie in between the 2nd standard deviation, and 99.7% of the data points will be between the 3rd standard deviation. Data points that lie outside the 3rd standard deviation can be treated as outliers. As 99.7% of the data will lie within the 3 standard deviations, we can treat the rest of the data which lie outside the 3 standard deviations as outliers. Standardization or Z-Score Normalization is one of the feature scaling techniques, here, the transformation of features is done by subtracting from the mean and dividing by standard deviation. This is often called Z-score normalization. The resulting data will have the mean as 0 and the standard deviation as\u00a01. Standardization vs Normalization Let us look at the practical implementation of this technique. import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings(&#039;ignore&#039;) df = pd.read_csv(&#039;placement_dataset.csv&#039;) The dataset has 2 independent features cgpa and placement_exam_marks. plt.figure(figsize=(10,5)) plt.subplot(1,2,1)sns.distplot(df[&#039;cgpa&#039;]) plt.subplot(1,2,2)sns.distplot(df[&#039;placement_exam_marks&#039;]) The distribution of the data shows that the feature cgpa is normally distributed, and the other feature, placemet_exam_marks is\u00a0skewed. So, the feature cgpa qualifies for the Z Score-based method of the outlier detection technique. print(&#039;Mean value of CGPA {}&#039;.format(df[&#039;cgpa&#039;].mean()))print(&#039;Min value of CGPA {}&#039;.format(df[&#039;cgpa&#039;].min()))print(&#039;Max value of CGPA {}&#039;.format(df[&#039;cgpa&#039;].max()))print(&#039;Standard deviation value of CGPA {}&#039;.format(round(df[&#039;cgpa&#039;].std(),2))) # the boundary values are: print(&#039;Highest value of cgpa&#039;, round(df[&#039;cgpa&#039;].mean()+3*df[&#039;cgpa&#039;].std(),3))print(&#039;Lowest value of cgpa&#039;, round(df[&#039;cgpa&#039;].mean()-3*df[&#039;cgpa&#039;].std(),3)) Below are the 5 data points which are detected as outliers. This can also be achieved using the Z score formula, which is shown\u00a0below. Outlier Treatment: Trimming: In this method, we can remove all the data points that are outside the 3 standard deviations. Sometimes, if the dataset has a large number of outliers, then we lose a significant amount of\u00a0data. Capping: In this method, the outlier data points are capped with the highest or lowest values, as shown\u00a0below. Why Is Multicollinearity A Problem? 2. IQR technique This method is used when the distribution of the data is\u00a0skewed. The IQR describes the middle 50% of values when ordered from lowest to highest. To find the interquartile range (IQR), \u200bfirst, find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and\u00a0Q1. IQR =\u00a0Q3-Q1 Minimum value = Q1\u20131.5\u00a0*IQR Maximum value = Q3+1.5*IQR The data points which are lesser than the minimum value and the data points which are greater than the maximum value are treated as outliers. Let us look at the practical implementation of this technique. plt.figure(figsize=(10,5)) plt.subplot(1,2,1)sns.distplot(df[&#039;cgpa&#039;]) plt.subplot(1,2,2)sns.distplot(df[&#039;placement_exam_marks&#039;]) The feature placement_exam_marks is skewed and qualifies for the IQR method of outlier detection. df[&#039;placement_exam_marks&#039;].skew()0.8356419499466834 #Finding the IQR Q1 = df[&#039;placement_exam_marks&#039;].quantile(0.25)Q3 = df[&#039;placement_exam_marks&#039;].quantile(0.75) IQR = Q3-Q1 upper_limit = Q3+1.5*IQRlower_limit = Q1-1.5*IQR print(&#039;lower limit: &#039;, lower_limit)print(&#039;upper limit: &#039;, upper_limit)print(&#039;IQR:&#039; , IQR) lower limit: -23.5upper limit: 84.5IQR: 27.0 The above 15 data points are detected as outliers. Trimming: In this method, we can remove all the data points that are outside the minimum and [&#8230;]",
            "pubdate": "Tue, 18 Oct 2022 13:18:27 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Encoding Categorical Data- The Right Way": {
            "url": "https://towardsai.net/p/l/encoding-categorical-data-the-right-way",
            "description": "Last Updated on October 18, 2022 by Editorial Team Author(s): Gowtham S R Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Photo from Unsplash by Icons8\u00a0Team Table of Contents \u00b7 Types of Data \u2218 Continuous Data \u2218 Discrete Data \u2218 Nominal Data \u2218 Ordinal Data \u00b7 How to Encode Categorical data? \u2218 Ordinal Encoding \u2218 Nominal Encoding \u2218 OneHotEncoding using Pandas \u2218 Dummy Variable Trap \u2218 OneHotEncoding using Sklearn Types of\u00a0Data In statistics and machine learning, we classify data into one of two types namely\u200a\u2014\u200aNumerical and Categorical Numerical data is categorized into discrete and continuous data. Categorical data is divided into two types, nominal data, and ordinal\u00a0data. Image by the author explaining Types of\u00a0Data Continuous Data Continuous data is data that takes any number(it can include decimal values). It has an infinite number of probable values that can be selected within a given specific\u00a0range. Example: Weight and height of a person, temperature, age, distance. Image by\u00a0author Discrete Data Discrete data can take only discrete values(it can not have decimal values). Discrete information contains only a finite number of possible values. Those values cannot be subdivided meaningfully. Here, things can be counted in whole\u00a0numbers. Example: Number of people in a family, number of bank accounts a person can have, number of students in a classroom, number of goals scored in a football\u00a0match. Image by\u00a0author How Should We Detect and Treat the Outliers? Nominal Data Nominal data is defined as data that is used for naming or labeling variables without any quantitative value. It is sometimes called \u201cnamed\u201d\u00a0data. There is usually no intrinsic ordering to nominal data. For example, Gender is a nominal variable having 2 categories, but there is no specific way to order from highest to lowest and vice\u00a0versa. Examples: letters, symbols, words, gender, color,\u00a0etc. Image by\u00a0author Ordinal Data Ordinal data is a type of data that follows a natural order. It is a type of categorical data with an order. The variables in ordinal data are listed in an ordered\u00a0manner. Examples: Customer rating(Good, Average, Bad), Medal category in Olympics(Gold, Silver,\u00a0Bronze) Image by\u00a0author Simple ways to write Complex Patterns in Python in just 4mins. How to Encode Categorical data? Encoding categorical data is a process of converting categorical data into integer format so that the data can be provided to different models. Categorical data will be in the form of strings or object data types. But, machine learning or deep learning algorithms can work only on numbers. So, being machine learning engineers, it is our duty to convert categorical data to numeric\u00a0form. Ordinal Encoding We need to maintain the intrinsic order while converting ordinal data to numeric data. So, each category will be given numbers from 0 to a number of categories. If we have 3 categories in the data, such as &#039;bad&#039;, &#039;average&#039;, and &#039;good&#039;, then bad will be encoded as 0, average as 1, and good as 2. So that the order is maintained. When we train the machine learning model, it will learn the pattern with the intrinsic order giving better\u00a0results. We make use of OrdinalEncoder class from sklearn to encode the ordinal\u00a0data. Standardization vs Normalization Let us see how we can encode ordinal data practically. import pandas as pdimport numpy as np df = pd.read_csv(&#039;customer.csv&#039;) df.head() df[&#039;age&#039;].unique()array([30, 68, 70, 72, 16, 31, 18, 60, 65, 74, 98, 51, 57, 15, 75, 59, 22,19, 97, 32, 96, 53, 69, 48, 83, 73, 92, 89, 86, 34, 94, 45, 76, 39,23, 27, 77, 61, 64, 38, 25], dtype=int64) df[&#039;gender&#039;].unique()array([&#039;Female&#039;, &#039;Male&#039;], dtype=object) df[&#039;review&#039;].unique()array([&#039;Average&#039;, &#039;Poor&#039;, &#039;Good&#039;], dtype=object) df[&#039;education&#039;].unique()array([&#039;School&#039;, &#039;UG&#039;, &#039;PG&#039;], dtype=object) df[&#039;purchased&#039;].unique()array([&#039;No&#039;, &#039;Yes&#039;], dtype=object) \u2018age\u2019\u200a\u2014\u200anumerical data \u2018gender\u2019\u200a\u2014\u200anominal categorical data \u2018review\u2019\u200a\u2014\u200aordinal categorical data \u2018education\u2019\u200a\u2014\u200aordinal categorical data \u2018purchased\u2019\u200a\u2014\u200athe target\u00a0variable Let us separate ordinal data and learn how to encode\u00a0them. df = df.iloc[:,2:]df.head() X = df.iloc[:,0:2]y = df.iloc[:,-1] from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2, random_state=1) from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder(categories=[[&#039;Poor&#039;,&#039;Average&#039;,&#039;Good&#039;],[&#039;School&#039;,&#039;UG&#039;,&#039;PG&#039;]]) ordinal_encoder.fit(X_train) X_train = ordinal_encoder.transform(X_train)X_test = ordinal_encoder.transform(X_test) In the above code, we can see that we are passing the categories in the order to the ordinalEncoder class, with poor being the lowest order and Good being the highest order in the case of feature\u00a0review. Similarly, in the case of the education column, school is the lowest order, and PG is in the highest\u00a0order. After transformation, we can observe the above result, which shows the first 5\u00a0rows. The first row had review = Average and education = UG, so it is transformed as\u00a0[1,1] The second row had review = Poor and education = PG, so it is transformed as\u00a0[0,2] Nominal Encoding Nominal data will not have intrinsic order, we make use of OneHotEncoder class from sklearn to encode the nominal\u00a0data. If we start encoding categories from 0,1,2 to all the categories, then the machine learning algorithm will give importance to 2 (more than 0 and 1), which will not be true as the data is nominal and will have no intrinsic order. So, this method will not work in the case of nominal\u00a0data. Here each of the categories will be transformed into a new column and will be given values of 0 or 1 depending on the occurrence of the respective category. So, the number of features will increase after the transformation. Let us see how we can encode nominal data practically. Let us take a titanic dataset with features \u2018Sex\u2019 and \u2018Embarked\u2019 (both are nominal features). import pandas as pdimport numpy as np df = pd.read_csv(&#039;titanic.csv&#039;) df.head() df[&#039;Sex&#039;].unique()array([&#039;male&#039;, &#039;female&#039;], dtype=object) df[&#039;Embarked&#039;].unique()array([&#039;S&#039;, &#039;C&#039;, &#039;Q&#039;], dtype=object) df[&#039;Survived&#039;].unique()array([0, 1], dtype=int64) OneHotEncoding using\u00a0Pandas pd.get_dummies(df,columns=[&#039;Sex&#039;,&#039;Embarked&#039;]) The below output shows how each of the categories is transformed into a column. The column \u2018Sex\u2019 has got transformed into \u2018Sex_female\u2019 and \u2018Sex_male\u2019 and the column \u2018Embarked\u2019 has got transformed into \u2018Embarked_C\u2019, \u2018Embarked_Q\u2019, and \u2018Embarked_S\u2019. These new attributes/columns created are called [&#8230;]",
            "pubdate": "Tue, 18 Oct 2022 13:04:23 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "How To Split The Data Effectively for Your Data Science Project": {
            "url": "https://towardsai.net/p/l/how-to-split-the-data-effectively-for-your-data-science-project",
            "description": "Last Updated on October 18, 2022 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Data is one of the most important resources for any data science project. But what good is abundant data if you can&#x2019;t use it effectively&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 18 Oct 2022 12:18:56 +0000",
            "pubdate_parsed": [
                2022,
                10,
                18
            ],
            "email_sent": true
        },
        "Genetic Algorithm Optimization": {
            "url": "https://towardsai.net/p/l/genetic-algorithm-optimization",
            "description": "Last Updated on October 22, 2022 by Editorial Team Author(s): Chinmay Bhalerao Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A detailed explanation of the evolutionary and nature-inspired optimization algorithm Photo by Sangharsh Lohakare on\u00a0Unsplash \u201cThe environment selects those few mutations that enhance survival, resulting in a series of slow transformations of one lifeform into another, the origin of a new species.\u201d- CARL SAGAN, 1934\u20131996 Evolution The concept of natural selection and biological evolution changed the perspective of thinking about evolution theory. Evolution is always a slow and gradual process that takes many centuries to work. Millions of species present on earth today arose from a single original life form through a branching process called speciation. complex creatures evolve from more simplistic ancestors naturally over time. In a nutshell, as random genetic mutations occur within an organism\u2019s genetic code, the beneficial mutations are preserved because they aid survival\u200a\u2014\u200aa process known as \u201cnatural selection.\u201d Photo by Johannes Plenio on\u00a0Unsplash DNA changes with time with different mutations and a combination of random inheritance, which is a recombination of parental DNA and mutational behaviors. This is conveniently described using tools from probability theory and stochastic processes. \u201cEvolution is the aggregation of thousands of semi-random events and the natural pressure to reproduce or die\u201d-Darwinian evolution https://medium.com/media/232b8f2e71b1022f8628652aaa260398/href To understand evolution, there is a great example of the prey and predator system. Fox eats rabbits, and faster rabbits tend to save their lives, whereas slower one has more probability of getting caught. Given a population, Smarter and quicker individuals are less likely to be consumed by foxes. As a result, they can continue to reproduce, which is what rabbits do best. Some of the less intelligent and slower rabbits also make it by chance. As the remaining population begins to reproduce, a good combination of rabbit genetic material is produced. Foxes and rabbits evolve with time [image by author created by Dall.\u00a0E] some slow rabbits breed with fast rabbits, some fast rabbits breed with fast rabbits, And on top of that, nature throws in a wild hare every once in a while by mutating some of the rabbit&#039;s genetic material. Because more parents who were quicker and smarter survived the foxes, the resulting rabbits (on average) are faster and smarter than those in the original group. The good thing is that the foxes are also undergoing a similar procedure. Otherwise, the rabbits would develop into creatures that are too quick and intelligent for the foxes to\u00a0capture. Genetic Algorithm optimization GAs was first proposed by John Holland in the 1960s. GA incorporates methods proposed by and inspired by the natural selection process. As I mentioned in the above example, the fittest individual has more chance or probability to survive. The same in GA. From the pool of solutions, the one who has more fitness has more chance to survive. Let&#039;s start the actual understanding of the Genetic Algorithm. Let&#039;s understand basic terminologies. Genetic Algorithm terminologies Parent: The one from which offspring is produced. member of the current generation. Offspring: Also known as a child. offspring is a member of the next generation Population: Population is a set of all possible solutions or chromosomes exhibiting similar gene structure Fitness: Fitness is a number assigned to an individual representing a measure of goodness. More fitter, the more chance of survival and reproduction. Chromosome: Chromosome is a coded form of a possible set of solutions consisting of genes made of one of two or more versions of DNA sequence (alleles). Crossover: Crossover is the phenomenon where generally two parents produce two offspring by gene exchange. Mutation: Mutation is a random change of the value of a gene we flip a bit and change 0 to 1 and 1 to\u00a0zero. Generation: Generation is a successively created population. In Genetic Algorithms, it is also termed as \u201citerations\u201d. Outline of genetic algorithm The genetic algorithm starts with defining a proper problem statement and creating a set of initial possible populations of solutions. The population is randomly generated chromosomes. like the evolution procedure, the procedure of natural selection starts. During successive generations, chromosomes in the population are rated for their fitness or rated for their chance to become the solution. Now, based on the evaluation of their fitness value, the new set of Chromosomes forms using a selection operation followed by crossover and mutation. The basic flow of Genetic Algorithm procedure [Image by\u00a0Author] Selection The first important step in Genetic algorithm operations is selection. You might have a question here! what are we selecting? I will answer this question. The fittest solution or fittest offspring/Child is our aim. for that, obviously, we have to select a parent depending on its fitness. If we have population \u201cX\u201d, then selection creates an intermediate population of \u201c X\u2019 \u201d [X_hash] with the copies of chromosomes of X. More fitter chromosomes will have more copies of it\u00a0!!! After this, the selection mechanism starts. The selection operation is carried out in two ways\u00a0: Roulette wheel selection You know the word Roulette wheel from casino or gambling, right? It&#039;s a much similar concept. In gambling, we have wheels, and we predict numbers. That is, the dice will land on that predicted number or not! In the GA roulette wheel selection, the wheel is the same. just a stop point is introduced at a fixed point. The chromosome takes the value on the pie or roulette wheel exactly equal to the fitness it\u00a0has. Chromosomes and their fitness values [Image by\u00a0author] It is obvious that a more physically fit individual has a larger pie on the wheel and a higher chance of landing in front of the fixed point when the wheel is revolved. As a result, an individual likelihood of selection is directly correlated with [&#8230;]",
            "pubdate": "Sat, 22 Oct 2022 12:19:06 +0000",
            "pubdate_parsed": [
                2022,
                10,
                22
            ],
            "email_sent": true
        },
        "Why AI Fairness Is Important in Telecom Personalization": {
            "url": "https://towardsai.net/p/l/why-ai-fairness-is-important-in-telecom-personalization",
            "description": "Last Updated on October 22, 2022 by Editorial Team Author(s): Arslan Shahid Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Personalization is the name of the game in the telecom\u00a0industry Image from\u00a0Pexel For the past two years, I have been working in the personalization and contextual marketing department of one of the major mobile operators in Pakistan. For people who don\u2019t know personalization is designing products, recommendations, and ads for individuals based on their attributes and preferences. Telecommunications is an oligopoly market with little to no product differentiation. Mobile data, minutes, and SMS can be thought of as commodities. One way to add value for customers and businesses is to construct personalized products for groups or individuals. Personalization is achieved by using tools such as machine learning and statistical analysis. However, as I will explain in this post, if not checked for, personalization could have adverse effects on individuals and society at\u00a0large. What do you mean by \u2018AI Fairness\u2019? Contrary to what some people believe, AI or statistical models are not free from biases or making discriminatory predictions or recommendations. All models represent statistical approximations of the data based on a set of variables we, as modelers, think are predictive of the thing we are interested in. By choosing the attributes that the modeler thinks are predictive and measuring data that they think is appropriate, a modeler often makes a choice that reflects their beliefs. Furthermore, the data itself could reflect a historic privilege or discrimination being done against a group of\u00a0people. AI fairness is the study of how algorithms treat groups of people. Attempting to make them less predatory or discriminatory against a set of protected groups or set attributes like gender, ethnicity, country of origin, and illnesses. Provided that we believe that being part of a certain group does not or should not be a basis for an algorithm to predict or make a decision that results in an adverse outcome. For example, a credit scoring algorithm assigning a lower credit score to a black person when compared to a white person with \u2018similar\u2019 attributes is \u2018unfair\u2019. The quotation marks signify that this depends on the definitions of similar and fairness. How is group fairness defined mathematically? Although there are plenty of ways to define fairness as a mathematical construct, below are a few of the most common, taken from CS 294: Fairness in Machine Learning, UC Berkeley, Fall\u00a02017 Fairness in Classification: Demographic Parity: Image from fairmlclass Simply explained, demographic parity means that the probability a certain classification algorithm predicts the true(C=1) class is the same when an individual is from group A=0 or group A=1. Where A could represent any arbitrary type of protected group, such as\u00a0gender. Accuracy Parity: Image from fairmlclass Intuitively, a classifier is accuracy parity-wise fair when it assigns all classes (represented by Y) with the same probability when a person has attribute a=0 or a=1. For example, the university admissions algorithm accepts, rejects, or waitlists, each with the same probability for a male or\u00a0female. Precision Parity Image from fairmlclass A classifier is precision-parity fair when the probability that an individual is from the true class, given that the classifier predicted that they are from the true class; is the same for an individual with attribute A=0 or A=1. Take the example of an algorithm that decides to predict a rare disease. Doctors are only allowed to give medicine to patients predicted to have the disease. If we want the algorithm to be precision-parity-wise fair, the proportion of people who had the disease and were predicted to have it should be the same across all protected groups. There are plenty of novel and use-case-specific definitions of fairness for classification problems. The above mention definitions include some of the most well-known and actively\u00a0used. Fairness in Regression: Statistical Parity: image from Microsoft Research For a regression problem, the modeler is trying to minimize the expected loss between the distribution of observed values (Y) and the distribution of predicted values (f(x)). For statistical parity to hold we minimize the loss subject to the constraint that the CDF conditioned on protected attribute A does not deviate from the unconditional CDF by a threshold epsilon. Bounded Group\u00a0Loss: Image from Microsoft Research Bounded group loss means that for every protected attribute a, the loss function is below a certain threshold. For example, we could require a regression to predict house prices have at least an RMSE of $2500 for all protected groups like\u00a0ethnicity. What do you mean by personalization in\u00a0Telecom? Considering GSM services are commodities, product differentiation is achieved in telecom using two broad categories: Price differentiation: You give services to customers at a different price point than immediate competitors. If your services are cheaper &#38; every network has the same service quality, you are likely to gain market\u00a0share. Network differentiation: A segment of customers will always be willing to pay a higher price for better quality. In telecom, quality is solely derived from spectrum allocation and network presence in the area. For example, each telecom operator in Pakistan has marked its territory where they provide the best services. Usually, it makes the most sense to buy services from the operator with the highest coverage in the\u00a0area. Personalization can help Telecos achieve product differentiation either through price or network on an individual level. For example, you can bundle together different GSM products like you can give a customer who is more inclined to use data but doesn\u2019t use call or SMS as much an \u2018averaged\u2019 bundled price where their data is subsidized but you charge more for\u00a0voice. How is personalization achieved? The following are some of the techniques and methods used in the telecom industry to enable personalization (not exhaustive). Dynamic pricing: GSM services [&#8230;]",
            "pubdate": "Sat, 22 Oct 2022 11:33:34 +0000",
            "pubdate_parsed": [
                2022,
                10,
                22
            ],
            "email_sent": true
        },
        "Medium API: Get Posts Using Python": {
            "url": "https://towardsai.net/p/l/medium-api-get-posts-using-python",
            "description": "Last Updated on October 22, 2022 by Editorial Team Author(s): Nishu Jain Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. User-written articles, publication articles, and top feeds Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 22 Oct 2022 11:33:31 +0000",
            "pubdate_parsed": [
                2022,
                10,
                22
            ],
            "email_sent": true
        },
        "Few-shot Financial Sentiment ClassificationDoes It Work?": {
            "url": "https://towardsai.net/p/l/few-shot-financial-sentiment-classification%e2%80%8a-%e2%80%8adoes-it-work",
            "description": "Last Updated on October 22, 2022 by Editorial Team Author(s): Neo Yi Peng Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. SetFit for Financial Sentiment Analysis Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 22 Oct 2022 07:03:52 +0000",
            "pubdate_parsed": [
                2022,
                10,
                22
            ],
            "email_sent": true
        },
        "AI Image Editing from Text! Imagic Explained": {
            "url": "https://towardsai.net/p/l/ai-image-editing-from-text-imagic-explained",
            "description": "Last Updated on October 23, 2022 by Editorial Team Author(s): Louis Bouchard Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Imagic: Manipulate images using pre-trained image generator models! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 23 Oct 2022 12:14:42 +0000",
            "pubdate_parsed": [
                2022,
                10,
                23
            ],
            "email_sent": true
        },
        "Top 5 Machine Learning Industries in 2022": {
            "url": "https://towardsai.net/p/l/top-5-machine-learning-industries-in-2022",
            "description": "Last Updated on October 24, 2022 by Editorial Team Author(s): Mostafa Ibrahim Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Machine learning is rapidly expanding into a lot of industries and sectors! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 24 Oct 2022 12:08:59 +0000",
            "pubdate_parsed": [
                2022,
                10,
                24
            ],
            "email_sent": true
        },
        "10 AI Websites That Will Excite You to The Core!": {
            "url": "https://towardsai.net/p/l/10-ai-websites-that-will-excite-you-to-the-core",
            "description": "Last Updated on October 25, 2022 by Editorial Team Author(s): Chinmay Bhalerao Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. interesting artificial intelligence-based websites and their\u00a0working Image created by the author on DALL\u00b7E\u00a02 1. VERSE BY\u00a0VERSE Are you struggling to write a nice poem for your girlfriend/boyfriend or your loved ones? Don&#039;t worry\u00a0!! Verse by Verse will completely help you to compose poems for yourself. Screen recording of Verse by Verse by\u00a0author With the help of Verse by Verse, you may create a poem using ideas from some of America\u2019s most well-known poets, including Dickinson, Whitman, Poe, Wheatley, Longfellow, and others. They trained AI systems to function as your muse while you write a poem of your own, providing recommendations in the manner of each individual poet to make this possible. Here is the link! enjoy and stretch your creativity with the help of legendary poets! Verse by Verse 2. DALL\u00b7E\u00a02 If there is a discussion about amazing websites in AI, then how can anyone forget DALL\u00b7E 2? Do you want to make a portrait or drawing or create an image of your complicated thoughts? But is it hard for you to draw it on paper? What if you can just describe your thoughts in words/texts and create images/drawings? Don&#039;t worry, DALL\u00b7E 2 will help you here in the same\u00a0fashion! Wired imagination is given by the author in the form of text [Screenshot by\u00a0author] Wired imagination is given by the author in the form of text [Screenshot by\u00a0author] Wired imagination is given by the author in the form of text [Screenshot by\u00a0author] According to openAI blog post, OpenAI\u2019s DALL\u00b7E 2 is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text\u2013image pairs. We\u2019ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing\u00a0images. Previously it had a waitlist. After registering, you have to wait for your turn, but recently it is free from the waitlist, and you can directly log in and create images from texts. You will get 50 credit points after registration and 15 points for each month. Each search will be equal to 1\u00a0point. DALL\u00b7E 2 3. Nightcafe Image created by the author on nightcafe using B/W fashion giving input \u201cold and veteran\u00a0people\u201d It works on a stable diffusion model, which will be released in 2022. Although it can be used for various tasks, including inpainting, outpainting, and creating image-to-image translations directed by text prompts, its primary usage is to generate detailed visuals conditioned on text descriptions. How does it\u00a0work? According to the stability.ai blog, The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to\u00a0all. Create Something Amazing 4. Let\u2019s\u00a0Enhance Want to make your distorted and blurry photos beautiful? you can use let\u2019s enhance! It makes low-res images into clear and sharp pics to be proud of, using cutting-edge image processing algorithms. Let&#039;s Enhance &#8211; Image Quality Online App &#38; Free Photo Enlarger 5. QuillBot\u00a0AI If you are unable to phrase your words and struggling to write, you can use the Paraphrasing tool of QuillBot\u00a0AI. screenshot by the author of Quillbot\u2019s paraphrasing tool the difference between other rephrasing tools is it uses state-of-the-art AI to predict, phrase, and rewrite your thoughts. It also gives us many options for your texts to a phrase, including synonyms analysis, grammar check, etc. It is very useful for new writers who want to write but don&#039;t know what perfect phrasing can\u00a0be! https://quillbot.com/ 6. Talk to\u00a0books Talk to the book is one of the most amazing websites that I have found! you can talk to a book like in the same way as we talk to humans. you can ask anything on the dialog box, and it will return the extracted semantic pieces of answers that fit best to your question. screen recording by the\u00a0author How does it\u00a0work? As research google blog says, the approach was to use billions of lines of dialogue to teach an AI how real human conversations flow. Once the AI has learned from that data, it is then able to predict how likely one statement would follow another as a response. In these demos, the AI is simply considering what you type to be an opening statement and looking across a pool of many possible responses to find the ones that would most likely\u00a0follow. Talk to Books 7. Teachable Machine Is model training quite expensive right? AI-ML-DL models require training which must have been faster and less expensive. what if we get an online website that will assist you with the training image, sound &#38; pose data? that&#039;s actually\u00a0cool! Teachable Machine 8. The Person Does Not\u00a0Exist It is so cool that AI can generate images of people that are not alive and have never existed in the world. It uses GAN ( Generative Adversarial Network) generated real \u201cFAKE PEOPLE\u201d. check the site and refresh the page. It generates a fake image of no-existing people in 5\u00a0seconds. This Person Does Not Exist 9.Resemble.ai Resemble\u2019s AI voice generator lets you create human-like voiceovers in seconds. EMOTIONS-Add an infinite amount of emotions to your voice without any new data. Happy, sad, angry, all preloaded, out of the box. SPEECH-TO-SPEECH Transform your voice into the target voice with real-time speech-to-speech. Granular control over [&#8230;]",
            "pubdate": "Tue, 25 Oct 2022 12:18:23 +0000",
            "pubdate_parsed": [
                2022,
                10,
                25
            ],
            "email_sent": true
        },
        "How to Tell if Properties are Under/Overvalued like a Data Scientist": {
            "url": "https://towardsai.net/p/l/how-to-tell-if-properties-are-under-overvalued-like-a-data-scientist",
            "description": "Last Updated on October 26, 2022 by Editorial Team Author(s): Diego Unzueta Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. I made a Bot that Scans the Web for the Best Property Deals Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 27 Oct 2022 00:08:23 +0000",
            "pubdate_parsed": [
                2022,
                10,
                27
            ],
            "email_sent": true
        },
        "Yes, We Need Statistical Significance Testing": {
            "url": "https://towardsai.net/p/l/yes-we-need-statistical-significance-testing",
            "description": "Last Updated on October 27, 2022 by Editorial Team Author(s): Benjamin Marie Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A rule of thumb may yield correct results but can&#x2019;t be scientifically credible Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 27 Oct 2022 12:13:17 +0000",
            "pubdate_parsed": [
                2022,
                10,
                27
            ],
            "email_sent": true
        },
        "Non Max Suppression (NMS) in PyTorch": {
            "url": "https://towardsai.net/p/l/non-max-suppression-nms-in-pytorch",
            "description": "Last Updated on October 31, 2022 by Editorial Team Author(s): Francesco Zuppichini Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Implementing non max suppression in PyTorch Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 31 Oct 2022 12:28:39 +0000",
            "pubdate_parsed": [
                2022,
                10,
                31
            ],
            "email_sent": true
        },
        "10 AI Websites That Will Excite You to The Core! Part:2": {
            "url": "https://towardsai.net/p/l/10-ai-websites-that-will-excite-you-to-the-core-part2",
            "description": "Last Updated on November 1, 2022 by Editorial Team Author(s): Chinmay Bhalerao Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Interesting artificial intelligence-based websites and their\u00a0working Image created by the author on DALL\u00b7E\u00a02 Before diving into actual websites, there is PART 1 of this series. You can go through it\u00a0also. 10 AI Websites That Will Excite You to The Core! 1] OpenAI playground What if a website can write code for you? a blog for you? resignation letter for you? love letter for your girlfriend/boyfriend or loved ones? Yes!!! all things are possible by OpenAI playground! Almost any activity that includes understanding or producing natural language or code can be used with the OpenAI API. We provide a range of models with varying degrees of power appropriate for various activities, as well as the option to fine-tune your own unique models. Everything from content creation to semantic search and classification may be done using these\u00a0models. OpenAI API Model: based on GPT-3 models called Davinci, Curie, Babbage, and Ada. You can choose any of the above according to your use\u00a0case. Asking to write resignation letter [Image by\u00a0author] Asking to write code [Image by\u00a0author] 2]Deepfakes web Video by deepfakes web Elon Musk or Leonardo DiCaprio\u00a0??? interestingly, both!!!!! what are deepfakes? Deepfakes are synthetic media in which a person in an existing image or video is replaced with someone else\u2019s likeness. The term \u201cdeepfakes\u201d is a combination of \u201cdeep learning\u201d and \u201cfake\u201d. Deepfakes use potent machine learning and artificial intelligence techniques to edit or synthesize visual and audio information that can more readily fool, even though the act of producing fake content is not\u00a0new. Make Your Own Deepfakes [Online App] 3] inferkit InferKit Using a cutting-edge neural network, the text generation tool from InferKit reads the text that you supply and generates what it believes will happen next. It can produce any amount of text about essentially any topic and is customizable. Who is this\u00a0for? Creative and fun uses of the network include writing stories or poetry. Other use cases might be marketing or auto-completion. Asking to write an essay on the topic \u2018my favorite cricketer\u2019 4]tensorflow playground Live training for classification problem TensorFlow playground is a website where you can visualize the training process with live changes in outputs with the help of numbers and actual diagrams. You can experiment with many things like changing activation, epochs, regularization, features, hidden layers, and many\u00a0more\u2026 The deep playground is an interactive visualization of neural networks written in TypeScript using d3.js. they use GitHub issues for tracking new requests and bugs. You can also contribute to their repository. Tensorflow &#8211; Neural Network Playground 5]Lalal.ai The stem separation problem has also been solved brilliantly by LALAL.AI if you need to preserve the instrumental backing track of a song but take out the vocals. One of the most powerful tools in\u00a0AI. It makes use of a neural network known as Cassiopeia, which was developed over numerous iterations using 20TB of training data. It can extract vocals, drums, guitar, piano, bass, synthesizer, and general accompaniment and is really easy to\u00a0use. Vocal Remover &#124; Isolate Voice &#38; Instrumental Online &#124; LALAL.AI The outcomes are remarkable and merit a try for yourself. LALAL.AI, thankfully, lets you input 10 minutes of music and provides a detailed preview of the extractions. Although you won\u2019t be able to download the extracted stems, it\u2019s still worth subscribing to the service if the results are good enough for your needs. You do not require a subscription, in contrast to other AI technologies. Instead, depending on your preference, LALAL.AI offers one-time packages for 90 or 300 minutes of extraction. So stretch your music senses to make your favorite music\u00a0piece! 6] Replika.com Feeling alone? want to chat with anyone? or a replica of you who thinks like you? visit this\u00a0website. An AI companion who is eager to learn and would love to see the world through your eyes. Replika is always ready to chat when you need an empathetic friend. Even though talking to Replika feels like talking to a human being, rest assured\u200a\u2014\u200ait\u2019s 100% artificial intelligence. Your Replika is unique to you and wants to know what your world is\u00a0like. creating my AI friend [Video by\u00a0author] My AI friend [Video by\u00a0author] Replika combines a sophisticated neural network machine learning model and scripted dialogue content. It has been trained on a large dataset to generate its own unique responses. Replika 7] Quick,\u00a0draw! Quick, Draw! is an online game developed by Google that challenges players to draw a picture of an object or idea and then uses a neural network artificial intelligence to guess what the drawings represent. The AI learns from each drawing, improving its ability to guess correctly in the future. Quick Draw is a new social space lottery game with drawings every five minutes that display winning numbers on monitors in-store. For a minimum of $1, players choose how many numbers (spots) they\u2019d like to play, which determines a player\u2019s odds and potential prizes. Quick, Draw! Video by\u00a0author 8] Hotpot.ai AI Art\u200a\u2014\u200ared-faced ball with blue eyes and blonde hair and two candle\u00a0sticks Hotpot uses AI and user-friendly tools to assist creators. Many people, especially those in developing countries, lack access to professional graphics, images, and text. This can be altered with more intelligent software. Our goal is to make video production and graphic design 10x faster and more accessible. their objective for professional designers is to augment the creative process by automating repetitive chores. The objective is to make design and image creation as simple as texting a friend for non-designers. text-to-image AI empowers anyone to create attractive paintings, illustrations, and images. Describe what you want, and watch Hotpot bring it to\u00a0life. Hotpot.ai You can create your own images [&#8230;]",
            "pubdate": "Wed, 02 Nov 2022 00:03:25 +0000",
            "pubdate_parsed": [
                2022,
                11,
                2
            ],
            "email_sent": true
        },
        "5 Types of ML Accelerators": {
            "url": "https://towardsai.net/p/l/5-types-of-ml-accelerators",
            "description": "Last Updated on November 2, 2022 by Editorial Team Author(s): Luhui Hu Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Comprehensive overview of machine learning accelerators for training and\u00a0serving The past decade has been the era of deep learning. We are thrilled with unstopping milestones from AlphaGo to DELL-E 2 and more. And we cannot count how many AI-powered things have happened in our daily lives, including Alexa devices, Ads recommendations, warehouse robots, self-driving cars, and\u00a0more. Photo by Joshua Woroniecki on\u00a0Unsplash Recent years have seen exponential growth in the scale of deep learning models. It is not news that Wu Dao 2.0 model contains 1.75 trillion parameters, and it takes about 25 days to train GPT-3 on 240 ml.p4d.24xlarge instances of the SageMaker training platform. But it becomes increasingly challenging as deep learning training and serving evolve. Scalability and efficiency are two major challenges for training and serving due to the growth of deep learning\u00a0models. Are deep learning systems stuck in a\u00a0rut? No! I introduced distributed parallel training for scaling out training in my earlier two articles: model parallelism and distributed parallel training. I shared ML compilers for accelerating training and\u00a0serving. Are these all solutions for the umbrella of deep learning? No! Here I\u2019ll summarize five primary types of ML accelerators or accelerating areas. Understand ML Lifecycle in AI Engineering Before fully covering ML accelerators, let\u2019s first visit the ML lifecycle. ML lifecycle is a lifecycle of data and models. Data is food for ML and can determine model quality. Every area in the lifecycle is full of opportunities for acceleration. MLOps can automate the process of ML model deployment and serving. But it is limited to the horizontal process of AI workflow and cannot improve training and serving fundamentally due to the nature of operations. AI engineering, far beyond MLOps, can holistically (both horizontally and vertically) engineer the process of ML workflow and the architecture of training and serving. Furthermore, it can accelerate serving and training through effective orchestration for the entire ML lifecycle. Based on the holistic ML lifecycle with AI engineering, there are five primary types of ML accelerators (or accelerating areas): hardware accelerators, AI computing platforms, AI frameworks, ML compilers, and cloud services. Please see their relationship diagram\u00a0below. Training and Serving Accelerators Relationship (by the\u00a0author) We can see hardware accelerators and AI frameworks are the mainstream of acceleration. But recently, ML compilers, AI computing platforms, and ML cloud services have become increasingly important. Let\u2019s take a closer look at them\u00a0below. 1. AI Frameworks We cannot skip choosing the right AI framework when talking about accelerating ML training and serving. Sadly, there is no perfect or best AI framework for all use cases. Three AI frameworks widely used in research and production are TensorFlow, PyTorch, and JAX. They lead from different perspectives, such as ease of use, production maturity, and scalability. TensorFlow: TensorFlow is the flagship AI framework. TensorFlow has dominated the deep learning open-source community since the beginning. TensorFlow Serving is a well-defined, mature platform. TensorFlow.js and TensorFlow Lite are also ripe for the web and\u00a0IoT. But due to the limitations of deep learning early exploration, TensorFlow 1.x was all about building static graphs in a very non-Pythonic way. This became a barrier for instant evaluation using the \u201ceager\u201d mode, which allowed PyTorch to ramp up quickly in research. TensorFlow 2.x tried to catch up, but unfortunately, upgrading from TensorFlow 1.x to 2.x has to be\u00a0brutal. TensorFlow also introduced Keras for easier use from the high level and XLA (Accelerated Linear Algebra) optimizing compiler to improve low-level speed. PyTorch: With its eager mode and Pythonic approach, PyTorch is a significant force in today\u2019s deep learning world, from research to production. In addition to TorchServe, PyTorch integrates with framework-agnostic platforms such as Kubeflow. Also, PyTorch\u2019s popularity was tied to the success of Hugging Face\u2019s Transformers library in the first\u00a0place. JAX: Based on device-accelerated NumPy and JIT (Just-In-Time), Google rolled out JAX. It is a more native framework for deep learning rapidly gaining traction in research, as PyTorch did a few years ago. But it\u2019s not an \u201cofficial\u201d Google product yet, as Google\u00a0claims. 2. Hardware Accelerators We can have a lengthy article on hardware accelerators. Undoubtedly, NVIDIA\u2019s GPUs ignited to speed up DL training, though it was initially intended for video\u00a0cards. The popularity of graphics cards for neural network training exploded after the advent of general-purpose GPUs. These GP-GPUs could execute arbitrary code, not just rendering subroutines. NVIDIA\u2019s CUDA programming language provided a way to write this arbitrary code in a C-like language. With their relatively convenient programming model, massive parallelism, and high memory bandwidth, GP-GPUs now o\ufb00er an ideal platform for neural network programming. Today, NVIDIA supports a range of GPUs from desktop to mobile, workstations, mobile workstations, consoles, and data\u00a0centers. With the success of NVIDIA\u2019s GPUs, there is no lack of successors along the way, such as AMD\u2019s GPUs, Google\u2019s TPU ASIC,\u00a0etc. 3. AI Computing Platforms As described above, the speed of ML training and serving significantly depends on hardware (e.g., GPU and TPU). These drivers (that is, AI computing platforms) become critical for performance. There are two well-known ones: CUDA and\u00a0OpenCL. CUDA: CUDA (Compute Unified Device Architecture) is a parallel programming paradigm released in 2007 by NVIDIA. It is designed for graphic processors and a vast array of general-purpose applications for GPUs. CUDA is a proprietary API only supporting NVIDIA\u2019s GPUs for Tesla Architecture. The CUDA-supported graphics cards include the GeForce 8 series, Tesla and\u00a0Quadro. OpenCL: OpenCL (Open Computing Language) was initially developed by Apple and is maintained by the Khronos group for heterogeneous computing, including CPUs, GPUs, DSPs, and other types of processors. This portable language is adaptable enough to allow each hardware platform to achieve high performance, [&#8230;]",
            "pubdate": "Wed, 02 Nov 2022 13:23:51 +0000",
            "pubdate_parsed": [
                2022,
                11,
                2
            ],
            "email_sent": true
        },
        "Jupyter Extensions to Improve your Data Workflow": {
            "url": "https://towardsai.net/p/l/jupyter-extensions-to-improve-your-data-workflow",
            "description": "Last Updated on November 2, 2022 by Editorial Team Author(s): Cornellius Yudha Wijaya Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Improve your workflow with these extensions Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 03 Nov 2022 00:08:33 +0000",
            "pubdate_parsed": [
                2022,
                11,
                3
            ],
            "email_sent": true
        },
        "Dont Frustrate Your Data Scientists (If You Want Them to Stay)": {
            "url": "https://towardsai.net/p/opinion/dont-frustrate-your-data-scientists-if-you-want-them-to-stay",
            "description": "Last Updated on November 3, 2022 by Editorial Team Author(s): Stu Bailey As I speak with data scientists, especially those working in Global 1000 companies, many express concerns about their situation. \u00a0 In some sense, they\u2019re victims of their own success:\u00a0 Data scientists are producing models that are making substantial contributions to the business, and thus more and more models are being used in production applications.\u00a0 But as a result, data scientists face several challenges.\u00a0 In my conversations, the following issues come up the most frequently: -Their organization lacks visibility to the business contributions being made by the models they produce -They\u2019re spending more and more time dealing with operational issues for their models in production The causes of both issues are very consistent across most organizations, and as such, lend themselves to straightforward solutions.\u00a0 This is good news for both data scientists and the organizations that they work for \u2013 provided that organizations act, and do so with some urgency. \u201cYou\u2019re Model is Broken \u2013 Let\u2019s Have a Meeting!\u201d Once developed and deployed into production, AI models can be very sensitive to a host of conditions that can compromise their effectiveness, reduce their value and increase their associated risks.\u00a0 Some of these items, such as data drift, relate directly to the work of the data scientist and require their expertise to address.\u00a0 But there are many other items that can impact models in production that have little to do with data science.\u00a0 For example, a problem with a production data pipeline can cause model outputs to deviate from accepted limits, or even produce erroneous inferences.\u00a0 A problem with the production IT infrastructure in which the model executes can cause performance issues.\u00a0 In many cases, there may be no meaningful role for the data scientist in addressing the problem.\u00a0 But that doesn\u2019t spare them from becoming involved. In many organizations, the response to a problem with an AI-driven application is to pull together a meeting with representatives from the data team, IT team, DevOps team, compliance team \u2013 as well as data science \u2013 in the hope of quickly identifying and addressing the root cause.\u00a0 These meetings often conjure the story of the blind men trying to describe an elephant:\u00a0 Each can describe the part of the elephant that they hold, but no one can describe the whole beast.\u00a0 As a result, a lot of time can be wasted \u2013 and value lost \u2013 as the group tries to assemble a complete picture of the problem and determine a fix. I\u2019ve yet to encounter a data scientist who isn\u2019t committed to making sure that their model is operating effectively and within its thresholds.\u00a0 What they don\u2019t appreciate is being called into situations in which the problems ultimately had nothing to do with the model.\u00a0 They\u2019re generally fine to play a role in monitoring their models that have reached production, but they don\u2019t want to spend their time chasing issues that they have no role in fixing. \u201cMy Models are Making Big Contributions \u2013 Believe Me\u201d Organizations have been pouring millions into AI initiatives in pursuit of big returns, and for the most mature organizations their investments are generating significant returns.\u00a0 But many organizations struggle to quantify the value that their initiatives are contributing. This is increasingly important as budgets tighten and there are more AI projects competing for funds.\u00a0 This directly impacts data scientists, who want their contributions to be recognized and for appropriate rewards to flow to them and their projects.\u00a0 Of course, a lack of visibility to the contributions of AI models is not just an issue for data scientists:\u00a0 The inability to accurately assess business contributions imperils all enterprise AI initiatives. ModelOps to the Rescue ModelOps is a core capability that enables organizations to govern and scale their AI initiatives.\u00a0 An effective ModelOps capability enables an organization to standardize and automate the operational processes for all models in production, but without restricting data scientists or any other team from using the most appropriate tooling and infrastructure for each use case.\u00a0 It also provides the enterprise \u2013 senior executives, IT staff, data teams, compliance teams, business teams, and of course data scientists \u2013 with business metrics that show the contributions, costs, and ROI of each production model. The most effective enterprise ModelOps capabilities are built around a platform that is independent of any data science tool, data system or execution infrastructure, but rather integrates with whatever tooling and systems are used across the enterprise, including enterprise systems for security and access management, ticketing, risk management, compliance, etc.\u00a0 The ModelOps platform maintains an evergreen database of all models in production, regardless of origin or execution environment, along with all artifacts including algorithms, training data, approvals and the like.\u00a0 It includes active monitors that continuously checks the full gamut of statistical, ethical, performance, security, business and compliance KPIs, and routes issues to those responsible and tracks resolution \u2013 eliminating the need for \u201chunting trips\u201d to find the root cause of problems and freeing data scientists, and everyone, to focus their time on their core responsibilities.\u00a0 A mature ModelOps platform also integrates with business systems to enable automated generation of model business metrics and ROI. For those organizations experiencing frustration with the demands of managing models in production and want to further scale their AI initiatives &#8211; and retain the best data scientists \u2013 there\u2019s an answer:\u00a0 Implement ModelOps today. Bio: Stu co-founded\u00a0ModelOp and serves as Chief Enterprise AI Architect. Stu\u2019s background as a technologist and entrepreneur, providing critical data-intensive infrastructure to the world\u2019s largest enterprises, gives him a unique perspective on how to help large, diversified enterprises become AI and Model-Driven. As the technical lead for the National Center for Data Mining from 1994-2000, Stu played key roles in the development of high-performance computing and distributed machine learning platforms, including the development of the Predictive Model Markup Language (PMML). In 2000 he founded the category defining and market leader Infoblox serving as Chief Technology Officer and Chief Scientist while [&#8230;]",
            "pubdate": "Fri, 04 Nov 2022 00:00:29 +0000",
            "pubdate_parsed": [
                2022,
                11,
                4
            ],
            "email_sent": true
        },
        "Deal With an Imbalanced Dataset With TensorFlow, LightGBM, and CatBoost": {
            "url": "https://towardsai.net/p/l/deal-with-an-imbalanced-dataset-with-tensorflow-lightgbm-and-catboost",
            "description": "Last Updated on November 8, 2022 by Editorial Team Author(s): Konstantin Pluzhnikov Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Add new instruments to your toolbox when customizing your\u00a0models Source: Photo by Brett Jordan on flickr.com You have an imbalanced dataset; you want to reduce the count of false negatives (FN) or even false positives (FP). Maybe you like custom things and want to practice adding changes to standard models. If so, this article is for\u00a0you. One way is to customize your model\u2019s loss function with a particular coefficient. This article aims to show customization approaches in TensorFlow, LightGBM, and Catboost. If you want to get a feeling of the whole idea with related math and see the same concept for XGBoost, look at my article on\u00a0Medium. Also, I aim to provide a way to embed a custom hyperparameter to a custom function, which opens the door to an advanced tuning of new parameters as ordinary\u00a0ones. I use the Titanic dataset for demonstration because it is approachable and imbalanced. Basic models, as well as customized models, are in my GitHub repository. LightGBM It is one of the most effective gradient-boosting algorithms developed by Microsoft. It outperforms XGBoost in speed and is comparable in accuracy. For more details, check this article by BexBoost. LightGBM is a younger brother of XGBoost, so it has all its achievements. https://medium.com/media/2316f962481510b88b0327b3907a27cf/href I have used embedded user-defined functions to introduce beta as a core part of the logloss function (it is no more an external hyperparameter). You can see that the outer function presents betato the internal, which calculates derivatives. The same applies to a custom metric. Now you can tune it with other hyperparameters with special packages like the Optuna\u00a0library. beta should be &#60; 1.0 to penalize FN. To punish FP, it should be more than 1.0. For details, please see my article on\u00a0Medium. There are some differences compared to the XGBoost custom loss function. Firstly, LightGBM puts y_predin logit_raw format, and the logit transformation is needed. Secondly, LightGBM custom metric outputs three results (the name of the custom metric (e.g., \u201clogreg_error\u201d), the value of metrics, and the boolean parameter that should be set Falsebecause our goal is to reduce custom metric\u00a0value). There is one more interesting detail in a logit transformation of predt\u00a0; I have used np.where function to ensure stability and avoid overflow when dealing with negatives logit_raw. It is mentioned as the best practice in different examples on Stackoverflow and models\u2019 documentation. Let\u2019s plot confusion matrices of the results of a standard LightGBM model and the one with custom\u00a0loss: (Left) Basic LightGBM model &#124; (Right) Tailored LightGBM model with beta = 0.4, Source: Images by\u00a0author The custom loss with beta&#60; 1 led to the growth of FPs and TPs; to the depletion of FN and\u00a0TN. CatBoost The full name is Categorical boosting, developed by Yandex. It has a massive advantage over other algorithms as you do not need to encode categorical features of your dataset; you list them in the model, and it deals with them on its own. Dmytro Iakubovskyi uses it broadly in his analysis of the different datasets (IMDB, wine, beer, and many more tables with statistics). CatBoost inherits the most perks of XGBoost and LightGBM. https://medium.com/media/bfc5f9029a5bc8d53b94d52855484f29/href You can see the difference between Catboost (using object-oriented programming) and LightGBM (a standard user-defined function) realizations. I take code for the CatBoost class from the official documentation. I only add the beta to the initialization of the class. You can write the code for these functions in any form you like (OOP or UDF). The choice is\u00a0yours! Plotting the\u00a0results: (Left) Basic CatBoost model &#124; (Right) Tailored CatBoost model with beta = 0.4, Source: Images by\u00a0author The logic of the results is the same as for a LightGBM\u00a0model. TensorFlow It is a well-known and super powerful family of algorithms by Google. Setting up a custom loss here is a kind of different story. You do not need to write down derivatives and a custom metric explicitly; there is no `beta` no more ( betais dead, long live to pos_weight!). TF has a suitable function, tf.nn.weighted_cross_entropy_with_logitswhich makes things much more manageable. https://medium.com/media/65e71e527f0b1945eebf7952c58dcaeb/href pos_weight should be &#62; 1.0 to penalize FN, and &#60; 1.0 to punish FP. It is the opposite situation compared to beta. pos_weight is a coefficient that multiplies FN part of logloss while beta is a factor of FP\u00a0part. Plotting the\u00a0results: (Left) Basic TensorFlow model &#124; (Right) Tailored TensorFlow model with pos_weight = 3.5, Source: Images by\u00a0author My custom model showed rather bad performance while the TF standard model has done great; I hope you excuse me for the poor results because the main goal here is demonstration. Conclusion Overall results are comparable for all models. The trade-off between FN and FP is also in place. But if reducing FN is your goal, these custom losses are at your disposal. Advantages Easy and fast to apply (use four user-defined functions and beta, and that\u2019s\u00a0it). There is no need to perform manipulation with underlying data before modeling (if a dataset is not highly imbalanced) It may be applied as a part of data exploration or as a part of model stacking. We may add it to the most popular machine-learning packages. With embedded beta or pos_weight we could tune them as usual hyperparameters. Shortcuts We should adjust beta to get optimal FN to FP trade-off. It may not provide meaningful results when a dataset is highly imbalanced (the dataset where the minor class is less than 10% of all samples). Exploratory data analysis is vital to make the model\u00a0work. If we penalize FN, it often leads to considerable FP growth and vice versa. You may need additional resources to compensate for that\u00a0growth. I hope this article [&#8230;]",
            "pubdate": "Tue, 08 Nov 2022 12:14:06 +0000",
            "pubdate_parsed": [
                2022,
                11,
                8
            ],
            "email_sent": true
        },
        "I Fine-Tuned GPT-2 on 110K Scientific Papers. Heres The Result": {
            "url": "https://towardsai.net/p/l/i-fine-tuned-gpt-2-on-110k-scientific-papers-heres-the-result",
            "description": "Last Updated on November 9, 2022 by Editorial Team Author(s): Edoardo Bianchi Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Content writing by AI is common, but is it possible for an AI to write technical essays? Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 10 Nov 2022 00:13:11 +0000",
            "pubdate_parsed": [
                2022,
                11,
                10
            ],
            "email_sent": true
        },
        "Getting Started With Stable Diffusion": {
            "url": "https://towardsai.net/p/l/getting-started-with-stable-diffusion",
            "description": "Last Updated on November 10, 2022 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Stable Diffusion is a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 11 Nov 2022 00:03:30 +0000",
            "pubdate_parsed": [
                2022,
                11,
                11
            ],
            "email_sent": true
        },
        "AnimeGAN effect with Python": {
            "url": "https://towardsai.net/p/l/animegan-effect-with-python",
            "description": "Last Updated on November 14, 2022 by Editorial Team Author(s): Rokas Balsys Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. I&#x2019;ll show you how you can easily apply the AnimeGAN effect on your media to get beautiful animated pictures, videos, or real-time camera Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 14 Nov 2022 12:48:15 +0000",
            "pubdate_parsed": [
                2022,
                11,
                14
            ],
            "email_sent": true
        },
        "Top 5 Upcoming Programming Languages for Web Development": {
            "url": "https://towardsai.net/p/l/top-5-upcoming-programming-languages-for-web-development",
            "description": "Last Updated on November 14, 2022 by Editorial Team Author(s): Amit Chauhan Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Programming languages for front-end, backend, and full-stack development Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 14 Nov 2022 12:38:58 +0000",
            "pubdate_parsed": [
                2022,
                11,
                14
            ],
            "email_sent": true
        },
        "From VGGNet to EfficientNet: Key Milestones in the Evolution of CNN Design": {
            "url": "https://towardsai.net/p/l/from-vggnet-to-efficientnet-key-milestones-in-the-evolution-of-cnn-design",
            "description": "Last Updated on November 16, 2022 by Editorial Team Author(s): Houssem Ben Braiek Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Modular, Multipath, Factorized, Compressed, Scalable&#x2026; All in CNN Nowadays, but What Led us Here&#x2026;We&#x2019;ll take you through the major&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 16 Nov 2022 12:43:43 +0000",
            "pubdate_parsed": [
                2022,
                11,
                16
            ],
            "email_sent": true
        },
        "A Step-by-Step Approach To Building a Text Summarization Webapp in Python From Scratch": {
            "url": "https://towardsai.net/p/l/a-step-by-step-approach-to-building-a-text-summarization-webapp-in-python-from-scratch",
            "description": "Last Updated on November 16, 2022 by Editorial Team Author(s): Dr. Dharini R Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Using Hugging Face Inference API, Flask, HTML &#38;\u00a0CSS Photo by Patrick Tomasso on\u00a0Unsplash What is the aim? To build a web application that can get an input text and show its\u00a0summary. What are we going to use? Hugging Face Accelerated Inference API, Python Flask Framework, HTML,\u00a0CSS. How is it done? By carrying out the following steps. The 3-Step\u00a0process 1. Identify the inference API of a text summarization model from the Hugging Face\u00a0library. 2. Build the front end with HTML and\u00a0CSS. 3. Build the back end with Python Flask and include the summarization task. Text summarization is the task of extracting a brief from a given set of sentences. A summary can be of two types\u200a\u2014\u200aan abstractive summary or an extractive summary. An extractive summary has words extracted from the given input, placing them together to form a brief. The abstractive summary generates the summary not only by replicating the words in the input, but also coining new words based on the understanding of the\u00a0text. We are going to build an abstractive summarization application using the Hugging Face Accelerated Inference API. To utilize a model by providing input to it and to get the model\u2019s output, we just have to make an API\u00a0call. The accelerated inference enables the \u2018plug-and-play\u2019 kind of usage to the machine learning models by means of API\u00a0calls. Kindly refer to the following blog link to understand the working of Inference API and its benefits with an implementation demo. Plug-and-Play ML Models with \u2018Accelerated Inference API\u2019 from Hugging Face The web application part of our project can easily be built with the Flask framework. The Flask helps with the development of a web application and renders an HTML file, which can be viewed in a web\u00a0browser. Welcome to Flask &#8211; Flask Documentation (2.2.x) The front end of our web application is built with HTML and CSS. The Hyper Text Markup Language (HTML) and Cascading Style Sheets (CSS) are used to design the structure of a webpage and presentation of a webpage, respectively. Having looked at the gist of everything we are going to use, let&#039;s have an idea of what we will build. Our web app is going to\u00a0have a front end\u200a\u2014\u200aa web page that gets the user input text and shows the summary as\u00a0output. a back end\u200a\u2014\u200awhere the user input is fed to the model, and the results are extracted from the\u00a0model. Now kindly make sure to go through the 3 step process of building our summarization application. 1. Identify and utilize a text summarization model from Hugging\u00a0Face For our project, we will be using the model facebook/bart-large-cnn provided by Lewis et al. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension] To use the model\u2019s Inference API, select the model from the Hugging Face library and then clickInference APIunder the Deploy\u00a0button. On clicking that, we will see a python script that can be utilized for the inference, as shown\u00a0below. Selecting the Inference API from Hugging\u00a0Face An Access Token is needed to get the API_URL and\u00a0headers. Create a profile in Hugging Face and create a new access token by following this path Profile -&#62; Settings -&#62; Access Tokens\u00a0Tab. The newly created access token can be used in place of Bearer and\u00a0headers Now let&#039;s move to the next part of our project to create a web application. 2. Build the front end with HTML and\u00a0CSS. The code for the front end is written in two parts \u200a\u2014\u200aA static file\u200a\u2014\u200aA HTML\u00a0file The static file consists of customizations to improvise the look of our front end. As the name suggests, the contents of a static file are not going to be changed according to the user\u2019s input or actions. A static file can include anything such as images, videos, cascading style sheets(CSS), flash files, etc., and these are not dynamically generated by the web server like a typical HTML response because they remain\u00a0static. In the desired location, create two folders named staticand templates In the static folder, create a new file named main.css, where CSS is the cascading style sheet that is used to style the\u00a0HTML. main.css consists of tags present in the HTML code with their corresponding styling and formatting. For a more detailed tutorial on how CSS is used, please refer to this\u00a0link. The code for the static file main.css is given below, followed by an explanation. https://medium.com/media/9085edd775ba2e82870a4b711d3e6523/href As we can see, the above CSS file consists of appearance-based formatting and customization for most of the tags used in our HTML\u00a0code. Code Explanation Lines 2 to 9\u200a\u2014\u200aconsist of styling and formatting aimed at the tag\u00a0header. Lines 11 to 25\u200a\u2014\u200aconsist of formatting for tag\u00a0h1. Lines 28 to 34\u200a\u2014\u200ahave the styling for body tag, along with a background image named image.gif Lines 35 to 39\u200a\u2014\u200aformatting for container Lines 41 to 47\u200a\u2014\u200astyling for div tag, which specifies the division of some\u00a0content Lines 49 to 64\u200a\u2014\u200aformatting for tags h2 and\u00a0h3 Lines 68 to 77\u200a\u2014\u200aconsist of formatting aimed at placing two text boxes side by side with the help of tags parent and child. Let&#039;s move on to creating the front end with our HTML\u00a0code. The HTML is built with the objective of providing a structured pleasant user interface, a space for users to provide input and a space for showing the\u00a0summary. In the already created folder named templates, create a new file named index.html. The code for index.html is given below, followed by the explanation. https://medium.com/media/259955078d0a4fa19f39d7196e7b124b/href Code Explanation Lines 4 to 11\u2014 Header of the\u00a0HTML Line 9\u200a\u2014\u200aWe have added the source of the static file (main.css). Line 10\u200a\u2014\u200aWe have given a title for our webpage named Summarization Application. Lines 13 to 42\u200a\u2014\u200aBody of [&#8230;]",
            "pubdate": "Wed, 16 Nov 2022 12:05:08 +0000",
            "pubdate_parsed": [
                2022,
                11,
                16
            ],
            "email_sent": true
        },
        "In-depth Azure Machine Learning Model Train, Test, and Deploy Pipelines on Cloud With Endpoints for Web APIs": {
            "url": "https://towardsai.net/p/cloud-computing/in-depth-azure-machine-learning-model-train-test-and-deploy-pipelines-on-cloud-with-endpoints-for-web-apis",
            "description": "Last Updated on November 20, 2022 by Editorial Team Author(s): Amit Chauhan An image by the Author The workspace consists of various artifacts Manage resources: It includes compute instances and compute clusters. Linked Services: Data Stores: It is a service to store various data. For example \u2014 Blob storage, hive storage, and SQL database. Compute targets: These are the machines where we run our model and do the train and test. Assets: Environments Experiments Pipeline Datasets Models Endpoints The whole scope of the workspace depends on some dependencies, there will be various logs, various notebooks, entries of the assets, etc. For them, the workspace requires storage. Dependencies Azure Storage account: Used for the administration and the working of the workspace. Azure container registry: When we deploy our model to the production and docker instances. Azure key vault: To store various keys, secret information, and privacy information. Azure application insight: It is used to monitor our machine learning applications and various information like response time, requests, failure conditions, performance, etc. Basic concepts Datasets It is information composed in the form of rows and columns, i.e., a collection of data. There are many methods in azure to upload/fetch the dataset for machine learning experiments. Data-stores When we want to fetch the dataset from the local system, then we need some storage that is where the data store comes into the picture. Data-store is just the connection to the various storage types like account storage, database, or analytics as a data lake. Various storage types Blob, file storage, data lake, Azure SQL, Azure PostgreSQL, MySQL, Azure Data bricks. These are supported by the azure system. Creating the machine learning workspace Below are the following steps to create the workspace Open the azure dashboard, search for the machine learning resource, click on it and then create. If you don\u2019t have an azure account, then follow the link below. How to Open an Azure cloud account with Debit Card A simple and easy process for all data scientist amitprius.medium.com An image by the Author 2. Fill in all the information. If there is no resource group name, then create a new one. When we will write the workspace name, the other information like a key vault, storage account, and application insight is filled automatically. We will keep the container registry to \u2018None\u2019 for now because it is required at the time of deployment. An image by the Author We can choose any region, but if we have a large amount of data, we can choose the nearest region of fast data transfer. In the Networking option, choose public access for practicing the experiment. n the Advanced option, there are many options, and keep it as it is, in data impact, if we enable then we are telling Microsoft that the data we will upload is sensitive. 3. After getting the Validation passed, click on create to make the workspace. It will create the four resources as shown below. An image by the Author 4. Now, click on the go to the resource, and the workspace dashboard will open with the launch studio option as shown below. An image by the Author In the above image, the access control (IAM) is used to create more users to use this workspace. Launching the machine learning studio After creating the workspace, it\u2019s time to launch the ML studio, and it will look like the image below. An image by the Author The author in the above image is responsible for making machine learning experiments and pipelines. 2. Make a new storage account to avoid the files of other storage systems. An image by the Author 3. Now, create a container inside this storage account. An image by the Author 4. Now, create a data store in the ML studio that will connect to this newly made storage account. An image by the Author 5. Fill in the information. An image by the Author To get the access key, go to the new storage account in step 2 and copy the key from the access key option, as shown below. An image by the Author Now, click on the create button of the data store. The data store is created and registered with the workspace along with the storage account. 6. Now, upload the dataset to the container we created in the storage account in step 3. An image by the Author We also check the file through the storage browser option in the storage account. An image by the Author 7. Now, create the dataset and choose the file from the data stores. An image by the Author Click the next button; when we will select \u2018From Azure storage\u2019 other options will come on the left side. We choose this option because our storage is a blob type. An image by the Author An image by the Author An image by the Author An image by the Author Now, we can deselect the Loan_ID and Gender column in the Schema option. An image by the Author Our data is uploaded in the dataset. An image by the Author An image by the Author Compute Resources In this topic, we will discuss the managed resources artifacts i.e. compute instances and compute clusters that come in the machine learning workspace. These are just different names for computers and virtual machines. The computed target is connected with linked services in the workspace. Why do we need computing resources? For any machine learning modeling, we need a computation resource that will train our model. Compute instance: It is a type of virtual machine/server or computer that is used for cloud computation. It is not a machine only but connected to the workspace and has Python, R, Docker, and Azure ML SDK configured. The default storage account while creating the workspace is attached to this instance means we can access all notebooks and other data stored. Mostly used in a development process training, testing, and inferencing. Inference means creating endpoints for web [&#8230;]",
            "pubdate": "Mon, 21 Nov 2022 01:10:46 +0000",
            "pubdate_parsed": [
                2022,
                11,
                21
            ],
            "email_sent": true
        },
        "Galactica: What Is It and What Happened?": {
            "url": "https://towardsai.net/p/l/galactica-what-is-it-and-what-happened",
            "description": "Last Updated on November 22, 2022 by Editorial Team Author(s): Louis Bouchard Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Galactica, Meta AI&#x2019;s most recent model: The AI Scientist Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 23 Nov 2022 00:18:30 +0000",
            "pubdate_parsed": [
                2022,
                11,
                23
            ],
            "email_sent": true
        },
        "How To Compare 2 Datasets With Pandas-profiling": {
            "url": "https://towardsai.net/p/l/how-to-compare-2-datasets-with-pandas-profiling",
            "description": "Last Updated on November 24, 2022 by Editorial Team Author(s): Fabiana Clemente Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A data quality use case with advanced\u00a0EDA Pandas-profiling compare report (screenshot by the\u00a0author) Visualization is the cornerstone of EDA. When facing a new, unknown dataset, visual inspection allows us to get a feel of the available information, draw some patterns regarding the data, and diagnose several issues that we might need to address. In that regard, Pandas Profiling has been the indispensable swiss-knife in every data scientist\u2019s tool belt. In my past articles, I\u2019ve mentioned how pandas profiling can be helpful while performing time-series EDA, but what if we could compare two datasets? How many of us have started the development of a data science project and struggle to understand how much are we getting from our data transformations and engineering? And that\u2019s exactly what I\u2019ll be covering in today\u2019s blog post\u200a\u2014\u200ahow to leverage the most famous single line of code EDA to boost the process of data science development and data quality improvements. I\u2019ll give you a tour of how to leverage Pandas-Profiling comparison report functionality to boost your EDA process and illustrate its potential in producing faster and smarter transformations on our\u00a0data. The dataset used in this article can be found in Kaggle, the HCC Dataset by Miriam Santos (License: CC0: Public Domain). For this particular use case, I\u2019ve artificially introduced some additional data quality issues to show you how visualization can help us detect them and guide us toward their efficient mitigation. All code and examples are available on GitHub and in case you need a little refresher, make sure to check this blog to dust off your pandas-profiling skills. So, on with our use\u00a0case! Pandas Profiling: EDA at your fingertip We\u2019ll start by profiling the HCC dataset and investigating the data quality issues suggested in the\u00a0report: pip install pandas-profiling==3.5.0 https://medium.com/media/aecbb2a0b206edc8b622a05049283be4/href Alerts shown in Pandas Profiling Report (scheenshot by\u00a0author) According to the \u201cAlerts\u201d overview, there are four main types of potential issues that need to be addressed: Duplicates: 4 duplicate rows in\u00a0data; Constant: Constant value \u201c999\u201d in\u00a0\u2018O2\u2019; High Correlation: Several features marked as highly correlated; Missing: Missing Values in \u2018Ferritin\u2019. The validity of each potential problem (as well as the need to find a mitigation strategy for it) depends on the specific use case and domain knowledge. In our case, with the exception of the \u201chigh correlation\u201d alerts, which would require further investigation, the remaining alerts seem to reflect true data quality issues and can be tackled using a few practical solutions: Removing Duplicate Rows: Depending on the nature of the domain, there might be records that have the same values without it being an error. However, considering that some of the features in this dataset are quite specific and refer to an individual\u2019s biological measurements (e.g., \u201cHemoglobin\u201d, \u201cMCV\u201d, \u201cAlbumin\u201d), it\u2019s unlikely that several patients report the same exact values for all features. Let\u2019s start by dropping these duplicates from the\u00a0data: https://medium.com/media/753e0ef447aeb6f486b7eafdcd565d3c/href Removing Irrelevant Features: The constant values in O2 also reflect a true inconsistency in data and do not seem to hold valuable information for model development. In real use case scenarios, it would be a good standard to iterate with a domain or a business experts, but for the purpose of this use case example, we\u2019ll go ahead and drop them from the analysis: https://medium.com/media/448ec4f47535e83238d6da48f166646e/href Missing Data Imputation: HCC dataset also seems extremely susceptible to missing data. A simple way to address this issue (avoiding removing incomplete records or entire features) is resorting to data imputation. We\u2019ll use mean imputation to fill in the absent observations, as it is the most common and simple of statistical imputation techniques and often serves as a baseline\u00a0method: https://medium.com/media/052d75becb1224d1766eb88cda7bc39f/href Side-by-side comparison: faster and smarter iterations on your\u00a0data Now for the fun part! After implementing the first batch of transformations to our dataset, we\u2019re ready to assess their impact on the overall quality of our data. This is where the pandas-profiling compare report functionality comes in handy. The code below depicts how to get\u00a0started: https://medium.com/media/092c7ff17bdb1027986911d700c4bfbc/href Here\u2019s how both reports are shown in the comparison: Comparing Original Data and Transformed Data (screencast by the\u00a0author) What can we right away understand from our dataset overview? The transformed dataset contains one less categorical feature (\u201cO2\u201d was removed), 165 observations (versus the original 171 containing duplicates), and no missing values (in contrast with the 79 missing observations in the original dataset). But how have this transformations impacted the quality of our data? And how good were those decisions? Let\u2019s dive deep into that. In what concerns duplicate records, there was no particular impact in what concerns variables distributions and dataset patterns after the drop. The missing values imputation that was done is a different story. As expected, there are no missing observations after the data imputation was performed. Note how both the nullity count and matrix show the differences between both versions of the data: in the transformed data, \u201cFerritin\u201d now has 165 complete values, and no blanks can be found in the nullity\u00a0matrix. Comparison Report: Missing Values (screencast by\u00a0Author) However, we can infer something else from the comparison report. If we were to inspect the \u201cFerritin\u201d histogram, we\u2019d see how imputing values with the mean has distorted the original data distribution, which is undesirable. Comparison Report: Ferritin\u200a\u2014\u200aimputed values seem to distort the original feature distribution (Screenshot by\u00a0author) This is also observed through the visualization of interactions and correlations, where daft interaction patterns and higher correlation values emerge in the relationship between \u201cFerritin\u201d and the remaining features. Comparison Report: Interactions between Ferritin and Age: imputed values are shown in a vertical line corresponding to the mean (Screenshot by\u00a0author) Comparison Report: Correlations\u200a\u2014\u200aFerritin correlation values seem to increase after data [&#8230;]",
            "pubdate": "Fri, 25 Nov 2022 00:13:05 +0000",
            "pubdate_parsed": [
                2022,
                11,
                25
            ],
            "email_sent": true
        },
        "I used AI to Generate Nietzschean Aphorisms": {
            "url": "https://towardsai.net/p/l/i-used-ai-to-generate-nietzschean-aphorisms",
            "description": "Last Updated on November 25, 2022 by Editorial Team Author(s): Yoo Byoung Woo Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Can large language models help mankind overcome the crisis of nihilism in the modern\u00a0world? Ever since OpenAI released GPT-3, a large language model (LLM) trained on a massive amount of text data, the natural language processing community has been excited about the potential of using this technology for computational creativity (e.g., generating blog posts, fiction, or poetry). I personally think that LLMs are not only convenient tools for synthesizing media content or writing works of literature but also have the potential to solve a more serious problem: the crisis of meaning due to the advent of nihilism in the modern\u00a0world. God is Dead\u200a\u2014\u200aNietzsche, the Herald of\u00a0Nihilism According to the 19th-century German philosopher Friedrich Nietzsche, best known for heralding the death of God, modern civilization is plagued by nihilism; the loss of all value structure and meaning in life. When Nietzsche proclaimed \u201cGod is dead\u201d, he wasn\u2019t merely observing the decline of religion but was also pointing out that the universal values which have been the cornerstone of Western civilization for centuries no longer hold any meaning, thus each individual is left to fend for themselves, trying to make sense of a world that seems increasingly chaotic and full of suffering. In the wake of the death of God, modern civilization is plagued by nihilism (created by author &#38; stable diffusion) Although the age of Enlightenment and the scientific revolution played a significant part in ushering the loss of faith, Nietzsche mainly blames the predominant worldview of the time, i.e., Platonic idealism, Christianity, and Utopianism, for the spiritual weakening of modern man. For Nietzsche, these philosophies are a form of escapism, as they are based on the belief in a perfect, idealistic world (e.g., Platonic Forms, Paradise, communist utopia, etc.) which is separate from and superior to the material world, in which we often feel our lives are cruel, harsh and painfully short. Their function is to provide grand narratives of the \u201ctrue world\u201d and give hope that there is something more to life than our earthly existence, but such a view is ultimately decadent, as it provides false hope and encourages people to withdraw from life and the real world. When these idealistic visions of reality are shown to be false (as they inevitably are), then the entire value structure that they support collapses, leading to a spiritual crisis and a loss of meaning in life. This is what Nietzsche called nihilism, and he saw it as the greatest threat to Western civilization. Ideologies aim to bring an end to history by creating a utopia (created by author &#38; stable diffusion) Nietzsche\u2019s warnings about the dangers of nihilism have proven to be prophetic, as the 20th century was marked by two world wars and the horrific genocide of millions of people, which were primarily fueled by fascism, totalitarianism, and communism; ideologies that aim to bring an end to history by creating a utopia, be it racial, national or class-based. According to philosopher Julian Young, although most of them rejected metaphysical ideas, they are nevertheless \u201ctrue world\u201d philosophies in disguise, in which the old distinction between physical and metaphysical is reinterpreted as a distinction between present and future. The modern man, after the death of God and consequent disasters, is left with a feeling of absurdity and pointlessness, and the need for a new grand narrative, or better yet, overcoming the dependence on such narratives altogether, arises. Revaluation of All Values\u200a\u2014\u200aGenerating Aphorisms with a\u00a0LLM How can man accomplish the task of overcoming nihilism? Nietzsche proposes that we must first carefully examine the ethical norms we take for granted, a process he calls the revaluation of all values. This process exposes how these \u201ctrue world\u201d grand narratives promote decadence, so that man can create values of his own without relying on such life-denying philosophies. For Nietzsche, the method of revaluation was to write aphorisms\u200a\u2014\u200abrief, pithy, highly condensed statements that identify the genealogy and psychological motivations behind systems of values and their futile attempts to cover up the existential uncertainties of the human condition. Summon Nietzsche from the abyss of madness via AI technology (created by author &#38; stable diffusion) Recently, I\u2019ve been reading a lot of Nietzsche\u200a\u2014\u200aI was fascinated by his style of writing in aphorisms and how he used this literary form to uncover truths that modernity would rather keep hidden. I thought it was a shame that, ever since Nietzsche suffered a severe mental breakdown at the age of forty-four, he couldn\u2019t produce remarkable works again. This had me thinking: what if we could summon Nietzsche from the abyss of madness and resurrect him via AI technology? Specifically, what if we could use LLMs to generate previously unheard-of Nietzschean aphorisms? NLP enthusiasts have shown that it is possible to get OpenAI\u2019s GPT-3 to generate works of poetry in the style of a specific poet by providing only a handful of examples. This approach is known as few-shot learning, where a LLM is able to learn to perform a certain task from a small amount of data (a.k.a. few-shot samples) during inference time. These few-shot samples are included in the input text (a.k.a. prompt), usually in the form of a numbered list. The LLM, a model which is primarily trained for natural language completion tasks, will then generate text that conforms to the format and style of the provided samples. I thought there would be no reason why the same approach couldn\u2019t be used to generate Nietzschean aphorisms. The prompt for Nietzschean aphorism generation (created by\u00a0author) The main idea for Nietzschean aphorism generation is as follows: I take a bunch of Nietzsche\u2019s aphorisms, format [&#8230;]",
            "pubdate": "Fri, 25 Nov 2022 12:13:49 +0000",
            "pubdate_parsed": [
                2022,
                11,
                25
            ],
            "email_sent": true
        },
        "Quartics-Built To Order At Any Address In The X-Y Grid": {
            "url": "https://towardsai.net/p/l/quartics-built-to-order-at-any-address-in-the-x-y-grid",
            "description": "Last Updated on November 26, 2022 by Editorial Team Author(s): Greg Oliver Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Build your own Quartic Polynomials to order at any address in the Grid! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 27 Nov 2022 00:13:05 +0000",
            "pubdate_parsed": [
                2022,
                11,
                27
            ],
            "email_sent": true
        },
        "Working on a Computer Vision project? These code chunks will help you!!!": {
            "url": "https://towardsai.net/p/l/working-on-a-computer-vision-project-these-code-chunks-will-help-you",
            "description": "Last Updated on November 27, 2022 by Editorial Team Author(s): Chinmay Bhalerao Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Working on a Computer Vision Project? These Code Chunks Will Help You\u00a0!!! An introduction to a few \u201cused to\u201d methods in a computer vision\u00a0project Computer vision projects\u00a0[Source] \u201cVR and AR will eventually converge, and smart glasses will take over our digital interactions.\u201d\u2015 Carlos L\u00f3pez (Founder @\u00a0Oarsis) The amazing thing about working in Computer vision and machine learning is that after every few years, somebody invents something crazy that makes you totally reconsider what&#039;s possible!!! The World got a new eye &#38; new way of thinking and tracking objects since the emergence of Computer vision algorithms. Starting from Region-Based Convolutional Neural Networks [RCNN] to YOLO V7, detectron-2, segformer, and classification architectures, computer vision changed drastically for higher efficiency of detection and higher latency with less requirement of time and computational expensiveness. A computer vision project is a combination of many things, from data collection to successful deployment. Understanding data and the right processing and training is the key to success. Below are a few code chunks with descriptions of their work that will ease your working on the\u00a0project. 1. Know your dataset\u2019s instances for the object detection or segmentation project, we annotate our dataset with the help of external annotation tools like makesense.ai, VGG annotator, LableIMG, etc. Example of an imbalance dataset in object detection [Image\u00a0Source] We know the exact number of images, but it&#039;s hard to know how many instances of each class we have. Knowing instances of the class will tell you if your dataset is imbalanced or not. It will have a deep impact on the learning model if your instances are not balanced. So after downloading annotated dataset and its annotation file, you can use the following chunk of code to see the class balance\u00a0status. import os#Give path of folder in which you stored images and annotations path = r\"Your dataset *folder* location\" # Change the directory to path os.chdir(path)x=[]# Spinning through all files for file in os.listdir():# Checking for text annotation file if file.endswith(\".txt\"): file_path = f\"{path}\\{file}\" with open(file_path, &#039;r&#039;) as f: for line in f: a=line[0] x.append(a)print(x)#to count instances from collections import CounterCounter(x) Image by\u00a0Author You can see, at last, the counter gives instance values for each class, and then on your model criteria, you can decide if the dataset needs further balancing or\u00a0not. 2. Preprocessing of\u00a0images In our image dataset, other than class instances, we have many other objects/things. If we take it for the learning purpose of the model, then these other items can be classified as noises. There are many use cases that claim that removing these noises and then sending them to the model for training improves the performance of the model. So how do preprocess images? See the below\u00a0code. #Writing a function to create mouse masking #We are using mouse click events hereimport numpy as npimport cv2 as cvdrawing = False # true if mouse is pressedmode = True # if True, draw rectangle. Press &#039;m&#039; to toggle to curveix,iy = -1,-1# mouse callback functiondef draw_circle(event,x,y,flags,param): global ix,iy,drawing,mode if event == cv.EVENT_LBUTTONDOWN: drawing = True ix,iy = x,y elif event == cv.EVENT_MOUSEMOVE: if drawing == True: if mode == True: cv.rectangle(img,(ix,iy),(x,y),(255,255,255),-1) #(255,255,255) represents white color but you can give any. # -1 represents filled box and 1 represents hollow box else: cv.circle(img,(x,y),5,(0,0,255),-1) elif event == cv.EVENT_LBUTTONUP: drawing = False if mode == True: cv.rectangle(img,(ix,iy),(x,y),(255,255,255),-1) else: cv.circle(img,(x,y),5,(0,0,255),-1) #storing final output cv2.imwrite(\"new_img.jpg\",img) #Calling function and using it on input image import cv2 img = cv2.imread(r\"Your image path\",1)#resizing to fit on screenimg = cv2.resize(img,(1200,800))cv.namedWindow(&#039;image&#039;)cv.setMouseCallback(&#039;image&#039;,draw_circle)while(1): cv.imshow(&#039;image&#039;,img) k = cv.waitKey(1) &#38; 0xFF if k == ord(&#039;m&#039;): mode = not mode elif k == 27: breakcv.destroyAllWindows() If you run the above code, then you will have your training image in front of you, and your mouse will act as a mask maker. After clicking and hovering the mouse on an unnecessary object will direct create a mask on that object. I took white color for use case purposes, but you can take any according to your problem. You can train a separate object detection model for noise, and below that, you can attach this code. At first, the model will detect noise, and then this code will mask that bounding box with your desired\u00a0color. Masking of noise objects [Image by\u00a0Author] There are many things you can do for image preprocessing, like cropping, making blur/contrast, etc. you can read my blog for more image preprocessing techniques. Do you know these basic image processing operations? 3. Data Augmentation In every computer vision project, you want to augment the dataset to make it bigger to make the model\u2019s work easier. There is much open-source software that does Augmentations for you, like Roboflow. But many times, there can be a problem with data security and confidentiality. So you can do your own dataset augmentation on your python editor. There is a library by TensorFlow known as \u201cImageDataGenerator\u201d which helps you to do this. See the below\u00a0code. # FOR COMPLETE FOLDER ANNOTATION#imports import tensorflowimport kerasimport numpy as npimport osfrom PIL import Imagefrom skimage import ioSIZE = 128dataset = []image_directory = &#039;Image folder address/&#039;from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img# Gving required augmentations to image #ImageDataGenerator has many Augmentations, choose those who are good for your conditiondatagen = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#039;nearest&#039;)my_images = os.listdir(image_directory)for i, image_name in enumerate(my_images): if (image_name.split(&#039;.&#039;)[1] == &#039;jpg&#039;): image = io.imread(image_directory + image_name) image = Image.fromarray(image,&#039;RGB&#039;) image = image.resize((SIZE,SIZE)) dataset.append(np.array(image)) x = np.array(dataset)i = 0for batch in datagen.flow(x, batch_size=20, save_to_dir=&#039;preview&#039;, save_prefix=&#039;Hard_Hat&#039;, save_format=&#039;jpeg&#039;): i += 1 if i &#62; 200: break #FOR SINGLE IMAGE ANNOTATIONimport tensorflowimport kerasfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#Adress of imageimg = load_img(&#039;Image address [should [&#8230;]",
            "pubdate": "Sun, 27 Nov 2022 12:23:46 +0000",
            "pubdate_parsed": [
                2022,
                11,
                27
            ],
            "email_sent": true
        },
        "How To Be a Machine Learning Engineer?": {
            "url": "https://towardsai.net/p/l/how-to-be-a-machine-learning-engineer",
            "description": "Last Updated on November 26, 2022 by Editorial Team Author(s): Gencay I. Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Here, we will discuss finding the necessary skills if you want to be a machine learning engineer, like SQL, Python, Django, or flask, and&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 27 Nov 2022 04:48:07 +0000",
            "pubdate_parsed": [
                2022,
                11,
                27
            ],
            "email_sent": true
        },
        "Microsoft Uses Transfer Learning to Train Autonomous Drones": {
            "url": "https://towardsai.net/p/l/microsoft-uses-transfer-learning-to-train-autonomous-drones",
            "description": "Last Updated on November 26, 2022 by Editorial Team Author(s): Jesus Rodriguez Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. The model is able to transfer knowledge between a simulated environment and real-world settings. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 27 Nov 2022 04:33:34 +0000",
            "pubdate_parsed": [
                2022,
                11,
                27
            ],
            "email_sent": true
        },
        "The Beauty of A.I. Is That It Lives in the Twilight Zone Between Atoms And Bits": {
            "url": "https://towardsai.net/p/l/the-beauty-of-a-i-is-that-it-lives-in-the-twilight-zone-between-atoms-and-bits",
            "description": "Last Updated on November 28, 2022 by Editorial Team Author(s): Tarek Amr Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. And this is probably its main weakness as well. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 28 Nov 2022 12:18:44 +0000",
            "pubdate_parsed": [
                2022,
                11,
                28
            ],
            "email_sent": true
        },
        "Object Tracking with Particle Filters In Python": {
            "url": "https://towardsai.net/p/l/object-tracking-with-particle-filters-in-python",
            "description": "Last Updated on November 29, 2022 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Computer vision has made rapid progress in the last few years, thanks to improvements in training data and algorithms, as well as the&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 29 Nov 2022 12:14:33 +0000",
            "pubdate_parsed": [
                2022,
                11,
                29
            ],
            "email_sent": true
        },
        "My Top 3 Tips for Getting Kaggle Expert Rank With Your First 5 Notebooks": {
            "url": "https://towardsai.net/p/l/my-top-3-tips-for-getting-kaggle-expert-rank-with-your-first-5-notebooks",
            "description": "Last Updated on December 1, 2022 by Editorial Team Author(s): Pere Martra Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Becoming a Kaggle expert requires work, but it is a very achievable goal. I\u2019ll tell you three tips that helped me the most to do it with only five notebooks. Photo by sporlab on\u00a0Unsplash Kaggle is the most prestigious data science competition site. Having a good profile on Kaggle can open many doors, and is one of the best places where you can showcase your Data Science problem-solving skills. The Kagglers got medals based on how well they did in the competitions. But Kaggle isn\u2019t just about competition. There are four categories in which you can progress. Competitions. Maybe the most prestigious category inside Kaggle. You can receive medals depending on the results of the competitions you participate in. Datasets. Data Scientists can publish their DataSet creations. Medals are awarded based on the votes of other Kagglers. Notebooks. Perhaps the most prestigious category after the competitions. Not all uploaded notebooks have to be associated with an active Kaggle competition. Any notebook can get votes from the community. As in the case of the DataSets, the more votes obtained, the higher the quality of the medals and the higher the\u00a0ranking. Discussion. It is the least prestigious of all the categories. Votes are obtained for comments made on the Kaggle platform. In each of these categories, you can opt for 5 ranks: Novice, Contributor, Expert, Master, and Grand\u00a0Master. The two initial ranks are really easy to obtain. Let\u2019s say that by filling out the profile, interacting a little with the community, and posting a job, you already reach the Contributor level. But obtaining the Expert rank requires effort, it is the first rank that must be obtained by earning\u00a0medals. In this article, I will discuss how to achieve the Expert rank in the Notebook category, which is possibly the most prestigious after competitions. What are the requirements to achieve Expert rank in the Notebook category on\u00a0Kaggle? Very easy! We need to get at least five bronze medals. To get a medal, we must obtain five votes from Kagglers with a higher category than Novice. The truth is that it seems simple, but we must consider that only 1 in 20 notebooks in Kaggle receive more than 2 votes. What we are going to attempt is to receive more than 5 votes from Kagglers in 100% of our notebooks. Image by\u00a0Author In the image, we can see the requirements for each of the categories. As you can see, I only have the check in the Notebooks category. So, I can only be considered an expert in that category. Data Scientists with more experience or who are more dedicated to Kaggle can earn the rank of an expert in more categories. With what is usually described as Kaggle Expert * n. Being n the number of categories in which he is considered an\u00a0expert. Competitions are the most prestigious category, followed by Notebooks and Datasets, and finally, discussions. My first five Notebooks. This was my first notebook. Currently, he has a silver medal. Awarded for getting more than 20 votes from Kagglers ranked higher than\u00a0Novice. I intended to get the highest score possible in the MNIST Competition with a Model created by me. In the Notebook, there are some techniques that aren\u2019t used often. For example, instead of using Dropout layers, I used SpatialDropout. I also struggled with the callback functions and tried to achieve the highest possible score. The most important thing is that the Notebook explains the reason for each of the techniques used. The second Notebook now also has a silver medal. Like the previous one, it was also part of one of Kaggle\u2019s basic competitions. The approach on this one was entirely different. I focused 100% on the data transformation, and on explaining step by step the reason for each of the modifications. The model used was a regression model from the SciKit library. I didn\u2019t waste time with him. I would say that the majority of the work was invested in data processing and the generation of graphs to understand the data and its transformations. Moreover, I attempted to make some functions customizable with variables. So that other Kagglers could play with the Notebook and test how it worked by altering these\u00a0values. This notebook was not part of any competition. But I used one of the best-known Kaggle Datasets: Card fraud detection. It was just an experiment using the SciKit Learn library, in which I tried to create a function that was capable of modifying the data by itself. Applying the algorithm to obtain good performance without the participation of the Data Scientist. The notebook was much better received than I expected. This notebook was also part of one of the basic competitions. It\u2019s a notebook that really surprised me that it got so many votes. Right now, it has a silver medal. In it, I\u2019m not trying to get a good score, but instead, I\u2019m manually generating a Simple Logistic Regression Model. It was intended as a simple guide to learning how to create simple models manually. By entering the same competition as the previous Notebook and scoring much better, the Notebook got far fewer votes. It surprised me at first, but later, after taking a tour of the other competing notebooks, I could say that this one contributed very little. It was just one more of the notebooks that used Transfer Leaning to solve the competition. It is true that the model used was not one of the most widely used and that I tried to make it a basic approach to Transfer Learning. Still, fortunately, he received a bronze medal, which [&#8230;]",
            "pubdate": "Fri, 02 Dec 2022 00:10:57 +0000",
            "pubdate_parsed": [
                2022,
                12,
                2
            ],
            "email_sent": true
        },
        "Efficient NeRFs for Real-Time Portrait Synthesis (RAD-NeRF)": {
            "url": "https://towardsai.net/p/l/efficient-nerfs-for-real-time-portrait-synthesis-rad-nerf",
            "description": "Last Updated on December 3, 2022 by Editorial Team Author(s): Louis Bouchard Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. From Audio to Talking Heads in Real-Time with AI! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 04 Dec 2022 00:14:22 +0000",
            "pubdate_parsed": [
                2022,
                12,
                4
            ],
            "email_sent": true
        },
        "DeepMinds DeepNash plays Stratego": {
            "url": "https://towardsai.net/p/l/deepminds-deepnash-plays-stratego",
            "description": "Last Updated on December 6, 2022 by Editorial Team Author(s): Mandar Karhade Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. DeepMind started December with a bang: DeepNash a Game-playing AI excels at playing games with imperfect information using Nash Equilibrium Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 06 Dec 2022 12:43:49 +0000",
            "pubdate_parsed": [
                2022,
                12,
                6
            ],
            "email_sent": true
        },
        "How to access Scientific Knowledge with Galactica": {
            "url": "https://towardsai.net/p/l/how-to-access-scientific-knowledge-with-galactica",
            "description": "Last Updated on December 6, 2022 by Editorial Team Author(s): Yoo Byoung Woo Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A tutorial for using Meta AI\u2019s large language model to perform scientific NLP\u00a0tasks The world of scientific knowledge is mind-bogglingly vast. Searching for relevant research papers, understanding complex concepts, and writing academic literature can be daunting and time-consuming, especially for inexperienced researchers. Fortunately, with the advent of Meta AI\u2019s large language model, accessing scientific knowledge has never been\u00a0easier. Galactica enables users to access scientific knowledge and tackle scientific NLP tasks (created by author &#38; stable diffusion) In this tutorial, I\u2019ll show you how to use Meta AI\u2019s Galactica to quickly and effectively perform various scientific NLP tasks. We\u2019ll cover topics such as finding relevant citations, generating academic papers, processing multi-modal data (e.g., LaTeX equations, code snippets, chemical formulas, etc.) frequently encountered during research, and\u00a0more. What is Meta AI\u2019s Galactica? Galactica trains on text sequences that represent scientific phenomena (Source: Galactica paper) Galactica is a 120B parameter large language model trained on a curated scientific corpus. The training data consists not only of massive volumes of scientific literature but also datasets for downstream scientific NLP tasks and special tokens representing scientific phenomena. Specialized tokenization is an integral part of Galactica as it enables the model to predict citations or process modalities such as protein sequences or SMILES formulas: Citations: citations are wrapped with reference tokens [START_REF] and [END_REF]. Reasoning: &#60;work&#62; token enables step-by-step reasoning by mimicking an internal working memory (not covered in this tutorial). SMILES Formula: SMILES formula sequences are wrapped with tokens [START_SMILES] and [END_SMILES]. For isomeric SMILES, tokens [START_I_SMILES] and [END_I_SMILES] are\u00a0used. Amino Acid Sequences: amino acid sequences are wrapped with tokens [START_AMINO] and [END_AMINO]. DNA Sequences: DNA sequences are wrapped with tokens [START_DNA] and [END_DNA]. Meta AI reports that Galactica\u2019s generative approach for citation prediction outperforms retrieval approaches, which demonstrates the potential for language models to replace search engines. Also, Galactica beats existing methods on reasoning task benchmarks (e.g., MMLU and MATH) and sets a new state-of-the-art on several downstream scientific NLP tasks (e.g., PubMedQA and MedMCOA). Despite its powerful capabilities, similar to most language models, Galactica is prone to hallucination; that is, the model outputs nonsensical results in some cases. Therefore, researchers who use Galactica should always fact-check the generated outputs. How to use Galactica Galactica is accesible via the galai Python library. You can download the model with load_model. import galai as galmodel = gal.load_model(name=\"standard\", num_gpus=2) The name arguement is for the name of the model version to use. There are five versions of the model available (\u2018mini\u2019, \u2018base\u2019, \u2018standard\u2019, \u2018large\u2019, \u2018huge\u2019), each with varying parameter size (125M, 1.3B, 6.7B, 30B, 120B, respectively). The num_gpus arguement is for the number of GPUs to use. I was able to load the \u2018standard\u2019 version on two NVIDIA RTX 3090 GPUs; the model took up about 19GB memory for each\u00a0device. Text Generation Similar to most large language models, Galactica frames every NLP task as text generation. You can use generate to generate\u00a0text. # free-form text generationinput_text = \"The reason why Transformers replaced RNNs was because\"generated_text = model.generate(input_text=input_text, max_length=256, new_doc=False, top_p=None)print(generated_text)\"\"\"The reason why Transformers replaced RNNs was because they were able to capture long-term dependencies in the input sequence.# 2.2.2. Attention MechanismThe attention mechanism was introduced in [START_REF] Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau[END_REF] to improve the performance of the encoder-decoder model...\"\"\" The input_text is the input context for the model to use for its generation. Galactica\u2019s other advanced capabilities can be accessed via prompt engineering of the input\u00a0context. The max_length modifies the maximum token length of the generated text. Default value is 60 tokens, so max_length should be set to a higher value for longer generations. The maximum context length of the model is 2048\u00a0tokens. If new_doc is set to True, a padding token is automatically appended to the front of the input text so that the model would treat it as the beginning of a new document. For free-form text generation, new_doc it should be set to\u00a0False. The top_p argument is for nucleus sampling. If the generated results seems too repetitive, set the value to a float between 1 and 0, such as 0.9. Otherwise top_p defaults to None and greedy decoding is\u00a0used. Papers and\u00a0Surveys You can generate various types of academic literature with Galactica via prompt engineering. If a prompt is designed to resemble a certain kind of document, so will its completion. For paper documents, use\u00a0Title:. # generate paper documentinput_text = \"Title: Self-Supervised Learning, A Survey\\n\\nAuthors: John Smith\\n\\n\"generated_text = model.generate(input_text, new_doc=True)print(generated_text)\"\"\"Title: Self-Supervised Learning, A SurveyAuthors: John Smith# AbstractSelf-supervised learning is a class of machine learning methods that learn representations of data without the need for human-provided labels.\\nIn this survey, we provide a comprehensive overview of the field\"\"\" This functionality is particularly useful when you need a comprehensive survey on a particular topic. Simply design the prompt as Title: TOPIC, A Survey and Galactica will automatically generate one for\u00a0you. Lecture Notes and Wikipedia Articles For Wikipedia-style articles or lecture notes, begin the prompt with\u00a0#. # generate wiki style articlesinput_text = \"# Multi-Head Attention\\n\\n\"generated_text = model.generate(input_text, new_doc=True)print(generated_text)\"\"\"# Multi-Head AttentionThe multi-head attention mechanism is a generalization of the single-head attention mechanism. The multi-head attention mechanism is a combination of multiple single-head attention mechanisms. The multi-head attention mechanism is shown in Figure 2.The multi- ...\"\"\"# generate lecture notesinput_text = \"# Lecture 1: The Ising Model\\n\\n\"generated_text = model.generate(input_text, new_doc=True)print(generated_text)\"\"\"# Lecture 1: The Ising Model# 1.1 The Ising ModelThe Ising model is a simple model of ferromagnetism. It was introduced by Lenz in 1920 [[START_REF] Beitrag zur Theorie des Ferromagnetismus, Ising[END_REF]]\"\"\" Citation Prediction Galactica is trained on a large scientific corpus comprising more than 360 million in-context citations and [&#8230;]",
            "pubdate": "Tue, 06 Dec 2022 12:43:47 +0000",
            "pubdate_parsed": [
                2022,
                12,
                6
            ],
            "email_sent": true
        },
        "OpenAIs Most Recent Conversational AI: ChatGPT": {
            "url": "https://towardsai.net/p/l/openais-most-recent-conversational-ai-chatgpt",
            "description": "Last Updated on December 6, 2022 by Editorial Team Author(s): Louis Bouchard Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. ChatGPT explained! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 07 Dec 2022 00:08:18 +0000",
            "pubdate_parsed": [
                2022,
                12,
                7
            ],
            "email_sent": true
        },
        "How to Quickly Build A Semantic Search System With txtai And Weaviate": {
            "url": "https://towardsai.net/p/l/how-to-quickly-build-a-semantic-search-system-with-txtai-and-weaviate",
            "description": "Last Updated on December 12, 2022 by Editorial Team Author(s): ___ Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. An introduction to the weaviate-txtai library Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 13 Dec 2022 02:41:41 +0000",
            "pubdate_parsed": [
                2022,
                12,
                13
            ],
            "email_sent": true
        },
        "End-to-End Machine Learning Project with Deployment Part 1: Project Set-Up": {
            "url": "https://towardsai.net/p/l/end-to-end-machine-learning-project-with-deployment-part-1-project-set-up",
            "description": "Last Updated on December 12, 2022 by Editorial Team Author(s): Abhishek Jana Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Get Your Project Ready for Machine Learning: A Step-by-Step Guide Many of us often make the mistake of jumping straight into coding when working on end-to-end projects. This approach can work well when dealing with small datasets that don\u2019t need much preprocessing. In these cases, we can quickly train a predictive machine learning model and deploy it in the cloud. But this approach has its limitations. If the project is not set up correctly, the code may not be \u201creusable\u201d or \u201cscalable\u201d, which can cause problems down the\u00a0road. What is the meaning of \u201creusable\u201d and \u201cscalable\u201d in a machine learning\u00a0project? \u201cReusable\u201d refers to the ability of a project or its components to be used again in future projects. Reusability can save time, money, and resources in future projects by reducing the need to start from\u00a0scratch. We say that a project is \u201cscalable\u201d when it can be easily adapted to work with larger or smaller datasets without significant changes to its overall design or structure. This is important because it allows the project to be used effectively in a wide range of situations, regardless of the size of the data it is working\u00a0with. If you\u2019re wondering how to get started, here\u2019s a step-by-step guide. Keep in mind that I won\u2019t be explaining the code in detail but rather provide an overview of the project\u00a0flow. Step 1. Don\u2019t\u00a0Code! It is important to carefully read and understand the problem statement and data description before starting to work on a dataset. Doing so can provide valuable information about the dataset, such as its origin, the number and names of columns, and how to access the data. In some cases, the description may even indicate that the dataset is outdated or commonly used and, therefore may not provide new insights. Let\u2019s look at an\u00a0example. I am currently working on a rental bike sharing which is a 10-year-old dataset and is used by many data science enthusiasts. So this is not going to give us any new information. So if you look at the dataset, it gives us a description of the dataset without even looking into the data. It tells us the source of the data which has the most up-to-date version. We can use\u00a0that. In industry, data descriptions are often provided along with the dataset. This is called a \u201cData Sharing Agreement,\u201d or DSA. It is important to read and understand this information before beginning your analysis. This brings us to our next\u00a0step. Step 2. Documentation! A data science or machine learning project typically involves multiple teams, such as a data maintenance team, a data analysis team, a model training team, and a front-end development team. It is important to document the project in a clear and organized way so that all team members can understand it and stay up to date with the latest developments. This is especially important when presenting the project to stakeholders or when new members join the team and need to quickly get up to speed. By documenting the project consistently and thoroughly, the team can ensure that everyone is on the same page and working towards the same\u00a0goals. There are five types of documents we need to maintain: High-Level Design Document: A high-level design document, or HLD, is a general document that outlines the overall flow of a project. It typically includes a description of the data that will be used, the steps involved in the project, and the tools and resources that will be required to complete it. This document provides a high-level overview of the project and is used to guide the development team in implementing the project. It may also be used to communicate the project\u2019s goals and objectives to stakeholders and other interested parties. Low-Level Design Document: The Low-Level Design (LLD) document is a more specific document that focuses on the details of data handling and machine learning model training. The LLD provides a more in-depth look at the technical aspects of the project and how the various components will work together. Architecture Design Document: AD provides a detailed description of the internal structure of a program. It includes a class diagram with methods and their relationships, as well as a description of program specifications. This document serves as a guide for the programmer, allowing them to write code directly from the\u00a0design. Wireframe Document: This is a preview of how the front-end will look after the project is deployed. Detailed Project Report: DPR is mostly geared towards the stakeholders about the overall findings of the\u00a0project. High-level design (HLD) and low-level design (LLD) are early planning stages in a project where the overall structure and detailed specifications of the project are laid out, respectively. Once the HLD and LLD are approved, the development team can begin writing code and creating application design (AD) and wireframe documents. The project progress and findings are typically summarized in a final document called the Detailed Project Report\u00a0(DPR). Step 3. Select a Template! Now to begin coding, we can create a GitHub repository and push our work\u00a0there. project structure Here is a project template that can help you when starting a new project. In the following parts, I will explain the purpose of each directory and file in the template. For now, you can clone this repository and explore the \u201cdocuments\u201d directory. After reading this, you should be able to use the template to create the high-level design (HLD) and low-level design (LLD) for your own project. Give it a try, and let me know how it\u00a0goes. You can follow me on GitHub, LinkedIn, and medium for the latest updates and stay informed about [&#8230;]",
            "pubdate": "Tue, 13 Dec 2022 00:11:01 +0000",
            "pubdate_parsed": [
                2022,
                12,
                13
            ],
            "email_sent": true
        },
        "A Journey into the Fabulous Applications of TransformersPart 1": {
            "url": "https://towardsai.net/p/l/a-journey-into-the-fabulous-applications-of-transformers%e2%80%8a-%e2%80%8apart-1",
            "description": "Last Updated on December 13, 2022 by Editorial Team Author(s): Dr. Dharini R Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A Journey Into the Fabulous Applications of Transformers\u200a\u2014\u200aPart\u00a01 Demo with Emphasis on NLP using Python, Hugging\u00a0Face. Photo by Arseny Togulev on\u00a0Unsplash The introduction of transformers has made a huge impact on Artificial Intelligence, especially in the Natural Language Processing domain. Transformers paved way for the most awaited success of transfer learning in Natural Language Processing. As a result, many large language models came into existence, and now we are able to build beneficial applications on top of these cutting-edge models. A transformer is, in simpler language, an encoder-decoder architecture with a self-attention mechanism on both sides. The encoder block takes input and converts it into numerical form, and the decoder block takes that numerical form and converts it to text. To constrain the article to the specific applications of transformers, we will not delve into the depth of its architecture. Kindly go through the links in the Reference section to understand the architecture, evolution, and fundamentals of transformers. Why Transformers? Transformers aided in successfully establishing \u201cTransfer Learning\u201d in NLP by enabling the usability of features extracted from a pretrained model. The idea is to utilize a model that is pretrained with humongous text data (also called as Large Language Models), by fine-tuning it to our own purpose. Along with saving a lot of time and expense, the need for large training data has reduced considerably since we are using an already pretrained model. This, in turn, triggered the formation of widespread research into transformers and brought the existence of numerous NLP pretrained models. The implementation of numerous applications with these models is made possible and bloomed the transfer learning in\u00a0NLP. Hugging Face Hugging Face is a library built by artificial intelligence enthusiasts where myriad models are built and shared with the community. Hugging Face comprises pretrained models for domains such as Computer Vision, Natural Language Processing, Audio Processing, Tabular data, Reinforcement Learning, and Multimodal applications. All the models can easily be accessed using the API(Application Programming Interface) The aim of the article is to harness the NLP domain and explore the possible applications with a demo code and explanation. To utilize any of the Hugging Face models, the first step is to install transformers library as\u00a0follows. pip install transformers The next step is to utilize pipeline which helps in hiding all the complex steps behind the model implementation and providing a simple, easy-to-use API for the models. For each of the major applications mentioned below, we will see the code along with an explanation. There are many models available in Hugging Face for every task in every domain. Since this article is about awareness of the applications, we will utilize the default model set by the library itself. Also, considering that this article is about the introduction to these possibilities, the details regarding the models can be referred to from the links given in each\u00a0section. Applications In all of the applications discussed in the article, we are going to follow these\u00a0steps. 1. Import\u00a0pipeline 2. Invoke a model for the corresponding task by instantiatingpipeline. The model weights of the default model for the task are downloaded in this\u00a0step. 3. Utilize that model by giving the required\u00a0input. 4. Review the results and try different inputs. The code and output for all the applications are given in this GitHub repository, along with a Google Colab\u00a0link. The applications discussed in this article\u00a0are 1. Text Classification 2. Text Summarization 3. Question Answering 4. Text Generation 5. Named Entity Recognition 1. Text Classification Classification is the process of placing the given text in any one of the informed categories. Let us start with importing the required library as\u00a0follows. The next step is to initialize a variable named text_classifier that represents the model being invoked in the text-classification category. As we know, the model is invoked bypipeline function using the task name. Since a specific model name is not given, the default model (distilbert-base-uncased-finetuned-sst-2-english) will be downloaded, which can be seen in the following output. from transformers import pipelinetext_classifier = pipeline(\"text-classification\")Output:No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).Using a pipeline without specifying a model name and revision in production is not recommended.Downloading: 100%629/629 [00:00&#60;00:00, 24.4kB/s]Downloading: 100%268M/268M [00:05&#60;00:00, 62.8MB/s]Downloading: 100%48.0/48.0 [00:00&#60;00:00, 1.70kB/s]Downloading: 100%232k/232k [00:00&#60;00:00, 6.48MB/s] As the next step, we give input to the text classification model using the initialized variable text_classifier. The model being invoked helps us to classify the given text and produce a score on POSITIVE and NEGATIVE sentiments on the text. For more information about the model, please click this\u00a0link. In the code given below, a sentence representing a negative sentiment is given, and the results are stored in clf_result. On printing the output, we can see that the model classified the sentence into a NEGATIVE category and gives a score (representing sentiment score). clf_result = text_classifier(\"Oh God!!!! Its so horrible to hear about the news of aircraft\")print(clf_result)Output[{&#039;label&#039;: &#039;NEGATIVE&#039;, &#039;score&#039;: 0.9992474317550659}] Now let us try with another sentence and see the result. In the snippet below, it can be seen that the result is POSITIVE label as we have given a statement that expresses relief. clf_result = text_classifier(\"Oh God!!!! So relieved to hear about the aircraft\")print(clf_result)Output[{&#039;label&#039;: &#039;POSITIVE&#039;, &#039;score&#039;: 0.9974811673164368}] 2. Text Summarization Text summarization is the task of extracting a summary from a given set of sentences. The first step is to import pipeline and the next, we instantiate with the task of our choice (summarization). The default model for text summarization is sshleifer/distilbart-cnn-12\u20136 and to know more about the model, please check this link. The model is invoked under the variable text_summarizer. from transformers import pipelinetext_summarizer = pipeline(\"summarization\")Output:No model was supplied, defaulted to [&#8230;]",
            "pubdate": "Wed, 14 Dec 2022 00:12:05 +0000",
            "pubdate_parsed": [
                2022,
                12,
                14
            ],
            "email_sent": true
        },
        "10 + Politics Related Data Visuals In A Single Line Of Code": {
            "url": "https://towardsai.net/p/l/10-politics-related-data-visuals-in-a-single-line-of-code",
            "description": "Last Updated on December 14, 2022 by Editorial Team Author(s): Adam Ross Nelson Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. For data scientists + data pros who need a quick viz on-they-fly for testing, demonstration, or training purposes Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 15 Dec 2022 00:10:59 +0000",
            "pubdate_parsed": [
                2022,
                12,
                15
            ],
            "email_sent": true
        },
        "A guide to Persistent storage in Docker": {
            "url": "https://towardsai.net/p/l/a-guide-to-persistent-storage-in-docker",
            "description": "Last Updated on December 15, 2022 by Editorial Team Author(s): Prithivee Ramalingam Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What is the need for persistent storage in\u00a0Docker? Applications generate 2 kinds of data, persistent and non-persistent. Non-persistent data can be ignored, and they don\u2019t have to be saved anywhere. On the other hand, persistent data needs to be saved for future use; it can\u2019t be lost at any cost. If the application is hosted as a container, persistent data must be accessible to multiple containers as they share the load and storage. The data must persist, devoid of the status of the container. Since we have understood the need for persisting data, let\u2019s look at how data is stored inside a container. A container consists of multiple layers, and the files inside the container are stored in the writable layer. The data can only be persisted as long as the container exists, which means when the container is deleted, all the data inside it will be lost. Which presents the following problems, It would be difficult for another container to access the data which is present inside the container. Since the container\u2019s writable layer is tightly coupled to the host machine, it would be difficult to move data to a different system. To solve this problem, docker came up with 2 ways of persistent storage, Volumes and bind mounts. Docker also supports temporary file storage for in-memory use\u00a0cases. In this article we will be learning about the different persistent storage options, their implementation, their use case along with code\u00a0samples. Photo by John Salvino on\u00a0Unsplash Table of\u00a0contents 1. Code walkthrough 2. Bind\u00a0Mounts 3. Volumes 4. Temporary file storage\u00a0mounts 5. Conclusion 1. Code walkthrough For this article, we have taken the example of a simple python application that takes in the file name and content of the file as parameters and creates the file with the specified content. The source code for this application can be found\u00a0here. from flask import Flask, requestimport osapp = Flask(__name__)if not os.path.exists(\"docker_bind\"): os.makedir(\"docker_bind\")@app.route(\"/create_file\",methods=[\"POST\"])def run(): data = request.get_json() file_name, content = data[&#039;file_name&#039;], data[&#039;content&#039;] file_path = f\"docker_bind/{file_name}\" with open(file_path,&#039;w&#039;) as write_file: write_file.write(content) return {\"Status\":\"Success\"}app.run(debug=False,host=&#039;0.0.0.0&#039;,port = 5000) To run this application as a container, the prerequisite is that docker has to be installed. After installing docker, open the command prompt and execute the following commands. The list of all the commands can be found\u00a0here. To build the container. docker build -t create_file_py_image . To run the container. docker run --name create_file_py_container -p 5001:5000 create_file_py_image After running the above command, we can open postman and send a request to the running container with file name and content as parameters. The application takes in the parameters, creates the file with the specified name and content, and returns the Status as success. We will be using the same code to explain both volume and bind\u00a0mounts. Image by Author\u200a\u2014\u200aSending request to the containers 2. Bind\u00a0Mounts 2.1 What are Bind\u00a0Mounts? 2.2 Creating a Bind\u00a0Mount. 2.3 Multiple containers accessing the same Bind\u00a0mount. 2.4 Demonstrating persistence with Bind\u00a0mount. 2.5 Where can we use Bind\u00a0Mounts 2.1 What are Bind\u00a0Mounts? Bind mounts are used for persistence, and they have been available since the early days of docker. When we use a bind mount, a directory on the host is mounted into a container. In bind mounts, the directory is managed by us and not by\u00a0docker. Bind Mounts also come up with a slight disadvantage as the containers have the ability to modify, delete and create resources in the host OS. Attention has to be provided if non-docker elements need to access the mount\u00a0folder. 2.2 Creating a Bind\u00a0Mount The mount flag is used to mention the kind of persistence we require. It could be bind, tmpfs, or volume. In this case, we set it to bind. For creating a bind mount, we need to provide the source path explicitly. It has to be an absolute path and not a relative. The source path is the path in the host. Similarly, we need to provide the target path. This is the path inside the container which we want to\u00a0mount. docker run -d -it -p 5000:5000 --name create_file_py_container1 --mount type=bind,source=\"C:\\Users\\prithiveer\\Documents\\Docker_Bind\",target=/app/docker_bind create_file_py_image With the above command, a container will be created. We sent a request from Postman to create a file named \u201csample_1.txt\u201d with the content \u201cMy first file\u201d. As shown below, we can use exec inside the container and find the file which we\u00a0created. Image by Author\u200a\u2014\u200aCreating a file using create_file_py_container1 2.3 Multiple containers accessing the same Bind\u00a0mount In real life, an application would be hosted in multiple containers, and we require them to be mounted to a single bind mount. So, for demonstration purposes, we create two more containers. docker run -d -it -p 5002:5000 --name create_file_py_container2 --mount type=bind,source=\"C:\\Users\\prithiveer\\Documents\\Docker_Bind\",target=/app/docker_bind create_file_py_imagedocker run -d -it -p 5003:5000 --name create_file_py_container3 --mount type=bind,source=\"C:\\Users\\prithiveer\\Documents\\Docker_Bind\",target=/app/docker_bind create_file_py_image Like the first container, we send requests from containers 2 and 3 and generate files sample_2.txt and sample_3.txt, respectively. While creating the bind mount, we provided a source address. We can find all the files which we created inside the containers in the specified location. Similarly, we can also find all the files in the docker_bind folder of all the 3 containers, irrespective of the files which were created by each container. Image by Author\u200a\u2014\u200aSource Folder has all the files which were created inside the 3 containers 2.4 Demonstrating persistence with Bind\u00a0mount. To demonstrate persistence, we delete all the 3 containers and create them again. Since the data is written to the writable layer when we delete the container, all the data inside the container should be lost. But due to bind mounts, we will be able to see all the files\u00a0created. Image by Author\u200a\u2014\u200aCreating the container again and finding the files created\u00a0earlier 2.5 [&#8230;]",
            "pubdate": "Fri, 16 Dec 2022 00:04:57 +0000",
            "pubdate_parsed": [
                2022,
                12,
                16
            ],
            "email_sent": true
        },
        "The Dark Side of OpenAIs ChatGPT": {
            "url": "https://towardsai.net/p/l/the-dark-side-of-openais-chatgpt",
            "description": "Last Updated on December 16, 2022 by Editorial Team Author(s): Taimur Ijlal Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. How this revolutionary new tool can usher in a new era of cybercrime Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 17 Dec 2022 00:09:49 +0000",
            "pubdate_parsed": [
                2022,
                12,
                17
            ],
            "email_sent": true
        },
        "10 Essential Skills for AI Leaders": {
            "url": "https://towardsai.net/p/l/10-essential-skills-for-ai-leaders",
            "description": "Last Updated on December 19, 2022 by Editorial Team Author(s): Dr. Mandar Karhade, MD. PhD. Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Or Not, No pressure and be an Individual Contributor&#xa0;;) Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 19 Dec 2022 05:13:38 +0000",
            "pubdate_parsed": [
                2022,
                12,
                19
            ],
            "email_sent": true
        },
        "Design your AI Art Generator Prompt Using ChatGPT": {
            "url": "https://towardsai.net/p/l/design-your-ai-art-generator-prompt-using-chatgpt",
            "description": "Last Updated on December 19, 2022 by Editorial Team Author(s): Bildea Ana Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A short guide on how to use ChatGPT to elaborate your text prompts Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 19 Dec 2022 05:13:35 +0000",
            "pubdate_parsed": [
                2022,
                12,
                19
            ],
            "email_sent": true
        },
        "A Visual Journey in What Vision-Transformers See": {
            "url": "https://towardsai.net/p/l/a-visual-journey-in-what-vision-transformers-see",
            "description": "Last Updated on December 22, 2022 by Editorial Team Author(s): Salvatore Raieli Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. How some of the largest models see the\u00a0world image from the original article:\u00a0source Visualizing CNN&#039;s allowed us to learn more about how these models work. Now that Vision Transformers are taking the stage, a new article explains how we can see what these broad models see the world\u00a0as. Visualize the vision transformers image from the original article:\u00a0source Since convolution neural networks (CNN) have emerged as a winning model in computer vision, different research groups have focused on understanding what these models\u00a0learn. On the one hand, neural networks have emerged in several fields (from language analysis to computer vision) but have been considered \u201cblack boxes.\u201d In contrast to many other algorithms, they are much more difficult to interpret. In fact, the more capable the models become (growth in the number of parameters), the more difficult it becomes to be able to understand what is going on\u00a0inside. Therefore, several methods have been developed to visualize what a convolutional neural network learns. Some of the most\u00a0used: Visualize the filters (or visualize the weights). Visualize layer activation To retrieve an image that maximally activates a\u00a0neuron Embedding the feature vectors with\u00a0t-SNE. GradCAM, saliency\u00a0maps. In 2016, transformers appeared on the scene. These wide models based on self-attention have been shown to achieve much superior performance in NLP (machine translation, language classification, and so on). Soon, they became the standard for NLP, and with the introduction of vision transformers, they were also applied to computer\u00a0vision. from the original transformer article:\u00a0here Therefore different researchers have tried to visualize what vision transformers (ViTs) learn. ViTs have proven to be much more difficult to analyze, and so far, the methods used have shown limitations. Understanding the inner workings of these models could be helpful in explaining their success and potential corner\u00a0cases. Previous work had focused on observing the activation of keys, queries, and values from the self-attention layer, but the result was unsuccessful. Visualizing the self-attention weights it is not leading to insightful visualization. caption and image from the original article:\u00a0source A paper has recently been published by researchers at New York University and the University of Maryland that provides a better understanding of what happens inside the model (whether they are vision transformers or models such as\u00a0CLIP). In the article, the researchers summarize their contribution: While standard methods lead to uninterpretable results (especially when applied to keys, queries, and values), it is possible to obtain informative visualizations by applying the same techniques to the next feed-forward layer of the same transformer block (and they demonstrated this using different models: ViTs, DeiT, CoaT, ConViT, PiT, Swin, and Twin transformers). Patch-wise image activation patterns for ViT features behave like saliency maps demonstrating that the model preserves positional relationships between patches (and learns this during training). CNN&#039;s and ViTs construct a complex and progressive representation (in CNNs, the first layers represent edges and textures, while later layers learn more complex patterns, and the authors show that the same happens in ViTs). ViTs, in contrast to CNN&#039;s are better able to use background information. The authors also applied their method to models using language supervision (such as CLIP) and showed that features could be extracted from these models that are associable with caption text (such as prepositions, adjectives, and conceptual categories). The authors compared ViTs to convolutional networks and noted that the representation increases in complexity along the pattern (earlier layers learn simpler structures while more sophisticated patterns are learned by more advanced layers). In practice, both CNN and ViTs share what is called progressive specialization. \u201cThe progression for visualized features of ViT B-32. Features from early layers capture general edges and textures. Moving into deeper layers, features evolve to capture more specialized image components and finally concrete objects.\u201d caption and image from the original article:\u00a0source \u201cComplexity of features vs depth in ViT B-32. Visualizations suggest that ViTs are similar to CNNs in that they show a feature progression from textures to parts to objects as we progress from shallow to deep features.\u201d caption and image from the original article:\u00a0source There are also differences. The authors investigated the reliance of ViTs and CNNs on background and foreground image features (using bounding boxes on ImageNet). ViTs are able to detect background information present in the image (in the image, for example, grass and snow). In addition, by masking the background or foreground in the image the researchers showed that ViTs not only use the background information better but are also less affected by its\u00a0removal. \u201c ViT-B16 detects background features. Left: Image optimized to maximally activate a feature from layer 6. Center: Corresponding maximally activating example from ImageNet. Right: The image\u2019s patch-wise activation map. (b): An example of an original image and masked-out foreground and background.\u201d caption and image from the original article:\u00a0source We find it surprising that even though every patch can influence the representation of every other patch, these representations remain local, even for individual channels in deep layers in the network. While a similar finding for CNNs, whose neurons may have a limited receptive field, would be unsurprising, even neurons in the first layer of a ViT have a complete receptive field. In other words, ViTs learn to preserve spatial information, despite lacking the inductive bias of CNNs. -source: original\u00a0article In other words, during training, the model learns how to preserve spatial information. In addition, the last layer instead has a uniform activation pattern and learns how to classify the image (according to the authors, the last layer has the function of globalizing information). Based on the preservation of spatial information in patches, we hypothesize that the CLS token plays a relatively minor role throughout the [&#8230;]",
            "pubdate": "Thu, 22 Dec 2022 12:15:09 +0000",
            "pubdate_parsed": [
                2022,
                12,
                22
            ],
            "email_sent": true
        },
        "All About Git Branches And Git pull vs fetch": {
            "url": "https://towardsai.net/p/l/all-about-git-branches-and-git-pull-vs-fetch",
            "description": "Last Updated on December 25, 2022 by Editorial Team Author(s): Muttineni Sai Rohith Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. All About Git Branches&#8230; And Git pull vs.\u00a0fetch With Branches in Git, we can create a copy of the file we would like to work on without messing up with the original file. Later we can merge them to the original\u00a0copy. This is the second addition to the series of articles about Git\u200a\u2014\u200aComplete Git Tutorial for Beginners with Examples. The link is available below\u00a0\u2014 Complete Git Tutorial for Beginners with Examples So far, we have created the main branch, pushed our first commit to the branch, and then 2nd commit. Our repo looks like this\u00a0\u2014 Branch Flow Now let&#039;s create a test branch and push our new changes to that branch. We can create a new branch named \u201ctest\u201d using the below command\u00a0&#8211; git checkout -b test In the above command, checkout makes us switch to a branch, and -b helps us in creating the\u00a0branch. Now Let\u2019s add some changes, then add them to our local repo commit and push them to the remote repo with the test\u00a0branch. def add(a, b): return a+b+10 We can add, commit and push changes to the remote repo using the following commands\u00a0\u2014 git add .git commit -m \u201cUpdating the add function with offset 10\u201dgit push origin test Instead of git add\u00a0. and git commit -m \u201cmessage\u201d we can use the following command\u00a0\u2014 git commit -am \"Updating the add function with offset 10\"git push origin test Now our repo looks like this\u00a0&#8211; Branch Flow Now Let\u2019s make another change by changing the offset to 20 and commit and push to the test branch\u00a0again. def add(a, b): return a+b+20 Let\u2019s add, commit and push into the remote\u00a0Repo git commit -a -m \"changing the offset to 20\"git push origin test Output Now as we have some modified code in the test branch, I feel my code is ready, and let\u2019s merge our code with the main\u00a0branch. This can be done using this command\u00a0&#8211; git checkout maingit merge test Here we used checkout to switch the branch, we are not using -b as it is used to create new. And then merge the code using the git merge\u00a0command. Now Below is what our repo looks\u00a0like- Now that we have made many changes to the main branch, let&#039;s see the commit history using the command\u00a0&#8211; git log We can press \u201cq\u201d to come out of the\u00a0log. To make our test branch track the main branch, we can use the below command\u00a0&#8211; git branch - set-upstream-to=origin/main test Pulling a git repo\u00a0branch Now let\u2019s make some changes in the main branch and commit\u00a0this. def add(a, b): return a+bdef sub(a, b): return a-bdef mul(a, b): return a * ba = int(input())b = int(input())print(add(a, b))print(sub(a, b))print(mul(a, b)) Now let&#039;s commit this and push this to the main\u00a0branch. git add .git commit -m \"adding multiplication code\"git push origin main Now in the main branch, we have made our\u00a0changes. Now let\u2019s suppose we need to add another change in the main branch, but as it is in production we have to make sure the testing doesn\u2019t cause any problems. So let\u2019s implement that change in the test branch and then create a pull request to merge that in the main\u00a0branch. But before that, let\u2019s check the code in the test branch. So for that, let\u2019s switch back to the test branch using the command\u00a0&#8211; git checkout test Here we can see the code is not updated. And our test branch Lags one commit behind the main\u00a0branch. To update the code from the main branch we need to give this command\u00a0&#8211; git pull origin main or git pull Pull command helps in pulling the latest changes from our own branch or from the remote\u00a0branch. Whenever we are working in the branches, it is a good practice to pull our changes regularly. Git fetch vs. Git\u00a0pull Git pull will check for any lagging code commits, and if our local repo lags behind any commit, then using Git pull, we can retrieve the updated changes and get them merged. At the same time, git fetch will only retrieve the updated changes in the lagging commits. And then, we should use git merge to merge them in our\u00a0repo. As we can see, after a commit is made, there is no update in the local repo using git status, but when we used git fetch, we got info that our local repo is lagging by 1 commit behind the main repo. So then, we should use the git merge command to merge the\u00a0changes. But all this can be implemented using one command\u00a0&#8211; git pull It will fetch the changes and then merge the changes in our local repo from the remote/origin repo. git pull = git fetch + git merge So that\u2019s all about git branches in this article&#8230; But there is more to cover on Git rebase, Stash, revert and reset. Those will be covered in the below article\u00a0\u2014 Git Rebase, Merge, and Stash Happy Learning\u2026 Stay\u00a0tuned\u2026 All About Git Branches&#8230; And Git pull vs fetch was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 25 Dec 2022 12:23:53 +0000",
            "pubdate_parsed": [
                2022,
                12,
                25
            ],
            "email_sent": true
        },
        "Complete Git Tutorial for Beginners with Examples": {
            "url": "https://towardsai.net/p/l/complete-git-tutorial-for-beginners-with-examples",
            "description": "Last Updated on December 25, 2022 by Editorial Team Author(s): Muttineni Sai Rohith Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Git is a version control system that lets us track the changes we make to our files over time. With Git we can revert to various states of our files. We can make a copy of our files and make changes to that copy and then merge these changes to the original copy.We can install git using this official\u00a0website. Photo by Luke Chesser on\u00a0Unsplash Configuring git:To verify git installation, we can open git bash and type the git\u200a\u2014\u200aversion command. This shows the latest version of git installed on our\u00a0PC. git --version Output The first thing we have to do is to set our username and email address. Git uses this information to identify who made changes to specific files. To set our username and email id, we can use the below commands\u00a0&#8211; git config --global user.name \u201cOur_username\u201dgit config --global user.email \u201cOur_email\u201d Output This will configure our Git environment. To check whether the configuration is succeeded, we can still check our configuration in the local machine using this command\u00a0\u2014 git config -l We can also store credentials in the cache, Just to avoid giving them each time using the below command\u00a0&#8211; git config --global credential.helper cache Cloning existing git repo to local &#8211;We can clone the existing git repo into the local machine using the command\u00a0&#8211; git clone https://ourrepo Output Creating and Initializing a new project in GitCreate a new folder or navigate to the folder where you want to create your first project. I have used git bash to create a folder test_calculator using the below commands\u00a0\u2014 mkdir test_calculatorcd test_calculator Alternatively, you can Open the folder you like through File Explorer, Right click on it and select Git Bash\u00a0Here Output Now once the folder is ready, we need to initialize the project, For this, we need to run the git init command. This will tell git to start watching our files, for every change that\u00a0occurs. Output Now let\u2019s add a small block of code to our Folder.I am writing a small python program calculator.py for that, which contains a function, that will take two numbers, a and b as arguments and return the\u00a0result. def add(a, b): return a+b a = int(input())b = int(input())print(add(a, b)) Now that we have added our changes to the folder, let\u2019s check whether git has tracked our changes. This can be done using the git status\u00a0command. git status Output We can see our branch name, if there are any past commits to our branch done us, And then any updated/untracked files that are not synced with the\u00a0git. To add the updated changes or untracked files, we need to use the command\u00a0\u2014 git add . This will add all the untracked changes. Output Now after performing git add\u00a0.\u00a0; We can see that our change is tracked with the git. And we can see the changes that are yet to be committed.To commit our changes, we can use this command git commit -m \u201cfirst commit\u201d\u00a0command. git commit -m \u201cfirst commit\u201d -m is the shorthand for the message and the text inside the parentheses is the commit\u00a0message. Output Now Our code is ready and committed. Now let\u2019s push our code to the\u00a0repo. For that to happen, let\u2019s create a Repo in GitHub. This can be done by logging in to GitHub and clicking on Repos -&#62; new -&#62; Enter Repo Name and click on create. This is my Repo link\u200a\u2014\u200ahttps://github.com/muttinenisairohith/test_calculator.git Later we need to create a connection between our local repo and the remote repo in Github. For that, we can use this command\u00a0&#8211; git remote add origin https://github.com/muttinenisairohith/test_calculator.git As we have our remote connection ready now. By default, git will point to the Master branch, but instead of the master branch, I want to push my changes to the main branch. For that, we can use the command\u00a0\u2014 git branch -b main Now We have our repo and branch ready, Let\u2019s push our modified code to our Branch in the Repo Created. For this, we can use the following command\u00a0&#8211; git push -u origin main This will push our changes to the main branch, and if it is our first time, it will ask us to Login into Git. We can log in to git using our email id and password. Output Git vs\u00a0GitHub Generally, people think Git and Github are similar, But they are not. As said, Git is a version control system that tracks our changes to the repo and provides us various functionalities like time traveling etc., Whereas Github is an Online hosting service for Git Repositories. Similar to Azure DevOps Services, bitbucket, etc., Github helps us to store our repo in their platform and allow other developers to contribute at the same time from any location. As we have pushed our first git project to Github, Moving further, Let\u2019s understand the stages of a file being tracked by\u00a0Git. Committed State\u200a\u2014\u200aA file is in the committed state when all the changes made to the file have been saved in the local repo. Files in the committed stage are files ready to be pushed to the remote repo (on\u00a0GitHub) Modified state\u200a\u2014\u200aA file in the modified state has some changes made to it, but it\u2019s not yet saved. This means that the state of the file has been altered from its previous state in the committed state Staged state\u200a\u2014\u200aA file in the staged state means it is ready to be committed. In this state, all necessary changes have been made, so the next step is to move the file to the commit\u00a0state. Now Moving Further let\u2019s add a functionality subtraction to our code and save that again\u00a0&#8211; def add(a, b):return [&#8230;]",
            "pubdate": "Sun, 25 Dec 2022 12:23:50 +0000",
            "pubdate_parsed": [
                2022,
                12,
                25
            ],
            "email_sent": true
        },
        "Maximizing ROI with Digital Twins: A Cost-Benefit Analysis": {
            "url": "https://towardsai.net/p/l/maximizing-roi-with-digital-twins-a-cost-benefit-analysis",
            "description": "Last Updated on December 25, 2022 by Editorial Team Author(s): Ishaan Gill Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Digital twins have been gaining significant traction in a wide range of industries in recent years, and for a good reason. A digital twin&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 25 Dec 2022 12:23:47 +0000",
            "pubdate_parsed": [
                2022,
                12,
                25
            ],
            "email_sent": true
        },
        "Evolution of AI and Data Science in 2022": {
            "url": "https://towardsai.net/p/l/evolution-of-ai-and-data-science-in-2022",
            "description": "Last Updated on December 25, 2022 by Editorial Team Author(s): Anmol Tomar Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. What can we expect in 2023 Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 25 Dec 2022 12:05:25 +0000",
            "pubdate_parsed": [
                2022,
                12,
                25
            ],
            "email_sent": true
        },
        "How to Identify Objects at Pixel Level using Deep Learning in Java": {
            "url": "https://towardsai.net/p/l/how-to-identify-objects-at-pixel-level-using-deep-learning-in-java",
            "description": "Last Updated on December 25, 2022 by Editorial Team Author(s): Xin Yang Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. with Deep Java\u00a0Library Semantic segmentation is a powerful technique in deep learning that allows for the identification of objects in images at the pixel level. The goal of semantic segmentation is to label each pixel of an image with a corresponding class. In this blog post, we will explore how to use semantic segmentation to identify objects in images in\u00a0Java. Note that semantic segmentation is different from instance segmentation, which is able to distinguish several instances belonging to the same class. Therefore, if we have two objects of the same category in the input image, the segmentation map will give the same label for the two objects. To distinguish separate instances of the same class, please refer to instance segmentation. Semantic segmentation can be applied to a wide range of use cases like self-driving cars, visual image search, medical imaging, and so on. For example, semantic segmentation can be used to accurately identify and classify different objects in the environment, such as pedestrians, vehicles, traffic signs, and buildings. Semantic segmentation for self-driving cars\u00a0(Source) Deep Java Library (DJL) is a Java-based Deep Learning (DL) Framework. It can be used to create and train models, as well as run inference. DJL provides enriched functionalities to apply Semantic Segmentation to use cases. In this post, we are going to demo how to employ these functionalities to achieve some common use\u00a0cases. Prerequisites To get started, we first need to declare the DJL dependencies in the module\u2019s build.gradle file: dependencies { implementation \"ai.djl:api:0.20.0\" runtimeOnly \"ai.djl.pytorch:pytorch-engine:0.20.0\" runtimeOnly \"ai.djl.android:pytorch-native:0.20.0\"} Run Inference Once we have dependencies set up, we can start writing code to run inference. In this example, we will use the DeepLabV3 model, which is a state-of-art model for semantic segmentation. To run inference for Semantic Segmentation, first load the Semantic Segmentation model. Then create a predictor with the given Model and Translator. In this case, SemanticSegmentationTranslator will be\u00a0used. After loading the model, feed the model an image and receive a \"segmentation map\" as output. This can be done by calling Predictor.predict(). The predictor takes an Image as input, and returns a CategoryMask as output. The CategoryMask contains a 2D array representing the class of each pixel in the original image. We can use the following code to do\u00a0this: String url = \"https://mlrepo.djl.ai/model/cv/semantic_segmentation/ai/djl/pytorch/deeplabv3/0.0.1/deeplabv3.zip\";Criteria&#60;Image, CategoryMask&#62; criteria = Criteria.builder() .setTypes(Image.class, CategoryMask.class) .optModelUrls(url) .optTranslatorFactory(new SemanticSegmentationTranslatorFactory()) .optEngine(\"PyTorch\") .optProgress(new ProgressBar()) .build();try (ZooModel&#60;Image, CategoryMask&#62; model = criteria.loadModel(); Predictor&#60;Image, CategoryMask&#62; predictor = model.newPredictor()) { CategoryMask mask = predictor.predict(img); // Do something with `mask`} For example, suppose we have a 600x800x3 RGB color\u00a0image: The output CategoryMask contains a 600\u00d7800\u00d71 mask array representing the class labels as integers. Below is the downsampled mask\u00a0array: DJL also provides utilities for visualizing the results of semantic segmentation, such as the ability to overlay the segmentation map on top of the original image to highlight the areas that the model has classified as belonging to a specific class. These can be useful for a variety of use\u00a0cases. Use Cases Use Case 1: Self-Driving Cars One use case for semantic segmentation is to enable self-driving cars to perceive and understand their surroundings. For example, by accurately identifying the positions of other vehicles on the road, the self-driving car can make informed decisions about how to navigate through traffic. Similarly, by accurately identifying pedestrians and other obstacles, the self-driving car can take appropriate actions to avoid collisions. To identify the objects in an image, call Predictor.predict() to feed the model with the image\u00a0below: CategoryMask mask = predictor.predict(img); Street scene\u00a0(Source) Then, to visualize the result, call CategoryMask.drawMask() to highlight the detected objects on the\u00a0image. mask.drawMask(img, 180, 0); Use Case 2: Extract object from\u00a0photo Another use case for semantic segmentation is in the process of extracting objects from photos for passport applications. For example, consider a scenario where an individual is required to submit a passport-style photo as part of their application. In this case, the goal might be to use semantic segmentation to extract the individual\u2019s face from the photo and use it to generate a passport-style photo that meets the required specifications. To extract the face in an image, call Predictor.predict() to feed the model with the image\u00a0below: CategoryMask mask = predictor.predict(img); Portrait image\u00a0(Source) Then call the CategoryMask.getMaskImage() method. Note that 15 is the ID of the person&#039;s\u00a0class. Image person = mask.getMaskImage(img, 15);person = person.resize(img.getWidth(), img.getHeight(), true); Use Case 3: Replace background for video conferences The third use case for semantic segmentation is in the process of replacing the background in an image for video conferences. For example, consider a scenario where an individual is participating in a video conference and wants to replace the background behind them with a more professional or appealing image. In this case, we could use semantic segmentation to automatically extract their foreground (e.g., their body and any objects they are holding) from the background of the\u00a0image. To achieve this, we can extract the individual\u2019s foreground from the background of the image. The extracted foreground can then be composited onto a new background image. This allows the individual to replace the background in the image with a more professional or appealing image, which can be useful for video conferences where the individual wants to present a more polished appearance. To extract the foreground in an image, call Predictor.predict() to feed the model with the image\u00a0below: CategoryMask mask = predictor.predict(img); Video conference scene\u00a0(Source) Then replace the background with another\u00a0image: Image background = ImageFactory.getInstance().fromFile(Paths.get(\"image_path\"));Image person = mask.getMaskImage(img, 15);person = person.resize(img.getWidth(), img.getHeight(), true);background.drawImage(person, true); You can find more DJL example code\u00a0here. DJL also provided an Android app with semantic_segmentation which can take a picture and run semantic segmentation with a variety of\u00a0options. Conclusion [&#8230;]",
            "pubdate": "Mon, 26 Dec 2022 00:13:08 +0000",
            "pubdate_parsed": [
                2022,
                12,
                26
            ],
            "email_sent": true
        },
        "My Pandas Cheatsheet for Exploratory Analysis and Data Manipulation": {
            "url": "https://towardsai.net/p/l/my-pandas-cheatsheet-for-exploratory-analysis-and-data-manipulation",
            "description": "Last Updated on December 28, 2022 by Editorial Team Author(s): Eugenia Anello Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Top 12 Pandas functions that can help you to analyze and manipulate fastly your dataset Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 29 Dec 2022 00:18:37 +0000",
            "pubdate_parsed": [
                2022,
                12,
                29
            ],
            "email_sent": true
        },
        "AI Anyone Can Understand: Part 8The Monte Carlo Method": {
            "url": "https://towardsai.net/p/l/ai-anyone-can-understand-part-8%e2%80%8a-%e2%80%8athe-monte-carlo-method",
            "description": "Last Updated on December 29, 2022 by Editorial Team Author(s): Andrew Austin Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Understanding the basics of the Monte Carlo method Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 29 Dec 2022 12:15:56 +0000",
            "pubdate_parsed": [
                2022,
                12,
                29
            ],
            "email_sent": true
        },
        "A Guide to MLOps in Production": {
            "url": "https://towardsai.net/p/l/a-guide-to-mlops-in-production",
            "description": "Last Updated on January 1, 2023 by Editorial Team Author(s): Prithivee Ramalingam Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. with Azure DevOps, Azure Kubernetes Service, and Azure Machine\u00a0Learning Introduction Countless hours of organized effort are required to bring a model to the production stage. The efforts which were spent on all the previous steps would turn out to be fruitful only if the model is successfully deployed. For deploying the model to production, we must consider an architecture that is secure, highly available, fault-tolerant, and capable of autoscaling. If you have figured out where I am going with this, then kudos to you. Yes, Kubernetes is the ideal candidate for our job. But setting up a Kubernetes cluster is not an easy task. We have to set up and configure load balancers, ingresses, role-based authentication, Virtual Machines, network policies, and so on. This requires a considerable amount of time which can be better spent perfecting our model. That\u2019s why Kubernetes as a Service (Kaas) is the preferred option under these circumstances. This article illustrates the whole lifecycle of MLOps, starting from training the model to deploying it in a production environment. The whole process is automated. The CI pipeline gets triggered whenever we make a code change, and the CD pipeline gets triggered whenever there is a new artifact available or if the CI pipeline gets completed successfully. In this way, the new functionality can be deployed with just a single commit. The below image provides a high-level overview of the whole\u00a0process. Image By Author\u200a\u2014\u200aHigh-level overview of MLOps\u00a0example If you require an End-to-End example of a CI-CD pipeline for development and QA environments, you can refer to my article -&#62; End-to-end example of CI-CD pipeline using Azure Machine Learning &#124; by Prithivee Ramalingam &#124; Apr, 2022 &#124; Towards\u00a0Dev Content Prerequisites. Code walkthrough. Creating a CI pipeline in Azure DevOps to train the model and publish the artifact. Creating a CD pipeline to deploy the model in Azure Kubernetes Service\u00a0(AKS). Testing the model deployed in\u00a0AKS. 1. Prerequisites: 1.1 Azure Devops\u00a0account 1.2 Azure Machine Learning\u00a0resource 1.3 Azure Kubernetes Service (AKS)\u00a0cluster 1.4 Resource Manager connection 1.1 Azure DevOps\u00a0account Azure DevOps Server is a Microsoft product that provides version control, project management, automated builds, testing, and release management capabilities. To create an Azure DevOps account, please follow these instructions. In a production setup, automation must be preferred over manual interference. We need to automate the process of creating builds and deploying them to a Kubernetes cluster. For this use case, we make use of the pipelines provided by Azure DevOps. We create a Continuous Integration (CI) pipeline for creating a model and packaging it and a Continuous Deployment (CD) pipeline for deploying the model in Kubernetes. 1.2 Azure Machine Learning\u00a0resource Azure Machine Learning is a cloud service for accelerating and managing the machine learning project lifecycle. Azure Machine Learning, teamed up with MLFlow, providing a centralized repository to host docker images, runs, environments and artifacts. It provides functionality to create compute clusters and instances. For additional information regarding Azure Machine Learning, you can refer to my article https://medium.com/mlearning-ai/features-and-capabilities-of-azure-machine-learning-868eb3b4d333. In this production setup, we are going to use Azure ML for logging metrics, saving model artifacts, creating an AKS cluster, and deploying the model in the created cluster. While creating an Azure Machine Learning workspace, Blob storage, a Key vault, a Container registry, and an application insights service are created along with\u00a0it. 1.3 Azure Kubernetes Service (AKS)\u00a0cluster Azure Kubernetes Service (AKS) offers the quickest way to start developing and deploying cloud-native apps in Azure. With the abstraction provided by Azure Machine Learning, we can manage deployment in AKS by configuring just a few variables. We can attach an existing AKS cluster or create a new one with Azure\u00a0ML. Image by Author\u200a\u2014\u200aInference clusters 1.4 Resource Manager connection A Resource manager connection is essential to automate model training, model packaging, model deployment, etc.. Azure ML is the place that centralizes the whole MLOps process. So, we need to access Azure ML securely to perform the above steps. One way is going to Azure ML workspace and setting the resources manually. But in order to do it in an automated manner, we require a resource manager connection which will help us manage the resources in Azure ML from Azure\u00a0DevOps. We should create the Resource Manager connection in Azure DevOps. We can assign the scope to the Subscription level, Management Group level, or to Machine Learning Workspace level. We can also limit access to the pipelines which can access this Service Principal. Project Settings -&#62; Service Connections -&#62; New Service connections -&#62; Azure Resource Manager -&#62; Service Principal (automatic) -&#62; Scope\u00a0level 2. Code walkthrough. After creating all the required prerequisites, the next step is to write code for training and inference and to push it into a version control system like git or Azure DevOps. You can access the source code\u00a0here. Image by Author\u200a\u2014\u200aFolder structure 2.1 Training\u00a0script 2.2 Inference scripts 2.1 Training\u00a0script The training script consists of training.py and files in the environment_setup directory. The environment_setup directory consists of install-requirements.sh, conda_dependencies.yml, and runconfig files. install-requirements.sh -&#62; Has the dependencies which have to be installed in the\u00a0agent. conda_dependencies.yml -&#62; Has the dependencies which have to be installed in the compute target. The dependencies are abstracted as environments. runconfig file (titanic_survival_prediction.runconfig) -&#62; The driver file for the whole training logic. During execution, the runconfig file gets visited first. From there information such as the location of training.py file, environment details, docker image details, and location of conda dependency files are obtained. framework: Pythonscript: training.pycommunicator: NoneautoPrepareEnvironment: truemaxRunDurationSeconds:nodeCount: 1environment: name: titanic_prediction_environment python: userManagedDependencies: false interpreterPath: python condaDependenciesFile: .azureml/conda_dependencies.yml baseCondaEnvironment: docker: enabled: true baseImage: mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04 sharedVolumes: true gpuSupport: false shmSize: 1g arguments: []history: outputCollection: true snapshotProject: true [&#8230;]",
            "pubdate": "Mon, 02 Jan 2023 00:08:32 +0000",
            "pubdate_parsed": [
                2023,
                1,
                2
            ],
            "email_sent": true
        },
        "Building A LSTM From Scratch In Python": {
            "url": "https://towardsai.net/p/l/building-a-lstm-from-scratch-in-python",
            "description": "Last Updated on January 2, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. How to build a basic LSTM using Basic Python libraries Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 02 Jan 2023 12:05:14 +0000",
            "pubdate_parsed": [
                2023,
                1,
                2
            ],
            "email_sent": true
        },
        "Generative AI: What Will Change in 2023": {
            "url": "https://towardsai.net/p/l/generative-ai-what-will-change-in-2023",
            "description": "Last Updated on January 5, 2023 by Editorial Team Author(s): Rafe Brena, PhD Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. It&#x2019;s not just GPT-4 Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 06 Jan 2023 00:19:07 +0000",
            "pubdate_parsed": [
                2023,
                1,
                6
            ],
            "email_sent": true
        },
        "Topic Modeling for E-commerce Reviews using BERTopic": {
            "url": "https://towardsai.net/p/l/topic-modeling-for-e-commerce-reviews-using-bertopic",
            "description": "Last Updated on January 6, 2023 by Editorial Team Author(s): Eugenia Anello Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. A step-by-step guide for training your unsupervised model using a new and improved way to deal with the dataset Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 06 Jan 2023 12:16:46 +0000",
            "pubdate_parsed": [
                2023,
                1,
                6
            ],
            "email_sent": true
        },
        "Conversational AI: 7 Trends and Predictions for 2023": {
            "url": "https://towardsai.net/p/l/conversational-ai-7-trends-and-predictions-for-2023",
            "description": "Last Updated on January 6, 2023 by Editorial Team Author(s): Patrick Meyer Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. I present seven new trends and predictions on the evolution of the conversational assistant market (commonly called Chatbots) for 2023. In&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 07 Jan 2023 00:18:43 +0000",
            "pubdate_parsed": [
                2023,
                1,
                7
            ],
            "email_sent": true
        },
        "The Art and Science of Regularization in Machine Learning: A Comprehensive Guide": {
            "url": "https://towardsai.net/p/l/the-art-and-science-of-regularization-in-machine-learning-a-comprehensive-guide",
            "description": "Last Updated on January 6, 2023 by Editorial Team Author(s): Data Science meets Cyber Security Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. REGULARIZATION IN MACHINE LEARNING: A PRACTICAL GUIDE INTRODUCTION: Are you tired of your machine learning models performing poorly on new data? Are you sick of seeing your model\u2019s validation accuracy skyrocket only to crash and burn on the test set? If so, it\u2019s time to learn about regularisation! Regularisation is a technique that is used to prevent overfitting in machine learning\u00a0models. SOURCE: https://giphy.com/ LET\u2019S GET OUR BASICS CLEAR FIRST\u00a0: We\u2019ll go through some interesting analogies and practical implementation of concepts to get things crystal clear in our\u00a0heads. SOURCE: https://giphy.com/ OVERFITTING\u00a0: Overfitting occurs when a model is too complex and fits the training data too well, but is not able to generalise well to new, unseen data. This can lead to poor performance on the test set or in real-world applications. LET\u2019S UNDERSTAND IT WITH MORE BETTER\u00a0ANALOGY: Imagine a child who has learned to recognize dogs by seeing pictures of only golden retrievers. When shown a picture of a different dog breed, such as a poodle, the child is unable to recognize it as a dog because they have only learned to recognize a specific type of dog rather than having a general understanding of what features define a dog. This is similar to how a model can overfit the training data and be unable to generalize well to new, unseen\u00a0data. LET\u2019S LOOK THROUGH IT PRACTICALLY: import numpy as npimport matplotlib.pyplot as plt # Generate datax = np.linspace(-5, 5, 100)y = x**3 + np.random.normal(0, 10, 100) # Create figure with larger sizefig, ax = plt.subplots(figsize=(10, 6)) # Plot dataax.plot(x, y, &#039;o&#039;)plt.show() This generates a set of 100 x-values and corresponding y-values, with the y-values being a cubic function of the x-values with some added noise. The data is plotted as\u00a0follows: IMAGE SOURCE: BY\u00a0AUTHOR Next, we can fit a polynomial regression model to the\u00a0data: from sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import PolynomialFeatures # Create polynomial featurespoly = PolynomialFeatures(degree=10)x_poly = poly.fit_transform(x.reshape(-1, 1)) # Fit model to datamodel = LinearRegression()model.fit(x_poly, y) # Predict on original x valuesy_pred = model.predict(x_poly) # Create figure and plot datafig, ax = plt.subplots(figsize=(10, 6))ax.plot(x, y, &#039;o&#039;, label=&#039;True data&#039;)ax.plot(x, y_pred, &#039;o&#039;, label=&#039;Predicted data&#039;)ax.legend() # Display figureplt.show() This fits a polynomial regression model with a degree of 10 to the data, which is a very high degree and results in a very complex model. The model has plotted along with the original data as\u00a0follows: IMAGE SOURCE: BY\u00a0AUTHOR CONCLUSION: We can see that the model is able to fit the training data very well but is not able to generalize well to new, unseen data. This is an example of overfitting, as the model has learned the specific patterns in the training data but has not developed a general understanding of the underlying relationships in the\u00a0data. To prevent overfitting, we can use techniques such as regularisation or early stopping to reduce the complexity of the model and improve its generalization performance. UNDERFITTING: Under-fitting, on the other hand, occurs when a model is too simple and is unable to capture the underlying patterns in the data. This can also lead to poor performance, but for a different reason. LET\u2019S UNDERSTAND IT WITH MORE BETTER\u00a0ANALOGY: One way to understand underfitting is through the analogy of a student preparing for a test. Imagine that a student is studying for a history test and is given a textbook to read. However, the student only reads the first few pages of the textbook and does not fully grasp the material. On the day of the test, the student is unable to answer even the most basic questions because they have not learned enough about the\u00a0subject. This is similar to how a model can underfit the training data. The model is too simple and is unable to capture the underlying patterns in the data, leading to poor performance on the training set and poor generalization to new, unseen\u00a0data. On the other hand, if the student had taken the time to read and fully understand the entire textbook, they would have been better equipped to perform well on the test. Similarly, a model that has not underfitted to the training data and has a good generalization performance will be able to make accurate predictions on the training data and new, unseen\u00a0data. Here is a simple code example that demonstrates underfitting using a linear regression model: import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegression # Generate synthetic datax = np.linspace(-5, 5, 50)y = x**2 + np.random.normal(0, 10, 50) # Fit linear modelmodel = LinearRegression()model.fit(x.reshape(-1, 1), y) # Predict on training datay_pred = model.predict(x.reshape(-1, 1)) # Create figure with specified sizeplt.figure(figsize=(10, 6)) # Plot data and predictionsplt.plot(x, y, &#039;o&#039;, label=&#039;True data&#039;)plt.plot(x, y_pred, &#039;o&#039;, label=&#039;Predicted data&#039;)plt.legend() # Save figure to fileplt.savefig(&#039;figure.png&#039;, bbox_inches=&#039;tight&#039;) # Show plotplt.show() IMAGE SOURCE: BY\u00a0AUTHOR CONCLUSION\u00a0: In this example, we generate synthetic data that follows a quadratic trend and fit a linear model to the data. We can see that the linear model is unable to capture the underlying quadratic trend and is under-fitting to the data, resulting in poor performance on the training\u00a0set. \u201cFeeling lost when it comes to the connection between bias, variance, under-fitting, and overfitting? Don\u2019t worry, I\u2019ve got the answers you\u00a0need!\u201d BIAS, VARIANCE, AND ITS CONNECTION TO OVERFITTING AND UNDERFITTING: SOURCE: https://giphy.com/ BIAS and VARIANCE are two important concepts that describe the error of a machine learning model. Bias refers to the error due to incorrect assumptions in the model, while variance refers to the error due to the complexity of the\u00a0model. In general, we want to find a model that has low bias and low variance, as this will result in good generalization performance [&#8230;]",
            "pubdate": "Sat, 07 Jan 2023 00:18:41 +0000",
            "pubdate_parsed": [
                2023,
                1,
                7
            ],
            "email_sent": true
        },
        "5 Growing Libraries in Python for Causality Analysis": {
            "url": "https://towardsai.net/p/l/5-growing-libraries-in-python-for-causality-analysis",
            "description": "Last Updated on January 7, 2023 by Editorial Team Author(s): Dr. Alessandro Crimi Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. not only for neuroimaging Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 07 Jan 2023 12:15:58 +0000",
            "pubdate_parsed": [
                2023,
                1,
                7
            ],
            "email_sent": true
        },
        "Effective Categorical Variable Encoding for Machine Learning": {
            "url": "https://towardsai.net/p/l/effective-categorical-variable-encoding-for-machine-learning",
            "description": "Last Updated on January 7, 2023 by Editorial Team Author(s): Filipe Filardi Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Image by DCStudio on\u00a0Freepik A categorical variable is a common type of data found in many machine learning datasets. Effective handling of categorical variables can be crucial for building successful models since it contains rich information that can be used to predict outcomes in Machine Learning. However, working with categorical variables can be challenging, as many models are designed to handle numerical data. As a result, some people may need clarification about correctly processing categorical data, leading to confusion and potentially suboptimal model performance. This article aims to provide a clear and comprehensive overview of the most popular approaches to handling categorical data in Machine Learning. By understanding the different options available and their implications, I hope to provide readers the knowledge and tools they need to handle categorical data in their Machine Learning projects. Categorical Data in Machine\u00a0Learning Categorical data consists of data that can be classified into categories. In machine learning, it is common to encounter categorical data from variables such as gender, race, nationality, genre, or occupation. Categorical data is often present in real-world datasets, and it is vital to handle it properly. One of the main challenges of working with categorical data is that most machine learning algorithms are designed to work with numerical data. This means that categorical data must be transformed into a numerical format to be used as input to the\u00a0model. Dealing with categorical data This section will explore some popular methods for dealing with categorical data in machine learning. What is \u201cReplacing for Numbers\u201d? Replacing for numbers refers to the process of replacing a categorical variable with a numerical value. For example, continuing with the example above, if we replaced the categorical variable with numerical values, we would get the following: Example of Replacing &#124; Image by\u00a0Author Here\u2019s the python code using replace in a Pandas data frame as a reference: df.replace({&#039;rock&#039;: 0, &#039;jazz&#039;: 1, &#039;blues&#039;: 2}) What is a \u201cLabel Encoder\u201d? Label Encoder is another method for encoding categorical variables. It assigns a unique numerical value to each category in the categorical variable. Using Label Encoder on the previous example would result in the same values as replacing. While replace might be a suitable approach for a small number of categories, it can become impractical when dealing with many categories. Example of Label Encoder &#124; Image by\u00a0Author Here\u2019s the Python code using the Label\u00a0Encoder: from sklearn import preprocessingle = preprocessing.LabelEncoder()le.fit(df[&#039;genres&#039;])df[&#039;genres&#039;] = le.transform(df[&#039;genres&#039;]) What is converting to a \u201cdummy variable\u201c? It is the process of creating a new binary column for each category in a categorical variable, with a 0 or 1 indicating the presence or absence of that category, such\u00a0as: Example of Dummy &#124; Image by\u00a0Author There are two ways of doing that. The first is using get_dummies() of Pandas\u00a0library: import pandas as pdX_encoded = pd.get_dummies(df, columns=[&#039;genres&#039;]) The other is using OneHotEncoder() of Scikit-learn (sklearn): from sklearn.preprocessing import OneHotEncoderenc = OneHotEncoder()enc.fit(df)X_encoded = enc.transform(df).toarray() Dummifying and One Hot Encoding are essentially the same things. The main difference is that \u201cdummify\u201d is a more colloquial term, and \u201cOne Hot encoding\u201d is the technical term used in the machine learning literature. Why are Dummies Preferred Over the other solutions? There are several reasons why Dummies are generally preferred over other encoding\u00a0methods: Avoiding implied ordinal relationships and preventing bias Dummies create separate columns for each category, allowing the model to learn the relationships between the individual categories and the target variable. Replacing for numbers and label encoder, on the other hand, imply an ordinal relationship between the categories and does not create separate columns for each category, which can lead to misleading results if the categories do not have an inherent\u00a0order. For example, suppose you replace \u201crock\u201d with 1, \u201cjazz\u201d with 2, and \u201cblues\u201d with 3 in your dataset. In that case, your model may assume that \u201cjazz\u201d is twice as important as \u201crock\u201d and \u201cblues\u201d is three times as important as \u201crock\u201d. This can introduce bias into the model, as it makes assumptions about the order in which you assign the\u00a0numbers. Dummies allow the model to learn more complex relationships Because it creates separate columns for each category, the model can learn more complex relationships between the categories and the target variable. On the other hand, the other mentioned encoders only allow the model to learn the overall relationship between the numerical value and the target variable, which may not capture the full complexity of the\u00a0data. When to Avoid\u00a0Dummies There are certain situations in which Dummies may not be the best approach. Here are the most important ones: High cardinality: One Hot Encoding creates a separate column for each category in the categorical variable. This can lead to a high number of columns, especially if the categorical variable has many unique values. In such cases, One Hot Encoding may result in a sparse and unwieldy data set, which can be challenging to work\u00a0with. Memory constraints: One Hot Encoding can also be problematic if the data set is large and requires a lot of memory to store. The resulting data set can take up a lot of space, which may not be feasible if memory is\u00a0limited. Multicollinearity: Occurs when there is a high correlation between the dummy variables, which can cause the coefficients in the model to be unstable and difficult to interpret. Dummy variables are naturally correlated because they are created from the same categorical variable. In these situations, alternative encoding methods, such as label encoder or target encoding, may be more appropriate, which can handle high cardinality more efficiently. If you are interested in learning more about multicollinearity and target encoding, there are many other resources available. [&#8230;]",
            "pubdate": "Sun, 08 Jan 2023 00:13:46 +0000",
            "pubdate_parsed": [
                2023,
                1,
                8
            ],
            "email_sent": true
        },
        "Exposing the Racial Divide in Data Science: The Reality of Discrimination and How to Overcome It": {
            "url": "https://towardsai.net/p/l/exposing-the-racial-divide-in-data-science-the-reality-of-discrimination-and-how-to-overcome-it",
            "description": "Last Updated on January 9, 2023 by Editorial Team Author(s): Andrew Austin Originally published on Towards AI the World&#8217;s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses. Racism in Data Science: A Call to Address the Inequalities within the Field Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. It\u2019s free, we don\u2019t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 10 Jan 2023 00:13:54 +0000",
            "pubdate_parsed": [
                2023,
                1,
                10
            ],
            "email_sent": true
        },
        "Fully Understand Convolutional Neural Networks Components": {
            "url": "https://towardsai.net/p/l/fully-understand-convolutional-neural-networks-components",
            "description": "Last Updated on January 10, 2023 by Editorial Team Author(s): Amit Chauhan Originally published on Towards AI. Terms and technology in the artificial intelligence world Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 10 Jan 2023 12:12:14 +0000",
            "pubdate_parsed": [
                2023,
                1,
                10
            ],
            "email_sent": true
        },
        "Ultimate MLOps Learning Roadmap with Free Learning Resources In 2023": {
            "url": "https://towardsai.net/p/l/ultimate-mlops-learning-roadmap-with-free-learning-resources-in-2023",
            "description": "Last Updated on January 11, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI. In today&#x2019;s hype of Machine learning where many organizations have integrated or are trying to integrate ML systems into their products and&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 11 Jan 2023 12:11:05 +0000",
            "pubdate_parsed": [
                2023,
                1,
                11
            ],
            "email_sent": true
        },
        "Machine Learning for Documents": {
            "url": "https://towardsai.net/p/l/machine-learning-for-documents",
            "description": "Last Updated on January 11, 2023 by Editorial Team Author(s): Sean Benhur Originally published on Towards AI. Photo by Romain Dancre on\u00a0Unsplash Documents carry the essential source of vital information. Much of the structured and unstructured information of the enterprises is available as Documents. These are available in the form of native PDF documents and scanned PDF documents such as Bank Invoices, Legal documents, and verification ID cards, over the time information on these documents is used for many applications using techniques such as Optical Character Recognition(OCR), Computer Vision(CV) and Natural Language Processing(NLP) Document AI refers to the Artificial Intelligence techniques that are applied to analyze and understand documents for various tasks. Notable tasks include Form/Invoice extraction, Optical Character Recognition, Table Detection, and Table Extraction. In this article, we will look\u00a0on The major tasks and datasets that are prevalent in Document\u00a0AI. The Methodologies such as recent research papers, pretrained models, and existing techniques for each task are discussed. Current issues in this\u00a0domain. Tasks and\u00a0Datasets Different types of tasks are prevalent in Document AI to solve many business use cases. In many cases, some of the tasks are used together to solve one use case. For example, for an invoice extraction task, it is common to use an OCR system to extract the text from the pdf and a Visual Information Extraction system to recognize the entities. In this section, we will look over each task and the common dataset that is used for that\u00a0task. Optical Character Recognition Optical Character Recognition(OCR) refers to the texts in which we recognize and extract the text. It is an important task in the Document AI pipeline. OCR is also one of the hardest tasks since the text could be in different formats and quality of the scanned document can be low and the handwriting of the text can be in poor formats. There are many benchmarks and datasets available for this task; the famous dataset MNIST is a type of OCR dataset. Other benchmarks include IAM Handwriting which consists of handwritten document images, and ICDAR 2003, consisting of images of scene understanding. Document Layout\u00a0Analysis This task refers to identifying the structure and layout of the document, such as the paragraphs, tables, and charts identified. ICDAR 2013 is one of the popular benchmarks for this task which includes text images of word-level annotations; another dataset is PubLayNet which consists of document images annotated on the structure level, such as text, table, figure, and other similar categories. Visual Information Extraction This refers to the task of extracting key information from the documents. In this task, only key entities are extracted, unlike OCR, where the entire text is extracted but here, only the text of the key entities and the spatial information of the same. Invoice extraction, Form extraction are some of the Visual Information Extraction tasks. Benchmarks include FUNSD, which consists of annotated forms with information on semantic entities, Named Entities, and Spatial Information. CORD is another benchmark that consists of images of receipts annotated on each text region with spatial-level information. Image of an annotated receipt indicating the coordinates of the text in JSON format [Source]. Document Visual Question Answering This task refers to answering questions based on the text provided in the document. This task is different from the other Visual Question Answering tasks due to the complex nature of the document images. Usually, the text is extracted first with the OCR model, and then the modeling is done. DocVQA is the first dataset that introduced this task; it has two subtasks in which the first one contains a single document image and a question and the second one consists of a collection of document images and a single question. Example Image from the DocVQA\u00a0[Source] Document Image Classification In this task, the images of the documents are classified into the type of documents such as invoices, legal documents, resumes, and many others.RVL-CLIP is a popular benchmark used for this task, it consists of images of sixteen categories, such as memos, emails, scientific reports, and file\u00a0folders. Table Detection and Table Extraction Tables are an important source of information in any document, it mainly consists of numerical information. In this task, we focus on recognizing where the table is located on the document and extracting the information inside it. This task also has some subtasks, such as Table structure recognition, where the rows, the columns, and the cells of the table are identified, and another subtask, Table Functional Analysis, in which the key value is extracted. PubTables-1M is a recently released dataset that consists of 948K annotated PDFs for the tasks of Table Detection, Table Structure Recognition, and Table Functional Analysis. Different Tasks in Table Detection [Source] Methodologies The images of the documents are different from the normal images as it contains some tables, numerical information, and text. The location of these texts is also needed for some of the tasks mentioned above. Before the advent of deep learning, most of the above tasks were solved through rule-based systems and heuristics with several Image processing algorithms and OCR techniques. In this section, we will go over an overview of some methods to solve these tasks as well as the recent research breakthroughs in this\u00a0area. Deep Learning based techniques After the advent of Deep Learning and the rise of CNNs, many computer vision methods have been used for these tasks. Tasks such as Document Layout Analysis and Table Detection tasks are entirely treated as Object Detection tasks where object detection models such as RCNN, Faster-RCNN, and YOLO are\u00a0used. For Document Image Classification, the common approaches that are used for Natural Image Classification can be used. Some approaches, such as Dauphnee et al., used textual and visual content to classify the documents. Tasks on which the text is also an important source of information such as Visual Document Extraction and Document Visual Question Answering. The baseline approach is to use a pipeline consisting of an Object Detection model for locating the word labels, a NER model for [&#8230;]",
            "pubdate": "Thu, 12 Jan 2023 00:13:25 +0000",
            "pubdate_parsed": [
                2023,
                1,
                12
            ],
            "email_sent": true
        },
        "Can AI Models be Common Sense Enabled?": {
            "url": "https://towardsai.net/p/l/can-ai-models-be-common-sense-enabled",
            "description": "Last Updated on January 12, 2023 by Editorial Team Author(s): Poornachandra Sarang Originally published on Towards AI. Building machines with human-level intelligence Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 13 Jan 2023 00:14:03 +0000",
            "pubdate_parsed": [
                2023,
                1,
                13
            ],
            "email_sent": true
        },
        "Demystifying ChatGPT!": {
            "url": "https://towardsai.net/p/l/demystifying-chatgpt",
            "description": "Last Updated on January 13, 2023 by Editorial Team Author(s): Michele De Filippo, PhD Originally published on Towards AI. The Revolution of Conversational AI has just started! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 14 Jan 2023 00:13:45 +0000",
            "pubdate_parsed": [
                2023,
                1,
                14
            ],
            "email_sent": true
        },
        "Feature selection for unsupervised problems: the case of clustering": {
            "url": "https://towardsai.net/p/l/feature-selection-for-unsupervised-problems-the-case-of-clustering",
            "description": "Last Updated on January 14, 2023 by Editorial Team Author(s): Kevin Berlemont, PhD Originally published on Towards AI. Feature Selection for Unsupervised Problems: The Case of Clustering Photo by NASA on\u00a0Unsplash With the massive growth of data over the last decade, selecting the right feature is becoming a major challenge. A well-known technique in data processing consists of dimensionality reduction. This process tries to remove redundant and irrelevant features that would degrade the performance. These methods can be categorized between feature extraction/construction and feature selection. In the case of feature extraction, the dimensionality of the data is reduced by deriving new features based on the original ones. Examples of this process are Principal Component Analysis [1] and Singular Value Decomposition [2]. On the other hand, feature selection tries to select a subset, ideally small, of relevant features. This approach is needed when there is a large number of features in the dataset and the goal is to reduce the computational complexity and obtain generalizable models. Feature selection approaches usually require class labels to determine whether a feature is relevant or not. However, when the class labels are unknown, such as for clustering, how can a feature be classified as relevant? Feature selection can be categorized in four categories: Filters methods try to select an optimal feature subset according to the general characteristics of the data but not from a learning algorithm. In general, filters compute the score of a subset of features using specific evaluation criteria. Wrappers methods need a learner to evaluate the goodness of the feature subsets. Thus, they are computationally more expensive but will increase the performance of a specific learning algorithm. Hybrid methods try to get the advantages of both methods above by incorporating them in a two-stage process. Embedded methods embed features directly into the learning algorithm. However, they often don\u2019t reach better performances than wrappers. Next, I will describe specific feature selection methods for all of these different categories, highlighting when and how to use\u00a0them. Filter Approaches Filters select features in the data according to the characteristics of features. They directly evaluate the statistical performance of features in the data. A proposed filter approach [3] is to measure the dependencies between features based on a variance-based metric (maximal information compression index, MICI). This approach divides features into clusters in a similar way to the k-nearest neighbor algorithm. In each iteration, the k nearest features are found for each feature based on the MICI. Afterward, the feature which builds the most compact subset is selected, and the procedure is repeated until all features are selected or discarded. Another filtering method consists in selecting features using the Pearson correlation coefficient. First, all possible pairwise correlations between features and data are computed. Then, it removes the feature with the highest average dependency on other features. Afterward, the process is repeated until the number of features wanted is\u00a0reached. As shown with these two examples, filter methods are usually general as they don\u2019t rely on a specific learning algorithm. However, their clustering performances are usually lower than the ones of wrapper methods, which will be the focus of the next\u00a0section. Wrappers Approach for\u00a0K-means In this section, I will focus on the K-means algorithm for clustering, as the wrapper approaches are specific to the algorithm picked. For more details about other models, such as evolutionary algorithms, I recommend the following paper\u00a0[4]. K-means is one of the most popular clustering algorithms in Data Science, but one of its main deficiencies is that it evaluates all the features with equal importance. Thus, in the case of a significant number of irrelevant features, the quality of the clustering process will decrease. In this context, it is useful to give certain features more importance by weighting them. The convex K-means algorithm [5] improves the standard K-means algorithm by integrating an adaptive weighting scheme in K-means. It attempts to iteratively determine the optimal weights of a feature set by minimizing the average within-cluster distance. One caveat to this approach is that the minima search can be stuck in a local optimum due to gradient descent\u00a0search. Another well-known feature weighting approach for K-means consists in attribute weighting clustering. Each feature can have different weights at different clusters. The goal is then to minimize the sum of the weighted distances within the clusters. This method and variants have been really successful at clustering, but they are highly dependent on the hyperparameter keeping the weights at a reasonable level. Embedded Approaches For embedded approaches, the feature selection process is performed as a part of the learning process. Due to their performances and interpretability, embedded approaches usually make use of a sparse learning algorithm. First, it finds the cluster labels using a clustering algorithm, and it then transforms the unsupervised feature selection into a supervised context. One of the earliest sparse learning feature selection methods is multi-cluster feature selection. In the first step, the intrinsic structure of the data is explored using spectral analysis in order to measure the correlation between features. In the second step, the importance of the features is quantified using an L1-regularized regression model. The last step consists in selecting the specified number of features with the highest coefficients from the previous stage. This approach has been proven efficient at feature selection for clustering but is computationally expensive. The previous method consists in the conventional sparse learning feature selection approach that requires the cluster labels to be generated by a clustering algorithm before transforming the problem into a supervised feature selection problem. However, this approach has the tendency to cause non-optimal feature subsets. To address this, embedded unsupervised feature selection directly embeds feature selection into a clustering algorithm without the transformation. It applies K-means by minimizing the reconstruction error to obtain the cluster labels and select the features. However, it is necessary to be careful about the heterogeneity between clusters with this approach as it has the tendency to select non-discriminative features otherwise. Hybrid Approaches In recent years, hybrid approaches for feature selections have [&#8230;]",
            "pubdate": "Sat, 14 Jan 2023 12:15:25 +0000",
            "pubdate_parsed": [
                2023,
                1,
                14
            ],
            "email_sent": true
        },
        "How To Build A Data Science Portfolio That Will Land You A Job?": {
            "url": "https://towardsai.net/p/l/how-to-build-a-data-science-portfolio-that-will-land-you-a-job",
            "description": "Last Updated on January 15, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI. &#x650;A Step by Step Practical Guide to Building A Data Science Portfolio Projects Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 15 Jan 2023 12:10:50 +0000",
            "pubdate_parsed": [
                2023,
                1,
                15
            ],
            "email_sent": true
        },
        "How to Track ML Experiments With DVC Inside VSCode To Boost Your Productivity": {
            "url": "https://towardsai.net/p/l/how-to-track-ml-experiments-with-dvc-inside-vscode-to-boost-your-productivity",
            "description": "Last Updated on January 17, 2023 by Editorial Team Author(s): BEXGBoost Originally published on Towards AI. Manage ML experiments like a pro Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Tue, 17 Jan 2023 12:05:01 +0000",
            "pubdate_parsed": [
                2023,
                1,
                17
            ],
            "email_sent": true
        },
        "Learn Reinforcement Learning from Top Universities": {
            "url": "https://towardsai.net/p/l/learn-reinforcement-learning-from-top-universities",
            "description": "Last Updated on January 17, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI. Reinforcement learning (RL) is a rapidly growing field that is revolutionizing the way machines learn to make decisions. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 18 Jan 2023 00:08:13 +0000",
            "pubdate_parsed": [
                2023,
                1,
                18
            ],
            "email_sent": true
        },
        "Why Having the Right Strategy of MLOps is Important?": {
            "url": "https://towardsai.net/p/l/why-having-the-right-strategy-of-mlops-is-important",
            "description": "Last Updated on January 18, 2023 by Editorial Team Author(s): Sumit Singh Originally published on Towards AI. Photo by Kaleidico on\u00a0Unsplash The last decades have seen incredible advancement in the field of AI and machine learning. What previously has been a research topic and was accessible to only a handful of scientists and researchers is now accessible to entry-level engineers and students. But as technology grew and the industry-wide applications started rolling out, so did the complexities and challenges. Building an ML model to do basic stuff like identifying a few household objects or human activities to say 70% confidence is a common thing, but to achieve 99% accuracy in identifying cancers or tumors in a medical image is a different thing. What it takes to build a production-level AI To solve any industrial use case, ML teams have to manage a scale, which means a high volume of data to be processed, the right algorithm selection, and a vast amount of iterations. It involves a significantly bigger team with different expertise, and to manage the overall process, one needs to implement the right process and strategy. That process is called\u00a0MLOps. What are the MLOps components? Every team has its unique set of requirements, and based on their goals, the implementation might differ, but components of MLOps will largely remain the\u00a0same. MLOps can range from data pipeline to model output in some cases, while other projects may only require MLOps execution of the model deployment process. The majority of businesses use MLOps principles in the following areas: Exploratory data analysis\u00a0(EDA) Data preparation and feature development Model development and refinement Model evaluation and governance Serving and model inference Model monitoring Model retraining that is automated Let\u2019s understand each component one by\u00a0one- 1. Exploratory Data Analysis (EDA): EDA is a process to analyze and summarize a data set, typically with the goal of finding patterns or relationships between variables. It involves using visualizations, statistical techniques, and other methods to explore the data in order to gain insight and better understand the data\u00a0set. 2. Data preparation: It is the process of cleaning, transforming, and organizing raw data so that it can be used for analysis, modeling, and other\u00a0tasks. Let\u2019s say you have a data set of customer\u00a0records. You might need to preprocess the data by normalizing the values, removing invalid or missing data, converting categorical data into numerical values, and scaling the data so that the values fall within a specific\u00a0range. Once the data is preprocessed, it can then be used for machine learning or other data analysis\u00a0tasks. 3. Feature development: is the process of creating new variables from existing data, either through combining existing variables or by extracting new information from existing\u00a0data. To understand this, let\u2019s say you have a data set containing customer records that include information about their age, gender, location, and purchase\u00a0history. You can create new features from this data by combining existing variables, such as creating a \u201ctotal purchases\u201d feature by adding up all the customers\u2019 purchase histories, or by extracting new information from existing data, such as creating a \u201clocation density\u201d feature by counting the number of customers in a given location. These new features can then be used for machine learning or other data analysis\u00a0tasks. 4. Model development and refinement: is the process of creating, testing, and refining a mathematical model to fit observed\u00a0data. This process typically involves selecting the appropriate model type, specifying the parameters and variables, and fitting the model to the observed\u00a0data. Once the model has been created, it can be used to make predictions and to explain relationships between variables. One example of this would be a machine learning model trained on a data set to predict customer\u00a0churn. The model would be tested on a validation data set, and any errors or inconsistencies in the model\u2019s performance would be identified and addressed by refining the model parameters or introducing new features to improve the model\u2019s accuracy. Once the model has been refined and is performing appropriately, it can be used for predictive analytics and other applications. 5. Model evaluation: It involves assessing the accuracy, reliability, and validity of the model, while model governance involves establishing policies and procedures to ensure that the model is used in an ethical and responsible manner. Model evaluation and governance are important in ensuring that models are used responsibly and produce reliable\u00a0results. An example of model evaluation and governance is the use of a risk assessment process to evaluate the accuracy and reliability of a model before it is deployed for use in decision-making. This process involves assessing the data used to train the model, assessing the performance of the model on a validation data set, and assessing whether the model is ethically sound and is being used in a responsible manner. If any issues are identified, the model can be revised or adjusted to improve accuracy and reliability and to ensure that it is being used responsibly. 6. Serving and model inference: is the process of deploying a trained machine learning model and using it to make predictions and inferences about\u00a0data. This process typically involves deploying the model to a server, where it can be used to make predictions and inferences on new\u00a0data. Additionally, the process may involve collecting feedback from users to ensure that the model is providing accurate and useful predictions and inferences. Let\u2019s understand this with an example using a trained deep-learning model to classify\u00a0images. The model would be deployed to a server, where it could be used to make predictions on new\u00a0images. In order to ensure that the model\u2019s performance is accurate, the model would be evaluated on a validation data\u00a0set. The model would also be monitored to ensure that it is performing correctly and efficiently, and feedback from users would be collected to ensure that the model is providing accurate and useful predictions. 7. Model monitoring: is the process of tracking the performance of a machine learning model over\u00a0time. It involves tracking metrics such as accuracy, precision, recall, and false [&#8230;]",
            "pubdate": "Wed, 18 Jan 2023 12:05:40 +0000",
            "pubdate_parsed": [
                2023,
                1,
                18
            ],
            "email_sent": true
        },
        "Create Your Own YouTube Video Summarizer App in Just 3 Easy Steps": {
            "url": "https://towardsai.net/p/l/create-your-own-youtube-video-summarizer-app-in-just-3-easy-steps",
            "description": "Last Updated on January 18, 2023 by Editorial Team Author(s): Asish Biswas Originally published on Towards AI. Build GPT-3 powered video summarizer to identify the important glimpse of your favorite video. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 19 Jan 2023 00:06:29 +0000",
            "pubdate_parsed": [
                2023,
                1,
                19
            ],
            "email_sent": true
        },
        "LLMs Encode Clinical Knowledge:A Review": {
            "url": "https://towardsai.net/p/l/llms-encode-clinical-knowledge-a-review",
            "description": "Last Updated on January 19, 2023 by Editorial Team Author(s): Ronny Polle Originally published on Towards AI. LLMs Encode Clinical Knowledge: A Quick\u00a0Review Outline Introduction Contributions Limitations Conclusion References Introduction In the field of medicine, language is an enabler of key interactions for and between clinicians, researchers, and patients. This provides opportunities for leveraging LLMs for modeling properties of textual data in the medical\u00a0domain. There is evidence that LLMs can act as implicit knowledge bases. The weights of these networks store information, resulting in information that is pliable and hence can be operated upon in a representation space. This phenomenon equips LLMs with an ability to form associations between stored information to produce meaningful insights. The unfortunate news is that this associative capability can lead to hallucinations as information stored in weights is unreliable. Hence, current AI models for applications to medicine and healthcare lack the ability to address significant gaps in effectively leveraging language as a tool for mediating real world clinical workflows. For instance, LLMs are found to have the potential to mirror misinformation,bias, and stereotypes in the\u00a0corpus. With the profound advances made by LLMs, AI systems are undergoing innovative repurposing and helping address limitations posed by predominantly single-task AI\u00a0systems. Contribution Key contributions made in this study can be summarised across the following three\u00a0axes; Dataset benchmark Framework for human evaluation Modeling Firstly, this paper introduces a dataset benchmark for medical question answering called MultiMedQA. This benchmark is a collection of six open-question-answering datasets\u200a\u2014\u200aMedQA [jin2021disease], MedMCQA [pal2022medmcqa], PubMedQA [jin2019pubmedqa], LiveQA [abacha2017overview], MedicationQA [abacha2019bridging], and MMLU clinical topics [hendrycks2020measuring]). MedQA mirrors the US Medical License Exam (USMLE) style of questions. Furthermore, this benchmark is augmented by HealthSearchQA, the seventh dataset comprising curated commonly searched consumer health queries in\u00a0English. Secondly, a robust human-centric evaluation framework is proposed to address some of the current limitations with automated metrics for assessing long-form answer generation, such as the bilingual evaluation understanding metric (BLEU). Clinicians and lay users (non-experts) are captured in the evaluation of the models\u2019 generative output. The clinician\u2019s evaluation is run along twelve different evaluation axes whilst lay users are evaluated along two unique axes. These include\u200a\u2014\u200ahow well the models\u2019 output agrees with scientific consensus; the possibility and the likelihood of harm; evidence of comprehension, reasoning, and retrieval ability; the presence of inappropriate, incorrect, or missing content; the possibility of bias in the answer; answer captures user intent and helpfulness of the\u00a0answer. Thirdly, this paper highlighted how well LLMs encode clinical knowledge with major architectural modifications. The authors built on the Pathway Language Model (PaLM) and Flan-PaLM family of LLMs. PaLM is a transformer model architecture composed of a decoder-only setup, with key features such as\u200a\u2014\u200aSwiGLU activation function to substitute for standard activation functions(ReLU, Swish, GeLU), parallel transformer layers, multi-query attention mechanism, rotary position embeddings (ROPE) as a substitute for absolute or relative position embeddings, shared-input output embeddings, zero bias kernels, and layer normalizations, and use of a SentencePiece vocabulary. Leveraging the PaLM baseline and instruction prompt tuning paradigms, the authors demonstrated the Flan-PaLM variant to gain superior performance across a suite of evaluation tasks over the baseline. Comparison with prior\u00a0SOTA Moreover, given the key limitations of domain adaptation methods and end-to-end finetuning of the model using copious amounts of in-domain data, the authors successfully investigated prompting and prompt-tuning to aid Flan-PaLM in adapting to the medical domain. The instruction prompt tuning technique designed in this study incorporates soft-prompt learned by prompt-tuning as an initial prefix that is shared across multiple medical datasets, followed by a task-specific human-engineered prompt alongside the original question and/or\u00a0context. Instruction prompt tuning for\u00a0Med-PaLM Limitations Although promising, the dataset benchmark fails to cover multiple languages and excludes a larger variety of medical and scientific domains, hence partially reflecting real-world clinical workflows. Secondly, although Flan-PaLM was able to reach state-of-the-art performance on several medical questions and answering benchmarks, there are important gaps to be bridged in order for it to reach an expert clinician level on many clinically important axes. Important future directions proposed to help address this challenge include\u200a\u2014\u200aestablishing solid grounds for the models\u2019 responses in authoritative medical sources and accounting for the time-varying nature of medical consensus, the ability to effectively quantify and communicate uncertainty to a generic user-in-the-loop, and multilingual support for responses. Secondly, it is critical to conduct exhaustive work to improve the human evaluation framework. The pilot rating framework is not very exhaustive as it fails to capture important variations across diverse population groups. Also, the pool of clinicians and lay-users assessing the model responses is limited. Lastly, the evaluation framework failed to investigate the impact of variation in the clinician rater\u2019s medical specialty, demographics, and geography. Moreover, fairness and equity are underexplored in this study, especially the lacking understanding of how perturbations to demographic identifiers in prompts influence the model outputs. Also, the safety-critical and complex requirements of the medical domain pose an important question as to how the approach of sampling clinicians to participate in identifying the best-demonstration prompts examples and crafting few-shot prompts impacts the overall behavior of\u00a0LLMs. In conclusion, I am fascinated by the performance of LLMs demonstrated in this rigorous study. Not only does it exemplify a successful application and evaluation of LLMs in the medical context, but also, it demonstrates exciting directions for future research and improvements. Overview of Contributions Thank you for reading\u00a0\ud83d\ude42 References [1] Large Language Models (LLMs) encode clinical knowledge [2] Galactica: A Large Language Model for\u00a0Science [3] PaLM\u00a0: Scaling Language Modeling with\u00a0Pathways LLMs Encode Clinical Knowledge\u00a0:A Review was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 19 Jan 2023 13:19:40 +0000",
            "pubdate_parsed": [
                2023,
                1,
                19
            ],
            "email_sent": true
        },
        "Soccer and Data Science: Decision Tree explained by Ibra and Muriqi": {
            "url": "https://towardsai.net/p/l/soccer-and-data-science-decision-tree-explained-by-ibra-and-muriqi",
            "description": "Last Updated on January 19, 2023 by Editorial Team Author(s): Andrea Ianni Originally published on Towards AI. The Decision Tree explained by our Pirate Vedat, Felipe and Ibrahimovic. Machine Learning and Artificial Intelligence playing in Serie A..! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 19 Jan 2023 13:19:36 +0000",
            "pubdate_parsed": [
                2023,
                1,
                19
            ],
            "email_sent": true
        },
        "How To Create Highly-Organized ML Projects Anyone Can Reproduce With DVC Pipelines": {
            "url": "https://towardsai.net/p/l/how-to-create-highly-organized-ml-projects-anyone-can-reproduce-with-dvc-pipelines",
            "description": "Last Updated on January 19, 2023 by Editorial Team Author(s): BEXGBoost Originally published on Towards AI. What is a machine learning pipeline? Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 19 Jan 2023 12:24:33 +0000",
            "pubdate_parsed": [
                2023,
                1,
                19
            ],
            "email_sent": true
        },
        "Meta-Learning for Time Series Forecasting (DeepTime) in PyTorch Lightning": {
            "url": "https://towardsai.net/p/l/meta-learning-for-time-series-forecasting-deeptime-in-pytorch-lightning",
            "description": "Last Updated on January 19, 2023 by Editorial Team Author(s): Reza Yazdanfar Originally published on Towards AI. This article is devoted to describing a new type of deep learning model to cope with the usual problems in time series (covariate shift&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 20 Jan 2023 00:02:41 +0000",
            "pubdate_parsed": [
                2023,
                1,
                20
            ],
            "email_sent": true
        },
        "Simple But Effective Free Roadmap to Start A Career in Data Science & AI In 2023": {
            "url": "https://towardsai.net/p/l/simple-but-effective-free-roadmap-to-start-a-career-in-data-science-ai-in-2023",
            "description": "Last Updated on January 20, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI. Whether you&#x2019;re a recent graduate or a professional looking to make a career change, the field of Data Science and AI offers a wide range&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 21 Jan 2023 00:16:29 +0000",
            "pubdate_parsed": [
                2023,
                1,
                21
            ],
            "email_sent": true
        },
        "As a Data Scientist, You Should Know About a Clever Horse Called Hans": {
            "url": "https://towardsai.net/p/l/as-a-data-scientist-you-should-know-about-a-clever-horse-called-hans",
            "description": "Last Updated on January 22, 2023 by Editorial Team Author(s): Leon Eversberg Originally published on Towards AI. Learn about the Clever Hans effect in machine learning Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 23 Jan 2023 00:21:27 +0000",
            "pubdate_parsed": [
                2023,
                1,
                23
            ],
            "email_sent": true
        },
        "An Introduction to CNNs: Understanding the Basics": {
            "url": "https://towardsai.net/p/l/an-introduction-to-cnns-understanding-the-basics",
            "description": "Last Updated on January 25, 2023 by Editorial Team Author(s): Pranay Rishith Originally published on Towards AI. Exploring how powerful CNN is: from\u00a0basics Photo by Denys Nevozhai on\u00a0Unsplash Introduction CNN Source Convolutional neural networks are a deep learning concept that was specifically built for processing images. Machine learning is a concept where a computer learns from past experiences. Deep Learning is an advanced part of machine learning. CNN is designed to find visual patterns. When we humans see images, we see objects, colors, etc. We learn these things as we grow up, but computers can only understand 0\u2019s and 1\u2019s, i.e., binary values. Then how will computers see\u00a0images? Every image is made up of pixels. The below image is a good depiction of how a computer reads images. There are two types of images, Grayscale and Color. Grayscale(black and white) is made up of an array of values that range from 0 to 255(black to white). Color images have 3 arrays, red array, green array, and blue array(RGB). Also each of those arrays ranging from 0 to 255(black to corresponding colors). MIT deep learning lecture 3\u00a0source If a grayscale image has a size of 1080&#215;1080 then the total number of values is 1080x1080x1 whereas a color image has 1080x1080x3(3 as in\u00a0R+G+B). Architecture A convolutional neural network has 3 types of layers: convolution layer, pooling layers, and fully connected layers. Convolutional Layers The convolutional Layer is the layer where important features are extracted from input images. This layer uses a small square to extract features from the input image. This small square is called a kernel or filter. To explain, There is a mathematical operation in this layer between the input image and a filter in order to preserve and extract features. This is called Feature extraction in\u00a0CNN. Size of feature map:\u00a0n-f+1 n = size of\u00a0input f = size of\u00a0filter By author With different filters, different operations can be performed like edge detection, blur,\u00a0etc. MIT deep learning lecture 3\u00a0Source To perform a convolution operation, a filter should be specified as a certain size. The filter moves across the input image matrix and multiplies values with filter and summing. The result is smaller in size than the input image matrix size. To sum up, in CNN convolutional layer is the most important step or layer. This is used to extract important features from the input image matrix. A CNN can consist of any number of convolutional layers. Non-Linear Layer This layer is added after every convolutional layer to introduce non-linearity to the matrix. Non-linearity is introduced so that the output is not affected by the input or the output is not proportional to the input. This nonlinearity is done by activation functions. That topic is for another\u00a0article. Why do we need non-linearity in the neural network? might be a question. If the data doesn\u2019t have non-linearity, then the input is directly influencing the output, and it doesn\u2019t matter how many layers we use. The outcome will be the same. By increasing the power of non-linearity, the network is created to find more new and unique patterns in the\u00a0data. The commonly used activation functions are RELU, Tanh,\u00a0etc. Padding Now you have understood how important is the convolutional layer. A kernel or filter is used to extract important features. I mentioned that the convolutional layer could be used any number of times, and every time the size of the feature map decreases. We don\u2019t need that. Consider an input matrix of 5&#215;5 and a filter of size 3&#215;3. The size of the feature map is 5\u20133+1 = 3. If we add another layer, then the size is\u00a01. To make a feature map of size of same as the input matrix, we use padding. Let\u2019s reverse engineer. We need a feature map of size 5. The filter size is 3. from the above formula, n = 5+f-1 = 5+3\u20131 = 7. We need an input matrix of size 7 from a size 5 input matrix. We add padding, i.e., a row on top, bottom, and column on left and right, giving a matrix of size 7&#215;7. now the math, n-f+1 = 7\u20133+1 = 5. Hence\u00a0proved. Padding formula =\u00a0n+2p-f+1 p =\u00a0padding If p = 1, then one row and one column, so thats why we add 2p, so we get 2 rows and 2\u00a0columns. The above-added rows and columns are filled with zeros, called as zero\u00a0padding. by author This is how padding is\u00a0applied. Strides We talked about filters in the convolutional layer. Strides are defined as the number of pixels to move in any direction to apply the filter. If the stride is [1,1], then the filter moves 1 pixel at a time in either direction, and if it is [2,2] then the filter moves 2 pixels in either direction. This parameter is mainly useful when there is an input image with high resolution, then more pixels to filter. The larger the stride, the smaller the convolution features\u00a0map. by author A [1,1] looks like the\u00a0above. by author A [2,2] stride looks like the\u00a0above. To summarize, Strides is a value where the kernel or filter will move on the input\u00a0matrix. Pooling Layers If deducing the input image to 1/4 determines what the whole image depicts, then it is no good in processing the whole image. This is where pooling comes into\u00a0place. This is the layer where the large feature matrix is reduced by retaining features. This is called spatial spacing. Pooling also has a kernel and strides. There are different types of spatial\u00a0spacing. by author Max Pooling: This is where the largest element in the filter is selected. Min Pooling: This is where the least element in the filter is selected. Mean Pooling: This is the mean of all the elements in the\u00a0filter. Average Pooling: This is the average of all the elements in the\u00a0filter. This pooling layer is mainly used to connect Convolutional Layer and the Fully connected layer. The main reason the pooling layer is used after the convolutional layer because to reduce feature [&#8230;]",
            "pubdate": "Wed, 25 Jan 2023 13:22:18 +0000",
            "pubdate_parsed": [
                2023,
                1,
                25
            ],
            "email_sent": true
        },
        "MLflow: The Solution for Managing Complex Machine Learning Projects": {
            "url": "https://towardsai.net/p/l/mlflow-the-solution-for-managing-complex-machine-learning-projects",
            "description": "Last Updated on January 25, 2023 by Editorial Team Author(s): Himanshu Tripathi Originally published on Towards AI. MLflow is an open-source platform that streamlines machine learning development by managing the lifecycle of models, data sets&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 25 Jan 2023 13:22:16 +0000",
            "pubdate_parsed": [
                2023,
                1,
                25
            ],
            "email_sent": true
        },
        "This Is the Model That Will Power Googles Alternative To ChatGPT": {
            "url": "https://towardsai.net/p/l/this-is-the-model-that-will-power-googles-alternative-to-chatgpt",
            "description": "Last Updated on January 25, 2023 by Editorial Team Author(s): Jesus Rodriguez Originally published on Towards AI. LaMDA is positioned to power the next generation of large language models from Google. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 25 Jan 2023 13:03:01 +0000",
            "pubdate_parsed": [
                2023,
                1,
                25
            ],
            "email_sent": true
        },
        "Python: Top Programming Language for Data ScienceIntro and Implementation": {
            "url": "https://towardsai.net/p/l/python-top-programming-language-for-data-science%e2%80%8a-%e2%80%8aintro-and-implementation",
            "description": "Last Updated on January 27, 2023 by Editorial Team Author(s): Farzad Mahmoodinobar Originally published on Towards AI. Python: Language of Choice for Data Scientists Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 27 Jan 2023 13:19:02 +0000",
            "pubdate_parsed": [
                2023,
                1,
                27
            ],
            "email_sent": true
        },
        "ChatGPT on a Scientific Paper as a Co-author? Does It Even Make Sense?": {
            "url": "https://towardsai.net/p/l/chatgpt-on-a-scientific-paper-as-a-co-author-does-it-even-make-sense",
            "description": "Last Updated on January 27, 2023 by Editorial Team Author(s): The Tech Insider Originally published on Towards AI. Exploring the implications of using AI as a co-author in scientific research Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 27 Jan 2023 13:03:08 +0000",
            "pubdate_parsed": [
                2023,
                1,
                27
            ],
            "email_sent": true
        },
        "Police Fatalities Forecasting With Prophet": {
            "url": "https://towardsai.net/p/l/police-fatalities-forecasting-with-prophet",
            "description": "Last Updated on January 27, 2023 by Editorial Team Author(s): Andrea Ianni Originally published on Towards AI. At the end of the article there is a little surprise for you and&#x2026; no please, don&#x2019;t go directly there! Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 27 Jan 2023 13:03:05 +0000",
            "pubdate_parsed": [
                2023,
                1,
                27
            ],
            "email_sent": true
        },
        "How Exploratory Data Analysis Helped Me Solve Million-Dollar Business Problems": {
            "url": "https://towardsai.net/p/l/how-exploratory-data-analysis-helped-me-solve-million-dollar-business-problems",
            "description": "Last Updated on January 27, 2023 by Editorial Team Last Updated on January 27, 2023 by Editorial Team Author(s): Puneet Jindal Originally published on Towards AI. Photo by Luke Chesser on\u00a0Unsplash EDA is a powerful method to get insights from the data that can solve many unsolvable problems in business. In the increasingly competitive world, understanding the data and taking quicker actions based on that help create differentiation for the organization to stay\u00a0ahead! Before moving ahead, let me share the official definition mentioned on the\u00a0internet Exploratory Data Analysis (EDA) is a process of analyzing data sets in order to summarize their main characteristics [1][2], often using statistical or graphical techniques. It is used to discover trends [2], patterns, relationships, and anomalies in data, and can help inform the development of more complex models [3]. It can also be used to generate hypotheses and test them, identify important variables, detect outliers, and assess relationships among variables. EDA is an iterative process, and is used to uncover hidden insights and uncover relationships within the\u00a0data. Let me walk you through the definition of EDA in the form of a story. This story is very personal to me due to the fact that this moment added to my conviction to enter the data science field and explore it further to create a social impact in the\u00a0world. Disclaimer:- i would not name the organization to which this story belongs to avoid any sort of revealing any confidential information. When I interview many newbies, their story is usually about entering Data Science because it is termed the sexiest job of the 21st century. But I didn\u2019t about data science in a way on how it is known. My case was purely accidental and driven by curiosity. I started my journey as a software engineer around technologies such as web stack including python, javascript, and java stack. I got very passionate about building products where I could see the impact in front of me. To that end, I started picking up more responsibilities such as managing databases both SQL and\u00a0NoSQL. One day, I finished my assigned work, and I heard a senior business colleague of mine complaining about data accessibility to my dept head. He mentioned that his team was trying to download business reports. The majority of the downloads were failing, or downloads were very slow and this was impacting his team\u2019s efficiency and leading to job dissatisfaction every\u00a0day. Because of this, they asked my tech dept. head to add more high-end servers, and high-speed internet, and purchase high-memory laptops for his team to look at the reporting data as they were struggling to even the downloaded files in case it gets downloaded. This meant a huge IT investment ask. I volunteered to look at the issue and did the following steps. First, I got access to the data reporting system so that I could download the data from the server logging database. The data was in the form of JSON, and so it had to be converted into some easy-to-understand format such as CSV or any other tabular\u00a0format. So I planned to use pandas, and it was in a few MBs, so I could do the analysis on my laptop with 16GB\u00a0RAM. Then, I loaded these server logs JSON into Jupyter Notebook and installed various libraries such as Pandas and Matplotlib. With Pandas, I loaded JSON files into a data frame so that I could perform data transformations such as extracting hours of the day for all the records,\u00a0etc. You can learn about such transformation operations at https://towardsdatascience.com/how-to-convert-json-into-a-pandas-dataframe-100b2ae1e0d8 Pandas helped me reformat data in a easy to analyze format and matplotlib helped me plot the charts on the\u00a0data If you want to dive deep into Pandas and Matplotlib on a sample dataset as a beginner, you can follow the video\u00a0below. What I did next found patterns and basis that I plotted charts such as downloads happening by user or server CPU utilization by hours of the day etc. What I got was something mind-boggling. 3 simple insights but actionable ones! All of the users across the company were downloading all the data in the morning between say 10 am -11 am, and the rest of the day, servers were completely idle. Further, when I had a discussion with the team, they had to remove the irrelevant data after downloading all of the data, and as the database was getting bigger, the download size was getting larger. This meant that they were unnecessarily downloading 10x more data, almost all of the database, including historical data, which they didn&#039;t even need for most of the day-to-day decision-making. There were users who didn\u2019t need the data but were still requesting the data and exchanging that data over emails as\u00a0well. Actions taken to above insights\u00a0gained Peak server capacity was only needed for a few minutes in the day when the user needed data access, so we built a scalable solution to adjust server size dynamically for optimum utilization, which is a version of today\u2019s serverless computing systems. So the server would only be billed for the time users requested for the reports according to the size of the\u00a0query. A UI interface was built to provide relevant filters such that only required data could be downloaded by the users, such as team-wise access to limited reports. Later we went to automate this to complete Business Intelligence and Reporting tool having aggregated and detailed\u00a0charts. data governance\u200a\u2014\u200aDifferent roles were assigned to users based on their needs such that they could only access the data they should have access\u00a0to. Benefits from the above\u00a0insights Optimum utilization of server capacity helped reduce server costs equivalent to 2 hours instead of 24\u00a0hours. Improved efficiency of the team member because of the speed of downloading while providing relevant data without purchasing new high-end laptops helping with unavoidable IT infra expenditure Enable data compliance and\u00a0security By now, whatever I explained is nothing but an Exploratory data analysis process. Loved the following depiction of a\u00a0workflow The image is [&#8230;]",
            "pubdate": "Fri, 27 Jan 2023 13:03:02 +0000",
            "pubdate_parsed": [
                2023,
                1,
                27
            ],
            "email_sent": true
        },
        "Deep Learning Explained: Perceptron": {
            "url": "https://towardsai.net/p/l/deep-learning-explained-perceptron",
            "description": "Last Updated on January 27, 2023 by Editorial Team Author(s): Cl\u00e9ment Delteil Originally published on Towards AI. Deep Learning Explained: Perceptron The key concept behind every neural\u00a0network. Source: Image by Gerd Altmann from\u00a0Pixabay Nowadays, frameworks such as Keras, TensorFlow, or PyTorch provide turnkey access to most deep learning solutions without necessarily having to understand them in\u00a0depth. But this can get problematic as soon as your model is not working as expected. You may need to tweak it yourself. So, if you are here to understand the concept of Perceptron in deep learning, I think you are on the right track if you want to be able to contribute one day to this ecosystem in any way, it is essential to understand the roots of these\u00a0systems. Otherwise, if you are already familiar with the concept of Perceptron, it\u2019s not a big deal. I still hope to surprise\u00a0you! In this article, I\u2019ll introduce the idea of the Perceptron. We\u2019ll see how it was thought back in 1950 and how it\u00a0works. Let\u2019s get into\u00a0it. A bit of\u00a0history Beginning Back in 1943, McCulloch and Pitts published a paper entitled A logical calculus of the ideas immanent in nervous activity\u200a\u2014\u200aknown today as the first mathematical model of a neural\u00a0network. The idea of this article is part of the dynamics of the time of wanting to create intelligent machines by reproducing the functioning of the human\u00a0brain. I take as evidence the beginning of the abstract of\u00a0it. Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. At that time, the functioning of the human brain was popularized as interconnected nerve cells transmitting electrical and chemical signals like a simple logic\u00a0gate! Source: Image by Vishnu Mohanan from\u00a0Unsplash The Perceptron itself Now let\u2019s jump forward 14 years to 1957 and the publication of an article by Rosenblatt called The Perceptron\u200a\u2014\u200aA Perceiving and Recognizing Automaton. It is in this article that we find the perceptron as it is understood today. A system to learn the optimal weights to multiply with the inputs to determine whether a neuron activates or\u00a0not. Below you can see the first perceptron trained to recognize objects or patterns, in this case, the letters of the alphabet. Source: Flickr\u200a\u2014\u200aThe camera system of the Mark 1 Perceptron (Public\u00a0Domain) Now that you have an idea of the history of this concept let\u2019s move on to its application in deep learning. Perceptron applied to Deep Learning. The basic perceptron is used for binary classification in supervised machine learning. As a reminder, binary classification implies that there are only two classes to predict 1 and -1, for\u00a0example. And supervised machine learning refers to training the model via already labeled data (with their associated classes). Mathematical definition We define the inputs \ud835\udc65, outputs y, and weights \ud835\udc64 the following way. Source: Image by\u00a0author Where m is the size of the vector \ud835\udc64, \ud835\udc65 or\u00a0y. Let \ud835\udc67 be the net input composed of a linear combination of \ud835\udc65 and\u00a0\ud835\udc64. Source: Image by\u00a0author The classification is defined by an activation function phi: \ud835\udf19 (\ud835\udc67) with a threshold theta: \ud835\udf03 corresponding to the so-called bias, we will see it\u00a0later. Source: Image by\u00a0author The activation function defines in a way how the incoming element will be classified. If the neuron activates, that is to say, if z \u2265 \ud835\udf03, then the current input will be assigned class 1, -1 otherwise. This kind of function is called a Heaviside step function. Source: Image by author\u200a\u2014\u200aHeaviside step function illustration Above, theta is equal to 0. By changing this value, we shift the curve to the left or the\u00a0right. To recap, now that we have added theta, the equation for the net input z changes a little\u00a0bit. We now have\u00a0: Source: Image by\u00a0author With\u00a0: Source: Image by\u00a0author Congratulations! You now know the mathematical definition of a perceptron. Here is the graphical equivalent: Source: Chang et al\u200a\u2014\u200aCreative Commons Attribution 4.0 International\u200a\u2014\u200aPerceptron illustration But how do you train a perceptron? Training a perceptron Here are the training\u00a0steps: Initialization of the weights to 0 (or a small random\u00a0number) 2. For each training example x\u207d\u2071\u207e\u00a0: \u2013 Calculate the estimated output y\u0302\u207d\u2071\u207e \u2013 Update the\u00a0weights The update of each weight of the vector\u00a0w Source: Image by\u00a0author is done as follows\u00a0: Source: Image by\u00a0author Where we introduce eta: \ud835\udf02, the learning rate (between 0.0 and\u00a01.0). Depending on whether or not you are comfortable with these notations, you may have trouble imagining how a perceptron is\u00a0trained. Let\u2019s take some examples. Example For the sake of simplicity, let us assume that the learning rate is equal to 1 and that we know the following values. Source: Image by\u00a0author We consider that there is only one feature in the dataset to simplify the calculations. Here are some examples of the calculation of the delta of the first weight in the perceptron. Source: Image by\u00a0author You can see that the estimated output value given by the activation function is systematically subtracted from the real output\u00a0value. When the estimated value is the same as the real value, it is equal to 0 so there is no\u00a0update. Otherwise, the weight must be\u00a0updated. This is the case in the last two examples. We can notice that the value scale of the input \ud835\udc65 makes the weight update vary more or\u00a0less. In example number 3, \ud835\udc65 = 3, and so we have a weight difference of 6, whereas in example number 4, \ud835\udc65 = 0.5, so the weight difference is only\u00a01. Bias Earlier I intentionally skipped the explanation of the bias so as not to overload you with information. As explained above, the bias is a scalar value that is added to the net input z before passing through the activation function. It allows the decision boundary of the perceptron to be shifted away from the origin, which can be useful in situations where the data is not linearly separable. Source: M.Grove and J.Blinkhor\u200a\u2014\u200aCC BY\u200a\u2014\u200aLinearly separable data vs non-linearly separable data The bias is an addition to the [&#8230;]",
            "pubdate": "Fri, 27 Jan 2023 12:14:54 +0000",
            "pubdate_parsed": [
                2023,
                1,
                27
            ],
            "email_sent": true
        },
        "Data Preprocessing in R Markdown": {
            "url": "https://towardsai.net/p/l/data-preprocessing-in-r-markdown",
            "description": "Last Updated on January 28, 2023 by Editorial Team Last Updated on January 28, 2023 by Editorial Team Author(s): Mohammed Fayiz Parappan Originally published on Towards AI. for Machine\u00a0Learning Photo by Scott Graham on\u00a0Unsplash Data preprocessing constitutes cleaning, sampling, analyzing, transforming, and encoding data so that it can be easily interpretable to provide insights or can be fed into a machine learning\u00a0model. Data is the new oil. It is crucial to have data in an interpretable form. In this article, I will discuss the implementation of Data Preprocessing methods in R. I will be using Heart Attack Analysis and Prediction Dataset provided by\u00a0Kaggle. Steps in Data Preprocessing Import the designated data file and\u00a0Explore Handle Missing Values, Remove duplicates and irrelevant observations Fix structural errors Filter unwanted\u00a0outliers Measures of central tendency (calculate mean, median, mode, and frequencies) Measures of dispersion (calculate variance, standard deviation, range, inter-quartile range, coefficient of variance) Calculate the correlation coefficient and correlation plot Check the distribution of features using histograms and a Normal Probability Plot Data Splitting Import the designated data file and\u00a0Explore You can find more details on the dataset here: https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/ Unlike many other programming languages, datasets in the form of CSV and TXT files can be directly imported without any library in\u00a0R. Top rows of the\u00a0dataset Structure of the\u00a0dataset 2. Handle Missing Values, Remove duplicates and irrelevant observations In R, missing values are represented by NA (not available). Number of missing\u00a0values As no missing values are there, no missing value techniques are used. In case missing values are found, either they are removed or replaced by mean or approximations. Duplicate data can contaminate the interpretability of the dataset and may also lead machine learning models to learn patterns that do not exist in\u00a0reality. The index of the only duplicate row is found and removed from the\u00a0dataset. 3. Fix structural errors As missing values and duplicates are now removed, let\u2019s check if the distribution of dataset w.r.t output is balanced or not. The dataset is labeled as 0\u2019s and\u00a01&#039;s. 0 = No Heart Attack\u00a0Occurs 1 = Heart Attack\u00a0Occur As there are a similar number of observations of both classes, the dataset is balanced\u00a0enough. 4. Filter unwanted\u00a0outliers Outliers are extreme data points that do not match with general trends seen in other points of the dataset. It can have a crucial impact on the interpretations and results given by ML models. It is important to note that the mere appearance of outliers doesn\u2019t mean they should be removed. Only those outliers which are irrelevant for data analysis should be\u00a0removed. Outlier data points in a dataset can be detected with the help of Cook\u2019s Distance which is a metric to measure the influence of each data point over the model (here, linear regression is shown) into which the dataset is fed. Cook\u2019s distances can be easily calculated in R using olsrr library that can be installed from Tools -&#62; Install Packages. Number of\u00a0Outliers Note that conditions for treating data points as outliers are subjective. Here, I have treated data points whose Cook\u2019s distances are more than five times the mean Cook\u2019s distance as outliers. There are 9 such points, and they were filtered from the\u00a0dataset. 5. Measures of central tendency (mean, median, mode, and frequencies) The mean, median, mode, minimum, maximum, and quartiles of each dataframe in the dataset can be extracted from the summary of the\u00a0dataset. 6. Measures of dispersion (variance, standard deviation, range, inter-quartile range, coefficient of variance) I have used sapply() function, which takes a list or vector or data frame as input and gives output as a vector or matrix to get the values of measures of dispersion. Standard Deviation of each\u00a0feature Variance of each\u00a0feature IQR of each\u00a0feature Coefficient of Variance of each\u00a0feature 7. Calculate the correlation coefficient and correlation plot A correlation coefficient is a number between -1 and 1 that tells the strength (along with direction) between features of the dataset. It is useful to detect multicollinearity, which kills independence between features of the dataset and can lead to inaccurate parameter estimates by ML\u00a0models. A correlation plot helps in visualizing correlation coefficients between features of the dataset. It is plotted in R using corrplot library, which can be installed from Tools -&#62; Install Packages. Correlation coefficients of each pair of\u00a0features Correlation Plot on\u00a0Dataset Notice that intensity of the blue color shows the strength of positive collinearity, while the intensity of the red color shows the strength of negative collinearity. 8. Check the distribution of features using Histograms and Normal Probability Plot Histograms show how the values of each feature are distributed, which can give interesting insights into the dataset. A normal probability plot tells us how close the feature distribution is to the normal distribution. I used ggplot2 and qqplotr libraries to plot\u00a0NPP\u2019s. Histogram Plot on the Age of\u00a0Patients Histogram Plot on the blood pressure of\u00a0Patients Histogram plot on the cholesterol level of\u00a0Patients Normal Probability plot on the age of\u00a0Patients Normal Probability plot on blood pressure of\u00a0Patients Normal Probability Plot on cholesterol level of\u00a0Patients 9. Data Splitting I have used caTools library to split the dataset into train and test sets with a ratio of\u00a080:20. All these techniques will help you to have better insights from data and also to prepare your dataset for feeding it into a machine learning model. If you know any other techniques, share them in the comments for everyone! Thanks For Reading, Follow Me For\u00a0More Data Preprocessing in R Markdown was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story. Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 28 Jan 2023 12:10:22 +0000",
            "pubdate_parsed": [
                2023,
                1,
                28
            ],
            "email_sent": true
        },
        "YOLOv8 Is Here and It Gets Better!": {
            "url": "https://towardsai.net/p/l/yolov8-is-here-and-it-gets-better",
            "description": "Last Updated on January 30, 2023 by Editorial Team Author(s): Puneet Jindal Originally published on Towards AI. YOLOv8 Is Here, and It Gets\u00a0Better! YOLOv8 is the latest installment in the highly influential family of models used for object detection and image segmentation. It features a new architecture, new convolutional layers, and a new detection head. It is also significantly faster and more accurate than previous versions of YOLO, making it an excellent choice for real-time object detection. Additionally, YOLOv8 supports the latest computer vision algorithms, including instance segmentation, which allows for the detection of multiple objects in an\u00a0image. How Yolov8 is better than previous popular versions of Yolo, such as Yolov5, Yolov7,\u00a0etc? Firstly, YOLOv8 introduces a new backbone network, Darknet-53, which is significantly faster and more accurate than the previous backbone used in YOLOv7. DarkNet-53 is a convolutional neural network that is 53 layers deep and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many\u00a0animals. Yolov8 makes bounding box predictions similar to image segmentation, i.e., pixel-wise. To achieve this, they have introduced anchor free detection head. To understand more about what is anchor free object detection, you can read\u00a0here Additionally, YOLOv8 is more efficient than previous versions because it uses a larger feature map and a more efficient convolutional network. This allows the model to detect objects in a more accurate and faster way. With a larger feature map, the model can capture more complex relationships between different features and can better recognize patterns and objects in the data. Additionally, a larger feature map also helps to reduce the amount of time it takes to train the model and can help to reduce overfitting. Additionally, YOLOv8 also uses feature pyramid networks, which helps to better recognize objects of different sizes, which improves its overall accuracy. Feature Pyramid networks are a concept that uses different scales of feature maps(similar to making predictions on different sizes of images) coupled with skip connections to predict smaller and bigger objects more accurately. More can be read\u00a0here Finally, YOLOv8 introduces a user-friendly API, allowing users to quickly and easily implement the model in their applications. Let&#039;s understand this in a bit more detail with a quick to-try, hands-on explanation You should visit https://github.com/ultralytics/ultralytics Before moving further, it&#039;s important to understand the relationship between Ultralytics and\u00a0Yolo. Ultralytics is the developer of YOLO (You Only Look Once), a popular object detection model used in computer vision applications. YOLO is a deep learning algorithm that is used to detect objects in images and videos, and the Ultralytics package provides tools and libraries to help developers create and deploy YOLO models. The package includes a range of pre-built models and tutorials, as well as tools for training, validating and inferring models. Scroll down\u00a0to As you can see above, click on \u201cOpen in Colab,\u201d as pointed out here. Otherwise, I am sharing the same link here https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb Click on connect to start using the Google Colab. If you are new to Google Colab, then click on the \u201cHow to start using Google Colab tutorial?\u201d The moment you visit the link, it will open the tutorial as below, where the first line is about\u00a0Setup Let&#039;s dive deep into each cell of the\u00a0code The above lines of code install the Ultralytics package so that we can use the YOLO algorithm. By default, it will install version 8 while I am writing this article. Alternatively, you can\u00a0do %pip install ultralytics==8.0.3 After installing the next line of code, importing the Ultralytics module and then we try to check whether all the required dependencies and compatible hardware are present or\u00a0not. E.g., the following is the output when I executed checks on my\u00a0machine. Ultralytics YOLOv8.0.3 \ud83d\ude80 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB) Setup complete \u2705 (2 CPUs, 12.7 GB RAM, 24.3/78.2 GB\u00a0disk) One thing to note here is the below code. Don\u2019t run this code in case you find it. A few others and I reported the issue at https://github.com/ultralytics/ultralytics/issues/232 Let\u2019s move on to the next\u00a0step # Run inference on an image with\u00a0YOLOv8n Inference means a prediction that we can run on an image to detect the label, whether classification or of a bounding box or a segmentation. Above statement is calling yolo(!yolo) to predict(mode=predict) for object detection task(task=detect) using yolov8 model(model=yolov8n.pt) to output labels only if confidence score is greater than 0.25(conf=0.25) on an image on a publicly accessible link (source = \u2018https://ultralytics.com/images/zidane.jpg\u2019) In case you want to try predictions on another type of task, the following options are available. The above statement will first download the referenced image, and then download the referenced pre-trained model checkpoint if not downloaded already, and then produce inference results. Here are the results in my case. It might differ, especially in the latency and number of labels detected. Results will be saved to the runs/detect/predict folder in case you want to cross-reference, and if you open the image, it might look like the following with detections printed on the\u00a0image. Let\u2019s move to the next step of testing predictions on the validation set. But before that, we first need to download the validation dataset, which is coco2017. You can see that PyTorch provides the functions to download the zip file from the URL. In the last statement below, we can unzip it into a folder location\u00a0../datasets and delete the zip file with the rm\u00a0command. The below command shows the predictions on the validation dataset. The output of the predictions on the validation set is\u00a0as Now a question could emerge \u201cHow do I interpret these numbers?\u201d To interpret the YOLOv8 prediction results in summary on a validation set, you need to look at the metrics such as mean Average Precision (mAP), precision, recall, and the false positive rate (FPR). The mAP is a measure of the model\u2019s overall performance, while the Precision, Recall, and FPR measure the model\u2019s accuracy in detecting different classes. Additionally, you should also look at the class-wise performance of the model, which is the performance of the model for each [&#8230;]",
            "pubdate": "Tue, 31 Jan 2023 00:11:36 +0000",
            "pubdate_parsed": [
                2023,
                1,
                31
            ],
            "email_sent": true
        },
        "Build Strong Deep Learning Foundations By Learning From Top Universities": {
            "url": "https://towardsai.net/p/l/build-strong-deep-learning-foundations-by-learning-from-top-universities",
            "description": "Last Updated on January 31, 2023 by Editorial Team Author(s): Youssef Hosni Originally published on Towards AI. Deep learning is a rapidly growing field with a high demand for experts in the industry. Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Wed, 01 Feb 2023 00:11:57 +0000",
            "pubdate_parsed": [
                2023,
                2,
                1
            ],
            "email_sent": true
        },
        "This AI newsletter is all you need #32": {
            "url": "https://towardsai.net/p/l/this-ai-newsletter-is-all-you-need-32",
            "description": "Last Updated on January 31, 2023 by Editorial Team Author(s): Towards AI Editorial Team Originally published on Towards AI. What happened this week in AI by\u00a0Louis Following recent advancements in image, code, and text generation, there have been new developments in AI-generated music and text-to-speech. In 2019, everyone was already impressed with OpenAI\u2019s Musenet, built on GPT-2 techniques applied to MIDI files. However, with recent advancements in AI, much more flexible and comprehensive music models are now possible. This week, AI has noted several interesting AI-generated music and text-to-speech models, including MusicLM, a model announced by Google Research that generates high-fidelity music from rich text descriptions. Although the model isn\u2019t released yet, dataset MusicCaps, consisting of 5.5k human-written music-text pairs, is available. The paper \u201cMake-An-Audio\u201d was also released this week, describing Text-To-Audio Generation with Prompt-Enhanced Diffusion Models. We expect to see a wave of AI music startups and open-source models released going forward, especially as annotated music data sets become accessible. We hope progress in AI music models can benefit musicians exploring new concepts and lower the cost and obstacles for new musicians entering the industry. Besides new music models this week, we also discovered an impressive and flexible text-to-audio model from elevenlabs. Excited about the potential of such models to increase accessibility of written content, we also see growing risks in text-to-audio, including voice cloning for fake quotes and voice-protected logins and verifications. Hottest News 1.ChatGPT is \u2018not particularly innovative,\u2019 and \u2018nothing revolutionary\u2019, says Meta\u2019s chief AI scientist Recently, there has been much discussion about the potential of OpenAI\u2019s ChatGPT program for generating natural language responses to human prompts. However, AI scholars have a different view. During a Zoom meeting with press and executives last week, Yann LeCun, the Chief AI Scientist at Meta, stated, \u201cIn terms of underlying techniques, ChatGPT is not particularly innovative.\u201d 2. The inside story of ChatGPT: How OpenAI founder Sam Altman built the world\u2019s hottest technology with billions from Microsoft Sam Altman believes that the future of AI could be exceptional\u200a\u2014\u200aunless things go astray. It is important to know the story of OpenAI\u2019s ChatGPT chatbot, which has been used for activities such as debugging code, writing recipes, scripts, and more, and how it has sparked a revolution. 3. AI adoption: is it obvious\u00a0yet? Regardless of one\u2019s opinion of ChatGPT, its release last year generated another buzz in the AI community. Its launch likely did more to promote the value of machine learning to non-experts than any other event. It is crucial to assess if we are ready for AI adoption in terms of technological and product maturity and to comprehend the excitement surrounding AI and the arguments for and against incorporating it into products. 4. AI21 Labs has created a co-writing bot that can suggest quotes, statistics, provide citations and\u00a0more AI21 Labs, a research lab specializing in NLP and generative AI, announced the launch of Wordtune Spices, a new feature for its popular Wordtune editing platform, to enhance the writing experience for writers of all types. Wordtune Spices is an AI-powered writing tool that comprehends content and meaning to assist users in expressing their ideas more effectively and compellingly. 5. The Human-AI Partnership In this podcast, Reid Hoffman speaks with ChatGPT about the partnership between humans and AI. He delves into the unique ways in which AI can enhance human capabilities as part of a miniseries focused on the future of AI and chatbots. Three 5-minute reads/videos to keep you\u00a0learning 1.A Brief History of Artificial Intelligence This article explores the rich history and evolution of AI, from its early origins to the current ethical debates surrounding its development. The article traces the journey of AI, starting with its first concepts and leading to the seminal conference organized by Allen Newell, Cliff Shaw, and Herbert Simon, which marked the beginning of its proof of concept. The article provides insight into the past, present, and future of\u00a0AI. 2. Introduction to embeddings\u200a\u2014\u200aThe bread and butter of language\u00a0models Word and sentence embeddings are the backbones of language models. This Twitter thread by Cohere offers a clear and simple introduction to these essential concepts, including examples, applications, and additional resources. It also explains how embeddings work and how they can be used in language\u00a0models. 3. Getting started with LLMs using LangChain Recently, there has been a surge of interest in generative AI and language models (LLMs). This article introduces LangChain, a library that enables the creation of advanced applications around LLMs such as OpenAI\u2019s GPT-3 models and the open-source alternatives provided by Hugging Face. It begins with a discussion on the simplest component offered by LangChain. 4. Five pieces of advice for those building in AI right\u00a0now In this Twitter thread, Nathan shares his thoughts on the process of building products. Some advice he shares in his thread: avoiding generalizations, recognizing that AI is not a unique advantage, treating AI-powered products as more than just \u201cwrappers\u201d, disregarding hype, and acknowledging that AI-powered applications are not primarily focused on\u00a0AI. 5. Manipulating Tensors in\u00a0PyTorch PyTorch is a deep-learning library that operates on numerical arrays known as tensors. This article provides a brief overview of what PyTorch offers for tensors and how to use them. It gives insight into how to create and perform operations on PyTorch tensors and the common functions available in PyTorch for manipulating tensors. Enjoy these papers and news summaries? Get a daily recap in your\u00a0inbox! The Learn AI Together Community section! Upcoming Community Events The Learn AI Together Discord community hosts weekly AI seminars to help the community learn from industry experts, ask questions, and get a deeper insight into the latest research in AI. Join us for free, interactive video sessions hosted live on Discord weekly by attending our upcoming\u00a0events. 1.Convolution Networks: The Neural Network Architecture Seminar\u00a0(#4) This is the fourth session of a (free) nine-part series on Neural Networks Architectures presented by Pablo Duboue (DrDub), covering Convolution Networks. The session focus on CNNs, DL image processing, YOLO, U-Net, Retina-Net, and SpineNet. Find the link [&#8230;]",
            "pubdate": "Wed, 01 Feb 2023 00:11:54 +0000",
            "pubdate_parsed": [
                2023,
                2,
                1
            ],
            "email_sent": true
        },
        "Parquet Best Practices: The Art of Filtering": {
            "url": "https://towardsai.net/p/l/parquet-best-practices-the-art-of-filtering",
            "description": "Last Updated on February 1, 2023 by Editorial Team Author(s): Arli Originally published on Towards AI. Understanding how to filter Parquet files Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Thu, 02 Feb 2023 00:21:42 +0000",
            "pubdate_parsed": [
                2023,
                2,
                2
            ],
            "email_sent": true
        },
        "ChatGPT Is Overhyped. There Is A Reason For It.": {
            "url": "https://towardsai.net/p/l/chatgpt-is-overhyped-there-is-a-reason-for-it",
            "description": "Last Updated on February 4, 2023 by Editorial Team Author(s): Alexandros Zenonos, PhD Originally published on Towards AI. ChatGPT is making people smarter Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 04 Feb 2023 12:19:18 +0000",
            "pubdate_parsed": [
                2023,
                2,
                4
            ],
            "email_sent": true
        },
        "Technology Readiness Levels (TRL) in AI development": {
            "url": "https://towardsai.net/p/l/technology-readiness-levels-trl-in-ai-development",
            "description": "Last Updated on February 4, 2023 by Editorial Team Author(s): Stavros Theocharis Originally published on Towards AI. A framework for the future development of\u00a0AI Image generated by\u00a0DALL-E Artificial Intelligence (AI) is a subject that has been discussed almost everywhere. It continuously gets so popular that it permeates practically every field, from the business world to the entertainment industry. This technology, however, is more than simply a fad; it\u2019s a serious means through which businesses may boost their productivity. As a result of the proliferation of use cases demonstrating how AI improved various operations, an increasing number of businesses have realized that AI and other forms of cutting-edge technology are the new arenas in which to compete. Perhaps you are among the company owners or project planners who have realized the value of AI and are looking for ways to improve their operations by using AI software. But what is actually\u00a0AI? Artificial intelligence (AI) is a subfield of computer science and engineering that simulates intelligent behavior similar to that of humans. AI technology can be used in many different ways, such as to make fully autonomous systems that can think, learn, and act on their own. The powers of the human mind can be mimicked and even improved upon by computers thanks to AI\u2019s ability to do so. AI is being more fully integrated into many aspects of modern life, including the creation of self-driving automobiles and the widespread use of virtual assistants. As a direct consequence of this, several tech businesses operating in a wide variety of sectors are increasing their investments in technologies that are powered by AI. This includes a wide range of things, like being able to understand spoken language, recognize faces and objects, make plans, and solve problems. There is a substantial body of literature focused on the stages of an AI project\u2019s lifecycle. The main possible steps are the following, as referenced in\u00a0[1]: Exploratory data\u00a0analysis Data preparation Data preprocessing Modeling Testing These steps can certainly be supplemented with additional steps, but they define a basic structure. I would for sure add, on top of everything, \u201cunderstand the business problem\u201d. For many teams, this is obvious, but unfortunately, this isn\u2019t always the case, and many teams realize it when it is already too\u00a0late. Personally, I am one of the primary advocates of AI-based problem-solving solutions. On the other hand, I stress the significance of introducing AI only when it is really required. An actual issue or goal should serve as the impetus for any AI project. This is crucial since AI may not be necessary for all situations. This is an important part to be considered in order to understand the following parts of this\u00a0article. So, let\u2019s go\u00a0deeper\u2026 What is the Technology Readiness Level\u00a0(TRL)? Research and consulting in the field of information systems have relied on maturity models for many years. Therefore, there is an infinite amount of literature on them. A recent comprehensive review of the state of the art discovered 409 relevant papers and a multitude of classification techniques, many of which still need genuine validation [2]. At its core, a system\u2019s, subsystem\u2019s, or component\u2019s Technology Readiness Level (TRL) is only a description of its performance history in relation to a scale developed at NASA headquarters in the 1980s. The TRL is a measure of the development and maturity of technology [3]. Technology Readiness Levels (NASA,\u00a02003) The 9 levels of the scale illustrate how far down the path of maturity a given technology must go before it may be used in its designated operational setting. The TRL has been criticized for being utilized more as a decision-making aid in public funding for R&#38;D&#38;I than as an ontological explanation of the processes involved in bringing new technologies to\u00a0market. Since 2014, the TRL scale has been included in the European Union\u2019s Horizon 2020 Work Programmes and has been extensively implemented in the context of research, development, and innovation investments funded by the European Regional Development Fund, or \u201cERDF\u201d\u00a0[4]. But, what about using TRL in AI applications? Lavin A. et al. (2022) state that the default TRL process may come into sharp contrast with the rapid development and fast iteration that an AI project may need to follow. Because of this, they propose a simplified Machine Learning Technology Readiness Levels (MLTRL) framework to be put in place\u00a0[5]. I would also agree that fast prototyping in many AI projects can lead to faster market entry and engagement than the competition. However, what happens when we are talking specifically about healthcare diagnostic tools or, more generally, about ethics and fairness? In such cases, rapid prototyping without specific steps may have the opposite effect on both business and\u00a0society. Models, algorithms, data pipelines, software modules, and their many combinations all have different levels of maturity, which are measured by the\u00a0TRL. The ethics part is one of the pain points of today\u2019s AI systems. Organizational AI ethics procedures may differ, but discussions concerning ethical problems are needed to take place all the way through the TRL procedure, and in the majority of instances, these discussions are tied to a full ongoing ethics checklist from the beginning until the end. Discussions on ethics are needed with the involvement of more than one team (stakeholders, legal team, etc.) in order to get into serious consideration of the different ideas and suggestions. Let\u2019s focus on the different TRL levels from the MLTRL framework (process-desired outputs): Level 0: First principles This is the early phase of AI research for a new concept that needs to be implemented. Here, there won\u2019t be any full data to work with yet, but some sample data instead; therefore, much of the work will consist of literature study, laying down mathematical foundations, etc. Level 1: Goal-oriented research Instead of running end-to-end to get a performance benchmark score, try doing low-level experiments to investigate certain model or algorithm aspects. In order to train and test the model, it is necessary to gather and analyze sample data. This data may be a [&#8230;]",
            "pubdate": "Sun, 05 Feb 2023 00:11:31 +0000",
            "pubdate_parsed": [
                2023,
                2,
                5
            ],
            "email_sent": true
        },
        "How To Estimate FP, FN, TP, TN, TPR, TNR, FPR, FNR & Accuracy for Multi-Class Data in Python in 5": {
            "url": "https://towardsai.net/p/l/how-to-estimate-fp-fn-tp-tn-tpr-tnr-fpr-fnr-accuracy-for-multi-class-data-in-python-in-5",
            "description": "Last Updated on February 5, 2023 by Editorial Team Author(s): Serafeim Loukas Originally published on Towards AI. In this post, I explain how someone can read a confusion matrix and how to extract several performance metrics for a multi-class&#x2026; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sun, 05 Feb 2023 12:08:52 +0000",
            "pubdate_parsed": [
                2023,
                2,
                5
            ],
            "email_sent": true
        },
        "Ask Jeeves Has Been Re-Born with AI!": {
            "url": "https://towardsai.net/p/l/ask-jeeves-has-been-re-born-with-ai",
            "description": "Last Updated on February 5, 2023 by Editorial Team Author(s): Andrew Austin Originally published on Towards AI. I hope you wrote down everything you didn&#x2019;t get a chance to &#x201c;Ask Jeeves&#x201d; Continue reading on Towards AI \u00bb Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 06 Feb 2023 00:11:09 +0000",
            "pubdate_parsed": [
                2023,
                2,
                6
            ],
            "email_sent": true
        },
        "Inside BLOOM: How Thousands of AI Researchers Created an Open Source ChatGPT Alternative": {
            "url": "https://towardsai.net/p/l/inside-bloom-how-thousands-of-ai-researchers-created-an-open-source-chatgpt-alternative",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Jesus Rodriguez Originally published on Towards AI. An open-source LLM shows that tech incumbents are not the only companies able to create massive models. Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 13 Feb 2023 11:34:45 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "Will AI replace Cybersecurity jobs?": {
            "url": "https://towardsai.net/p/l/will-ai-replace-cybersecurity-jobs",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Taimur Ijlal Originally published on Towards AI. Do cybersecurity jobs have a limited lifespan as AI becomes better and better at securing stuff&#xa0;? Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 13 Feb 2023 11:34:43 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "A Cheat Sheet to Deep Learning Algorithms: Types, Applications, and Examples": {
            "url": "https://towardsai.net/p/l/a-cheat-sheet-to-deep-learning-algorithms-types-applications-and-examples",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): SPX Originally published on Towards AI. Introduction Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 13 Feb 2023 11:34:41 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "How to Maximize ML Project Success with Efficient Scoping? | MLOps 5": {
            "url": "https://towardsai.net/p/l/how-to-maximize-ml-project-success-with-efficient-scoping-mlops-5",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Akhil Theerthala Originally published on Towards AI. In our past articles of this series, we have seen many things. We started our journey by looking at the lifecycle of a project. Following this, we got an overview of each phase of the lifecycle. If you have followed this series till now, you should know about the standard practices in different lifecycle phases. Let\u2019s take a look at the lifecycle once. Lifecycle of a Machine Learning Project. Source: Deeplearning.AI, licensed under the Creative CommonsAttribution-ShareAlike 2.0\u00a0license. After a brief overview of the lifecycle, we looked into the individual phases. After moving back to back from one stage of the project to another, it is time for us to look into the project\u2019s very first stage,\u00a0Scoping. Fret not. Unlike the previous articles, this article is a short one. You can finish reading this within a few minutes!\u00a0\ud83d\ude0c Why should we scope a\u00a0project? Let\u2019s take a step back and think from a company\u2019s perspective. Companies generally invest vast amounts into any of their projects. Besides these costs, they must employ engineers and scientists to work on these projects. This requires additional capital. All this money will be invested into a project, which sometimes might yield abysmal results. Hence, companies are cautious before they start a new\u00a0project. They cross-check why they need the project, check for alternative cheaper options, list out additional benefits a project can bring, do market research for the projects, list out the different risks the project has, plan out the entire financial requirements, and make a profit estimate, and they estimate the time required for the project. They do all this before a project begins. However, even after doing this, the project might still fail or need more time or investments. In Aug 2022, Forbes stated that somewhere between 60\u201380% of AI projects are failing, according to different sources. This shows that scoping the projects is one of the most important steps before starting a machine learning\u00a0project. Let\u2019s speak examples. Now that we have seen why scoping a project is essential. Let\u2019s get into the details. Essentially, the question we have been asking till now is, \u201c How to pick a project to work\u00a0on?\u201d Let\u2019s use the example of an e-commerce retailer looking to increase sales. What can you recommend to him as an ML Engineer? You can guide\u00a0him, A better product recommendation system. A better search engine for his\u00a0site. To improve the catalog data of his\u00a0store. A better inventory management system,\u00a0etc. We can also keep giving hundreds of other suggestions to him, both belonging to ML and not belonging to ML. But, there are only a few efficient suggestions that he needs to focus on. So, our problem now is\u00a0to Identify the most profitable project to work\u00a0on. Explain to him how this is the most profitable project using different metrics. Estimate the resources and costs of this project and ask him the\u00a0amount. That\u2019s it. If you answer these three questions, you have finished scoping the\u00a0project. General Outline for\u00a0Scoping. Like all other fields, scoping ML projects have many community recommended outlines. Despite there being many outlines, their essence is the same. Now, we can look at one such strategy. Step-1: Brainstorm the business problems. Before beginning the project, it is first essential to have a goal or a problem to solve. There are countless problems found everywhere. However, only a few are profitable. In this step, we list all the problems that can profit the company when\u00a0solved. For our store, some problems can be reducing the inventory, increasing profit margin, increasing conversion, etc., Step-2: Brainstorm AI solutions. In this step, we identify different AI solutions for the identified problems. We pick the most feasible one among these in the later\u00a0stages. For our store, it can be things like,&#8211; Improving the recommendation model to increase the conversion rate.&#8211; Optimizing what to sell to reduce the profit margin.&#8211; Using applications of marketing and demand prediction to reduce inventory. Step-3: Feasibility Assessment AI is not a cure-all tool. It should not be applied to everything just because we\u00a0can. In our earlier AI solutions, many need unrealistic requirements. Hence we need to filter these\u00a0out. Since we are checking the possibility of an AI solution, we use different machine learning metrics to filter these\u00a0out. For filtering these problems, &#8211; We can use external benchmarks like research papers, other companies etc.&#8211; We can look into Human-Level Performance (HLP) for unstructured data (or) the availability of predictive features for structured data.&#8211; We also generally look at the history of existing projects in the market and their performance to assess the rate of future improvement. Step-4: Value Assessment Just as all the solutions are not feasible, all feasible solutions are not valuable. For example, building a linear regression to predict no. of A/Cs in the office is useless for an e-commerce store. In the value Assessment, we look at different Business metrics that affect the AI feasible projects. The business metrics include the metrics like User engagement, quality of the model compared to the market, possible revenue that can be generated from the model,\u00a0etc., In practical situations, the ML and Business teams compromise to some degree to find feasible and valuable projects. Step-5: Draw out a\u00a0plan. Determining the milestones generally involves determining the ML and software metrics of the projects. Some of such metrics were previously discussed in articles 2 and\u00a03.1. However, here, we also try to determine the Business metrics and resources needed for the project like estimating the possible revenue, data required, the personnel that is allocated, etc., Finally, after all the things are done. You now have a problem, a possible AI solution, and the metrics to evaluate the situation. But your job doesn\u2019t end here. You need to give the company a timeline within which you will complete the project. (Yup, they don\u2019t trust you with the\u00a0budget.) Step-6: Determine the Budget for the resources. Of course, doing all the above things and not having [&#8230;]",
            "pubdate": "Mon, 13 Feb 2023 11:34:37 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "An Intuitive Explanation of Policy Gradient": {
            "url": "https://towardsai.net/p/l/an-intuitive-explanation-of-policy-gradient",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Renu Khandelwal Originally published on Towards AI. A Simple Explanation of Policy Gradient for Reinforcement Learning with very little Math Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 13 Feb 2023 11:34:35 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "How to Use Hugging Face Pipelines?": {
            "url": "https://towardsai.net/p/nlp/how-to-use-hugging-face-pipelines",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Tirendaz AI Originally published on Towards AI. A practical guide on how to perform NLP tasks with Hugging Face Pipelines Image by\u00a0Canva With the libraries developed recently, it has become easier to perform deep learning analysis. One of these libraries is Hugging Face. Hugging Face is a platform that provides pre-trained language models for NLP tasks such as text classification, sentiment analysis, and\u00a0more. This blog will walk you through how to perform NLP tasks with Hugging Face Pipelines. Here are topics we\u2019ll discuss in this\u00a0blog. What is\u00a0NLP? What is Transformers? Performing various NLP tasks with Transformers. The NLP tasks we\u2019ll cover are text classification, named entity recognition, question answering, and text generation. Let\u2019s dive\u00a0in! What is\u00a0NLP? NLP is a subfield of AI that allows computers to interpret, manipulate and understand human language. The goal of NLP tasks is to analyze text and voice data like emails, social media newsfeeds, video, audio, and more. With the NLP techniques, you can handle various tasks such as text classification, generating text content, extracting an answer from a text,\u00a0etc. NLP doesn\u2019t just deal with written text. It also overcomes complex challenges in speech recognition and computer vision, such as creating a transcript of a sound sample or a description of an\u00a0image. Cool, we learned what NLP is in this section. Let\u2019s go ahead and have a look at what the Transformers library\u00a0is. What is the Transformers library? Transformers is a library in Hugging Face that provides APIs and tools. It allows you to easily download and train state-of-the-art pre-trained models. You may ask what pre-trained models are. Let me explain. A pre-trained model is actually a saved pre-trained network that was previously trained on a large dataset. Using pre-trained models, you can save the time and resources needed to train a model from\u00a0scratch. Nice, we looked at what the Transformers library is. Let\u2019s carry out some tasks to show how to use this\u00a0library. Transformer Applications Transformers library has great functions to handle various NLP tasks. The easiest way to tackle NLP tasks is to use the pipeline function. It connects a model with its necessary pre-processing and post-processing steps. This allows you to directly input any text and get an\u00a0answer. To use the Transformers library, you need to install it with the following command: pip install -q transformers To show how to utilize the pipeline function, let\u2019s import it from transformers. from transformers import pipeline Cool, we can now perform the NLP tasks with this object. Let\u2019s start with sentiment analysis. Sentiment Analysis Sentiment analysis is one of the most used NLP tasks. It is the process of detecting positive or negative sentiments in text. To show how to do this task, let\u2019s create a\u00a0text. text = \"This movie is beautiful. I would like to watch this movie again.\" Awesome, we now have a text. Let\u2019s find out the sentiment of this text. To do this, first, we instantiate a pipeline by calling the pipeline function. Next, we give the name of the task we are interested in. classifier = pipeline(\"sentiment-analysis\") Nice, we are ready to analyze our text using this\u00a0object. classifier(text)# Output:[{&#039;label&#039;: &#039;POSITIVE&#039;, &#039;score&#039;: 0.9998679161071777}] As you can see, our pipeline predicted the label and showed the score. The label is positive, and the score is 0.99. It turns out that the model is very confident that the text has a positive sentiment. Great, we have finished our sentiment analysis. It is simple,\u00a0right? Let\u2019s take a step back and think about what happened. This pipeline first selected a pretrained model that has been fine-tuned for sentiment analysis. Next, when creating the classifier object, the model was downloaded. Note that when passing some text to a pipeline, the text is preprocessed into a format the model can understand. In this analysis, we used a pipeline for sentiment analysis. You can also use it for other tasks. Some of the pipelines that have been developed recently are Sentiment-analysis; we just learned how to perform this pipeline, summarization, named entity recognition, question-answering, text generation, translation, feature extraction, zero-shot-classification, etc. Let\u2019s have a look at a few of these. The pipeline we\u2019re going to talk about now is zero-hit classification. Zero-Shot Classification Imagine you want to categorize unlabeled text. This is where the zero-shot classification pipeline comes in. It helps you label text. So, you don\u2019t have to depend on the labels of the pretrained model. Let\u2019s take a look at how to use this pipeline. First, we\u2019re going to instantiate by calling the pipeline function. classifier = pipeline(\"zero-shot-classification\") Now let\u2019s create a text to classify. text = \"This is a tutorial about Hugging Face.\" Let\u2019s define candidate labels. candidate_labels = [\"tech\", \"education\", \"business\"] Cool, we created our text and labels. Now, let\u2019s predict the label of this sentence. To do this, we\u2019re going to use the classifier object. classifier(text, candidate_labels)# Output:{&#039;sequence&#039;: &#039;This is a tutorial about Hugging Face&#039;, &#039;labels&#039;: [&#039;education&#039;, &#039;tech&#039;, &#039;business&#039;], &#039;scores&#039;: [0.8693577647209167, 0.11372026801109314, 0.016921941190958023]} As you can see, the text is about education. Here we didn\u2019t fine-tune the model on our data. Our pipeline directly returned probability scores. This is why this pipeline is called zero-shot. Let\u2019s move on and take a look at the text generation task. Text Generation Tools like ChatGPT are great for generating text, but sometimes you might want to generate text about a topic. The goal of text generation is to generate meaningful sentences. Our model gets a prompt and auto-completes it. Let\u2019s see how to perform a pipeline. First, we instantiate the pipelines with text-generation. generator = pipeline(\"text-generation\") Let\u2019s go ahead and create a\u00a0prompt. prompt= \"This tutorial will walk you through how to\" Now let\u2019s pass this prompt to our\u00a0object. generator(prompt)# Output:[{&#039;generated_text&#039;: &#039;This tutorial will walk you through how to setup a Python script to automatically find your favourite website using Python and JavaScript so you can build a web site that&#039;}] As you can see, a text was generated according to our sentence. Note that this [&#8230;]",
            "pubdate": "Mon, 13 Feb 2023 11:34:27 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "Unsupervised Sentiment Analysis With Real-World Data: 500,000 Tweets on Elon Musk": {
            "url": "https://towardsai.net/p/l/unsupervised-sentiment-analysis-with-real-world-data-500000-tweets-on-elon-musk",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Cl\u00e9ment Delteil Originally published on Towards AI. Guided walkthrough in a real-world Natural Language Processing project. Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Mon, 13 Feb 2023 11:34:20 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "Traffic Forecasting: The Power of Graph Convolutional Networks on Time Series": {
            "url": "https://towardsai.net/p/l/traffic-forecasting-the-power-of-graph-convolutional-networks-on-time-series",
            "description": "Last Updated on February 13, 2023 by Editorial Team Author(s): Barak Or Originally published on Towards AI. The Graph Convolutional Network (GCN) has revolutionized the field of deep learning by showcasing its versatility in solving real-world problems, including traffic prediction, which is a critical issue in transportation. Introduction The Graph Convolutional Network (GCN) is a revolutionary development in the field of deep learning, demonstrating its versatility and potential for application in addressing real-world problems. One such challenge is traffic prediction, which is a critical issue in transportation. The ability to adapt GCN algorithms for traffic prediction purposes holds immense promise and has the potential to significantly impact the transportation industry. It is important to note that this post assumes a prior understanding of GCN. For those who require an introduction to GCN, I strongly recommend exploring Michael Bronstein\u2019s post, \u201cDo we need deep GNN,\u201d as well as Tobias Skovgaard Jepsen&#039;s hands-on introductory post. The power of Graph Neural Networks (GNN) [1], particularly GCN, lies in their ability to model complex relationships between entities, as demonstrated through examples such as chemical bonds between atoms or traffic speeds between road segments. With its capability for feature learning through graph convolutional operations, GCN has seen wide research in vision and text data, yet there is still much potential for exploring its application in the time-series domain, unlocking new frontiers for innovation. Image by\u00a0author Exploring Time-Series Tasks with Temporal\u00a0GCN The advent of GCN has opened up new possibilities for analyzing and understanding graph-structured data, including time-series data. Time-series data presents unique challenges compared to other forms of data, such as images. It is often less intuitive, lacks clear visual representations, and requires a deep understanding of causality to be analyzed effectively. To address these challenges, a new field of research known as Temporal GCN (TGCN) has emerged, which combines the strengths of GCN with those of Recurrent Neural Networks (RNN), Long-Short Term Memory (LSTM), or Gated Recurrent Units (GRU). This allows TGCN to capture both the spatial and temporal aspects of time-series data, making it a powerful tool for analyzing complex and dynamic\u00a0systems. Next, we will explore the potential of TGCN for traffic prediction [2], which is a critical issue in transportation. By analyzing traffic patterns and dependencies between different road segments over time, TGCN can provide insights and make accurate predictions about future traffic conditions. Through this example, we aim to demonstrate the versatility and potential of TGCN for solving real-world problems and improving our understanding of time-series data. Temporal GCN For Traffic Prediction Traffic prediction is a critical issue in transportation, and finding an accurate solution is crucial for optimizing traffic flow and reducing congestion. Traditional methods of predicting traffic speed only consider the past values of the same time series, but with the advent of GCN and GRU, a more comprehensive approach is possible. In a recent study [2], a new approach to traffic prediction was proposed that leverages the strengths of GCN and GRU. The researchers modeled the traffic network as a graph, with each road segment being a node and the connections between segments being the edges. The traffic speed was then represented as a signal on this graph, allowing the model to capture both the spatial and temporal features of the\u00a0data. The architecture included a graph convolution layer and a GRU layer, which enabled the model to effectively capture the relationships between different road segments and predict future traffic speed. By taking into account the dependencies between road segments, this solution provides a more accurate and comprehensive traffic prediction than traditional methods. Image by\u00a0author Exploring the Road Segments\u00a0Dataset In order to understand if there is any dependency between different road segments, we need to analyze the correlations between them. To do this, we selected 26 roads out of the 228 available in the dataset and created a correlation map. By visualizing the correlations, we can see if there is any relationship between the traffic speed of different road segments. If there is a strong relationship, this provides a clear justification for the use of a GCN to model the dependencies. It\u2019s important to note that capturing these relationships is crucial in accurately predicting traffic speed. By using a GCN, we can consider not only the individual traffic speed of each road segment but also the interdependencies between segments. This holistic approach allows for a more accurate and robust prediction of traffic\u00a0speed. Image by\u00a0author To understand if there are any dependencies between different road segments in the dataset, we calculated the correlations. The results showed that segments 4\u20135\u20136\u20137 and 19\u201320\u201321 have a high correlation, highlighting the importance of incorporating spatial features. In their research, Ling Zhao and their team proposed a TGCN architecture that utilizes both spatial and temporal features, with a graph convolution layer for the former and a GRU layer for the latter. We played with their code [3] and made it accessible through a Google Colab notebook, providing an opportunity for you to easily experiment with it [4]. The results were promising, and we encourage you to further test and improve the model by adjusting the parameters and increasing the number of\u00a0epochs. Keras.io Time-series traffic forecasting Tutorial Summary The graph convolutional network (GCN) is a highly innovative and impactful development in deep learning. It has numerous applications in the time-series domain, where it is adapted to include temporal features. To showcase the capabilities of TGCN, the example of traffic speed prediction was used and the correlation map was shown to highlight the need for the inclusion of spatial features. follow me on Medium for more\u00a0posts About the\u00a0Author Barak Or is an Entrepreneur and AI &#38; navigation expert; Ex-Qualcomm. Barak holds M.Sc. and B.Sc. in Engineering and B.A. in Economics from the Technion. Winner of Gemunder prize. Barak finished his Ph.D. in the fields of AI and Sensor Fusion. Author of several papers and patents. He is the founder and CEO of ALMA Tech. LTD, an AI &#38; advanced navigation company. Further reading and\u00a0comments [1] GCNs [&#8230;]",
            "pubdate": "Mon, 13 Feb 2023 11:34:14 +0000",
            "pubdate_parsed": [
                2023,
                2,
                13
            ],
            "email_sent": true
        },
        "This AI newsletter is all you need #34": {
            "url": "https://towardsai.net/p/machine-learning/this-ai-newsletter-is-all-you-need-34",
            "description": "Last Updated on February 15, 2023 by Editorial Team What happened this week in AI by Louis This week was rather chaotic in the world of large language models (LLMs) and \u201cGenerative AI\u201d as large tech companies scrambled to display their technology in the wake of ChatGPT\u2019s success. Microsoft announced an AI-powered version of the Bing search engine that incorporates OpenAI\u2019s ChatGPT technology into its Edge browser. In response, Alphabet announced their alternative to ChatGPT, named Bard. However, promotional material for Bard contained inaccurate information raising questions as to whether Google rushed out its release. Meanwhile, Chinese web giant Baidu is preparing to launch a generative AI chatbot, ERNIE, later this year. What people call \u201cGenerative AI\u201d is increasingly looking to be the next major platform for founders and startups to use to build new products. The barriers to entry to starting a business have now been reduced. You can rapidly and affordably create a prototype or minimum viable product on the back of\u00a0prompting\u00a0or fine-tuning APIs of LLMs like ChatGPT. But it is difficult to know how the ecosystem will play out and what capabilities and products will be built into the LLMs and owned by the likes of OpenAI, Microsoft, and Google and which will be performed by the surrounding startup ecosystem. This week we published a new blog\u00a0Learn Prompting 101: Prompt Engineering Course &#38; Challenges\u00a0as a summary of Prompt Engineering and how to talk to LLMs and get the most out of them. This forms an introduction to the comprehensive open-source\u00a0Learn Prompting course\u00a0that we have contributed to. Hottest News 1. Alphabet Stock Plunge Erases $100 Billion After New AI Chatbot Gives Wrong Answer In Ad Questions were raised about whether Alphabet rushed the release of its new Bard LLM after promotional material contained inaccurate information. This comes as Microsoft begins to integrate ChatGPT into its Bing search engine and opens a debate about how generative AI will change the way people browse and interact with the internet going forward. 2. Generative AI: The Next Consumer Platform We\u2019ve entered the age of generative AI. It could be the next major platform upon which founders build category-defining products. This article explores the main consumer categories with opportunities like search and product discovery, education, dating, coaching, e-commerce, and more. 3. Hands-on with the new Bing: Microsoft\u2019s step beyond ChatGPT Microsoft announced a new AI-powered version of the Bing search engine using the same technology behind ChatGPT. The way Microsoft has integrated these chatbot powers into its Edge browser is different from ChatGPT. This version allows you to ask questions about real-time news and events unfolding by integrating chatbot capabilities into its Edge browser. 4. China\u2019s Baidu reveals generative AI chatbot based on language model bigger than GPT-3 The Chinese web giant, Baidu, has made AI the focus of its hyperscale cloud and is set to launch a generative AI chatbot later this year. According to a Baidu spokesperson, the company plans to complete internal testing in March before making the chatbot available to the public. The spokesperson added that what sets ERNIE apart from other language models is its exceptional understanding and generation capabilities, thanks to its ability to integrate extensive knowledge with massive data. 5. Announcing the launch of the Medical AI Research Center (MedARC) Medical AI Research Center (MedARC) announced a new open and collaborative research center dedicated to advancing the field of AI in healthcare. MedARC aims to develop large AI models, also known as foundation models, for use in medicine and to build interdisciplinary teams that can address clinical needs. Three 5-minute reads/videos to keep you learning 1. Understanding Large Language Models \u2014 A Transformative Reading List In just five years, large language models (transformers) have revolutionized the field of natural language processing. To help researchers and practitioners get started with these models, this article provides a chronological reading list of academic research papers. The list covers the main architecture and tasks, scaling laws, improving efficiency, and steering large language models to intended goals and interests. 2. Machines Learn Better if We Teach Them the Basics Although AI agents have shown impressive performance in certain tasks, they often struggle to generalize to new environments and lack the abstract skills necessary to succeed in diverse contexts. This limitation arises from their limited foundation of concepts and the vast space of possibilities they must explore. To overcome this limitation, computer scientists are developing new techniques to teach machines foundational concepts before unleashing them into the wild. This article delves into the details of these emerging approaches and their potential impact on AI development. 3. Boltus, The God of AI \u2014 A four-episode series of learning to use AI with funny production This Twitter series is a four-episode guide to using AI, covering a range of topics. These include deploying diffusion models at scale, building text-to-image generators, integrating Stable Diffusion into a Slack workspace, and improving the speed of serving Stable Diffusion by 3x. 4. Solving a machine-learning mystery Researchers are studying a new concept called in-context learning, where a large language model can learn a new task after seeing only a few examples, without updating its parameters. This phenomenon could be explained by smaller, simpler linear models embedded in the larger model that can be trained to complete the new task using only existing information. This research sheds light on the learning algorithms that large models can use and could help models complete new tasks without costly retraining. Scientists from MIT, Google Research, and Stanford University are working to unravel this mystery. 5. The Most Important Job Skill of This Century A product race is underway in the world of artificial intelligence. AI evangelists believe generative AI will become the overlay for search engines, as well as creative work, memo writing, research, homework, sketching, outlining, storyboarding, and teaching. This means that the future of work could depend on how well people can talk to AI and the skill required to do so: prompt engineering. This article explains why. [&#8230;]",
            "pubdate": "Wed, 15 Feb 2023 09:37:10 +0000",
            "pubdate_parsed": [
                2023,
                2,
                15
            ],
            "email_sent": true
        },
        "Generative AI: The Future of Artificial Intelligence (AI)": {
            "url": "https://towardsai.net/p/generative-ai/generative-ai-the-future-of-artificial-intelligence-ai",
            "description": "Last Updated on February 14, 2023 by Editorial Team Source: Image generated by author via Midjourney Generative AI: The Future of Artificial Intelligence (AI) Creating the Future: How Generative AI is Set to Revolutionize Industries and Transform Society TL;DR: This article explores generative AI and provides an overview of its capabilities and applications. Generative AI involves the use of neural networks to create new content such as images, videos, or text. Its ability to create realistic and novel content has promising applications in fields such as entertainment, design, and medicine. It also raises ethical concerns around issues such as bias and the potential misuse of generated content. Disclaimer: This article uses Cohere for text generation. Generative AI is a fascinating field that has gained a lot of attention in recent years. It involves using machine learning algorithms to generate new data based on existing data. This technology has the potential to transform a wide range of industries, including healthcare, finance, and entertainment. In this article, we will explore what generative AI is, how it is being used today, and what the future holds for this exciting field. What is Generative AI? Generative AI is a subset of artificial intelligence (AI) that involves using algorithms to create new data. This can include anything from generating new images and videos to creating new text or music. The key difference between generative AI and other types of AI is that generative AI is focused on the creation of new data, rather than simply analyzing or processing existing data. Generative AI works by training algorithms on large datasets, which the algorithm can then use to generate new data. For example, a generative AI algorithm could be trained on a large dataset of images, and then use that training to create new, never-before-seen images. This approach has been used to create some incredible works of art, as well as some impressive technological innovations. How Is Generative AI Being Used\u00a0Today? Generative AI is being used in a wide range of industries today, from entertainment to healthcare. One of the most notable applications of generative AI is in the field of art, where it is being used to create stunning works of art that would be impossible for a human artist to create. In addition, generative AI is being used to create new music and even entire films. Another exciting application of generative AI is in the field of healthcare. Generative AI algorithms can be used to create new drugs, based on existing drugs or other data. This approach has the potential to revolutionize the field of medicine, allowing researchers to discover new treatments and cures faster than ever before. In the finance industry, generative AI is being used to create new financial models and trading algorithms. These algorithms can help traders and investors make more informed decisions, based on a wider range of data. This has the potential to make the financial markets more efficient and more profitable for everyone. What Are the Best Platforms for Generative AI Nowadays?? Cohere and OpenAI are two of the most widely used tools and platforms for generative AI. Cohere, a startup that specializes in natural language processing, has developed a reputation for creating sophisticated applications that can generate natural language with great accuracy. Their technology has been used to create chatbots, automated content generation, and many other natural language processing applications. OpenAI, on the other hand, is an AI research laboratory that was founded in 2015. The organization is dedicated to developing AI technologies that are safe and beneficial for society, with a particular focus on generative AI. OpenAI has created several tools for generative AI, including GPT-3, a powerful autoregressive language model that has received a great deal of attention for its ability to generate coherent and natural-sounding text. Both Cohere and OpenAI have made significant contributions to the field of generative AI, and their platforms and tools are widely used by researchers, developers, and organizations around the world. With the continued growth and development of generative AI, it is likely that we will see even more innovative tools and platforms emerging in the years to come. How to Get Started With Generative AI? Getting started with generative AI can be a daunting task, but it is not as difficult as you might think. The first step is to learn the basics of machine learning and deep learning, which are the technologies that underpin generative AI. There are many resources available online, including free courses and tutorials. Once you have a basic understanding of machine learning, you can start exploring generative AI by experimenting with different algorithms and datasets. There are many open-source libraries and tools available that can help you get started, including Cohere, OpenAI, or AI2Labs. Source: Image generated by author via Midjourney What Is the Future of Generative AI? Looking ahead, the future of generative AI is undoubtedly bright. As technology continues to evolve, we can expect to see even more advanced and sophisticated applications emerging in a wide range of industries. One of the most exciting prospects for the future of generative AI is the development of even more powerful algorithms that are capable of generating more complex and nuanced outputs. This could include everything from virtual reality environments to music and art, and it has the potential to transform the way we experience and interact with technology. Another important trend to watch in the future of generative AI is the growing focus on ethical and responsible AI development. With the potential of AI to impact society in profound ways, it is crucial that we take a responsible approach to its development and use. This includes ensuring that AI is used in ways that benefit society, and that it is designed to be transparent and explainable. Overall, there is no doubt that generative AI will play an increasingly important role in shaping the future of technology and society. As more researchers and developers continue to explore this field, we can expect to see [&#8230;]",
            "pubdate": "Wed, 15 Feb 2023 03:25:02 +0000",
            "pubdate_parsed": [
                2023,
                2,
                15
            ],
            "email_sent": true
        },
        "Top AI Conferences in 2023": {
            "url": "https://towardsai.net/p/artificial-intelligence/top-ai-conferences-in-2023",
            "description": "Last Updated on February 19, 2023 by Editorial Team Source: Image generated by the author with generative AI via Midjourney. Exploring the Top AI Conferences in 2023: A Must-Attend for Researchers and Practitioners Alike TL;DR: Get ahead in the AI game by attending these top conferences in 2023! From NeurIPS to KDD, these events bring together the best and brightest minds in the field to share the latest research, developments, and insights. Whether you\u2019re a researcher or practitioner, don\u2019t miss your chance to stay up-to-date and network with other professionals in the field. Disclaimer: This article uses Cohere for text generation. The world of artificial intelligence (AI) is rapidly advancing with new discoveries and breakthroughs emerging at an unprecedented pace. For researchers and practitioners in the field, staying current and connected is vital, and attending top AI conferences in 2023 can offer unique opportunities for collaboration, inspiration, and professional growth. From NeurIPS to KDD, these conferences bring together leading experts in machine learning, deep learning, natural language processing, and more. Whether you\u2019re an established researcher, an aspiring practitioner, or just passionate about the latest AI developments, these conferences are a must-attend. So join the excitement and start planning your trip to one of these top AI conferences in 2023. NeurIPS NeurIPS (the Conference on Neural Information Processing Systems) is one of the premier AI conferences, and it brings together researchers, practitioners, and industry professionals from around the world. The conference features keynote talks, paper presentations, workshops, and tutorials covering a broad range of topics in machine learning, deep learning, computer vision, natural language processing, and more. NeurIPS 2023 is set to take place in Vancouver, Canada, in December. Location: New Orleans, LA Conference Dates: Dec 10\u201316, 2023 ICML The International Conference on Machine Learning (ICML) is a leading forum for researchers, practitioners, and industry professionals to share and discuss the latest research and applications in machine learning. The conference features paper presentations, workshops, and tutorials covering a wide range of topics, including deep learning, reinforcement learning, and probabilistic modeling. ICML 2023 is scheduled to take place in Amsterdam, Netherlands, in July. Location: Honolulu, HI Conference Dates: Jul 23\u201329, 2023 ICLR The International Conference on Learning Representations (ICLR) is a premier conference in the field of deep learning, featuring papers, posters, and invited talks from leading researchers and practitioners. The conference covers a wide range of topics, including computer vision, natural language processing, and reinforcement learning. ICLR 2023 will be held in Sydney, Australia, in April. Location: Kigali, Rwanda Conference Dates: May 1\u20135, 2023 AISTATS The Conference on Artificial Intelligence and Statistics (AISTATS) brings together researchers in machine learning, statistics, and related fields to discuss the latest advances in AI and its applications. The conference features invited talks, paper presentations, and poster sessions covering a wide range of topics, including Bayesian inference, graphical models, and deep learning. AISTATS 2023 will be held in Barbados in April. Location: Palau de Congressos, Valencia, Spain Conference Dates: Apr 25\u201327, 2023 AAAI The Association for the Advancement of Artificial Intelligence (AAAI) hosts an annual conference that brings together researchers and practitioners in AI and related fields. The conference features technical presentations, invited talks, and workshops on a wide range of topics, including machine learning, natural language processing, robotics, and AI ethics. AAAI 2023 will be held in Austin, Texas, in February. Location: Washington, DC Conference Dates: February 7\u201314, 2023 KDD The ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) is one of the premier conferences in data mining and machine learning, featuring keynote talks, paper presentations, and workshops on a wide range of topics. The conference covers a range of topics, including data mining, machine learning, and artificial intelligence. KDD 2023 will be held in San Diego, California, in August. Location: Long Beach, CA Conference Dates: Aug 6\u201310, 2023 Attending the top AI conferences in 2023 can offer a wealth of opportunities for researchers and practitioners in the field. These conferences bring together the brightest minds in the industry to share their latest research, insights, and innovations. Whether you\u2019re looking to network with other professionals, learn from leading experts, or simply stay up-to-date with the latest developments, these events are not to be missed. So start planning your trip, mark your calendar, and get ready to be inspired by the cutting-edge research and ideas presented at these top AI conferences in 2023! Don\u2019t miss out on the chance to be a part of the forefront of AI research and development. Support me in this generative AI journey by becoming a member or by buying me a coffee. Follow me on Linkedin or my website to stay tuned on generative AI.",
            "pubdate": "Mon, 20 Feb 2023 00:09:08 +0000",
            "pubdate_parsed": [
                2023,
                2,
                20
            ],
            "email_sent": true
        },
        "We employed ChatGPT as an ML Engineer. This is what we learned": {
            "url": "https://towardsai.net/p/machine-learning/we-employed-chatgpt-as-an-ml-engineer-this-is-what-we-learned",
            "description": "Author(s): Eric Landau, Co-founder and CEO, Encord TLDR; Among the proliferation of recent use cases using the AI application ChatGPT, we ask whether it can be used to make improvements in other AI systems. We test it on a practical problem in a modality of AI in which it was not trained, computer vision, and report the results. The average over ChatGPT metric suggestions, achieves on average 10.1% improvement in precision and 34.4% improvement in recall over our random sample, using a purely data-centric metric-driven approach. The code for the post is here. Introduction Few technological developments have captured the public imagination as quickly and as starkly as the release of ChatGPT by OpenAI. Within two months of launch, it had garnered over 100 million users, the fastest public application to do so in history. While many see ChatGPT as a leap forward technologically, its true spark wasn\u2019t based on a dramatic technological step change (GPT3, the model it is based on has been around for almost 3 years), but instead on the fact that it was an AI application perfectly calibrated towards individual human interactions. It was its ostentatiousness as an AI system that could demonstrate real-world value that so thoroughly awed the public. Forms of AI are present in various aspects of modern life but are mostly hidden away (Google searches, Youtube recommendations, spam filters, identity recognition) in the background. ChatGPT is one of the few that is blatantly artificial, but also intelligent. AI being in the limelight has spawned a deluge of thought pieces, articles, videos, blog posts, and podcasts. Amid this content rush have been renewed questions and concerns around the more provocative implications of AI advancement, progression towards AI consciousness, artificial general intelligence(AGI), and a technological singularity. In the most speculative scenarios, the fear (or hope depending on who you ask) is that the sophistication, power, and complexity of our models will eventually breach an event horizon of breakaway intelligence, where the system develops the capability to iteratively self-improve both it\u2019s core functionality and it\u2019s own ability to self-improve. This can create an exponentially growing positive reinforcement cycle spurring an unknowable and irreversible transition of society and human existence as we know it. Not yet, ChatGPT To be clear\u2026 that\u2019s not where we are. ChatGPT is not the dawn of the Terminator. Prognosticating on the direction of a technology still in rapid development is often a folly, and on its broader implications even more so. However, in the presence of a large looming and unanswerable question, we can still develop insights and intuition by asking smaller, more palatable ones. In that vein, we look for a simpler question we can pose and subsequently test on the topic of AI self-improvement: Can we find an avenue by which we can examine if one AI system can iteratively improve another AI system? We observe that the main agents at the moment for AI progression are people working in machine learning as engineers and researchers. A sensible proxy sub-question might then be: Can ChatGPT function as a competent machine learning engineer? The Set Up If ChatGPT is to function as an ML engineer, it is best to run an inventory of the tasks that the role entails. The daily life of an ML engineer includes among others: Manual inspection and exploration of data Training models and evaluating model results Managing model deployments and model monitoring processes. Writing custom algorithms and scripts. The thread tying together the role is the fact that machine learning engineers have to be versatile technical problem solvers. Thus, rather than running through the full gamut of ML tasks for ChatGPT, we can focus on the more abstract and creative problem-solving elements of the role. We will narrow the scope in a few ways by having ChatGPT: Work specifically in the modality of computer vision: We chose computer vision both as it is our expertise and because as a large language model, ChatGPT, did not (as far as we know) have direct access to any visual media in its training process. It thus approaches this field from a purely conceptual point of view. Reason over one concrete toy problem: In honor of both the Python library we all know and love and the endangered animal we also all know and love, we pick our toy problem to be building a robust object detector of pandas. We will use data from the open-source YouTube-VOS dataset, which we have relabelled independently and with deliberate mistakes. Take an explicitly data-centric approach: We choose a data-centric methodology as it is often what we find has the highest leverage for practical model development. We can strip out much of the complication of model and parameter selection so that we can focus more on improving the data and labels being input into the model for training. Taking a more model-centric approach of running through hyperparameters and model architectures, while important, will push less on testing abstract reasoning abilities from ChatGPT. Use existing tools: To further simplify the task, we remove any dependence on internal tools ML engineers often build for themselves. ChatGPT (un)fortunately can\u2019t spend time in a Jupyter Notebook. We will leverage the Encord platform to simplify the model training/inference by using Encord\u2019s micro-models and running data, label, and model evaluation through the open-source tool Encord Active. The code for running the model training is presented below. With this narrowed scope in mind, our approach will be to use ChatGPT to write custom quality metrics through Encord Active that we can run over the data, labels, and model predictions to filter and clean data in our panda problem. Quality metrics are additional parametrizations over your data, labels, and models; they are methods of indexing training data and predictions in semantically interesting and relevant ways. Examples can include everything from more general attributes like the blurriness of an image to arbitrarily specific ones like the number of average distance between pedestrians in an image. ChatGPT\u2019s job as our ML engineer [&#8230;]",
            "pubdate": "Tue, 21 Feb 2023 13:00:06 +0000",
            "pubdate_parsed": [
                2023,
                2,
                21
            ],
            "email_sent": true
        },
        "Proving the Convexity of Log-Loss for Logistic Regression": {
            "url": "https://towardsai.net/p/l/proving-the-convexity-of-log-loss-for-logistic-regression",
            "description": "Last Updated on February 25, 2023 by Editorial Team Author(s): Towards AI Editorial Team Originally published on Towards AI. Unpacking Log Loss Error Function\u2019s Impact on Logistic Regression Photo by DeepMind on\u00a0Unsplash Author(s): Pratik\u00a0Shukla \u201cCourage is like a muscle. We strengthen it by use.\u201d\u200a\u2014\u200aRuth\u00a0Gordo Table of Contents: Proof of convexity of the log-loss function for logistic regression A visual look at BCE for logistic regression Resources and references Introduction In this tutorial, we will see why the log-loss function works better in logistic regression. Here, our goal is to prove that the log-loss function is a convex function for logistic regression. Once we prove that the log-loss function is convex for logistic regression, we can establish that it\u2019s a better choice for the loss function. Logistic regression is a widely used statistical technique for modeling binary classification problems. In this method, the log-odds of the outcome variable is modeled as a linear combination of the predictor variables. To estimate the parameters of the model, the maximum likelihood method is used, which involves optimizing the log-likelihood function. The log-likelihood function for logistic regression is typically expressed as the negative sum of the log-likelihoods of each observation. This function is known as the log-loss function or binary cross-entropy loss. In this blog post, we will explore the convexity of the log-loss function and why it is an essential property in optimization algorithms used in logistic regression. We will also provide a proof of the convexity of the log-loss function. Proof of convexity of the log-loss function for logistic regression: Let\u2019s mathematically prove that the log-loss function for logistic regression is\u00a0convex. We saw in the previous tutorial that a function is said to be a convex function if its second derivative is &#62;0. So, here we\u2019ll take the log-loss function and find its second derivative to see whether it\u2019s &#62;0 or not. If it\u2019s &#62;0, then we can say that it is a convex function. Here we are going to consider the case of a single trial to simplify the calculations. Step\u200a\u2014\u200a1: The following is a mathematical definition of the binary cross-entropy loss function (for a single\u00a0trial). Figure\u200a\u2014\u200a1: Binary Cross-Entropy loss for a single\u00a0trial Step\u200a\u2014\u200a2: The following is the predicted value (\u0177) for logistic regression. Figure\u200a\u2014\u200a2: The predicted probability for the given\u00a0example Step\u200a\u2014\u200a3: In the following image, z represents the linear transformation. Figure\u200a\u2014\u200a3: Linear transformation in forward propagation Step\u200a\u2014\u200a4: After that, we are modifying Step\u200a\u2014\u200a1 to reflect the values of Step\u200a\u2014\u200a3 and Step\u200a\u2014\u200a2. Figure\u200a\u2014\u200a4: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a5: Next, we are simplifying the terms in Step\u200a\u2014\u200a4. Figure\u200a\u2014\u200a5: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a6: Next, we are further simplifying the terms in Step\u200a\u2014\u200a5. Figure\u200a\u2014\u200a6: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a7: The following is the quotient rule for logarithms. Figure\u200a\u2014\u200a7: The quotient rule for logarithms Step\u200a\u2014\u200a8: Next, we are using the equation from Step\u200a\u2014\u200a7 to further simplify Step\u200a\u2014\u200a6. Figure\u200a\u2014\u200a8: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a9: In Step\u200a\u2014\u200a8, the value of log(1) is going to be\u00a00. Figure\u200a\u2014\u200a9: The value of\u00a0log(1)=0 Step\u200a\u2014\u200a10: Next, we are rewriting Step\u200a\u2014\u200a8 with the remaining terms. Figure\u200a\u2014\u200a10: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a11: The following is the power rule for logarithms. Figure\u200a\u2014\u200a11: Power rule for logarithms Step\u200a\u2014\u200a12: Next, we will use the power rule of logarithms to simplify the equation in Step\u200a\u2014\u200a10. Figure\u200a\u2014\u200a12: Applying the power\u00a0rule Step\u200a\u2014\u200a13: Next, we are replacing the values in Step\u200a\u2014\u200a10 with the values in Step\u200a\u2014\u200a12. Figure\u200a\u2014\u200a13: Using the power rule for logarithms Step\u200a\u2014\u200a14: Next, we are substituting the value of Step\u200a\u2014\u200a13 into Step\u200a\u2014\u200a10. Figure\u200a\u2014\u200a14: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Step\u200a\u2014\u200a15: Next, we are multiplying Step\u200a\u2014\u200a14 by (-1) on both\u00a0sides. Figure\u200a\u2014\u200a15: Binary Cross-Entropy loss for logistic regression for a single\u00a0trial Finding the First Derivative: Step\u200a\u2014\u200a16: Next, we are going to find the first derivative of\u00a0f(x). Figure\u200a\u2014\u200a16: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a17: Here we are distributing the partial differentiation sign to each\u00a0term. Figure\u200a\u2014\u200a17: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a18: Here we are applying the derivative rules. Figure\u200a\u2014\u200a18: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a19: Here we are finding the partial derivative of the last term of Step\u200a\u2014\u200a18. Figure\u200a\u2014\u200a19: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a20: Here we are finding the partial derivative of the first term of Step\u200a\u2014\u200a18. Figure\u200a\u2014\u200a20: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a21: Here we are putting together the results of Step\u200a\u2014\u200a19 and Step\u200a\u2014\u200a20. Figure\u200a\u2014\u200a21: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a22: Next, we are rearranging the terms of the equation in Step\u200a\u2014\u200a21. Figure\u200a\u2014\u200a22: Finding the first derivative of\u00a0f(w) Step\u200a\u2014\u200a23: Next, we are rewriting the equation in Step\u200a\u2014\u200a22. Figure\u200a\u2014\u200a23: Finding the first derivative of\u00a0f(w) Finding the Second Derivative: Step\u200a\u2014\u200a24: Next, we are going to find the second derivative of the function\u00a0f(x). Figure\u200a\u2014\u200a24: Finding the second derivative of\u00a0f(w) Step\u200a\u2014\u200a25: Here we are distributing the partial derivative to each\u00a0term. Figure\u200a\u2014\u200a25: Finding the second derivative of\u00a0f(w) Step\u200a\u2014\u200a26: Next, we are simplifying the equation in Step\u200a\u2014\u200a25 to remove redundant terms. Figure\u200a\u2014\u200a26: Finding the second derivative of\u00a0f(w) Step\u200a\u2014\u200a27: Here is the derivative rule for\u00a01/f(x). Figure\u200a\u2014\u200a27: The derivative rule for\u00a01/f(x) Step\u200a\u2014\u200a28: Next, we are finding the relevant term to plug-in in Step\u200a\u2014\u200a27. Figure\u200a\u2014\u200a28: Value of p(w) for derivative of\u00a01/p(w) Step\u200a\u2014\u200a29: Here we are finding the partial derivative term for Step\u200a\u2014\u200a27. Figure\u200a\u2014\u200a29: Value of p\u2019(w) for derivative of\u00a01/p(w) Step\u200a\u2014\u200a30: Here we are finding the squared term for Step\u200a\u2014\u200a27. Figure\u200a\u2014\u200a30: Value of p(w)\u00b2 for derivative of\u00a01/p(w) Step\u200a\u2014\u200a31: Here we are putting together all the terms of Step\u200a\u2014\u200a27. Figure\u200a\u2014\u200a31: Calculating the value of the derivative of\u00a01/p(w) Step\u200a\u2014\u200a32: Here we are simplifying the equation in Step\u200a\u2014\u200a31. Figure\u200a\u2014\u200a32: Calculating the value of the derivative of\u00a01/p(w) Step\u200a\u2014\u200a33: Next, we are putting together all the values in Step\u200a\u2014\u200a26. Figure\u200a\u2014\u200a33: Finding the second derivative of\u00a0f(w) Step\u200a\u2014\u200a34: Next, we are further simplifying the terms in Step\u200a\u2014\u200a33. Figure\u200a\u2014\u200a34: Finding the second derivative of\u00a0f(w) Alright! So, now we have the second derivative of the function f(x). Next, we need to find out whether this will be &#62;0 for all the values of x or not. If it is &#62;0 [&#8230;]",
            "pubdate": "Sat, 25 Feb 2023 09:00:33 +0000",
            "pubdate_parsed": [
                2023,
                2,
                25
            ],
            "email_sent": true
        },
        "A Web App for Automated E-mail Writing From Voice Notes, Using GPT-3": {
            "url": "https://towardsai.net/p/l/a-web-app-for-automated-e-mail-writing-from-voice-notes-using-gpt-3",
            "description": "Last Updated on February 25, 2023 by Editorial Team Author(s): LucianoSphere Originally published on Towards AI. I coupled Chrome&#x2019;s speech recognition engine with GPT-3 to create a web app that writes e-mails from your spoken notes and indications&#x2026; Continue reading on Towards AI Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Sat, 25 Feb 2023 09:00:26 +0000",
            "pubdate_parsed": [
                2023,
                2,
                25
            ],
            "email_sent": true
        },
        "Best Laptops for Deep Learning, Machine Learning (ML), and Data Science for2023": {
            "url": "https://towardsai.net/p/news/best-laptops-for-machine-learning-deep-learning-data-science-ml-f55602197593",
            "description": "Last Updated on March 5, 2023 by Editorial Team Source: Image generated with generative AI via Midjourney.\u00a0 Get ahead in the AI game with our top picks for laptops that are perfect for machine learning, data science, and deep learning at every budget. After analyzing over 8,000 options [8], we\u2019ve identified the best of the best to help future-proof your AI\u00a0rig. Last updated March 5, 2023 Are you tired of endlessly scouring the internet for the perfect laptop to power your machine learning, deep learning, and data science projects? Well, search no further! We\u2019ve done the heavy lifting for you by sifting through a whopping 8,000 laptops to bring you only the most powerful and efficient machines for every budget. Whether you\u2019re looking for top-of-the-line laptops or more affordable options, we\u2019ve got you covered. With our expert recommendations, you can stay ahead of the game and future-proof your AI setup. We\u2019ll keep updating this resource as technology evolves to provide you with even more powerful and efficient laptops for every budget. Our inbox is inundated with emails from fellow AI enthusiasts who are seeking the best laptops for their AI projects, which inspired us to create this list. If you have any suggestions to add to the list, please feel free to shoot us an email at pub@towardsai.net. Disclosure: Our editorial team at Towards AI writes authentic and trustworthy reviews and may receive a small compensation for products we select to support Towards AI\u2019s efforts. For this article, as an Amazon Associate, Towards AI may receive a small commission from qualifying purchases made from it (at no extra cost to the buyer). For feedback, questions, or concerns, please email us at pub@towardsai.net. Key Considerations for Choosing a Laptop for ML and Data\u00a0Science Processor: A powerful CPU is crucial for running complex ML algorithms and data analysis. Look for a laptop with an Intel Core i7 or i9 processor or an AMD Ryzen 7 or 9 processor. GPU: The GPU (graphics processing unit) is important for running deep learning algorithms, as it can handle parallel processing much faster than a CPU. Look for a laptop with a dedicated GPU, such as an NVIDIA GeForce or Quadro, or an AMD Radeon. RAM: The more RAM your laptop has, the better it can handle large data sets and complex ML models. Look for a laptop with at least 16GB of RAM, but 32GB or more is ideal. Storage: Data sets for deep learning and data science can be massive, so you\u2019ll want a laptop with plenty of storage. Look for a laptop with at least a 512GB SSD, or consider one with multiple drives or the ability to add external storage. Display: A high-quality display is important for visualizing data and models. Look for a laptop with a resolution of at least 1080p, or consider a 4K display for even more detail. You may also want to consider a laptop with a high color gamut, such as Adobe RGB or DCI-P3. By considering these factors, you can find a laptop that will handle the demands of deep learning, machine learning and data science, allowing you to work efficiently and effectively. ???? Check out our editorial recommendations for the best deep learning workstations. ???? Let\u2019s get started! For Budgets under $ 1,000.00\u00a0\u2193 Source: Amazon Lenovo Legion\u00a05 Updated: [Tie] Best laptop under $ 1k. Ideal for data leaders who care about AMD processors, excellent RAM size, and an RTX 3050ti GPU under a $ 1k budget. Specs: Processor: AMD Ryzen 5 5600H (Hexa-core, 12 Threads, base clock speed 3.3 GHz, max turbo to 4.2GHz, 16MB L3 Cache) Memory: 32GB (16GB x 2) DDR4 3200MHz Hard Drive: 2TB PCIe SSD GPU: Dedicated NVIDIA GeForce RTX 3050 Ti 4 GB GDDR6, Boost clock up to 1695MHz, TGP up to 95W Computing Power: 8.6 [9] Ports: 1x USB-C 3.2 Gen 2 (support data transfer, Power Delivery, and DisplayPort 1.4), 1x USB-C 3.2 Gen 2 (support data transfer and DisplayPort 1.4), 1x USB 3.2 Gen 1 (Always On), 3x USB 3.2 Gen 1 1x HDMI 2.1, 1x Ethernet (RJ-45), 1x Headphone/microphone combo jack (3.5mm) OS: Windows 11 Home Weight: 5.3 lbs Display: 15.6&#8243; Full HD (1920&#215;1080) IPS 250nits Anti-glare, 45% NTSC, 120Hz, FreeSync Connectivity: Wi-Fi 6, 11ax 2&#215;2 + Bluetooth 5.1 Battery life: Average ~ 4 hours. Grab one on Amazon Source: Amazon ASUS TUF Gaming\u00a0A15 Updated: [Tie] Best laptop under $ 1k. Ideal for data leaders who care about Intel processors, suitable RAM size, and RTX 3050ti GPUs under a $ 1k budget. Specs: Processor: AMD Ryzen 7 8-core Processor AMD R7\u20136800H 16 MB Cache, Base Clock 3.2Ghz, Max Boost Clock 4.7Ghz, Memory: 32GB DDR5 Memory Hard Drives: 1TB SSD GPU: NVIDIA GeForce RTX 3050 Ti 4 GB. Computing Power: 8.6 [9] Ports: 2 X USB 3.1 Type A &#124; 1 X DisplayPort &#124; 1 X RJ-45 &#124; 1 X Headphone/Speaker/Line-Out Jack &#124; 2 X USB 3.1 TYPE-C &#124; 1 X HDMI &#124; OS: Windows 10 Home. Weight: 4.85 lbs. Display: 144Hz Full HD 1920&#215;1080 display Connectivity: WiFi 802.11ax, Gigabit LAN (Ethernet), Bluetooth. Battery life: Average ~ 4 hours. Grab one on Amazon Source: Amazon Dell Mytrix\u00a0G15 Updated: Fantastic laptop under $ 1k. Ideal for those who care about AMD processors, suitable RAM size, and RTX 30XX GPUs under a $ 1k budget. Specs: Processor: AMD Ryzen 5 5600H 3.30 GHz Memory: 16 GB DDR4. Hard Drives: 512 GB NVMe SSD. GPU: NVIDIA GeForce RTX 3050 Ti 4 GB. Computing Power: 8.6 [9] Ports: 1x HDMI 2.0, 1x USB 3.1 Type-C, 2x USB 3.1, 1x USB 2.0. OS: Windows 10 Home. Weight: 5.39 lbs. Display: 15.6, 1920 x 1080. Connectivity: WiFi 802.11ax, Gigabit LAN (Ethernet), Bluetooth. Battery life: Average ~ 4 hours. Grab one on Amazon Performance comparison RTX 3060 vs. RTX 2070\u00a0[11] For Budgets under $ 2,000.00\u00a0\u2193 Source: Amazon Acer Predator Helios\u00a0300 Updated: [Tie] Best laptop under $ 2k. Ideal for data leaders who want the best under a $ 2k budget, care about Intel processors, [&#8230;]",
            "pubdate": "Mon, 06 Mar 2023 01:00:49 +0000",
            "pubdate_parsed": [
                2023,
                3,
                6
            ],
            "email_sent": true
        },
        "Correlation and Causation: What are the Differences?": {
            "url": "https://towardsai.net/p/machine-learning/correlation-and-causation-what-are-the-differences",
            "description": "Last Updated on March 23, 2023 by Editorial Team Author(s): Cornellius Yudha Wijaya &#160; Originally published on Towards AI. Learn the differences between both concepts Photo by Sam Moghadam Khamseh on Unsplash In statistics and data science, we often encounter correlation and causation terms. Albeit, the correlation was mentioned more often compared to causation. Nevertheless, their meaning is often associated with how two variables are related. However, correlation and causation are two different terms that stand by their definition. It&#8217;s two different concepts that might be intertwined and used together, but we still need to know the differences. So, what are correlation and causation? Also, what are the differences between them and their usefulness? Let&#8217;s explore the concept a little bit. Correlation and Causation Correlation Let&#8217;s&#8230; Read the full blog for free on Medium. Join thousands of data leaders on the AI newsletter. Join over 80,000 subscribers and keep up to date with the latest developments in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a\u00a0sponsor. Published via Towards AI",
            "pubdate": "Fri, 24 Mar 2023 00:11:07 +0000",
            "pubdate_parsed": [
                2023,
                3,
                24
            ],
            "email_sent": true
        },
        "Trends in AI  April 2023 // GPT-4, New Prompting Tricks, Zero-shot Video Generation": {
            "url": "https://towardsai.net/p/machine-learning/trends-in-ai-april-2023-gpt-4-new-prompting-tricks-zero-shot-video-generation",
            "description": "Last Updated on April 12, 2023 by Editorial Team Author(s): Sergi Castella i Sap\u00e9 Originally published on Towards AI. GPT-4 has arrived; it\u2019s already everywhere. ChatGPT plugins bring augmented LMs to the masses, new Language Model tricks are discovered, Diffusion models for video generation, Neural Radiance Fields, and more. Just three weeks after the announcement of GPT-4, it already feels like it\u2019s been with us forever. Meanwhile, an open letter with high-profile signatories calling for a stop on giant AI experiments went viral, and subsequently, the AGI discourse has been unleashed and Eliezer Yudkowsky\u2019s imminent superintelligence doom existential risk theories have made it to Times Magazine. If you\u2019ve got a case of existential angst over a hypothetical intelligence explosion, here\u2019s a phenomenal based take from Julian Togelius that can soothe your soul. With that out of the way, let\u2019s start looking at what happened recently in the AI world. ????\ufe0f News ChatGPT plugins: ChatGPT can now interact with external modules via natural language and act as an augmented language model. For instance, using WolframAlpha for information about the world and sound computation, or Kayak to search for flights, stays or rental cars. Italy banned ChatGPT temporarily last week on the grounds that it violates GDPR. While OpenAI complied with the ban, this has left the EU in a weird spot with growing uncertainties about what Language Model technologies will be allowed in the old continent. Stanford\u2019s Center for Research on Foundation Models (CRFM) unveiled Alpaca, an instruction-following model trained by distilling OpenAI\u2019s models using Meta\u2019s LLAMA as a base model. Since then, the past couple of weeks has seen a good amount of similar open-source distillations from GPT models, such as Vicuna (Post, Demo, Repo) an up to 13B instruction-following model trained by distilling from conversations people have shared from ChatGPT (via ShareGPT). Stanford released their annual AI Index Report for 2023, highlighting, among others, how much AI research has shifted from academia into the industry and quantifying the growth that the field has experienced in the past decade. Midjourney (an independent research lab) has the world in awe with its new v5 image generation model. Adobe is building competing products for its creative suit, but it looks like they are struggling to have on-par quality, as they\u2019re more cautious with training data to avoid using copyrighted data inadvertently. Runway \u2014 the company behind Stable Diffusion \u2014 has been touting their new video generation product Gen 2. Nvidia announced during their latest developer conference their efforts to become the leading foundry for large foundation models. Clients will be able to define a model they want to train and Nvidia will use their infrastructure and expertise to train the model for them. Meanwhile, Google outlined in more detail their latest TPU v4 accelerators in their latest paper. GitHub announced Copilot X, a big update to Copilot that adds chat and voice interface features, supports pull request completions, question answering on documentation, and adopts GPT-4. ???? Research This month, our selection of research includes GPT-4, applications of language models, diffusion models, computer vision, video generation, recommender systems, and neural radiance fields. 1. GPT-4 Technical Report // Sparks of Artificial General Intelligence: Early experiments with GPT-4 By S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang. \u2753 Why \u2192 The already famous GPT-4 from OpenAI has been the uncontested star this past month. But its release has generated more questions than its technical report chose to answer. Hence the addition here of the massive evaluation paper that examines its behavior in more detail. Of course, the writing of this very text was aided by GPT-4. ???? Key insights \u2192 This evaluation paper is filled with samples and anecdotes from GPT-4 experiments. While this cherry-picking approach is unapologetically motivated and biased, it turns out to be an essential tool for grasping the behavior of this powerful model. Not a replacement for the big tables with bold numbers, but a necessary companion. The hilarious example: how GPT-4 ability to draw a unicorn in TikZ (LaTeX) improved over time while the model was still under active development. Source: https://arxiv.org/pdf/2303.08774.pdf The 155-page evaluation report covers a vast range of topics, such as multimodal capabilities, mathematical reasoning, coding, human interaction, and societal influences. The authors argue that GPT-4 shows some behavior that could be labeled general intelligence while acknowledging its limitations and caveats. The cluelessness of Microsoft\u2019s researchers highlights the secrecy involved in this project: the authors from the very tech giant who partnered with OpenAI and provided the infrastructure for training GPT-4 didn\u2019t seem to have details of GPT-4 beyond having access to a mysterious API endpoint. 2. Larger language models do in-context learning differently By Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma. \u2753 Why \u2192 The emergence of complex in-context learning in large language models has piqued everyone\u2019s interest. This article delves into some niche but fascinating emerging capabilities from large language models that are not present in their smaller counterparts. ???? Key insights \u2192 Larger models possess unique abilities that smaller models simply can\u2019t replicate, no matter how much data and effort is put to into it. For example, large models can learn within the prompt to flip labels and learn new mappings, such as reversing the sentiment labels of sentences (e.g., positive sentences are labeled negative and vice versa). Source: https://arxiv.org/pdf/2303.03846.pdf The main emergence study reveals: Large models learn to flip labels, while smaller models stick to their pre-trained knowledge, continuing to label positive as positive and negative as negative. Semantically unrelated labels (SUL) emerge with scale, where the models label things with tokens that are not words. Instruction-tuned models strengthen both the use of semantic priors and the capacity to learn input-label mappings. However, they place more emphasis on the former aspect. 3. Reflexion: an autonomous [&#8230;]",
            "pubdate": "Wed, 12 Apr 2023 04:01:45 +0000",
            "pubdate_parsed": [
                2023,
                4,
                12
            ],
            "email_sent": true
        }
    },
    "Louis Bouchard Blog": {
        "An AI that Automatically Summarizes your Documents": {
            "url": "https://www.louisbouchard.ai/google-docs-summary/",
            "description": "A new model for automatically generating summaries using machine learning, released in Google Docs that you can already\u00a0use!",
            "pubdate": "Thu, 21 Apr 2022 12:22:43 GMT",
            "pubdate_parsed": [
                2022,
                4,
                21
            ],
            "email_sent": true
        },
        "Your Personal Photoshop Expert with AI!": {
            "url": "https://www.louisbouchard.ai/mystyle/",
            "description": "This AI can reconstruct, enhance and edit your\u00a0images!",
            "pubdate": "Thu, 28 Apr 2022 00:58:15 GMT",
            "pubdate_parsed": [
                2022,
                4,
                28
            ],
            "email_sent": true
        },
        "Meta's new model OPT is GPT-3's closest competitor! (and is open source)": {
            "url": "https://www.louisbouchard.ai/opt-meta/",
            "description": "An open-source model that is as powerful as\u00a0GPT-3!",
            "pubdate": "Fri, 06 May 2022 01:50:37 GMT",
            "pubdate_parsed": [
                2022,
                5,
                6
            ],
            "email_sent": true
        },
        "Deepmind's new model Gato isamazing!": {
            "url": "https://www.louisbouchard.ai/deepmind-gato/",
            "description": "Gato: A single Transformer to RuLe them all! The first generalist RL agent using transformers!",
            "pubdate": "Fri, 13 May 2022 13:38:56 GMT",
            "pubdate_parsed": [
                2022,
                5,
                13
            ],
            "email_sent": true
        },
        "This is a BIG step for GANs! BlobGAN Explained": {
            "url": "https://www.louisbouchard.ai/blobgan/",
            "description": "A GAN model that uses simple blobs to manipulate objects in images\u2026",
            "pubdate": "Fri, 13 May 2022 00:57:25 GMT",
            "pubdate_parsed": [
                2022,
                5,
                13
            ],
            "email_sent": true
        },
        "How Uber uses AI to serve you better": {
            "url": "https://www.louisbouchard.ai/uber-deepeta/",
            "description": "How can Uber deliver food and always arrive on time or a few minutes before?",
            "pubdate": "Sat, 21 May 2022 11:40:23 GMT",
            "pubdate_parsed": [
                2022,
                5,
                21
            ],
            "email_sent": true
        },
        "Google Brain's Answer to Dalle-e 2: Imagen": {
            "url": "https://www.louisbouchard.ai/google-brain-imagen/",
            "description": "An AI that creates photorealistic images from input text better than Dall-e\u00a02!",
            "pubdate": "Tue, 24 May 2022 03:29:37 GMT",
            "pubdate_parsed": [
                2022,
                5,
                24
            ],
            "email_sent": true
        },
        "How does dalle-mini work?": {
            "url": "https://www.louisbouchard.ai/dalle-mini/",
            "description": "Dalle mini is a free, open-source AI that produces amazing images from text inputs. Here's how it\u00a0works.",
            "pubdate": "Thu, 16 Jun 2022 02:40:55 GMT",
            "pubdate_parsed": [
                2022,
                6,
                16
            ],
            "email_sent": true
        },
        "No Language LeftBehind": {
            "url": "https://www.louisbouchard.ai/no-language-left-behind/",
            "description": "Translating 200 languages with a single model\u200a-\u200aMeta\u00a0AI",
            "pubdate": "Wed, 06 Jul 2022 13:11:04 GMT",
            "pubdate_parsed": [
                2022,
                7,
                6
            ],
            "email_sent": true
        },
        "What is data-centric AI?": {
            "url": "https://www.louisbouchard.ai/data-centric-ai/",
            "description": "The beginning of data-centric AI with data programming",
            "pubdate": "Fri, 08 Jul 2022 00:11:13 GMT",
            "pubdate_parsed": [
                2022,
                7,
                8
            ],
            "email_sent": true
        },
        "CVPR 2022 Best Paper Honorable Mention: Dual-Shutter Optical Vibration Sensing": {
            "url": "https://www.louisbouchard.ai/cvpr-2022-best-paper/",
            "description": "They reconstruct sound using cameras and a laser beam on any vibrating surface, allowing them to isolate music instruments, focus on a specific speaker, remove ambient noises, and many more amazing applications.",
            "pubdate": "Wed, 13 Jul 2022 13:36:57 GMT",
            "pubdate_parsed": [
                2022,
                7,
                13
            ],
            "email_sent": true
        },
        "How OpenAI Reduces risks for DALLE2": {
            "url": "https://www.louisbouchard.ai/how-openai-reduces-risks-for-dall-e-2/",
            "description": "DALL\u00b7E 2 Pre-Training Mitigations",
            "pubdate": "Sat, 16 Jul 2022 16:00:54 GMT",
            "pubdate_parsed": [
                2022,
                7,
                16
            ],
            "email_sent": true
        },
        "Produce Amazing Artworks with Text and Sketches!": {
            "url": "https://www.louisbouchard.ai/make-a-scene/",
            "description": "\"Make-A-Scene\": a fantastic blend between text and sketch-conditioned image generation.",
            "pubdate": "Tue, 19 Jul 2022 12:16:59 GMT",
            "pubdate_parsed": [
                2022,
                7,
                19
            ],
            "email_sent": true
        },
        "Build Animatable 3D Models with AI": {
            "url": "https://www.louisbouchard.ai/banmo/",
            "description": "Create deformable 3D models from pictures with\u00a0BANMo!",
            "pubdate": "Sat, 13 Aug 2022 13:39:19 GMT",
            "pubdate_parsed": [
                2022,
                8,
                13
            ],
            "email_sent": true
        },
        "Latent Diffusion Models: The Architecture behind Stable Diffusion": {
            "url": "https://www.louisbouchard.ai/latent-diffusion-models/",
            "description": "A High-Resolution Image Synthesis Architecture: Latent Diffusion",
            "pubdate": "Sat, 27 Aug 2022 14:41:59 GMT",
            "pubdate_parsed": [
                2022,
                8,
                27
            ],
            "email_sent": true
        },
        "One of the Most Challenging Tasks for AI": {
            "url": "https://www.louisbouchard.ai/psg/",
            "description": "A New Challenging Task for\u00a0AI:  panoptic scene graph generation",
            "pubdate": "Thu, 01 Sep 2022 00:19:49 GMT",
            "pubdate_parsed": [
                2022,
                9,
                1
            ],
            "email_sent": true
        },
        "Guiding Stable Diffusion with yourImages": {
            "url": "https://www.louisbouchard.ai/imageworthoneword/",
            "description": "Personalizing Text-to-Image Generation using Textual Inversion",
            "pubdate": "Fri, 02 Sep 2022 02:08:32 GMT",
            "pubdate_parsed": [
                2022,
                9,
                2
            ],
            "email_sent": true
        },
        "General Video Recognition with AI": {
            "url": "https://www.louisbouchard.ai/general-video-recognition/",
            "description": "What does such a model understand when it sees such a picture or, even more complex, a video?",
            "pubdate": "Thu, 08 Sep 2022 01:50:38 GMT",
            "pubdate_parsed": [
                2022,
                9,
                8
            ],
            "email_sent": true
        },
        "OpenAI's Most Recent Model: Whisper (explained)": {
            "url": "https://www.louisbouchard.ai/whisper/",
            "description": "A good transcription tool that would accurately understand what you say and write it\u00a0down",
            "pubdate": "Thu, 06 Oct 2022 01:25:29 GMT",
            "pubdate_parsed": [
                2022,
                10,
                6
            ],
            "email_sent": true
        },
        "3D Models from Text! DreamFusion Explained": {
            "url": "https://www.louisbouchard.ai/dreamfusion/",
            "description": "How AI generates 3d models from only text!",
            "pubdate": "Sat, 15 Oct 2022 01:08:42 GMT",
            "pubdate_parsed": [
                2022,
                10,
                15
            ],
            "email_sent": true
        },
        "Diffusion models: Everything you need to know": {
            "url": "https://www.louisbouchard.ai/diffusion-models/",
            "description": "Here's every vision application Diffusion models were a game changer in 2022: image, text, video, 3D, and more!",
            "pubdate": "Thu, 03 Nov 2022 10:19:55 GMT",
            "pubdate_parsed": [
                2022,
                11,
                3
            ],
            "email_sent": true
        },
        "InfiniteNature-Zero: Fly Into Your Pictures With AI!": {
            "url": "https://www.louisbouchard.ai/infinitenature-zero/",
            "description": "Generate infinite new frames as if you would be flying into your image!",
            "pubdate": "Thu, 17 Nov 2022 02:11:33 GMT",
            "pubdate_parsed": [
                2022,
                11,
                17
            ],
            "email_sent": true
        },
        "Galactica: What is it and What Happened?": {
            "url": "https://www.louisbouchard.ai/galactica/",
            "description": "Galactica, Meta AI's most recent model: The AI Scientist",
            "pubdate": "Tue, 22 Nov 2022 03:46:58 GMT",
            "pubdate_parsed": [
                2022,
                11,
                22
            ],
            "email_sent": true
        },
        "What is ChatGPT?": {
            "url": "https://www.louisbouchard.ai/chatgpt/",
            "description": "OpenAI's most recent conversational AI explained",
            "pubdate": "Tue, 06 Dec 2022 01:24:41 GMT",
            "pubdate_parsed": [
                2022,
                12,
                6
            ],
            "email_sent": true
        },
        "Prompting Explained: How to talk toChatGPT": {
            "url": "https://www.louisbouchard.ai/prompting-explained/",
            "description": "What is a prompt engineer and how to improve at\u00a0it\u2026",
            "pubdate": "Tue, 13 Dec 2022 01:53:03 GMT",
            "pubdate_parsed": [
                2022,
                12,
                13
            ],
            "email_sent": true
        },
        "Automatic Re-Aging with AI! Disneys FRAN Model Explained": {
            "url": "https://www.louisbouchard.ai/disney-re-age/",
            "description": "Disney's New Model Explained",
            "pubdate": "Thu, 22 Dec 2022 00:42:54 GMT",
            "pubdate_parsed": [
                2022,
                12,
                22
            ],
            "email_sent": true
        },
        "An interview with the Director of Perception at Zoox: Ruijie (RJ) He: What is an ML Engineer and more...": {
            "url": "https://www.louisbouchard.ai/rj-he-zoox/",
            "description": "An interview with the Director of Perception at Zoox, Ruijie (RJ) He, with the goal of demystifying what is a good profile to get an ML engineer job and perform at the interviews.",
            "pubdate": "Tue, 10 Jan 2023 02:19:34 GMT",
            "pubdate_parsed": [
                2023,
                1,
                10
            ],
            "email_sent": true
        },
        "Image Editing from Text Instructions: InstructPix2Pix": {
            "url": "https://www.louisbouchard.ai/instructpix2pix/",
            "description": "The hottest image editing AI, InstructPix2Pix, explained!",
            "pubdate": "Thu, 26 Jan 2023 02:18:42 GMT",
            "pubdate_parsed": [
                2023,
                1,
                26
            ],
            "email_sent": true
        },
        "Generating music withAI!": {
            "url": "https://www.louisbouchard.ai/musiclm/",
            "description": "MusicLM explained",
            "pubdate": "Tue, 31 Jan 2023 01:28:14 GMT",
            "pubdate_parsed": [
                2023,
                1,
                31
            ],
            "email_sent": true
        },
        "Gen-1, the future of storytelling?": {
            "url": "https://www.louisbouchard.ai/gen-1/",
            "description": "Turn mockups into videos automatically!",
            "pubdate": "Mon, 06 Mar 2023 12:51:05 GMT",
            "pubdate_parsed": [
                2023,
                3,
                6
            ],
            "email_sent": true
        },
        "Interview with NVIDIA Data Scientist Meriem Bendris & RTX 4080 Giveaway!": {
            "url": "https://www.louisbouchard.ai/nvidia-meriem-bendris/",
            "description": "What is it like, and how to get there (and RTX 4080 giveaway)",
            "pubdate": "Tue, 07 Mar 2023 12:37:36 GMT",
            "pubdate_parsed": [
                2023,
                3,
                7
            ],
            "email_sent": true
        },
        "How to Build a strong Data Science Resume with Kaggle": {
            "url": "https://www.louisbouchard.ai/build-a-strong-data-science-resume-with-kaggle/",
            "description": "An interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA",
            "pubdate": "Thu, 09 Mar 2023 03:38:32 GMT",
            "pubdate_parsed": [
                2023,
                3,
                9
            ],
            "email_sent": true
        },
        "How good is GPT-4?": {
            "url": "https://www.louisbouchard.ai/gpt-4/",
            "description": "If you thought ChatGPT was good, wait before you try this\u00a0one\u2026",
            "pubdate": "Wed, 15 Mar 2023 01:36:24 GMT",
            "pubdate_parsed": [
                2023,
                3,
                15
            ],
            "email_sent": true
        },
        "What is a Solution Architect atNVIDIA?": {
            "url": "https://www.louisbouchard.ai/solution-architect/",
            "description": "An interview with Adam Grzywaczewski, senior data scientist at NVIDIA",
            "pubdate": "Sun, 19 Mar 2023 03:30:57 GMT",
            "pubdate_parsed": [
                2023,
                3,
                19
            ],
            "email_sent": true
        },
        "Googles New AI Robot Can See and Understands Language! (PaLM-E)": {
            "url": "https://www.louisbouchard.ai/palm-e/",
            "description": "Google's New AI Model PaLM-E  Explained",
            "pubdate": "Thu, 23 Mar 2023 02:04:31 GMT",
            "pubdate_parsed": [
                2023,
                3,
                23
            ],
            "email_sent": true
        }
    },
    "Computational Intelligence Blog": {
        "Evolving Systems, Volume 13, issue 3, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/05/evolving-systems-volume-13-issue-3-june.html",
            "description": "<div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09396-z\">Dimensionality reduction in the context of dynamic social media data streams</a></div><div><b>Author(s): </b>Moritz Heusinger, Christoph Raab, Frank-Michael Schleif</div><div><b>Pages: </b>387 - 401</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09403-3\">Evolving and explainable clinical risk assessment at the edge</a></div><div><b>Author(s):&nbsp;</b>Andrea Pazienza, Roberto Anglani...Felice Vitulano</div><div><b>Pages:&nbsp;</b>403 - 422</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09422-8\">Evolving hyperbox fuzzy modeling</a></div><div><b>Author(s):&nbsp;</b>Alisson Porto, Fernando Gomide</div><div><b>Pages:&nbsp;</b>423 - 434</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09435-3\">Deep recurrent Gaussian Nesterovs recommendation using multi-agent in social networks</a></div><div><b>Author(s):&nbsp;</b>Vinita Tapaskar, Mallikarjun M. Math</div><div><b>Pages:&nbsp;</b>435 - 452</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09402-4\">A novel improved arithmetic optimization algorithm for optimal design of PID controlled and Bode\u2019s ideal transfer function based automobile cruise control system</a></div><div><b>Author(s):&nbsp;</b>Davut Izci, Serdar Ekinci...Erdal Eker</div><div><b>Pages:&nbsp;</b>453 - 468</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09402-4\">Score level fusion of major and minor finger knuckle patterns based symmetric sum-based rules for person authentication</a></div><div><b>Author(s):&nbsp;</b>Rabah Hammouche, Abdelouahab Attia, Samir Akhrouf</div><div><b>Pages:&nbsp;</b>469 - 483</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09416-y\">Ant colony optimization technique for integrating renewable DG in distribution system with techno-economic objectives</a></div><div><b>Author(s):&nbsp;</b>Nisha R. Godha, Vishram N. Bapat, Iranna Korachagaon</div><div><b>Pages:&nbsp;</b>485 - 498</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09438-0\">An evolutionary approach with reliability priority to design Scada systems for water reservoirs</a></div><div><b>Author(s):&nbsp;</b>Ali Dolatshahi Zand, Kaveh Khalili-Damghani, Sadigh Raissi</div><div><b>Pages:&nbsp;</b>499 - 517</div><div><br /></div>",
            "pubdate": "2022-05-23T21:22:00.000+12:00",
            "pubdate_parsed": [
                2022,
                5,
                23
            ],
            "email_sent": true
        },
        "IEEE Transactions on Emerging Topics in Computational Intelligence, Issue 3, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/05/ieee-transactions-on-emerging-topics-in.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9432948/\">Training a Quantum Annealing Based Restricted Boltzmann Machine on Cybersecurity Data</a></div><div><b>Author(s): </b>Vivek Dixit, Raja Selvarajan, Tamer Aldwairi, Yaroslav Koshka, Mark A. Novotny, Travis S. Humble, Muhammad A. Alam, Sabre Kais</div><div><b>Pages: </b>417 - 428</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9457027/\">Deep Learning in Physics: A Study of Dielectric Quasi-Cubic Particles in a Uniform Electric Field</a></div><div><b>Author(s):&nbsp;</b>Zhe Wang, Claude Guet</div><div><b>Pages:&nbsp;</b>429 - 438</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9456987/\">Double Fourier Integral Analysis Based Convolutional Neural Network Regression for High-Frequency Energy Disaggregation</a></div><div><b>Author(s):&nbsp;</b>Pascal A. Schirmer, Iosif Mporas</div><div><b>Pages:&nbsp;</b>439 - 449</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9455355/\">TDM: Trustworthy Decision-Making Via Interpretability Enhancement</a></div><div><b>Author(s):&nbsp;</b>Daoming Lyu, Fangkai Yang, Hugh Kwon, Wen Dong, Levent Yilmaz, Bo Liu</div><div><b>Pages:&nbsp;</b>450 - 461</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9460321/\">Privacy-Preserving Context-Based Electric Vehicle Dispatching for Energy Scheduling in Microgrids: An Online Learning Approach</a></div><div><b>Author(s):&nbsp;</b>Yichen Liu, Pan Zhou, Lei Yang, Yulei Wu, Zichuan Xu, Kai Liu, Xiumin Wang</div><div><b>Pages:&nbsp;</b>462 - 478</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9448192/\">Reward-Reinforced Generative Adversarial Networks for Multi-Agent Systems</a></div><div><b>Author(s):&nbsp;</b>Changgang Zheng, Shufan Yang, Juan Marcelo Parra-Ullauri, Antonio Garcia-Dominguez, Nelly Bencomo</div><div><b>Pages:&nbsp;</b>479 - 488</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9448189/\">Multi-UAV Mobile Edge Computing and Path Planning Platform Based on Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Huan Chang, Yicheng Chen, Baochang Zhang, David Doermann</div><div><b>Pages:&nbsp;</b>489 - 498</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9543522/\">Deep Reinforcement Learning Based Pricing Strategy of Aggregators Considering Renewable Energy</a></div><div><b>Author(s):&nbsp;</b>Yu-Chieh Chuang, Wei-Yu Chiu</div><div><b>Pages:&nbsp;</b>499 - 508</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9615099/\">Person Re-Identification With Multi-Features Based on Evolutionary Algorithm</a></div><div><b>Author(s):&nbsp;</b>Lixia Zhang, Kangshun Li, Yu Qi</div><div><b>Pages:&nbsp;</b>509 - 518</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9521230/\">VRConvMF: Visual Recurrent Convolutional Matrix Factorization for Movie Recommendation</a></div><div><b>Author(s):&nbsp;</b>Zhu Wang, Honglong Chen, Zhe Li, Kai Lin, Nan Jiang, Feng Xia</div><div><b>Pages:&nbsp;</b>519 - 529</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9525180/\">Intelligent Decision Support and Fusion Models for Fault Detection and Location in Power Grids</a></div><div><b>Author(s):&nbsp;</b>Hossein Hassani, Roozbeh Razavi-Far, Mehrdad Saif, Jafar Zarei, Frede Blaabjerg</div><div><b>Pages:&nbsp;</b>530 - 543</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9336053/\">Furnace-Grouping Problem Modeling and Multi-Objective Optimization for Special Aluminum</a></div><div><b>Author(s):&nbsp;</b>Hao Zhang, Lianbo Ma, Junyi Wang, Liang Wang</div><div><b>Pages:&nbsp;</b>544 - 555</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9314924/\">O-SegNet: Robust Encoder and Decoder Architecture for Objects Segmentation From Aerial Imagery Data</a></div><div><b>Author(s):&nbsp;</b>Karuna Kumari Eerapu, Shyam Lal, A. V. Narasimhadhan</div><div><b>Pages:&nbsp;</b>556 - 567</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9739038/\">Multiagent Reinforcement Learning for Community Energy Management to Mitigate Peak Rebounds Under Renewable Energy Uncertainty</a></div><div><b>Author(s):&nbsp;</b>Bo-Chen Lai, Wei-Yu Chiu, Yuan-Po Tsai</div><div><b>Pages:&nbsp;</b>568 - 579</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9422365/\">Attend to Where and When: Cascaded Attention Network for Facial Expression Recognition</a></div><div><b>Author(s):&nbsp;</b>Xiaoye Qu, Zhikang Zou, Xinxing Su, Pan Zhou, Wei Wei, Shiping Wen, Dapeng Wu</div><div><b>Pages:&nbsp;</b>580 - 592</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9512550/\">TMFNet: Three-Input Multilevel Fusion Network for Detecting Salient Objects in RGB-D Images</a></div><div><b>Author(s):&nbsp;</b>Wujie Zhou, Sijia Pan, Jingsheng Lei, Lu Yu</div><div><b>Pages:&nbsp;</b>593 - 601</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9349964/\">Optimized Bezier Curve Based Intensity Mapping Scheme for Low Light Image Enhancement</a></div><div><b>Author(s):&nbsp;</b>Magudeeswaran Veluchamy, Ashish Kumar Bhandari, Bharath Subramani</div><div><b>Pages:&nbsp;</b>602 - 612</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9354230/\">A Generalized Deep Neural Network Approach for Digital Watermarking Analysis</a></div><div><b>Author(s):&nbsp;</b>Weiping Ding, Yurui Ming, Zehong Cao, Chin-Teng Lin</div><div><b>Pages:&nbsp;</b>613 - 627</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9386216/\">Remodelling State-Space Prediction With Deep Neural Networks for Probabilistic Load Forecasting</a></div><div><b>Author(s):&nbsp;</b>Parul Arora, Abbas Khosravi, B. K. Panigrahi, P. N. Suganthan</div><div><b>Pages:&nbsp;</b>628 - 637</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9395505/\">Frequent Pattern Mining in Big Social Graphs</a></div><div><b>Author(s):&nbsp;</b>Lei Li, Ping Ding, Huanhuan Chen, Xindong Wu</div><div><b>Pages:&nbsp;</b>638 - 648</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9410461/\">Anomaly Detection in Resource Constrained Environments With Streaming Data</a></div><div><b>Author(s):&nbsp;</b>Prarthi Jain, Seemandhar Jain, Osmar R. Za\u00efane, Abhishek Srivastava</div><div><b>Pages:&nbsp;</b>649 - 659</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9439895/\">Information Cartography in Association Rule Mining</a></div><div><b>Author(s):&nbsp;</b>Iztok Fister, Iztok Fister</div><div><b>Pages:&nbsp;</b>660 - 676</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9430169/\">Multi-Label Classification Using Binary Tree of Classifiers</a></div><div><b>Author(s):&nbsp;</b>Anwesha Law, Ashish Ghosh</div><div><b>Pages:&nbsp;</b>677 - 689</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9576079/\">A New Semi-Supervised Fault Diagnosis Method via Deep CORAL and Transfer Component Analysis</a></div><div><b>Author(s):&nbsp;</b>Xinyu Li, Zhao Zhang, Liang Gao, Long Wen</div><div><b>Pages:&nbsp;</b>690 - 699</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9509400/\">StreamSoNG: A Soft Streaming Classification Approach</a></div><div><b>Author(s):&nbsp;</b>Wenlong Wu, James M. Keller, Jeffrey Dale, James C. Bezdek</div><div><b>Pages:&nbsp;</b>700 - 709</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9509578/\">Granular Symmetric Implicational Method</a></div><div><b>Author(s):&nbsp;</b>Yiming Tang, Witold Pedrycz, Fuji Ren</div><div><b>Pages:&nbsp;</b>710 - 723</div><div><br /></div>",
            "pubdate": "2022-05-27T09:24:00.000+12:00",
            "pubdate_parsed": [
                2022,
                5,
                26
            ],
            "email_sent": true
        },
        "IEEE Transactions on Artificial Intelligence, Volume 3, Issue 3, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/05/ieee-transactions-on-artificial.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9664332/\">A Survey on Masked Facial Detection Methods and Datasets for Fighting Against COVID-19</a></div><div><b>Author(s): </b>Bingshu Wang, Jiangbin Zheng, C. L. Philip Chen</div><div><b>Pages: </b>323 - 343</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9645324/\">FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning</a></div><div><b>Author(s):&nbsp;</b>Indro Spinelli, Simone Scardapane, Amir Hussain, Aurelio Uncini</div><div><b>Pages:&nbsp;</b>344 - 354</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9566791/\">Assessment of the Clusterability of Data Using a Multimodal Convolutional Neural Network</a></div><div><b>Author(s):&nbsp;</b>Niko Reunanen, Tomi R\u00e4ty, Timo Lintonen, Juho J. Jokinen</div><div><b>Pages:&nbsp;</b>355 - 369</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9744422/\">Unsupervised Domain-Adaptation-Based Tensor Feature Learning With Structure Preservation</a></div><div><b>Author(s):&nbsp;</b>Ali Braytee, Mohamad Naji, Paul J. Kennedy</div><div><b>Pages:&nbsp;</b>370 - 380</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9612606/\">Quick Learning Mechanism With Cross-Domain Adaptation for Intelligent Fault Diagnosis</a></div><div><b>Author(s):&nbsp;</b>Arun K. Sharma, Nishchal K. Verma</div><div><b>Pages:&nbsp;</b>381 - 390</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9640487/\">Distributed GAN: Toward a Faster Reinforcement-Learning-Based Architecture Search</a></div><div><b>Author(s):&nbsp;</b>Jiachen Shi, Yi Fan, Guoqiang Zhou, Jun Shen</div><div><b>Pages:&nbsp;</b>391 - 401</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9668998/\">Spherical Linguistic Petri Nets for Knowledge Representation and Reasoning Under Large Group Environment</a></div><div><b>Author(s):&nbsp;</b>Xun Mou, Ling-Xiang Mao, Hu-Chen Liu, MengChu Zhou</div><div><b>Pages:&nbsp;</b>402 - 413</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9699063/\">Distributed Semisupervised Partial Label Learning Over Networks</a></div><div><b>Author(s):&nbsp;</b>Ying Liu, Zhen Xu, Chen Zhang</div><div><b>Pages:&nbsp;</b>414 - 425</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9684723/\">System Neural Network: Evolution and Change Based Structure Learning</a></div><div><b>Author(s):&nbsp;</b>Animesh Chaturvedi, Aruna Tiwari, Shubhangi Chaturvedi, Pietro Li\u00f2</div><div><b>Pages:&nbsp;</b>426 - 435</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9648029/\">Evolutionary Neural Architecture Search for Automatic Esophageal Lesion Identification and Segmentation</a></div><div><b>Author(s):&nbsp;</b>Yao Zhou, Xianglei Yuan, Xiaozhi Zhang, Wei Liu, Yu Wu, Gary G. Yen, Bing Hu, Zhang Yi</div><div><b>Pages:&nbsp;</b>436 - 450</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9576645/\">Robot Path Planning via Neural-Network-Driven Prediction</a></div><div><b>Author(s):&nbsp;</b>Jiankun Wang, Jianbang Liu, Weinan Chen, Wenzheng Chi, Max Q.-H. Meng</div><div><b>Pages:&nbsp;</b>451 - 460</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9612011/\">Bidirectional Gated Recurrent Unit-Based Lower Upper Bound Estimation Method for Wind Power Interval Prediction</a></div><div><b>Author(s):&nbsp;</b>Fang Liu, Qing Tao, Dechang Yang, Denis Sidorov</div><div><b>Pages:&nbsp;</b>461 - 469</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9691915/\">Attacks on Data-Driven Process Monitoring Systems: Subspace Transfer Networks</a></div><div><b>Author(s):&nbsp;</b>Xiaoyu Jiang, Zhiqiang Ge</div><div><b>Pages:&nbsp;</b>470 - 484</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9653852/\">Deep Multiscale Multi-Instance Networks With Regional Scoring for Mammogram Classification</a></div><div><b>Author(s):&nbsp;</b>Wenjie Liu, Xin Shu, Lei Zhang, Dong Li, Qing Lv</div><div><b>Pages:&nbsp;</b>485 - 496</div><div><br /></div><div><br /></div>",
            "pubdate": "2022-05-30T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                5,
                30
            ],
            "email_sent": true
        },
        "Weekly Review 3 June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/weekly-review-3-june-2022.html",
            "description": "Some interesting links that I&nbsp;<a href=\"https://twitter.com/DrMikeWatts\">Tweeted</a>&nbsp;about this week:<br /><p></p><div> <b>1)</b> An improved method for detecting deepfake images, using an improved training set: <a href=\"https://dataconomy.com/2022/05/detecting-deepfakes-self-blended-images/\">https://dataconomy.com/2022/05/detecting-deepfakes-self-blended-images/</a></div><div><br /></div><div> <b>2)</b> Why am I not surprised that it's Amazon that is leading the way in using AI enabled cameras to monitor its staff? <a href=\"https://www.theregister.com/2022/05/24/ai_cameras_amazon/\">https://www.theregister.com/2022/05/24/ai_cameras_amazon/</a></div><div><br /></div><div><b>3)</b> Google has demoed a new conversational AI, but it's still not available to anyone else: <a href=\"https://www.datanami.com/2022/05/16/google-debuts-lamda-2-conversational-ai-system-and-ai-test-kitchen/\">https://www.datanami.com/2022/05/16/google-debuts-lamda-2-conversational-ai-system-and-ai-test-kitchen/</a></div><div><br /></div><div><b>4)</b> Two opposing schools of thought on the future of artificial general intelligence: <a href=\"https://dataconomy.com/2022/05/future-of-artificial-general-intelligence/\">https://dataconomy.com/2022/05/future-of-artificial-general-intelligence/</a></div><div><br /></div><div><b>5)</b> Yet another supercomputer for AI. <a href=\"https://www.theregister.com/2022/05/25/hpe_cerebras_lrz/\">https://www.theregister.com/2022/05/25/hpe_cerebras_lrz/</a> At which point do these supercomputers start to encounter diminishing returns? That is, when will throwing more processing power at a problem no longer yield better results?</div><div><br /></div><div><b>6)</b> The ethics of AI must also or even primarily include the ethics of the data sets used. This increasingly means respect the privacy of data: <a href=\"https://www.theregister.com/2022/05/23/clearview_ai_ico_fine/\">https://www.theregister.com/2022/05/23/clearview_ai_ico_fine/</a></div><div><br /></div><div><b>7)</b> The advantages and disadvantages of using AI in manufacturing: <a href=\"https://www.datasciencecentral.com/pros-and-cons-of-ai-in-manufacturing/\">https://www.datasciencecentral.com/pros-and-cons-of-ai-in-manufacturing/</a></div><div><br /></div><div><b>8)</b> Another cloud-based #AI platform launches: <a href=\"https://www.datanami.com/2022/05/11/graft-emerges-from-stealth-to-make-the-ai-of-the-1-accessible-to-the-99/\">https://www.datanami.com/2022/05/11/graft-emerges-from-stealth-to-make-the-ai-of-the-1-accessible-to-the-99/</a></div><div><br /></div><div><b>9)</b> A COVID19 treatment discovered by AI: <a href=\"https://www.theregister.com/2022/05/25/covid19_medication_ai/\">https://www.theregister.com/2022/05/25/covid19_medication_ai/</a></div><div><br /></div><div><b>10)</b> You cannot trust a&nbsp; Machine Learning model you didn't train yourself: <a href=\"https://dataconomy.com/2022/05/undetectable-backdoors-machine-learning/\">https://dataconomy.com/2022/05/undetectable-backdoors-machine-learning/</a></div><div><br /></div><div style=\"text-align: left;\"><b>11)</b> A tool from Google that uses AI to help you prepare for job interviews: <a href=\"https://dataconomy.com/2022/05/google-interview-warmup-ai-in-recruitment/\">https://dataconomy.com/2022/05/google-interview-warmup-ai-in-recruitment/</a>&nbsp;</div><p><b>12)</b>&nbsp;Google has banned the creation of DeepFake models on its Colab platform: <a href=\"https://techcrunch.com/2022/06/01/2328459/\">https://techcrunch.com/2022/06/01/2328459/&nbsp;</a></p><p><b>13)</b>&nbsp;A list of articles on ethics in AI: <a href=\"https://www.informationweek.com/big-data/quick-study-artificial-intelligence-ethics-and-bias\">https://www.informationweek.com/big-data/quick-study-artificial-intelligence-ethics-and-bias</a></p><p><b>14)</b>&nbsp;If an AI defamed someone, who would be liable? <a href=\"https://techcrunch.com/2022/06/01/whos-liable-for-ai-generated-lies/\">https://techcrunch.com/2022/06/01/whos-liable-for-ai-generated-lies/</a></p><p></p>",
            "pubdate": "2022-06-03T23:00:00.013+12:00",
            "pubdate_parsed": [
                2022,
                6,
                3
            ],
            "email_sent": true
        },
        "IEEE Transactions on Evolutionary Computation, Volume 26, Issue 3, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/ieee-transactions-on-evolutionary.html",
            "description": "<div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9507530/\">PSO-<i>X</i>: A Component-Based Framework for the Automatic Design of Particle Swarm Optimization Algorithms</a></div><div><b>Author(s): </b>Christian L. Camacho-Villal\u00f3n, Marco Dorigo, Thomas St\u00fctzle</div><div><b>Pages: </b>402 - 416</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9477421/\">MO4: A Many-Objective Evolutionary Algorithm for Protein Structure Prediction</a></div><div><b>Author(s):&nbsp;</b>Zhenyu Lei, Shangce Gao, Zhiming Zhang, MengChu Zhou, Jiujun Cheng</div><div><b>Pages:&nbsp;</b>417 - 430</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9512284/\">Dynamic Optimization in Fast-Changing Environments via Offline Evolutionary Search</a></div><div><b>Author(s):&nbsp;</b>Xiaofen Lu, Ke Tang, Stefan Menzel, Xin Yao</div><div><b>Pages:&nbsp;</b>431 - 445</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9496593/\">Evolutionary Multitasking for Feature Selection in High-Dimensional Classification via Particle Swarm Optimization</a></div><div><b>Author(s):&nbsp;</b>Ke Chen, Bing Xue, Mengjie Zhang, Fengyu Zhou</div><div><b>Pages:&nbsp;</b>446 - 460</div><div><br /></div><div><b>5) </b><a href=\"https://ieeexplore.ieee.org/document/9518387/\">A Cooperative Memetic Algorithm With Learning-Based Agent for Energy-Aware Distributed Hybrid Flow-Shop Scheduling</a></div><div><b>Author(s):&nbsp;</b>Jing-Jing Wang, Ling Wang</div><div><b>Pages:&nbsp;</b>461 - 475</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9481257/\">Analyzing Dominance Move (MIP-DoM) Indicator for Multiobjective and Many-Objective Optimization</a></div><div><b>Author(s):&nbsp;</b>Claudio Lucio do Val Lopes, Fl\u00e1vio Vin\u00edcius Cruzeiro Martins, Elizabeth Fialho Wanner, Kalyanmoy Deb</div><div><b>Pages:&nbsp;</b>476 - 489</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9524335/\">Black-Box Optimization Revisited: Improving Algorithm Selection Wizards Through Massive Benchmarking</a></div><div><b>Author(s):&nbsp;</b>Laurent Meunier, Herilalaina Rakotoarison, Pak Kan Wong, Baptiste Roziere, J\u00e9r\u00e9my Rapin, Olivier Teytaud, Antoine Moreau, Carola Doerr</div><div><b>Pages:&nbsp;</b>490 - 500</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9531957/\">Theory of (1 + 1) ES on the RIDGE</a></div><div><b>Author(s):&nbsp;</b>Alexandru Agapie, Ovidiu Solomon, Marius Giuclea</div><div><b>Pages:&nbsp;</b>501 - 511</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9486864/\">Multipopulation Ant Colony System With Knowledge-Based Local Searches for Multiobjective Supply Chain Configuration</a></div><div><b>Author(s):&nbsp;</b>Xin Zhang, Zhi-Hui Zhan, Wei Fang, Pengjiang Qian, Jun Zhang</div><div><b>Pages:&nbsp;</b>512 - 526</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9555836/\">Static and Dynamic Multimodal Optimization by Improved Covariance Matrix Self-Adaptation Evolution Strategy With Repelling Subpopulations</a></div><div><b>Author(s):&nbsp;</b>Ali Ahrari, Saber Elsayed, Ruhul Sarker, Daryl Essam, Carlos A. Coello Coello</div><div><b>Pages:&nbsp;</b>527 - 541</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9669184/\">Dynamic Transfer Reference Point-Oriented MOEA/D Involving Local Objective-Space Knowledge</a></div><div><b>Author(s):&nbsp;</b>Yingbo Xie, Shengxiang Yang, Ding Wang, Junfei Qiao, Baocai Yin</div><div><b>Pages:&nbsp;</b>542 - 554</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9499117/\">Dual-Tree Genetic Programming for Few-Shot Image Classification</a></div><div><b>Author(s):&nbsp;</b>Ying Bi, Bing Xue, Mengjie Zhang</div><div><b>Pages:&nbsp;</b>555 - 569</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9612583/\">Memetic EDA-Based Approaches to QoS-Aware Fully Automated Semantic Web Service Composition</a></div><div><b>Author(s):&nbsp;</b>Chen Wang, Hui Ma, Gang Chen, Sven Hartmann</div><div><b>Pages:&nbsp;</b>570 - 584</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9530458/\">Toward Large-Scale Evolutionary Multitasking: A GPU-Based Paradigm</a></div><div><b>Author(s):&nbsp;</b>Yuxiao Huang, Liang Feng, Alex Kai Qin, Meng Chen, Kay Chen Tan</div><div><b>Pages:&nbsp;</b>585 - 598</div><div><br /></div>",
            "pubdate": "2022-06-06T12:00:00.004+12:00",
            "pubdate_parsed": [
                2022,
                6,
                6
            ],
            "email_sent": true
        },
        "IEEE Transactions on Fuzzy Systems, Volume 30, Issue 6, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/ieee-transactions-on-fuzzy-systems.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9786023/\">Guest Editorial Special Issue on Cyborg Intelligence: Human Enhancement With Fuzzy Sets</a></div><div><b>Author(s): </b>Zhijun Li, Jian Huang, Hang Su, Zhaojie Ju</div><div><b>Pages: </b>1502 - 1505</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9662203/\">Incremental Motor Skill Learning and Generalization From Human Dynamic Reactions Based on Dynamic Movement Primitives and Fuzzy Logic System</a></div><div><b>Author(s):&nbsp;</b>Zhenyu Lu, Ning Wang, Miao Li, Chenguang Yang</div><div><b>Pages:&nbsp;</b>1506 - 1515</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9690032/\">Explainable CNN With Fuzzy Tree Regularization for Respiratory Sound Analysis</a></div><div><b>Author(s):&nbsp;</b>Jianqiang Li, Cheng Wang, Jie Chen, Heng Zhang, Yuyan Dai, Lingwei Wang, Li Wang, Asoke K. Nandi</div><div><b>Pages:&nbsp;</b>1516 - 1528</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9737383/\">Generalized Point Set Registration With Fuzzy Correspondences Based on Variational Bayesian Inference</a></div><div><b>Author(s):&nbsp;</b>Ang Zhang, Zhe Min, Zhengyan Zhang, Max Q.-H. Meng</div><div><b>Pages:&nbsp;</b>1529 - 1540</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9743751/\">Fuzzy Enhanced Adaptive Admittance Control of a Wearable Walking Exoskeleton With Step Trajectory Shaping</a></div><div><b>Author(s):&nbsp;</b>Pengbo Huang, Zhijun Li, MengChu Zhou, Xiang Li, Mengyue Cheng</div><div><b>Pages:&nbsp;</b>1541 - 1552</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9735420/\">A Muscle Synergy-Driven ANFIS Approach to Predict Continuous Knee Joint Movement</a></div><div><b>Author(s):&nbsp;</b>Wenjuan Zhong, Xueming Fu, Mingming Zhang</div><div><b>Pages:&nbsp;</b>1553 - 1563</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9729601/\">Fuzzy Approximation-Based Task-Space Control of Robot Manipulators With Remote Center of Motion Constraint</a></div><div><b>Author(s):&nbsp;</b>Hang Su, Wen Qi, Jiahao Chen, Dandan Zhang</div><div><b>Pages:&nbsp;</b>1564 - 1573</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9757861/\">Adaptability Control Towards Complex Ground Based on Fuzzy Logic for Humanoid Robots</a></div><div><b>Author(s):&nbsp;</b>Chencheng Dong, Zhangguo Yu, Xuechao Chen, Huanzhong Chen, Yan Huang, Qiang Huang</div><div><b>Pages:&nbsp;</b>1574 - 1584</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9366404/\">Reducing Criteria in Multicriteria Group Decision-Making Methods Using Hierarchical Clustering Methods and Fuzzy Ontologies</a></div><div><b>Author(s):&nbsp;</b>Juan Antonio Morente-Molinera, Yinglin Wang, Zai-Wu Gong, A. Morfeq, Rami Al-Hmouz, Enrique Herrera-Viedma</div><div><b>Pages:&nbsp;</b>1585 - 1598</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9366349/\">Building Trend Fuzzy Granulation-Based LSTM Recurrent Neural Network for Long-Term Time-Series Forecasting</a></div><div><b>Author(s):&nbsp;</b>Yuqing Tang, Fusheng Yu, Witold Pedrycz, Xiyang Yang, Jiayin Wang, Shihu Liu</div><div><b>Pages:&nbsp;</b>1599 - 1613</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9366943/\">Membership-Function-Dependent Control Design and Stability Analysis of Interval Type-2 Sampled-Data Fuzzy-Model-Based Control System</a></div><div><b>Author(s):&nbsp;</b>Ming Chen, Hak-Keung Lam, Bo Xiao, Chengbin Xuan</div><div><b>Pages:&nbsp;</b>1614 - 1623</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9369886/\">A Novel Kernelized Total Bregman Divergence-Driven Possibilistic Fuzzy Clustering With Multiple Information Constraints for Image Segmentation</a></div><div><b>Author(s):&nbsp;</b>Chengmao Wu, Xue Zhang</div><div><b>Pages:&nbsp;</b>1624 - 1639</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9369889/\">Stability Analysis for Mamdani-Type Integral Fuzzy-Based Sliding-Mode Control of Systems Under Persistent Disturbances</a></div><div><b>Author(s):&nbsp;</b>Pablo J. Prieto, Luis T. Aguilar, Selene L. Cardenas-Maciel, Jorge A. Lopez-Renteria, Nohe R. Cazarez-Castro</div><div><b>Pages:&nbsp;</b>1640 - 1647</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9369865/\">Exponential Stability of Stochastic Takagi-Sugeno Fuzzy Systems Under Intermittent Dynamic Event-Triggered Control</a></div><div><b>Author(s):&nbsp;</b>Yongbao Wu, Sai Hu, Wenxue Li</div><div><b>Pages:&nbsp;</b>1648 - 1659</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9372886/\">Novel Pythagorean Fuzzy Correlation Measures Via Pythagorean Fuzzy Deviation, Variance, and Covariance With Applications to Pattern Recognition and Career Placement</a></div><div><b>Author(s):&nbsp;</b>Paul Augustine Ejegwa, Shiping Wen, Yuming Feng, Wei Zhang, Ning Tang</div><div><b>Pages:&nbsp;</b>1660 - 1668</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9373968/\">Inverse Optimal Design of Direct Adaptive Fuzzy Controllers for Uncertain Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Kaixin Lu, Zhi Liu, C. L. Philip Chen, Yaonan Wang, Yun Zhang</div><div><b>Pages:&nbsp;</b>1669 - 1682</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9373964/\">Incremental Feature Selection Using a Conditional Entropy Based on Fuzzy Dominance Neighborhood Rough Sets</a></div><div><b>Author(s):&nbsp;</b>Binbin Sang, Hongmei Chen, Lei Yang, Tianrui Li, Weihua Xu</div><div><b>Pages:&nbsp;</b>1683 - 1697</div><div><br /></div><div><b>18) </b><a href=\"https://ieeexplore.ieee.org/document/9373994/\">A Novel Extension of Best-Worst Method With Intuitionistic Fuzzy Reference Comparisons</a></div><div><b>Author(s):&nbsp;</b>Shuping Wan, Jiuying Dong</div><div><b>Pages:&nbsp;</b>1698 - 1711</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9373926/\">Fractional Sliding-Mode Control for Microgyroscope Based on Multilayer Recurrent Fuzzy Neural Network</a></div><div><b>Author(s):&nbsp;</b>Juntao Fei, Zhe Wang, Xiao Liang, Zhilin Feng, Yuncan Xue</div><div><b>Pages:&nbsp;</b>1712 - 1721</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9376301/\">Static Output-Feedback Tracking Control for Positive Polynomial Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Lining Fu, H. K. Lam, Fucai Liu, Bo Xiao, Zhixiong Zhong</div><div><b>Pages:&nbsp;</b>1722 - 1733</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9376264/\">Fuzzy Intermittent Extended Dissipative Control for Delayed Distributed Parameter Systems With Stochastic Disturbance: A Spatial Point Sampling Approach</a></div><div><b>Author(s):&nbsp;</b>Kui Ding, Quanxin Zhu</div><div><b>Pages:&nbsp;</b>1734 - 1749</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9376268/\">Event-Triggered Adaptive Fault-Tolerant Control for Nonaffine Uncertain Systems With Output Tracking Errors Constraints</a></div><div><b>Author(s):&nbsp;</b>Yang Wu, Guoshan Zhang, Li-Bing Wu</div><div><b>Pages:&nbsp;</b>1750 - 1761</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9378943/\">Ordinal Sum of Two Binary Operations Being a T-Norm on Bounded Lattice</a></div><div><b>Author(s):&nbsp;</b>Xinxing Wu, Qin Zhang, Xu Zhang, G\u00fcl Deniz \u00c7ayl\u0131, Lidong Wang</div><div><b>Pages:&nbsp;</b>1762 - 1772</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9381663/\">Switched-Observer-Based Event-Triggered Adaptive Fuzzy Funnel Control for Switched Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Fenglan Wang, Lijun Long</div><div><b>Pages:&nbsp;</b>1773 - 1787</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9382881/\">Design and Optimization of Robust Path Tracking Control for Autonomous Vehicles With Fuzzy Uncertainty</a></div><div><b>Author(s):&nbsp;</b>Zeyu Yang, Jin Huang, Diange Yang, Zhihua Zhong</div><div><b>Pages:&nbsp;</b>1788 - 1800</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9382904/\">Finite-Time Almost Sure Stability of a Markov Jump Fuzzy System With Delayed Inputs</a></div><div><b>Author(s):&nbsp;</b>He Zhang, Shengyuan Xu</div><div><b>Pages:&nbsp;</b>1801 - 1808</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9388897/\">Adaptive Fuzzy Decentralized Sampled-Data Control for Large-Scale Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Yongming Li, Kunting Yu</div><div><b>Pages:&nbsp;</b>1809 - 1822</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9388934/\">High-Efficient Fuzzy Querying With HiveQL for Big Data Warehousing</a></div><div><b>Author(s):&nbsp;</b>Bo\u017cena Ma\u0142ysiak-Mrozek, Jadwiga Wieszok, Witold Pedrycz, Weiping Ding, Dariusz Mrozek</div><div><b>Pages:&nbsp;</b>1823 - 1837</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9388899/\">Design of a Fuzzy Adaptive Sliding Mode Control System for MEMS Tunable Capacitors in Voltage Reference Applications</a></div><div><b>Author(s):&nbsp;</b>Ehsan Ranjbar, Amir Abolfazl Suratgar, Mohammad Bagher Menhaj, Mukesh Prasad</div><div><b>Pages:&nbsp;</b>1838 - 1852</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9388942/\">Event-Based H\u221e Control for Discrete-Time Fuzzy Markov Jump Systems Subject to DoS Attacks</a></div><div><b>Author(s):&nbsp;</b>Pengyu Zeng, Feiqi Deng, Hongyang Zhang, Xiaobin Gao</div><div><b>Pages:&nbsp;</b>1853 - 1863</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9392358/\">Further Results on Sampled-Data H\u221e Filtering for T-S Fuzzy Systems With Asynchronous Premise Variables</a></div><div><b>Author(s):&nbsp;</b>Yongsik Jin, Wookyong Kwon, Sangmoon Lee</div><div><b>Pages:&nbsp;</b>1864 - 1874</div><div><br /></div><div><b>32)</b>&nbsp;<a href=\"https://ieeexplore.ieee.org/document/9392325/\">H\u221e PID Control for Discrete-Time Fuzzy Systems With Infinite-Distributed Delays Under Round-Robin Communication Protocol</a></div><div><b>Author(s):&nbsp;</b>Yezheng Wang, Zidong Wang, Lei Zou, Hongli Dong</div><div><b>Pages:&nbsp;</b>1875 - 1888</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9392283/\">Observer-Based Sliding Mode Control for Networked Fuzzy Singularly Perturbed Systems Under Weighted Try-Once-Discard Protocol</a></div><div><b>Author(s):&nbsp;</b>Jing Wang, Chengyu Yang, Jianwei Xia, Zheng-Guang Wu, Hao Shen</div><div><b>Pages:&nbsp;</b>1889 - 1899</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9392347/\">Interactive Transfer Learning-Assisted Fuzzy Neural Network</a></div><div><b>Author(s):&nbsp;</b>Honggui Han, Hongxu Liu, Zheng Liu, Junfei Qiao</div><div><b>Pages:&nbsp;</b>1900 - 1913</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9394772/\">Fault-Tolerant Tracking Control of Discrete-Time T-S Fuzzy Systems With Input Constraint</a></div><div><b>Author(s):&nbsp;</b>Iman Zare, Peyman Setoodeh, Mohammad Hassan Asemani</div><div><b>Pages:&nbsp;</b>1914 - 1928</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9394750/\">Fuzzy Lyapunov-Based Model Predictive Sliding-Mode Control of Nonlinear Systems: An Ellipsoid Recursive Feasibility Approach</a></div><div><b>Author(s):&nbsp;</b>Mohsen Farbood, Mokhtar Shasadeghi, Taher Niknam, Behrouz Safarinejadian</div><div><b>Pages:&nbsp;</b>1929 - 1938</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9394792/\">Event-Based Adaptive Fuzzy Fixed-Time Secure Control for Nonlinear CPSs Against Unknown False Data Injection and Backlash-Like Hysteresis</a></div><div><b>Author(s):&nbsp;</b>Shuai Song, Ju H. Park, Baoyong Zhang, Xiaona Song</div><div><b>Pages:&nbsp;</b>1939 - 1951</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9400708/\">Characterizations and Applications of Fuzzy Implications Generated by a Pair of Generators of <i>T</i>-Norms and the Usual Addition of Real Numbers</a></div><div><b>Author(s):&nbsp;</b>Hongjun Zhou</div><div><b>Pages:&nbsp;</b>1952 - 1966</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9400738/\">Fast Training of Adversarial Deep Fuzzy Classifier by Downsizing Fuzzy Rules With Gradient Guided Learning</a></div><div><b>Author(s):&nbsp;</b>Suhang Gu, Chi Man Vong, Pak Kin Wong, Shitong Wang</div><div><b>Pages:&nbsp;</b>1967 - 1980</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9403972/\">Reachability Analysis-Based Interval Estimation for Discrete-Time Takagi-Sugeno Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Shenghui Guo, Weijie Ren, Choon Ki Ahn, Chenglin Wen, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>1981 - 1992</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9403990/\">Adaptive Fuzzy Prescribed Finite-Time Tracking Control for Nonlinear System With Unknown Control Directions</a></div><div><b>Author(s):&nbsp;</b>Yang Liu, Huaguang Zhang, Yingchun Wang, He Ren, Qiaochu Li</div><div><b>Pages:&nbsp;</b>1993 - 2003</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9404860/\">H-Rank Consensus Models for Fuzzy Preference Relations Considering Eliminating Rank Violations</a></div><div><b>Author(s):&nbsp;</b>Jiancheng Tu, Zhibin Wu</div><div><b>Pages:&nbsp;</b>2004 - 2018</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9404849/\">Social Trust Driven Consensus Reaching Model With a Minimum Adjustment Feedback Mechanism Considering Assessments-Modifications Willingness</a></div><div><b>Author(s):&nbsp;</b>Hengjie Zhang, Fang Wang, Yucheng Dong, Francisco Chiclana, Enrique Herrera-Viedma</div><div><b>Pages:&nbsp;</b>2019 - 2031</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9404851/\">Event-Based Secure Control of T-S Fuzzy-Based 5-DOF Active Semivehicle Suspension Systems Subject to DoS Attacks</a></div><div><b>Author(s):&nbsp;</b>Zhou Gu, Xiang Sun, Hak-Keung Lam, Dong Yue, Xiangpeng Xie</div><div><b>Pages:&nbsp;</b>2032 - 2043</div><div><br /></div><div><b>45)</b> <a href=\"https://ieeexplore.ieee.org/document/9406360/\">Fuzzy-Control-Based Chance-Constrained Programming for Humanitarian Relief Allocation Problem</a></div><div><b>Author(s):&nbsp;</b>Jianghua Zhang, Yang Liu, Xiaojie Su, Peng Shi</div><div><b>Pages:&nbsp;</b>2044 - 2054</div><div><br /></div><div><b>46)</b> <a href=\"https://ieeexplore.ieee.org/document/9408425/\">Spatial Colocation Pattern Discovery Incorporating Fuzzy Theory</a></div><div><b>Author(s):&nbsp;</b>Xiaoxuan Wang, Le Lei, Lizhen Wang, Peizhong Yang, Hongmei Chen</div><div><b>Pages:&nbsp;</b>2055 - 2072</div><div><br /></div><div><b>47)</b> <a href=\"https://ieeexplore.ieee.org/document/9409687/\">A Novel Data-Driven Approach to Autonomous Fuzzy Clustering</a></div><div><b>Author(s):&nbsp;</b>Xiaowei Gu, Qiang Ni, Guolin Tang</div><div><b>Pages:&nbsp;</b>2073 - 2085</div><div><br /></div><div><b>48)</b> <a href=\"https://ieeexplore.ieee.org/document/9416157/\">Membership-Function-Dependent Design of L1-Gain Output-Feedback Controller for Stabilization of Positive Polynomial Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Aiwen Meng, Hak-Keung Lam, Fucai Liu, Yingjie Yang</div><div><b>Pages:&nbsp;</b>2086 - 2100</div><div><br /></div><div><b>49)</b> <a href=\"https://ieeexplore.ieee.org/document/9416181/\">Robust Actor-Critic Learning for Continuous-Time Nonlinear Systems With Unmodeled Dynamics</a></div><div><b>Author(s):&nbsp;</b>Yongliang Yang, Weinan Gao, Hamidreza Modares, Cheng-Zhong Xu</div><div><b>Pages:&nbsp;</b>2101 - 2112</div><div><br /></div><div><b>50)</b> <a href=\"https://ieeexplore.ieee.org/document/9376312/\">Some Consistency Properties and Individual Preference Monotonicity for Weighted Aggregation Operators</a></div><div><b>Author(s):&nbsp;</b>Lesheng Jin</div><div><b>Pages:&nbsp;</b>2113 - 2117</div><div><br /></div><div><b>51)</b> <a href=\"https://ieeexplore.ieee.org/document/9380521/\">Decentralized Event-Triggered Adaptive Fuzzy Control for Nonlinear Switched Large-Scale Systems With Input Delay Via Command-Filtered Backstepping</a></div><div><b>Author(s):&nbsp;</b>Jing Zhang, Shi Li, Choon Ki Ahn, Zhengrong Xiang</div><div><b>Pages:&nbsp;</b>2118 - 2123</div><div><br /></div><div><b>52)</b> <a href=\"https://ieeexplore.ieee.org/document/9388891/\">Expected Shortfall for the Makespan in Activity Networks With Fuzzy Durations</a></div><div><b>Author(s):&nbsp;</b>Gabriella Dellino, Carlo Meloni, Marco Pranzo, Marcella Sam\u00e0</div><div><b>Pages:&nbsp;</b>2124 - 2128</div><div><br /></div><div><b>53)</b> <a href=\"https://ieeexplore.ieee.org/document/9403919/\">Finite-Time Fuzzy Control for Nonlinear Singularly Perturbed Systems With Input Constraints</a></div><div><b>Author(s):&nbsp;</b>Feng Li, Wei Xing Zheng, Shengyuan Xu</div><div><b>Pages:&nbsp;</b>2129 - 2134</div><div><br /></div><div><b>54)</b> <a href=\"https://ieeexplore.ieee.org/document/9419725/\">An Asymmetric Lyapunov-Krasovskii Functional Method on Stability and Stabilization for T-S Fuzzy Systems With Time Delay</a></div><div><b>Author(s):&nbsp;</b>Zhaoliang Sheng, Chong Lin, Bing Chen, Qing-Guo Wang</div><div><b>Pages:&nbsp;</b>2135 - 2140</div><div><br /></div></div>",
            "pubdate": "2022-06-07T12:00:00.198+12:00",
            "pubdate_parsed": [
                2022,
                6,
                7
            ],
            "email_sent": true
        },
        "Complex & Intelligent Systems, Volume 8, issue 3, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/complex-intelligent-systems-volume-8.html",
            "description": "<div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00736-3\">Guest Editorial on \u201cComputational intelligence in analysis and integration of complex systems\u201d</a></div><div><b>Author(s): </b>Bo Zhao, Wenyi Zeng...Qichao Zhang</div><div><b>Pages: </b>1819 - 1821</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00323-y\">FSD-SLAM: a fast semi-direct SLAM algorithm</a></div><div><b>Author(s):&nbsp;</b>Xiang Dong, Long Cheng...Teng Li</div><div><b>Pages:&nbsp;</b>1823 - 1834</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00330-z\">A tractor-trailer parking control scheme using adaptive dynamic programming</a></div><div><b>Author(s):&nbsp;</b>Chenyong Guan, Yu Jiang</div><div><b>Pages:&nbsp;</b>1835 - 1845</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00322-z\">Remote sensing image building detection method based on Mask R-CNN</a></div><div><b>Author(s):&nbsp;</b>Qinzhe Han, Qian Yin...Ziyi Chen</div><div><b>Pages:&nbsp;</b>1847 - 1855</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00345-6\">A game strategy model in the digital curling system based on NFSP</a></div><div><b>Author(s):&nbsp;</b>Yuntao Han, Qibin Zhou, Fuqing Duan</div><div><b>Pages:&nbsp;</b>1857 - 1863</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00347-4\">ResCaps: an improved capsule network and its application in ultrasonic image classification of thyroid papillary carcinoma</a></div><div><b>Author(s):&nbsp;</b>Xiongzhi Ai, Jiawei Zhuang...Yu Fu</div><div><b>Pages:&nbsp;</b>1865 - 1873</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00360-7\">Deep transfer learning: a novel glucose prediction framework for new subjects with type 2 diabetes</a></div><div><b>Author(s):&nbsp;</b>Xia Yu, Tao Yang...Jian Zhou</div><div><b>Pages:&nbsp;</b>1875 - 1887</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00334-9\">Sliding-mode observers based distributed consensus control for nonlinear multi-agent systems with disturbances</a></div><div><b>Author(s):&nbsp;</b>Yulian Jiang, Yuhang Zhang...Keping Liu</div><div><b>Pages:&nbsp;</b>1889 - 1897</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00366-1\">Collision-free path planning for welding manipulator via hybrid algorithm of deep reinforcement learning and inverse kinematics</a></div><div><b>Author(s):&nbsp;</b>Jie Zhong, Tao Wang, Lianglun Cheng</div><div><b>Pages:&nbsp;</b>1899 - 1912</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00359-0\">Compensator-critic structure-based event-triggered decentralized tracking control of modular robot manipulators: theory and experimental verification</a></div><div><b>Author(s):&nbsp;</b>Bing Ma, Yuanchun Li</div><div><b>Pages:&nbsp;</b>1913 - 1927</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00376-z\">A deep learning-based computer-aided diagnosis method of X-ray images for bone age assessment</a></div><div><b>Author(s):&nbsp;</b>Shaowei Li, Bowen Liu...Dongxu Zhang</div><div><b>Pages:&nbsp;</b>1929 - 1939</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00370-5\">Neural network based asynchronous synchronization for fuzzy hidden Markov jump complex dynamical networks</a></div><div><b>Author(s):&nbsp;</b>Chao Ma, Liziyi Hao, Hang Fu</div><div><b>Pages:&nbsp;</b>1941 - 1948</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00338-5\">A novel sEMG-based force estimation method using deep-learning algorithm</a></div><div><b>Author(s):&nbsp;</b>Shaoyang Hua, Congqing Wang, Xuewei Wu</div><div><b>Pages:&nbsp;</b>1949 - 1961</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00364-3\">Sliding mode-based online fault compensation control for modular reconfigurable robots through adaptive dynamic programming</a></div><div><b>Author(s):&nbsp;</b>Hongbing Xia, Ping Guo</div><div><b>Pages:&nbsp;</b>1963 - 1973</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00382-1\">Dueling deep Q-networks for social awareness-aided spectrum sharing</a></div><div><b>Author(s):&nbsp;</b>Yonghua Wang, Xueyang Li...Xia Deng</div><div><b>Pages:&nbsp;</b>1975 - 1986</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00389-8\">A neuro-swarming intelligent heuristic for second-order nonlinear Lane\u2013Emden multi-pantograph delay differential system</a></div><div><b>Author(s):&nbsp;</b>Zulqurnain Sabir, Muhammad Asif Zahoor Raja...Ayman A. Aly</div><div><b>Pages:&nbsp;</b>1987 - 2000</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00444-4\">Reinforcement learning for the traveling salesman problem with refueling</a></div><div><b>Author(s):&nbsp;</b>Andr\u00e9 L. C. Ottoni, Erivelton G. Nepomuceno...Daniela C. R. de Oliveira</div><div><b>Pages:&nbsp;</b>2001 - 2015</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00502-x\">An integrated fuzzy model for evaluation and selection of mobile banking (m-banking) applications using new fuzzy-BWM and fuzzy-TOPSIS</a></div><div><b>Author(s):&nbsp;</b>Pranith Kumar Roy, Krishnendu Shaw</div><div><b>Pages:&nbsp;</b>2017 - 2038</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00516-5\">DensePILAE: a feature reuse pseudoinverse learning algorithm for deep stacked autoencoder</a></div><div><b>Author(s):&nbsp;</b>Jue Wang, Ping Guo, Yanjun Li</div><div><b>Pages:&nbsp;</b>2039 - 2049</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00734-5\">Self-adaptive opposition-based differential evolution with subpopulation strategy for numerical and engineering optimization problems</a></div><div><b>Author(s):&nbsp;</b>Jiahang Li, Yuelin Gao...Qinwen Yang</div><div><b>Pages:&nbsp;</b>2051 - 2089</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00632-2\">Motion intensity modeling and trajectory control of upper limb rehabilitation exoskeleton robot based on multi-modal information</a></div><div><b>Author(s):&nbsp;</b>WenDong Wang, JunBo Zhang...Peng Zhang</div><div><b>Pages:&nbsp;</b>2091 - 2103</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00636-y\">Multi-label feature selection based on fuzzy neighborhood rough sets</a></div><div><b>Author(s):&nbsp;</b>Jiucheng Xu, Kaili Shen, Lin Sun</div><div><b>Pages:&nbsp;</b>2105 - 2129</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00626-0\">Single-valued neutrosophic Einstein interactive aggregation operators with applications for material selection in engineering design: case study of cryogenic storage tank</a></div><div><b>Author(s):&nbsp;</b>Hafiz Muhammad Athar Farid, Muhammad Riaz</div><div><b>Pages:&nbsp;</b>2131 - 2149</div><div><br /></div><div><b>24) </b><a href=\"https://link.springer.com/article/10.1007/s40747-021-00623-3\">A decomposition structure learning algorithm in Bayesian network based on a two-stage combination method</a></div><div><b>Author(s):&nbsp;</b>Huiping Guo, Hongru Li</div><div><b>Pages:&nbsp;</b>2151 - 2165</div><div><br /></div><div><b>25) </b><a href=\"https://link.springer.com/article/10.1007/s40747-021-00639-9\">A state of health estimation method for electric vehicle Li-ion batteries using GA-PSO-SVR</a></div><div><b>Author(s):&nbsp;</b>Yue Zhi, Heqi Wang, Liang Wang</div><div><b>Pages:&nbsp;</b>2167 - 2182</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00647-3\">Modeling students\u2019 performance using graph convolutional networks</a></div><div><b>Author(s):&nbsp;</b>Ahmed A. Mubarak, Han Cao...Fei Hao</div><div><b>Pages:&nbsp;</b>2183 - 2201</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00585-6\">Group decision making method with hesitant fuzzy preference relations based on additive consistency and consensus</a></div><div><b>Author(s):&nbsp;</b>Jian Li, Li-li Niu...Wenjing Li</div><div><b>Pages:&nbsp;</b>2203 - 2225</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00628-y\">Path planning of a manipulator based on an improved P_RRT* algorithm</a></div><div><b>Author(s):&nbsp;</b>Junhui Yi, Qingni Yuan...Huan Bai</div><div><b>Pages:&nbsp;</b>2227 - 2245</div><div><br /></div><div><b>29) </b><a href=\"https://link.springer.com/article/10.1007/s40747-021-00638-w\">ASN-SMOTE: a synthetic minority oversampling method with adaptive qualified synthesizer selection</a></div><div><b>Author(s):&nbsp;</b>Xinkai Yi, Yingying Xu...Zhenzhou Tang</div><div><b>Pages:&nbsp;</b>2247 - 2272</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00648-2\">Safe-Nav: learning to prevent PointGoal navigation failure in unknown environments</a></div><div><b>Author(s):&nbsp;</b>Sheng Jin, Qinghao Meng...Huirang Hou</div><div><b>Pages:&nbsp;</b>2273 - 2290</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00644-6\">Content-adaptive image compression and encryption via optimized compressive sensing with double random phase encoding driven by chaos</a></div><div><b>Author(s):&nbsp;</b>Zhihua Gan, Xiuli Chai...Xiuhui Chen</div><div><b>Pages:&nbsp;</b>2291 - 2309</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00645-5\">Enhanced graph recommendation with heterogeneous auxiliary information</a></div><div><b>Author(s):&nbsp;</b>Fulian Yin, Meiqi Ji...Sitong Li</div><div><b>Pages:&nbsp;</b>2311 - 2324</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00646-4\">Decision-making methods based on fuzzy soft competition hypergraphs</a></div><div><b>Author(s):&nbsp;</b>Muhammad Akram, Sundas Shahzadi...Musavarah Sarwar</div><div><b>Pages:&nbsp;</b>2325 - 2348</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00622-4\">Cq-ROFRS: covering q-rung orthopair fuzzy rough sets and its application to multi-attribute decision-making process</a></div><div><b>Author(s):&nbsp;</b>Harish Garg, Mohammed Atef</div><div><b>Pages:&nbsp;</b>2349 - 2370</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00633-1\">Hybrid attention network with appraiser-guided loss for counterfeit luxury handbag detection</a></div><div><b>Author(s):&nbsp;</b>Jianbiao Peng, Beiji Zou...Chengzhang Zhu</div><div><b>Pages:&nbsp;</b>2371 - 2381</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00655-3\">Convergence and gradient algorithm of a class of neural networks based on the polygonal fuzzy numbers representation</a></div><div><b>Author(s):&nbsp;</b>Gang Sun, Mingxin Wang, Xiaoping Li</div><div><b>Pages:&nbsp;</b>2383 - 2404</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00627-z\">Two-layer LSTM network-based prediction of epileptic seizures using EEG spectral features</a></div><div><b>Author(s):&nbsp;</b>Kuldeep Singh, Jyoteesh Malhotra</div><div><b>Pages:&nbsp;</b>2405 - 2418</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00653-5\">Lifetime prolongation of a wireless charging sensor network using a mobile robot via linear Diophantine fuzzy graph environment</a></div><div><b>Author(s):&nbsp;</b>Karthikeyan Prakash, Mani Parimala...Muhammad Riaz</div><div><b>Pages:&nbsp;</b>2419 - 2434</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00656-2\">Decision-making based on probabilistic linguistic term sets without loss of information</a></div><div><b>Author(s):&nbsp;</b>Zhihong Yi</div><div><b>Pages:&nbsp;</b>2435 - 2449</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00657-1\">A hybrid model integrating FMEA and HFACS to assess the risk of inter-city bus accidents</a></div><div><b>Author(s):&nbsp;</b>James J. H. Liou, Perry C. Y. Liu...Yu-Zeng Wu</div><div><b>Pages:&nbsp;</b>2451 - 2470</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00658-0\">A deep learning method DCWR with HANet for stock market prediction using news articles</a></div><div><b>Author(s):&nbsp;</b>Saleh Albahli, Awais Awan...Waleed Albattah</div><div><b>Pages:&nbsp;</b>2471 - 2487</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00660-6\">TAUNet: a triple-attention-based multi-modality MRI fusion U-Net for cardiac pathology segmentation</a></div><div><b>Author(s):&nbsp;</b>Dapeng Li, Yanjun Peng...Jindong Sun</div><div><b>Pages:&nbsp;</b>2489 - 2505</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00661-5\">Memory-based variable neighborhood search for green vehicle routing problem with passing-by drivers: a comprehensive perspective</a></div><div><b>Author(s):&nbsp;</b>Lei Cao, Chun-ming Ye...Zhen-kun Wang</div><div><b>Pages:&nbsp;</b>2507 - 2525</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00643-7\">Neuroadaptive control of information-poor servomechanisms with smooth and nonsmooth uncertainties</a></div><div><b>Author(s):&nbsp;</b>Guichao Yang, Hua Wang</div><div><b>Pages:&nbsp;</b>2527 - 2539</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00681-1\">An integrated quality-function-deployment and stochastic-dominance-based decision-making approach for prioritizing product concept alternatives</a></div><div><b>Author(s):&nbsp;</b>Zeng-Qiang Wang, Zhen-Song Chen...Kwai-Sang Chin</div><div><b>Pages:&nbsp;</b>2541 - 2556</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00642-8\">Optimization on dual-channel supply chain model with pricing decision and trapezoidal fuzzy demand under a controllable lead time</a></div><div><b>Author(s):&nbsp;</b>B. Karthick, R. Uthayakumar</div><div><b>Pages:&nbsp;</b>2557 - 2591</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00629-x\">Data collection protocols for VANETs: a survey</a></div><div><b>Author(s):&nbsp;</b>Maryam Gillani, Hafiz Adnan Niaz...Ata Ullah</div><div><b>Pages:&nbsp;</b>2593 - 2622</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00665-1\">Computational intelligence in processing of speech acoustics: a survey</a></div><div><b>Author(s):&nbsp;</b>Amitoj Singh, Navkiran Kaur...Munish Kumar</div><div><b>Pages:&nbsp;</b>2623 - 2661</div><div><br /></div><div><b>49) </b><a href=\"https://link.springer.com/article/10.1007/s40747-021-00637-x\">Feature dimensionality reduction: a review</a></div><div><b>Author(s):&nbsp;</b>Weikuan Jia, Meili Sun...Sujuan Hou</div><div><b>Pages:&nbsp;</b>2663 - 2693</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00654-4\">Correction to: Efficient structural pseudoinverse learning-based hierarchical representation learning for skin lesion classification</a></div><div><b>Author(s):&nbsp;</b>Xiaodan Deng, Qian Yin, Ping Guo</div><div><b>Pages:&nbsp;</b>2695 - 2695</div><div><br /></div>",
            "pubdate": "2022-06-08T12:00:00.167+12:00",
            "pubdate_parsed": [
                2022,
                6,
                8
            ],
            "email_sent": true
        },
        "IEEE Transactions on Neural Networks and Learning Systems, Volume 33, Issue 6, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/ieee-transactions-on-neural-networks.html",
            "description": "<div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9786561/\">Editorial Deep Learning for Anomaly Detection</a></div><div><b>Author(s): </b>Guansong Pang, Charu Aggarwal, Chunhua Shen, Nicu Sebe</div><div><b>Pages: </b>2282 - 2286</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9565147/\">SmithNet: Strictness on Motion-Texture Coherence for Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Trong-Nguyen Nguyen, S\u00e9bastien Roy, Jean Meunier</div><div><b>Pages:&nbsp;</b>2287 - 2300</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9446996/\">Robust Unsupervised Video Anomaly Detection by Multipath Frame Prediction</a></div><div><b>Author(s):&nbsp;</b>Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, Qi Qi</div><div><b>Pages:&nbsp;</b>2301 - 2312</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9640579/\">MOCCA: Multilayer One-Class Classification for Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Fabio Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, \u015eeymanur Akti, Hazim Kemal Ekenel, Giuseppe Amato</div><div><b>Pages:&nbsp;</b>2313 - 2323</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9664442/\">Memory-Augmented Generative Adversarial Networks for Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Ziyi Yang, Teng Zhang, Iman Soltani Bozchalooi, Eric Darve</div><div><b>Pages:&nbsp;</b>2324 - 2334</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9513473/\">Memorizing Structure-Texture Correspondence for Image Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Kang Zhou, Jing Li, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Jiang Liu, Shenghua Gao</div><div><b>Pages:&nbsp;</b>2335 - 2349</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9556483/\">Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples</a></div><div><b>Author(s):&nbsp;</b>David Mac\u00eado, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, Teresa Ludermir</div><div><b>Pages:&nbsp;</b>2350 - 2364</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9526875/\">Automated Anomaly Detection via Curiosity-Guided Search and Self-Imitation Learning</a></div><div><b>Author(s):&nbsp;</b>Yuening Li, Zhengzhang Chen, Daochen Zha, Kaixiong Zhou, Haifeng Jin, Haifeng Chen, Xia Hu</div><div><b>Pages:&nbsp;</b>2365 - 2377</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9395172/\">Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning</a></div><div><b>Author(s):&nbsp;</b>Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, George Karypis</div><div><b>Pages:&nbsp;</b>2378 - 2392</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9525041/\">A Synergistic Approach for Graph Anomaly Detection With Pattern Mining and Feature Learning</a></div><div><b>Author(s):&nbsp;</b>Tong Zhao, Tianwen Jiang, Neil Shah, Meng Jiang</div><div><b>Pages:&nbsp;</b>2393 - 2405</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9556511/\">Cross-Domain Graph Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Kaize Ding, Kai Shu, Xuan Shan, Jundong Li, Huan Liu</div><div><b>Pages:&nbsp;</b>2406 - 2415</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9669110/\">Graph Convolutional Adversarial Networks for Spatiotemporal Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Leyan Deng, Defu Lian, Zhenya Huang, Enhong Chen</div><div><b>Pages:&nbsp;</b>2416 - 2428</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9533187/\">Learning to Classify With Incremental New Class</a></div><div><b>Author(s):&nbsp;</b>Da-Wei Zhou, Yang Yang, De-Chuan Zhan</div><div><b>Pages:&nbsp;</b>2429 - 2443</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9492295/\">Semisupervised Training of Deep Generative Models for High-Dimensional Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Qin Xie, Peng Zhang, Boseon Yu, Jaesik Choi</div><div><b>Pages:&nbsp;</b>2444 - 2453</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9465358/\">Feature Encoding With Autoencoders for Weakly Supervised Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Yingjie Zhou, Xucheng Song, Yanru Zhang, Fanxing Liu, Ce Zhu, Lingqiao Liu</div><div><b>Pages:&nbsp;</b>2454 - 2465</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9524840/\">Attract\u2013Repel Encoder: Learning Anomaly Representation Away From Landmarks</a></div><div><b>Author(s):&nbsp;</b>Jiachen Zhao, Fang Deng, Yongling Li, Jie Chen</div><div><b>Pages:&nbsp;</b>2466 - 2479</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9608947/\">Center-Aware Adversarial Autoencoder for Anomaly Detection</a></div><div><b>Author(s):&nbsp;</b>Daoming Li, Qinghua Tao, Jiahao Liu, Huangang Wang</div><div><b>Pages:&nbsp;</b>2480 - 2493</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9569766/\">Comparison of Anomaly Detectors: Context Matters</a></div><div><b>Author(s):&nbsp;</b>V\u00edt \u0160kv\u00e1ra, Jan Franc\u00e5, Mat\u011bj Zorek, Tom\u00e1\u0161 Pevn\u00fd, V\u00e1clav \u0160m\u00eddl</div><div><b>Pages:&nbsp;</b>2494 - 2507</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9525836/\">An Evaluation of Anomaly Detection and Diagnosis in Multivariate Time Series</a></div><div><b>Author(s):&nbsp;</b>Astha Garg, Wenyu Zhang, Jules Samaran, Ramasamy Savitha, Chuan-Sheng Foo</div><div><b>Pages:&nbsp;</b>2508 - 2517</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9596571/\">Learning Fast and Slow: Propedeutica for Real-Time Malware Detection</a></div><div><b>Author(s):&nbsp;</b>Ruimin Sun, Xiaoyong Yuan, Pan He, Qile Zhu, Aokun Chen, Andr\u00e9 Gr\u00e9gio, Daniela Oliveira, Xiaolin Li</div><div><b>Pages:&nbsp;</b>2518 - 2529</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9594513/\">Joint Stance and Rumor Detection in Hierarchical Heterogeneous Graph</a></div><div><b>Author(s):&nbsp;</b>Chen Li, Hao Peng, Jianxin Li, Lichao Sun, Lingjuan Lyu, Lihong Wang, Philip S. Yu, Lifang He</div><div><b>Pages:&nbsp;</b>2530 - 2542</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9709524/\">Deep Graph Learning for Anomalous Citation Detection</a></div><div><b>Author(s):&nbsp;</b>Jiaying Liu, Feng Xia, Xu Feng, Jing Ren, Huan Liu</div><div><b>Pages:&nbsp;</b>2543 - 2557</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9512275/\">A Novel Deep Class-Imbalanced Semisupervised Model for Wind Turbine Blade Icing Detection</a></div><div><b>Author(s):&nbsp;</b>Xu Cheng, Fan Shi, Xiufeng Liu, Meng Zhao, Shengyong Chen</div><div><b>Pages:&nbsp;</b>2558 - 2570</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9786562/\">Guest Editorial Special Issue on New Frontiers in Extremely Efficient Reservoir Computing</a></div><div><b>Author(s):&nbsp;</b>Gouhei Tanaka, Claudio Gallicchio, Alessio Micheli, Juan-Pablo Ortega, Akira Hirose</div><div><b>Pages:&nbsp;</b>2571 - 2574</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9482563/\">Reservoir Memory Machines as Neural Computers</a></div><div><b>Author(s):&nbsp;</b>Benjamin Paa\u00dfen, Alexander Schulz, Terrence C. Stewart, Barbara Hammer</div><div><b>Pages:&nbsp;</b>2575 - 2585</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9585304/\">Consistency Hierarchy of Reservoir Computers</a></div><div><b>Author(s):&nbsp;</b>Thomas J\u00fcngling, Thomas Lymburn, Michael Small</div><div><b>Pages:&nbsp;</b>2586 - 2595</div><div><br /></div><div><b>27) </b><a href=\"https://ieeexplore.ieee.org/document/9502408/\">An Echo State Network Imparts a Curve Fitting</a></div><div><b>Author(s):&nbsp;</b>G. Manjunath</div><div><b>Pages:&nbsp;</b>2596 - 2604</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9531523/\">Adaptive Practical Nonlinear Model Predictive Control for Echo State Network Models</a></div><div><b>Author(s):&nbsp;</b>Bernardo Barancelli Schwedersky, Rodolfo C\u00e9sar Costa Flesch, Samuel Bahu Rovea</div><div><b>Pages:&nbsp;</b>2605 - 2614</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9664461/\">Echo State Networks for Practical Nonlinear Model Predictive Control of Unknown Dynamic Systems</a></div><div><b>Author(s):&nbsp;</b>Jean Panaioti Jordanou, Eric Aislan Antonelo, Eduardo Camponogara</div><div><b>Pages:&nbsp;</b>2615 - 2629</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9452793/\">Real-Time and Adaptive Reservoir Computing With Application to Profile Prediction in Fusion Plasma</a></div><div><b>Author(s):&nbsp;</b>Azarakhsh Jalalvand, Joseph Abbate, Rory Conlin, Geert Verdoolaege, Egemen Kolemen</div><div><b>Pages:&nbsp;</b>2630 - 2641</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9476188/\">Multiresolution Reservoir Graph Neural Network</a></div><div><b>Author(s):&nbsp;</b>Luca Pasa, Nicol\u00f2 Navarin, Alessandro Sperduti</div><div><b>Pages:&nbsp;</b>2642 - 2653</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9548713/\">Hierarchical-Task Reservoir for Online Semantic Analysis From Continuous Speech</a></div><div><b>Author(s):&nbsp;</b>Luca Pedrelli, Xavier Hinaut</div><div><b>Pages:&nbsp;</b>2654 - 2663</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9525045/\">High-Performance Reservoir Computing With Fluctuations in Linear Networks</a></div><div><b>Author(s):&nbsp;</b>Johannes Nokkala, Rodrigo Mart\u00ednez-Pe\u00f1a, Roberta Zambrini, Miguel C. Soriano</div><div><b>Pages:&nbsp;</b>2664 - 2675</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9453819/\">Neuromorphic Time-Multiplexed Reservoir Computing With On-the-Fly Weight Generation for Edge Devices</a></div><div><b>Author(s):&nbsp;</b>Sarthak Gupta, Satrajit Chakraborty, Chetan Singh Thakur</div><div><b>Pages:&nbsp;</b>2676 - 2685</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9600461/\">Neural Schr\u00f6dinger Equation: Physical Law as Deep Neural Network</a></div><div><b>Author(s):&nbsp;</b>Mitsumasa Nakajima, Kenji Tanaka, Toshikazu Hashimoto</div><div><b>Pages:&nbsp;</b>2686 - 2700</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9586079/\">Cellular Automata Can Reduce Memory Requirements of Collective-State Computing</a></div><div><b>Author(s):&nbsp;</b>Denis Kleyko, Edward Paxon Frady, Friedrich T. Sommer</div><div><b>Pages:&nbsp;</b>2701 - 2713</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9578941/\">Information Processing Capacity of a Single-Node Reservoir Computer: An Experimental Evaluation</a></div><div><b>Author(s):&nbsp;</b>Benedikt Vettelschoss, Andr\u00e9 R\u00f6hm, Miguel C. Soriano</div><div><b>Pages:&nbsp;</b>2714 - 2725</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9319556/\">Vertebrae Labeling via End-to-End Integral Regression Localization and Multi-Label Classification Network</a></div><div><b>Author(s):&nbsp;</b>Chunli Qin, Ji Zhou, Demin Yao, Han Zhuang, Hui Wang, Shiyao Chen, Yonghong Shi, Zhijian Song</div><div><b>Pages:&nbsp;</b>2726 - 2736</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9316887/\">Reachable Set Estimation of Delayed Markovian Jump Neural Networks Based on an Improved Reciprocally Convex Inequality</a></div><div><b>Author(s):&nbsp;</b>Guoqiang Tan, Zhanshan Wang</div><div><b>Pages:&nbsp;</b>2737 - 2742</div><div><br /></div><div><br /></div>",
            "pubdate": "2022-06-17T09:08:00.000+12:00",
            "pubdate_parsed": [
                2022,
                6,
                16
            ],
            "email_sent": true
        },
        "IEEE Transactions on Cognitive and Developmental Systems, Volume 14, Issue 2, June 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/ieee-transactions-on-cognitive-and.html",
            "description": "<div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9793369/\">Special Issue on Emerging Topics on Development and Learning</a></div><div><b>Author(s): </b>Mar\u00eda-Jos\u00e9 Escobar, Nicol\u00e1s Navarro-Guerrero, Javier Ruiz-Del-Solar, Giulio Sandini</div><div><b>Pages: </b>255 - 257</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9570769/\">A Biologically Inspired Computational Model of Time Perception</a></div><div><b>Author(s):&nbsp;</b>In\u00eas Louren\u00e7o, Robert Mattila, Rodrigo Ventura, Bo Wahlberg</div><div><b>Pages:&nbsp;</b>258 - 268</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9446597/\">A Sensorimotor Perspective on Contrastive Multiview Visual Representation Learning</a></div><div><b>Author(s):&nbsp;</b>Alban Laflaqui\u00e8re</div><div><b>Pages:&nbsp;</b>269 - 278</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9762837/\">In Search of a Neural Model for Serial Order: A Brain Theory for Memory Development and Higher Level Cognition</a></div><div><b>Author(s):&nbsp;</b>Alexandre Pitti, Mathias Quoy, Catherine Lavandier, Sofiane Boucenna, Wassim Swaileh, Claudio Weidmann</div><div><b>Pages:&nbsp;</b>279 - 291</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9495946/\">Philosophical Specification of Empathetic Ethical Artificial Intelligence</a></div><div><b>Author(s):&nbsp;</b>Michael Timothy Bennett, Yoshihiro Maruyama</div><div><b>Pages:&nbsp;</b>292 - 300</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9353685/\">Time Perception: A Review on Psychological, Computational, and Robotic Models</a></div><div><b>Author(s):&nbsp;</b>Hamit Basgol, Inci Ayhan, Emre Ugur</div><div><b>Pages:&nbsp;</b>301 - 315</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9380459/\">Vision-Based Gaze Estimation: A Review</a></div><div><b>Author(s):&nbsp;</b>Xinming Wang, Jianhua Zhang, Hanlin Zhang, Shuwen Zhao, Honghai Liu</div><div><b>Pages:&nbsp;</b>316 - 332</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9390376/\">The Why, What, and How of Artificial General Intelligence Chip Development</a></div><div><b>Author(s):&nbsp;</b>Alex P. James</div><div><b>Pages:&nbsp;</b>333 - 347</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9430619/\">Deep Learning in EEG: Advance of the Last Ten-Year Critical Period</a></div><div><b>Author(s):&nbsp;</b>Shu Gong, Kaibo Xing, Andrzej Cichocki, Junhua Li</div><div><b>Pages:&nbsp;</b>348 - 365</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9246953/\">To Move or Not to Move: Development of Fine-Tuning of Object Motion in Haptic Exploration</a></div><div><b>Author(s):&nbsp;</b>Alessandra Sciutti, Giulio Sandini</div><div><b>Pages:&nbsp;</b>366 - 374</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9270601/\">A Matrix Determinant Feature Extraction Approach for Decoding Motor and Mental Imagery EEG in Subject-Specific Tasks</a></div><div><b>Author(s):&nbsp;</b>Muhammad Tariq Sadiq, Xiaojun Yu, Zhaohui Yuan, Muhammad Zulkifal Aziz, Siuly Siuly, Weiping Ding</div><div><b>Pages:&nbsp;</b>375 - 387</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9273020/\">A Compressed Sensing Network for Acquiring Human Pressure Information</a></div><div><b>Author(s):&nbsp;</b>Tao Han, Kuangrong Hao, Xue-Song Tang, Xin Cai, Tong Wang, Xiaoyan Liu</div><div><b>Pages:&nbsp;</b>388 - 402</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9288837/\">Imitation Learning-Based Algorithm for Drone Cinematography System</a></div><div><b>Author(s):&nbsp;</b>Yuanjie Dang, Chong Huang, Peng Chen, Ronghua Liang, Xin Yang, Kwang-Ting Cheng</div><div><b>Pages:&nbsp;</b>403 - 413</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9292658/\">Learning Multipart Attention Neural Network for Zero-Shot Classification</a></div><div><b>Author(s):&nbsp;</b>Min Meng, Jie Wei, Jigang Wu</div><div><b>Pages:&nbsp;</b>414 - 423</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9300220/\">Motor-Cortex-Like Recurrent Neural Network and Multitask Learning for the Control of Musculoskeletal Systems</a></div><div><b>Author(s):&nbsp;</b>Jiahao Chen, Hong Qiao</div><div><b>Pages:&nbsp;</b>424 - 436</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9314882/\">Holistically Associated Transductive Zero-Shot Learning</a></div><div><b>Author(s):&nbsp;</b>Yangyang Xu, Xuemiao Xu, Guoqiang Han, Shengfeng He</div><div><b>Pages:&nbsp;</b>437 - 447</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9314893/\">In Situ Learning in Hardware Compatible Multilayer Memristive Spiking Neural Network</a></div><div><b>Author(s):&nbsp;</b>Jiwei Li, Hui Xu, Sheng-Yang Sun, Nan Li, Qingjiang Li, Zhiwei Li, Haijun Liu</div><div><b>Pages:&nbsp;</b>448 - 461</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9316712/\">An Empirical Study of Active Inference on a Humanoid Robot</a></div><div><b>Author(s):&nbsp;</b>Guillermo Oliver, Pablo Lanillos, Gordon Cheng</div><div><b>Pages:&nbsp;</b>462 - 471</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9319691/\">Spontaneous Temporal Grouping Neural Network for Long-Term Memory Modeling</a></div><div><b>Author(s):&nbsp;</b>Dongjing Shan, Xiongwei Zhang, Chao Zhang</div><div><b>Pages:&nbsp;</b>472 - 484</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9319729/\">Augmented Memory Replay in Reinforcement Learning With Continuous Control</a></div><div><b>Author(s):&nbsp;</b>Mirza Ramicic, Andrea Bonarini</div><div><b>Pages:&nbsp;</b>485 - 496</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9321385/\">Cogno-Vest: A Torso-Worn, Force Display to Experimentally Induce Specific Hallucinations and Related Bodily Sensations</a></div><div><b>Author(s):&nbsp;</b>Atena Fadaei Jouybari, Kenny Jeanmonod, Olivier A. Kannape, Jevita Potheegadoo, Hannes Bleuler, Masayuki Hara, Olaf Blanke</div><div><b>Pages:&nbsp;</b>497 - 506</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9328469/\">Multiscale Brain-Like Neural Network for Saliency Prediction on Omnidirectional Images</a></div><div><b>Author(s):&nbsp;</b>Dandan Zhu, Yongqing Chen, Defang Zhao, Yucheng Zhu, Qiangqiang Zhou, Guangtao Zhai, Xiaokang Yang</div><div><b>Pages:&nbsp;</b>507 - 518</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9328521/\">A Dissemination Model Based on Psychological Theories in Complex Social Networks</a></div><div><b>Author(s):&nbsp;</b>Tianyi Luo, Zhidong Cao, Daniel Zeng, Qingpeng Zhang</div><div><b>Pages:&nbsp;</b>519 - 531</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9328807/\">Neural Correlates of Single-Task Versus Cognitive-Motor Dual-Task Training</a></div><div><b>Author(s):&nbsp;</b>Jiaxing Wang, Weiqun Wang, Shixin Ren, Weiguo Shi, Zeng-Guang Hou</div><div><b>Pages:&nbsp;</b>532 - 540</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9330624/\">RCIT: An RSVP-Based Concealed Information Test Framework Using EEG Signals</a></div><div><b>Author(s):&nbsp;</b>Hanwen Wang, Yu Qi, Hang Yu, Yueming Wang, Cong Liu, Guoping Hu, Gang Pan</div><div><b>Pages:&nbsp;</b>541 - 551</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9335985/\">Resting-State Brain Connectivity via Multivariate EMD in Mild Cognitive Impairment</a></div><div><b>Author(s):&nbsp;</b>Haifeng Wu, Lingxu Kong, Yu Zeng, Han Bao</div><div><b>Pages:&nbsp;</b>552 - 564</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9336705/\">Toward Emerging Cubic-Spline Patterns With a Mobile Robotics Swarm System</a></div><div><b>Author(s):&nbsp;</b>Belkacem Khaldi, Fouzi Harrou, Foudil Cherif, Ying Sun</div><div><b>Pages:&nbsp;</b>565 - 577</div><div><br /></div><div><b>28) </b><a href=\"https://ieeexplore.ieee.org/document/9337927/\">Bicriteria Velocity Minimization Approach of Self-Motion for Redundant Robot Manipulators With Varying-Gain Recurrent Neural Network</a></div><div><b>Author(s):&nbsp;</b>Xiaohui Ren, Pengchao Zhang, Zhijun Zhang</div><div><b>Pages:&nbsp;</b>578 - 587</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9340391/\">Cross-Corpus Speech Emotion Recognition Based on Joint Transfer Subspace Learning and Regression</a></div><div><b>Author(s):&nbsp;</b>Weijian Zhang, Peng Song, Dongliang Chen, Chao Sheng, Wenjing Zhang</div><div><b>Pages:&nbsp;</b>588 - 598</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9340402/\">A Portable Device for Hand Rehabilitation With Force Cognition: Design, Interaction, and Experiment</a></div><div><b>Author(s):&nbsp;</b>Lei Yang, Fuhai Zhang, Jingbin Zhu, Yili Fu</div><div><b>Pages:&nbsp;</b>599 - 607</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9360879/\">Evaluation of Mental Workload Associated With Time Pressure in Rapid Serial Visual Presentation Tasks</a></div><div><b>Author(s):&nbsp;</b>Weibo Yi, Shuang Qiu, Xinan Fan, Lijian Zhang, Dong Ming</div><div><b>Pages:&nbsp;</b>608 - 616</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9361701/\">A Brain-Inspired Self-Organizing Episodic Memory Model for a Memory Assistance Robot</a></div><div><b>Author(s):&nbsp;</b>Chiao-Yu Yang, Edwinn Gamborino, Li-Chen Fu, Yu-Ling Chang</div><div><b>Pages:&nbsp;</b>617 - 628</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9363315/\">Feature-Specific Denoising of Neural Activity for Natural Image Identification</a></div><div><b>Author(s):&nbsp;</b>Hao Wu, Nanning Zheng, Badong Chen</div><div><b>Pages:&nbsp;</b>629 - 638</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9364289/\">Robust Heart Rate Estimation With Spatial\u2013Temporal Attention Network From Facial Videos</a></div><div><b>Author(s):&nbsp;</b>Min Hu, Fei Qian, Xiaohua Wang, Lei He, Dong Guo, Fuji Ren</div><div><b>Pages:&nbsp;</b>639 - 647</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9365692/\">MFFFLD: A Multimodal-Feature-Fusion-Based Fingerprint Liveness Detection</a></div><div><b>Author(s):&nbsp;</b>Chengsheng Yuan, Shengming Jiao, Xingming Sun, Q. M. Jonathan Wu</div><div><b>Pages:&nbsp;</b>648 - 661</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9366777/\">Conditional Generative Adversarial Networks for Optimal Path Planning</a></div><div><b>Author(s):&nbsp;</b>Nachuan Ma, Jiankun Wang, Jianbang Liu, Max Q.-H. Meng</div><div><b>Pages:&nbsp;</b>662 - 671</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9372969/\">Data-Fusion-Based Two-Stage Cascade Framework for Multimodality Face Anti-Spoofing</a></div><div><b>Author(s):&nbsp;</b>Weihua Liu, Xiaokang Wei, Tao Lei, Xingwu Wang, Hongying Meng, Asoke K. Nandi</div><div><b>Pages:&nbsp;</b>672 - 683</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9372334/\">Epileptic Classification With Deep-Transfer-Learning-Based Feature Fusion Algorithm</a></div><div><b>Author(s):&nbsp;</b>Jiuwen Cao, Dinghan Hu, Yaomin Wang, Jianzhong Wang, Baiying Lei</div><div><b>Pages:&nbsp;</b>684 - 695</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9372336/\">Attention-Mechanism-Based Real-Time Gaze Tracking in Natural Scenes With Residual Blocks</a></div><div><b>Author(s):&nbsp;</b>Lihong Dai, Jinguo Liu, Zhaojie Ju, Yang Gao</div><div><b>Pages:&nbsp;</b>696 - 707</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9380656/\">Effect of Physical and Virtual Feedback on Reach-to-Grasp Movements in Virtual Environments</a></div><div><b>Author(s):&nbsp;</b>Qiuwen Cai, Junhua Li, Jinyi Long</div><div><b>Pages:&nbsp;</b>708 - 714</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9395500/\">Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition</a></div><div><b>Author(s):&nbsp;</b>Wei Liu, Jie-Lin Qiu, Wei-Long Zheng, Bao-Liang Lu</div><div><b>Pages:&nbsp;</b>715 - 729</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9405306/\">Uncertainty Modeling for Multicenter Autism Spectrum Disorder Classification Using Takagi\u2013Sugeno\u2013Kang Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Zhongyi Hu, Jun Wang, Chunxiang Zhang, Zhenzhen Luo, Xiaoqing Luo, Lei Xiao, Jun Shi</div><div><b>Pages:&nbsp;</b>730 - 739</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9406414/\">Time-Spatial Multiscale Net for Vehicle Counting and Traffic Volume Estimation</a></div><div><b>Author(s):&nbsp;</b>Shuang Li, Chunsheng Liu, Faliang Chang</div><div><b>Pages:&nbsp;</b>740 - 751</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9427962/\">Multiscale Conditional Relationship Graph Network for Referring Relationships in Images</a></div><div><b>Author(s):&nbsp;</b>Jian Zhu, Hanli Wang</div><div><b>Pages:&nbsp;</b>752 - 760</div><div><br /></div><div><b>45) </b><a href=\"https://ieeexplore.ieee.org/document/9502838/\">Investigating Neural Substrates of Individual Independence and Interdependence Orientations via Efficiency-Based Dynamic Functional Connectivity: A Machine Learning Approach</a></div><div><b>Author(s):&nbsp;</b>Yifan Zhu, Xuesong Li, Yang Sun, Haixu Wang, Hua Guo, Jie Sui</div><div><b>Pages:&nbsp;</b>761 - 771</div><div><br /></div><div><b>46)</b> <a href=\"https://ieeexplore.ieee.org/document/9576513/\">EMRES: A New EMotional RESpondent Robot</a></div><div><b>Author(s):&nbsp;</b>Elena Battini S\u00f6nmez, Hasan Han, O\u01e7uzcan Karadeniz, Tu\u01e7ba Dalyan, Baykal Sar\u0131o\u01e7lu</div><div><b>Pages:&nbsp;</b>772 - 780</div><div><br /></div><div style=\"text-align: left;\">&nbsp;</div>",
            "pubdate": "2022-06-18T12:00:00.161+12:00",
            "pubdate_parsed": [
                2022,
                6,
                18
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 13, July 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/soft-computing-volume-26-issue-13-july.html",
            "description": "<div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07052-5\">Implication in finite posets with pseudocomplemented sections</a></div><div><b>Author(s): </b>Ivan Chajda, Helmut L\u00e4nger</div><div><b>Pages: </b>5945 - 5953</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07059-y\">Chain complex and quotient structure of fuzzy hypergroups</a></div><div><b>Author(s):&nbsp;</b>F. Barkhori Mehni, S. Ostadhadi-Dehkordi</div><div><b>Pages:&nbsp;</b>5955 - 5963</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07063-2\">Decompositions of average probability uninitialized sequential quantum machines</a></div><div><b>Author(s):&nbsp;</b>Feidan Huang</div><div><b>Pages:&nbsp;</b>5965 - 5974</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07078-9\">Pythagorean fuzzy inequality derived by operation, equality and aggregation operator</a></div><div><b>Author(s):&nbsp;</b>Xindong Peng, Zhigang Luo</div><div><b>Pages:&nbsp;</b>5975 - 6018</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07113-9\">Review: a generalized belief interval-valued soft set with applications in decision making</a></div><div><b>Author(s):&nbsp;</b>G\u00f6zde Yaylal\u0131, Nazan \u00c7akmak Polat, Bekir Tanay</div><div><b>Pages:&nbsp;</b>6019 - 6020</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07049-0\">A New single switch DC-DC converter for PEM fuel cell-based electric vehicle system with an improved beta-fuzzy logic MPPT controller</a></div><div><b>Author(s):&nbsp;</b>C. H. Hussaian Basha, C. Rani</div><div><b>Pages:&nbsp;</b>6021 - 6040</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07114-8\">An interval method to measure the uncertainty of basic probability assignment</a></div><div><b>Author(s):&nbsp;</b>Jinyan Su, Yong Deng</div><div><b>Pages:&nbsp;</b>6041 - 6050</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07120-w\">An iterative method for solving linear fuzzy fractional integral equation</a></div><div><b>Author(s):&nbsp;</b>Alexandru Mihai Bica, Shokrollah Ziari, Zoltan Satmari</div><div><b>Pages:&nbsp;</b>6051 - 6062</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07123-7\">Mining fuzzy high average-utility itemsets using fuzzy utility lists and efficient pruning approach</a></div><div><b>Author(s):&nbsp;</b>Manijeh Hajihoseini, Mohammad Karim Sohrabi</div><div><b>Pages:&nbsp;</b>6063 - 6086</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07126-4\">q-Rung orthopair fuzzy N-soft aggregation operators and corresponding applications to multiple-attribute group decision making</a></div><div><b>Author(s):&nbsp;</b>Haidong Zhang, TaiBen Nan, Yanping He</div><div><b>Pages:&nbsp;</b>6087 - 6099</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07013-y\">Sustainability assessment of supply chains by a novel robust two-stage network DEA model: a case study in the transport industry</a></div><div><b>Author(s):&nbsp;</b>Amirali Fathi, Balal Karimi, Reza Farzipoor Saen</div><div><b>Pages:&nbsp;</b>6101 - 6118</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07102-y\">Optimal scale combination selection for inconsistent multi-scale decision tables</a></div><div><b>Author(s):&nbsp;</b>Zhu Yingjie, Yang Bin</div><div><b>Pages:&nbsp;</b>6119 - 6129</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07075-y\">Gradient eigendecomposition invariance biogeography-based optimization for mobile robot path planning</a></div><div><b>Author(s):&nbsp;</b>Xiaodong Na, Jiaqian Wang...Decai Li</div><div><b>Pages:&nbsp;</b>6131 - 6144</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07110-y\">ML-WiGR: a meta-learning-based approach for cross-domain device-free gesture recognition</a></div><div><b>Author(s):&nbsp;</b>Zhenyue Gao, Jianqiang Xue...Wendong Xiao</div><div><b>Pages:&nbsp;</b>6145 - 6155</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07129-1\">Ensemble of hybrid neural networks to compensate for epistemic uncertainties: a case study in system prognosis</a></div><div><b>Author(s):&nbsp;</b>Arinan Dourado, Felipe Viana</div><div><b>Pages:&nbsp;</b>6157 - 6173</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07130-8\">Chronic diseases monitoring and diagnosis system based on features selection and machine learning predictive models</a></div><div><b>Author(s):&nbsp;</b>Sahar A. EL-Rahman, Ala Saleh Alluhaidan...Duna N. AlZunaytan</div><div><b>Pages:&nbsp;</b>6175 - 6199</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07138-0\">LM-MFP: large-scale morphology and multi-criteria-based feature pooling for image parsing</a></div><div><b>Author(s):&nbsp;</b>Vishal Srivastava, Bhaskar Biswas</div><div><b>Pages:&nbsp;</b>6201 - 6218</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07140-6\">Unsupervised abnormal detection using VAE with memory</a></div><div><b>Author(s):&nbsp;</b>Xin Xie, Xinlei Li...Huiping Li</div><div><b>Pages:&nbsp;</b>6219 - 6231</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07149-x\">Exploring different interaction among features for CTR prediction</a></div><div><b>Author(s):&nbsp;</b>Leilei Yang, Wenguang Zheng, Yingyuan Xiao</div><div><b>Pages:&nbsp;</b>6233 - 6243</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07163-z\">Brain tumor magnetic resonance image classification: a deep learning approach</a></div><div><b>Author(s):&nbsp;</b>Machiraju Jaya Lakshmi, S. Nagaraja Rao</div><div><b>Pages:&nbsp;</b>6245 - 6253</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07164-y\">An effective text compression\u2013encryption using tight and indirect encryptions</a></div><div><b>Author(s):&nbsp;</b>Ranganath Ponnaboyina, Ramesh Makala...Venkata Ramana Gupta Nallagattla</div><div><b>Pages:&nbsp;</b>6255 - 6264</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07002-1\">Power system security enhancement in FACTS devices based on Yin\u2013Yang pair optimization algorithm</a></div><div><b>Author(s):&nbsp;</b>A. Amarendra, L. Ravi Srinivas, R. Srinivasa Rao</div><div><b>Pages:&nbsp;</b>6265 - 6291</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07033-8\">Enhancing the contrast of the grey-scale image based on meta-heuristic optimization algorithm</a></div><div><b>Author(s):&nbsp;</b>Ali Hussain Khan, Shameem Ahmed...Ram Sarkar</div><div><b>Pages:&nbsp;</b>6293 - 6315</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07039-2\">2-Tuple unbalanced linguistic multiple-criteria group decision-making using prospect theory data envelopment analysis</a></div><div><b>Author(s):&nbsp;</b>Imran Khan, Anjana Gupta, Aparna Mehra</div><div><b>Pages:&nbsp;</b>6317 - 6332</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06610-7\">Offline parameter estimation of a modified permanent magnet generator using GSA and GSA-PSO</a></div><div><b>Author(s):&nbsp;</b>Vinod Puri, Yogesh K. Chauhan</div><div><b>Pages:&nbsp;</b>6333 - 6345</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06613-4\">A survey on soft computing-based high-utility itemsets mining</a></div><div><b>Author(s):&nbsp;</b>Rajiv Kumar, Kuldeep Singh</div><div><b>Pages:&nbsp;</b>6347 - 6392</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07157-x\">Limestone supplier selection for coal thermal power plant by applying integrated PF-SAW and PF-EDAS approach</a></div><div><b>Author(s):&nbsp;</b>Fethullah G\u00f6\u00e7er</div><div><b>Pages:&nbsp;</b>6393 - 6414</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07165-x\">Consensus tracking-based clock synchronization for the Internet of Things</a></div><div><b>Author(s):&nbsp;</b>Yuqing Niu, Ting Yang...Wei Li</div><div><b>Pages:&nbsp;</b>6415 - 6428</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07167-9\">Blockchain multi-objective optimization approach-enabled secure and cost-efficient scheduling for the Internet of Medical Things (IoMT) in fog-cloud system</a></div><div><b>Author(s):&nbsp;</b>Abdullah Lakhan, Mazin Abed Mohammed...Karrar Hameed Abdulkareem</div><div><b>Pages:&nbsp;</b>6429 - 6442</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06973-5\">Periodic event-triggered modified repetitive control with equivalent-input-disturbance estimator based on T-S fuzzy model for nonlinear systems</a></div><div><b>Author(s):&nbsp;</b>Sameh Abd-Elhaleem, Mohamed Soliman, Mohamed Hamdy</div><div><b>Pages:&nbsp;</b>6443 - 6459</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07205-6\">Correction to: Periodic event-triggered modified repetitive control with equivalent-input-disturbance estimator based on T-S fuzzy model for nonlinear systems</a></div><div><b>Author(s):&nbsp;</b>Sameh Abd-Elhaleem, Mohamed Soliman, Mohamed Hamdy</div><div><b>Pages:&nbsp;</b>6461 - 6462</div><div><br /></div><div><br /></div>",
            "pubdate": "2022-06-19T12:00:00.113+12:00",
            "pubdate_parsed": [
                2022,
                6,
                19
            ],
            "email_sent": true
        },
        "Programming languages for AI: a meta-analysis of popularity": {
            "url": "https://computational-intelligence.blogspot.com/2022/06/programming-languages-for-ai-meta.html",
            "description": "<div style=\"text-align: left;\"><u><b>Introduction</b></u></div><p>There are many articles about which programming languages an AI engineer should be familiar with. While there are frequent similarities between these lists, comparing them is difficult. In this post, I'll briefly report on a quick metanalysis of these articles, with an eye to producing a ranked list of programming languages that balances popularity and rating.</p><div style=\"text-align: left;\"><b><u>Methodology</u></b></div><p>The data I used in this analysis came from <a href=\"https://computational-intelligence.blogspot.com/feeds/posts/default#References\">24 different articles</a>. To ensure that the data was mostly current, I restricted the articles to those that had been either published or updated since the beginning of 2020. I also checked that each article was from a different author, to prevent duplication. Where several languages were ranked the same on the list, I recorded them as separate entries with the same rank.</p><div style=\"text-align: left;\"><u><b>Results</b></u></div><p>The lists were of varying lengths, the minimum was five and the maximum was 12. The most frequent list length was ten, with a mean of 8 and median of 9.</p><p>I analysed the lists in three ways:</p><p></p><ol style=\"text-align: left;\"><li>The <a href=\"https://computational-intelligence.blogspot.com/feeds/posts/default#frequency\">frequency</a> at which a language appeared in the lists, regardless of position on the list;</li><li>The <a href=\"https://computational-intelligence.blogspot.com/feeds/posts/default#median\">median rank</a> assigned to each language across all lists in which it appears, and;</li><li>A <a href=\"https://computational-intelligence.blogspot.com/feeds/posts/default#weighted\">weighted median</a> rank, where the median rank of the language was weighted according to the frequency at which it appeared in lists. This corrects for outliers that were highly ranked on only a small number of lists.</li></ol><div>Below are the top ten ranked languages, for each analysis method.</div><p></p><p>In order of <a name=\"frequency\">frequency</a>, the top ten languages for AI are:</p><p></p><ol style=\"text-align: left;\"><li>Python</li><li>Java</li><li>R</li><li>C++</li><li>Lisp</li><li>Julia</li><li>Prolog</li><li>Haskell</li><li>JavaScript</li><li>Scala</li></ol><p></p><p>In order of <a name=\"median\">median rank</a>:</p><p></p><ol style=\"text-align: left;\"><li>Python</li><li>Java</li><li>C</li><li>C#</li><li>R</li><li>C++</li><li>Lisp</li><li>JavaScript</li><li>Prolog</li><li>Julia</li></ol><div>Note that this is only the median ranks of languages, regardless of how frequently they are listed. This has the effect of pushing some languages, such as C, higher up the list than they would otherwise be. This is corrected by the <a name=\"weighted\">weighted median rank</a>.</div><div><br /></div><div>The top ten languages for AI, as ordered by weighted median rank, are:</div><div><ol style=\"text-align: left;\"><li>Python</li><li>Java</li><li>R</li><li>C++</li><li>Lisp</li><li>Prolog</li><li>Julia</li><li>Haskell</li><li>JavaScript</li><li>Scala</li></ol></div><p></p><p>As this listing accounts for both rankings of languages, and the frequency at which the language appears in the articles, I consider this to be the most informative.</p><div style=\"text-align: left;\"><a name=\"References\"><u><b>References</b></u></a></div><ul style=\"text-align: left;\"><li><a href=\"https://www.itproportal.com/features/top-five-programming-languages-for-ai-and-machine-learning-you-should-learn-this-year/\">https://www.itproportal.com/features/top-five-programming-languages-for-ai-and-machine-learning-you-should-learn-this-year/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.yoh.com/blog/10-best-programming-languages-for-ai-development\">https://www.yoh.com/blog/10-best-programming-languages-for-ai-development</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.trio.dev/blog/best-languages-for-ai\">https://www.trio.dev/blog/best-languages-for-ai</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.makeuseof.com/best-programming-languages-ai-development/\">https://www.makeuseof.com/best-programming-languages-ai-development/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.moveoapps.com/blog/best-programming-languages-ai-development/\">https://www.moveoapps.com/blog/best-programming-languages-ai-development/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.bairesdev.com/blog/top-6-languages-for-artificial-intelligence/\">https://www.bairesdev.com/blog/top-6-languages-for-artificial-intelligence/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.turing.com/blog/best-programming-languages-for-ai-development/\">https://www.turing.com/blog/best-programming-languages-for-ai-development/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.springboard.com/blog/data-science/best-programming-language-for-ai/\">https://www.springboard.com/blog/data-science/best-programming-language-for-ai/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.ksolves.com/blog/artificial-intelligence/top-8-programming-languages-for-artificial-intelligence-projects\">https://www.ksolves.com/blog/artificial-intelligence/top-8-programming-languages-for-artificial-intelligence-projects</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.ideamotive.co/blog/the-best-programming-language-for-ai-development\">https://www.ideamotive.co/blog/the-best-programming-language-for-ai-development</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.spec-india.com/blog/ai-programming-languages\">https://www.spec-india.com/blog/ai-programming-languages</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://invozone.com/blog/top-8-programming-languages-for-ai-development-in-2022\">https://invozone.com/blog/top-8-programming-languages-for-ai-development-in-2022</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://careerkarma.com/blog/best-programming-languages-for-ai/\">https://careerkarma.com/blog/best-programming-languages-for-ai/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.datasciencecentral.com/best-programming-languages-for-ai-amp-ml-artificial-intelligence/\">https://www.datasciencecentral.com/best-programming-languages-for-ai-amp-ml-artificial-intelligence/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.toolbox.com/tech/artificial-intelligence/articles/ai-programming-languages/\">https://www.toolbox.com/tech/artificial-intelligence/articles/ai-programming-languages/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://blog.robotiq.com/what-is-the-best-programming-language-for-robotics\">https://blog.robotiq.com/what-is-the-best-programming-language-for-robotics</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.linkedin.com/pulse/top-9-ai-programming-languages-2021-george-burlakov/\">https://www.linkedin.com/pulse/top-9-ai-programming-languages-2021-george-burlakov/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://pixelplex.io/blog/top-ai-programming-languages/\">https://pixelplex.io/blog/top-ai-programming-languages/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://iglu.net/best-programming-languages-for-ai/\">https://iglu.net/best-programming-languages-for-ai/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.zfort.com/blog/best-programming-language-for-ai\">https://www.zfort.com/blog/best-programming-language-for-ai</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.orientsoftware.com/blog/best-programming-language-for-ai/\">https://www.orientsoftware.com/blog/best-programming-language-for-ai/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://readwrite.com/top-10-programming-languages-to-become-an-ai-developer/\">https://readwrite.com/top-10-programming-languages-to-become-an-ai-developer/</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://towardsdatascience.com/top-programming-languages-for-ai-engineers-in-2020-33a9f16a80b0\">https://towardsdatascience.com/top-programming-languages-for-ai-engineers-in-2020-33a9f16a80b0</a><span style=\"white-space: pre;\">\t\t</span></li><li><a href=\"https://www.rankred.com/best-artificial-intelligence-programming-language/\">https://www.rankred.com/best-artificial-intelligence-programming-language/</a><span style=\"white-space: pre;\">\t</span></li></ul><p></p>",
            "pubdate": "2022-06-20T15:41:00.004+12:00",
            "pubdate_parsed": [
                2022,
                6,
                20
            ],
            "email_sent": true
        },
        "IEEE Transactions on Fuzzy Systems, Volume 30, Issue 7": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/ieee-transactions-on-fuzzy-systems.html",
            "description": "<div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9419714/\">Novel Heterogeneous Mode-Dependent Impulsive Synchronization for Piecewise T-S Fuzzy Probabilistic Coupled Delayed Neural Networks</a></div><div><b>Author(s): </b>Xiangxiang Wang, Yongbin Yu, Shouming Zhong, Kaibo Shi, Nijing Yang, Dingfa Zhang, Jingye Cai, Nyima Tashi</div><div><b>Pages: </b>2142 - 2156</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9419710/\">Nonlinear Dimensionality Reduction for Data Visualization: An Unsupervised Fuzzy Rule-Based Approach</a></div><div><b>Author(s):&nbsp;</b>Suchismita Das, Nikhil R. Pal</div><div><b>Pages:&nbsp;</b>2157 - 2169</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9423589/\">An Efficient Self-Organizing Deep Fuzzy Neural Network for Nonlinear System Modeling</a></div><div><b>Author(s):&nbsp;</b>Gongming Wang, Junfei Qiao</div><div><b>Pages:&nbsp;</b>2170 - 2182</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9424990/\">Adaptive Event-Triggered Fuzzy Tracking Control of Uncertain Stochastic Nonlinear Systems With Unmeasurable States</a></div><div><b>Author(s):&nbsp;</b>Rui-Yan Zhang, Li-Bing Wu, Nan-Nan Zhao, Yan Yan</div><div><b>Pages:&nbsp;</b>2183 - 2196</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9425433/\">Membership Function, Time Delay-Dependent \u03b7-Exponential Stabilization of the Positive Discrete-Time Polynomial Fuzzy Model Control System</a></div><div><b>Author(s):&nbsp;</b>Xiaomiao Li, Kamyar Mehran, Zhiyong Bao</div><div><b>Pages:&nbsp;</b>2197 - 2209</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9425590/\">A Novel Three-Way Decision Model Based on Utility Theory in Incomplete Fuzzy Decision Systems</a></div><div><b>Author(s):&nbsp;</b>Jianming Zhan, Jin Ye, Weiping Ding, Peide Liu</div><div><b>Pages:&nbsp;</b>2210 - 2226</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9427360/\">Finite-Time Adaptive Fuzzy Prescribed Performance Control for High-Order Stochastic Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Shuai Sui, C. L. Philip Chen, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>2227 - 2240</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9427125/\">Maximum Number of Line Faults in a P2P Network System Based on the Addition-Min Fuzzy Relation Inequalities</a></div><div><b>Author(s):&nbsp;</b>Xiao-Peng Yang, Gengzhong Zheng</div><div><b>Pages:&nbsp;</b>2241 - 2253</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9427225/\">Sampled-Data-Based H\u221e Fuzzy Pinning Synchronization of Complex Networked Systems With Adaptive Event-Triggered Communications</a></div><div><b>Author(s):&nbsp;</b>Xin Wang, Ju H. Park, Huilan Yang, Zhiqi Yu</div><div><b>Pages:&nbsp;</b>2254 - 2265</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9428619/\">Analysis of Ranking Consistency in Linguistic Multiple Attribute Decision Making: The Roles of Granularity and Decision Rules</a></div><div><b>Author(s):&nbsp;</b>Sihai Zhao, Yucheng Dong, Luis Mart\u00edne, Witold Pedrycz</div><div><b>Pages:&nbsp;</b>2266 - 2278</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9428344/\">Adaptive Fast Finite-Time Fuzzy Control of Stochastic Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Zhaoyang You, Fang Wang</div><div><b>Pages:&nbsp;</b>2279 - 2288</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9428513/\">Model-Based Fuzzy l2\u2212l\u221e Filtering for Discrete-Time Semi-Markov Jump Nonlinear Systems Using Semi-Markov Kernel</a></div><div><b>Author(s):&nbsp;</b>Jing Wang, Yigang Zhang, Lei Su, Ju H. Park, Hao Shen</div><div><b>Pages:&nbsp;</b>2289 - 2299</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9430728/\">Adaptive Fuzzy SOSM Controller Design With Output Constraints</a></div><div><b>Author(s):&nbsp;</b>Shihong Ding, Binbin Zhang, Keqi Mei, Ju H. Park</div><div><b>Pages:&nbsp;</b>2300 - 2311</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9430676/\">Fault-Tolerant Quantized Sliding Mode Observers Design for a Class of Takagi-Sugeno Fuzzy System With Unmeasurable Premise Variable</a></div><div><b>Author(s):&nbsp;</b>Ang Li, Guangren Duan, Ming Liu, Jingbo Fu</div><div><b>Pages:&nbsp;</b>2312 - 2324</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9432713/\">Fuzzy Energy-to-Peak Filtering for Continuous-Time Nonlinear Singular System</a></div><div><b>Author(s):&nbsp;</b>Xiao-Heng Chang, Ming-Yang Qiao, Xudong Zhao</div><div><b>Pages:&nbsp;</b>2325 - 2336</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9435080/\">An Analytical Method to Compute the Approximate Inverses of a Fuzzy Matrix With Max-Product Composition</a></div><div><b>Author(s):&nbsp;</b>Yan-Kuen Wu, Yung-Yih Lur, Hsun-Chih Kuo, Ching-Feng Wen</div><div><b>Pages:&nbsp;</b>2337 - 2346</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9436023/\">Fuzzy Event-Triggered Integral Sliding Mode Control of Nonlinear Continuous-Time Systems</a></div><div><b>Author(s):&nbsp;</b>Zeinab Echreshavi, Mohsen Farbood, Mokhtar Shasadeghi</div><div><b>Pages:&nbsp;</b>2347 - 2359</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9435959/\">Fuzzy Measures and Choquet Integrals Based on Fuzzy Covering Rough Sets</a></div><div><b>Author(s):&nbsp;</b>Xiaohong Zhang, Jingqian Wang, Jianming Zhan, Jianhua Dai</div><div><b>Pages:&nbsp;</b>2360 - 2374</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9436033/\">Fast Fuzzy Clustering Based on Anchor Graph</a></div><div><b>Author(s):&nbsp;</b>Feiping Nie, Chaodie Liu, Rong Wang, Zhen Wang, Xuelong Li</div><div><b>Pages:&nbsp;</b>2375 - 2387</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9437688/\">Multilabel Feature Selection Based on Relative Discernibility Pair Matrix</a></div><div><b>Author(s):&nbsp;</b>Erliang Yao, Deyu Li, Yanhui Zhai, Chao Zhang</div><div><b>Pages:&nbsp;</b>2388 - 2401</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9439174/\">Quantized Guaranteed Cost Output Feedback Control for Nonlinear Networked Control Systems and Its Applications</a></div><div><b>Author(s):&nbsp;</b>Qunxian Zheng, Shengyuan Xu, Baozhu Du</div><div><b>Pages:&nbsp;</b>2402 - 2411</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9439896/\">Fuzzy Bayesian Knowledge Tracing</a></div><div><b>Author(s):&nbsp;</b>Fei Liu, Xuegang Hu, Chenyang Bu, Kui Yu</div><div><b>Pages:&nbsp;</b>2412 - 2425</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9440662/\">Dwell-Time-Dependent H\u221e Bumpless Transfer Control for Discrete-Time Switched Interval Type-2 Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Siyuan Zhang, Jun Zhao</div><div><b>Pages:&nbsp;</b>2426 - 2437</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9442351/\">Fuzzy Multioutput Transfer Learning for Regression</a></div><div><b>Author(s):&nbsp;</b>Xiaoya Che, Hua Zuo, Jie Lu, Degang Chen</div><div><b>Pages:&nbsp;</b>2438 - 2451</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9442297/\">Stability and Control of Fuzzy Semi-Markov Jump Systems Under Unknown Semi-Markov Kernel</a></div><div><b>Author(s):&nbsp;</b>Zepeng Ning, Bo Cai, Rui Weng, Lixian Zhang, Shun-Feng Su</div><div><b>Pages:&nbsp;</b>2452 - 2465</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9446576/\">New Results on Dissipative Control for a Class of Singular Takagi\u2013Sugeno Fuzzy Systems With Time Delay</a></div><div><b>Author(s):&nbsp;</b>Zhiguang Feng, Huayang Zhang, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>2466 - 2475</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9447197/\">Finite-Time Dynamic Event-Triggered Distributed H\u221e Filtering for T-S Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Xiaoyuan Zheng, Hao Zhang, Zhuping Wang, Changzhu Zhang, Huaicheng Yan</div><div><b>Pages:&nbsp;</b>2476 - 2486</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9447907/\">Asynchronous Fault Detection for Interval Type-2 Fuzzy Nonhomogeneous Higher Level Markov Jump Systems With Uncertain Transition Probabilities</a></div><div><b>Author(s):&nbsp;</b>Xiang Zhang, Hai Wang, Vladimir Stojanovic, Peng Cheng, Shuping He, Xiaoli Luan, Fei Liu</div><div><b>Pages:&nbsp;</b>2487 - 2499</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9448475/\">Fuzzy Community Detection Based on Elite Symbiotic Organisms Search and Node Neighborhood Information</a></div><div><b>Author(s):&nbsp;</b>Jing Xiao, Yan-Jiao Wang, Xiao-Ke Xu</div><div><b>Pages:&nbsp;</b>2500 - 2514</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9449989/\">Cooperative Target Enclosing of Ring-Networked Underactuated Autonomous Surface Vehicles Based on Data-Driven Fuzzy Predictors and Extended State Observers</a></div><div><b>Author(s):&nbsp;</b>Yue Jiang, Zhouhua Peng, Dan Wang, Yong Yin, Qing-Long Han</div><div><b>Pages:&nbsp;</b>2515 - 2528</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9454304/\">A Simplified Finite-Time Fuzzy Neural Controller With Prescribed Performance Applied to Waverider Aircraft</a></div><div><b>Author(s):&nbsp;</b>Xiangwei Bu, Qiang Qi, Baoxu Jiang</div><div><b>Pages:&nbsp;</b>2529 - 2537</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9454283/\">Stability Criteria for Fuzzy-Based Sampled-Data Control Systems via a Fractional Parameter-Based Refined Looped Lyapunov Functional</a></div><div><b>Author(s):&nbsp;</b>Lakshmanan Shanmugam, Young Hoon Joo</div><div><b>Pages:&nbsp;</b>2538 - 2549</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9454293/\">Distributed Kalman Filter With Fuzzy Noises Over Multiagent Systems</a></div><div><b>Author(s):&nbsp;</b>Haoshen Lin, Chen Hu, Zhenhua Deng, Gang Liu</div><div><b>Pages:&nbsp;</b>2550 - 2562</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9454307/\">Self-Sustaining Oscillations With an Internal Two-Fuzzy Inference System Based on the Poincar\u00e9\u2013Bendixson Method</a></div><div><b>Author(s):&nbsp;</b>Jorge A. Lopez-Renteria, Lisdan Herrera-Garcia, Selene L. Cardenas-Maciel, Luis T. Aguilar, Nohe R. Cazarez-Castro</div><div><b>Pages:&nbsp;</b>2563 - 2573</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9454376/\">Admissibility and Design Issues for T-S Fuzzy Descriptor Systems With Perturbed Derivative Matrices in the Rules</a></div><div><b>Author(s):&nbsp;</b>Chih-Peng Huang</div><div><b>Pages:&nbsp;</b>2574 - 2582</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9454380/\">Guaranteed Cost Control for Interval Type-2 Fuzzy Semi-Markov Switching Systems Within a Finite-Time Interval</a></div><div><b>Author(s):&nbsp;</b>Linchuang Zhang, Yonghui Sun, Hak-Keung Lam, Hongyi Li, Jianxi Wang, Dongchen Hou</div><div><b>Pages:&nbsp;</b>2583 - 2594</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9457145/\">Finite-Frequency H\u2212/H\u221e Memory Fault Detection Filtering Design for Uncertain Takagi\u2013Sugeno Fuzzy Affine Systems</a></div><div><b>Author(s):&nbsp;</b>Rong Zhao, Lu Liu, Gang Feng</div><div><b>Pages:&nbsp;</b>2595 - 2609</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9457138/\">Event-Triggered Bipartite Consensus for Fuzzy Multiagent Systems Under Markovian Switching Signed Topology</a></div><div><b>Author(s):&nbsp;</b>Jiafeng Yu, Choon Ki Ahn, Peng Shi</div><div><b>Pages:&nbsp;</b>2610 - 2620</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9457143/\">Continuous Exp Strategy for Consumer Preference Analysis Based on Online Ratings</a></div><div><b>Author(s):&nbsp;</b>Long Ren, Bin Zhu, Zeshui Xu</div><div><b>Pages:&nbsp;</b>2621 - 2633</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9457170/\">Trust Cop-Kmeans Clustering Analysis and Minimum-Cost Consensus Model Considering Voluntary Trust Loss in Social Network Large-Scale Decision-Making</a></div><div><b>Author(s):&nbsp;</b>Su-Min Yu, Zhi-Jiao Du, Xue-Yang Zhang, Han-Yang Luo, Xu-Dong Lin</div><div><b>Pages:&nbsp;</b>2634 - 2648</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9462345/\">Statistically Evolving Fuzzy Inference System for Non-Gaussian Noises</a></div><div><b>Author(s):&nbsp;</b>Zhao-Xu Yang, Hai-Jun Rong, Plamen Angelov, Zhi-Xin Yang</div><div><b>Pages:&nbsp;</b>2649 - 2664</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9462321/\">A New Fuzzy Spiking Neural Network Based on Neuronal Contribution Degree</a></div><div><b>Author(s):&nbsp;</b>Fang Liu, Jie Yang, Witold Pedrycz, Wei Wu</div><div><b>Pages:&nbsp;</b>2665 - 2677</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9462470/\">Aperiodic Sampled-Data Takagi\u2013Sugeno Fuzzy Extended State Observer for a Class of Uncertain Nonlinear Systems With External Disturbance and Unmodeled Dynamics</a></div><div><b>Author(s):&nbsp;</b>Zhichen Li, Huaicheng Yan, Hao Zhang, Hak-Keung Lam, Congzhi Huang</div><div><b>Pages:&nbsp;</b>2678 - 2692</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9464680/\">Sampled-Data-Based Event-Triggered Fuzzy Control for PDE Systems Under Cyberattacks</a></div><div><b>Author(s):&nbsp;</b>Xiaona Song, Qiyuan Zhang, Shuai Song, Choon Ki Ahn</div><div><b>Pages:&nbsp;</b>2693 - 2705</div><div><br /></div><div><b>45)</b> <a href=\"https://ieeexplore.ieee.org/document/9466935/\">Interval-Valued Aggregation Functions Based on Moderate Deviations Applied to Motor-Imagery-Based Brain\u2013Computer Interface</a></div><div><b>Author(s):&nbsp;</b>Javier Fumanal-Idocin, Zdenko Tak\u00e1\u010d, Javier Fern\u00e1ndez, Jos\u00e9 Antonio Sanz, Harkaitz Goyena, Ching-Teng Lin, Yu-Kai Wang, Humberto Bustince</div><div><b>Pages:&nbsp;</b>2706 - 2720</div><div><br /></div><div><b>46)</b> <a href=\"https://ieeexplore.ieee.org/document/9468319/\">Noise-Tolerant Fuzzy-\u03b2-Covering-Based Multigranulation Rough Sets and Feature Subset Selection</a></div><div><b>Author(s):&nbsp;</b>Zhehuang Huang, Jinjin Li, Yuhua Qian</div><div><b>Pages:&nbsp;</b>2721 - 2735</div><div><br /></div><div><b>47)</b> <a href=\"https://ieeexplore.ieee.org/document/9470937/\">Expansive Errors-Based Fuzzy Adaptive Prescribed Performance Control by Residual Approximation</a></div><div><b>Author(s):&nbsp;</b>Shigen Gao, Mingjun Li, Yue Zheng, Hairong Dong</div><div><b>Pages:&nbsp;</b>2736 - 2746</div><div><br /></div><div><b>48)</b> <a href=\"https://ieeexplore.ieee.org/document/9477002/\">Fractional-Order Terminal Sliding-Mode Control Using Self-Evolving Recurrent Chebyshev Fuzzy Neural Network for MEMS Gyroscope</a></div><div><b>Author(s):&nbsp;</b>Zhe Wang, Juntao Fei</div><div><b>Pages:&nbsp;</b>2747 - 2758</div><div><br /></div><div><b>49)</b> <a href=\"https://ieeexplore.ieee.org/document/9477053/\">Relaxed Conditions of Observer Design of Discrete-Time Takagi\u2013Sugeno Fuzzy Systems via a New Multi-Instant Gain-Scheduling Scheme</a></div><div><b>Author(s):&nbsp;</b>Hongyu Lu, Xiangpeng Xie, Chen Peng</div><div><b>Pages:&nbsp;</b>2759 - 2768</div><div><br /></div><div><b>50)</b> <a href=\"https://ieeexplore.ieee.org/document/9426421/\">A Fuzzy Lyapunov Function Method to Stability Analysis of Fractional-Order T\u2013S Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Xiaofei Fan, Zhanshan Wang</div><div><b>Pages:&nbsp;</b>2769 - 2776</div><div><br /></div><div><b>51)</b> <a href=\"https://ieeexplore.ieee.org/document/9454282/\">Multi-Instant Gain-Scheduling Stabilization of Discrete-Time Takagi\u2013Sugeno Fuzzy Systems Based on a Time-Variant Balanced Matrix Approach</a></div><div><b>Author(s):&nbsp;</b>Xiangpeng Xie, Chengjie Bu, Chen Peng</div><div><b>Pages:&nbsp;</b>2777 - 2782</div><div><br /></div>",
            "pubdate": "2022-07-08T10:36:00.000+12:00",
            "pubdate_parsed": [
                2022,
                7,
                7
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 14, July 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/soft-computing-volume-26-issue-14-july.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07116-6\">Efficient data transfer supporting provable data deletion for secure cloud storage</a></div><div><b>Author(s): </b>Changsong Yang, Yueling Liu, Yong Ding</div><div><b>Pages: </b>6463 - 6479</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07119-3\">Green\u2019s relations in L-E-fuzzy skew lattices</a></div><div><b>Author(s):&nbsp;</b>Yuan Zhi, Xiangnan Zhou, Qingguo Li</div><div><b>Pages:&nbsp;</b>6481 - 6494</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07134-4\">Regular and strongly regular relations induced by fuzzy subhypermodules</a></div><div><b>Author(s):&nbsp;</b>N. Rakhsh Khorshid, S. Ostadhadi-Dehkordi</div><div><b>Pages:&nbsp;</b>6495 - 6506</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07127-3\">Another view on knowledge measures in atanassov intuitionistic fuzzy sets</a></div><div><b>Author(s):&nbsp;</b>Muhammad Irfan Ali, Jianming Zhan...Haider Faizan</div><div><b>Pages:&nbsp;</b>6507 - 6517</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07128-2\">Effect of fuzziness in fuzzy rule-based classifiers defined by strong fuzzy partitions and winner-takes-all inference</a></div><div><b>Author(s):&nbsp;</b>Gabriella Casalino, Giovanna Castellano...Corrado Mencar</div><div><b>Pages:&nbsp;</b>6519 - 6527</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07168-8\">Nonlinear interval regression analysis with neural networks and grey prediction for energy demand forecasting</a></div><div><b>Author(s):&nbsp;</b>Yi-Chung Hu, Wen-Bao Wang</div><div><b>Pages:&nbsp;</b>6529 - 6545</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07178-6\">Knowledge transfer learning from multiple user activities to improve personalized recommendation</a></div><div><b>Author(s):&nbsp;</b>Mingxin Gan, Yingxue Ma</div><div><b>Pages:&nbsp;</b>6547 - 6566</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07179-5\">A quantum system control method based on enhanced reinforcement learning</a></div><div><b>Author(s):&nbsp;</b>Wenjie Liu, Bosi Wang...Mohammed Zidan</div><div><b>Pages:&nbsp;</b>6567 - 6575</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07181-x\">On-demand DWDM design using machine learning</a></div><div><b>Author(s):&nbsp;</b>K. Venkatesan, A. Chandrasekar, P. G. V. Ramesh</div><div><b>Pages:&nbsp;</b>6577 - 6589</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07040-9\">Ramp loss KNN-weighted multi-class twin support vector machine</a></div><div><b>Author(s):&nbsp;</b>Huiru Wang, Yitian Xu, Zhijian Zhou</div><div><b>Pages:&nbsp;</b>6591 - 6618</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07042-7\">A revisited fuzzy DEMATEL and optimization method for strategy map design under the BSC framework: selection of objectives and relationships</a></div><div><b>Author(s):&nbsp;</b>H\u00e9ctor L\u00f3pez-Ospina, Daniela Pardo...Luis Quezada</div><div><b>Pages:&nbsp;</b>6619 - 6644</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07043-6\">A novel linear representation for evolving matrices</a></div><div><b>Author(s):&nbsp;</b>Connor Gregor, Daniel Ashlock...David Kribs</div><div><b>Pages:&nbsp;</b>6645 - 6657</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07050-7\">MOTEO: a novel multi-objective thermal exchange optimization algorithm for engineering problems</a></div><div><b>Author(s):&nbsp;</b>Nima Khodadadi, Siamak Talatahari...Armin Dadras Eslamlou</div><div><b>Pages:&nbsp;</b>6659 - 6684</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07061-4\">A novel incremental cost consensus approach for distributed economic dispatch over directed communication topologies in a smart grid</a></div><div><b>Author(s):&nbsp;</b>Um-E-Habiba Alvi, Waqas Ahmed...Ijaz Ahmed</div><div><b>Pages:&nbsp;</b>6685 - 6700</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07065-0\">Intelligent computing technique for solving singular multi-pantograph delay differential equation</a></div><div><b>Author(s):&nbsp;</b>Zulqurnain Sabir, Hafiz Abdul Wahab...Mohamed R. Ali</div><div><b>Pages:&nbsp;</b>6701 - 6713</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07066-z\">Optimization on the multi-period empty container repositioning problem in regional port cluster based upon inventory control strategies</a></div><div><b>Author(s):&nbsp;</b>Jiaxin Cai, Yubo Li...Zhihong Jin</div><div><b>Pages:&nbsp;</b>6715 - 6738</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07072-1\">A simple solution to technician routing and scheduling problem using improved genetic algorithm</a></div><div><b>Author(s):&nbsp;</b>Engin Pekel</div><div><b>Pages:&nbsp;</b>6739 - 6748</div><div><br /></div><div><b>18) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07079-8\">Fusion of modern meta-heuristic optimization methods using arithmetic optimization algorithm for global optimization tasks</a></div><div><b>Author(s):&nbsp;</b>Shubham Mahajan, Laith Abualigah...Maryam Altalhi</div><div><b>Pages:&nbsp;</b>6749 - 6763</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07089-6\">Identification of nonlinear discrete systems using a new Hammerstein model with Volterra neural network</a></div><div><b>Author(s):&nbsp;</b>Wei-Der Chang</div><div><b>Pages:&nbsp;</b>6765 - 6775</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07103-x\">Environmental assisted cracking and strength attenuation effect computing on the mechanical properties of casing steel P110 for industrial revolution 5.0 applications in sour well environments</a></div><div><b>Author(s):&nbsp;</b>Duo Hou, Zhongling Xiao...Taihe Shi</div><div><b>Pages:&nbsp;</b>6777 - 6787</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06906-2\">Comparison of subsidy strategies on the green supply chain under a behaviour-based pricing model</a></div><div><b>Author(s):&nbsp;</b>Kanying Liu, Wei Li...Yong Lan</div><div><b>Pages:&nbsp;</b>6789 - 6809</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06921-3\">Attack detection and prevention in IoT-SCADA networks using NK-classifier</a></div><div><b>Author(s):&nbsp;</b>Y. JustindhasP. Jeyanthi</div><div><b>Pages:&nbsp;</b>6811 - 6823</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06926-y\">A method to determine the integrated weights of cross-efficiency aggregation</a></div><div><b>Author(s):&nbsp;</b>Mei-Juan LiJin-Cheng LuLei Chen</div><div><b>Pages:&nbsp;</b>6825 - 6837</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06628-x\">Load-settlement response of a footing over buried conduit in a sloping terrain: a numerical experiment-based artificial intelligent approach</a></div><div><b>Author(s):&nbsp;</b>Muhammad Umer Arif Khan, Sanjay Kumar Shukla, Muhammad Nouman Amjad Raja</div><div><b>Pages:&nbsp;</b>6839 - 6856</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06629-w\">Recognition of shed damage on 11-kV polymer insulator using Bayesian optimized convolution neural network</a></div><div><b>Author(s):&nbsp;</b>B. Vigneshwaran, M. Willjuice Iruthayarajan, R. V. Maheswari</div><div><b>Pages:&nbsp;</b>6857 - 6869</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06630-3\">Enhanced heat transfer search and enriched replicated coronary circulation system optimization algorithms for real power loss reduction</a></div><div><b>Author(s):&nbsp;</b>Lenin Kanagasabai</div><div><b>Pages:&nbsp;</b>6871 - 6888</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06636-x\">A novel quality evaluation method for standardized experiment teaching</a></div><div><b>Author(s):&nbsp;</b>Luxin Yang, Yutong Chun...Jing Yang</div><div><b>Pages:&nbsp;</b>6889 - 6906</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06639-8\">Feature extraction-based intelligent algorithm framework with neural network for solving conditional nonlinear optimal perturbation</a></div><div><b>Author(s):&nbsp;</b>Shijin Yuan,Huazhen Zhang...Bin Mu</div><div><b>Pages:&nbsp;</b>6907 - 6924</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06646-9\">Computational intelligence in software defects rules discovery</a></div><div><b>Author(s):&nbsp;</b>Andreea VescanCamelia \u015eerbanGloria Cerasela Cri\u015fan</div><div><b>Pages:&nbsp;</b>6925 - 6939</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06648-7\">Fuzzy transfer learning in time series forecasting for stock market prices</a></div><div><b>Author(s):&nbsp;</b>Shanoli Samui PalSamarjit Kar</div><div><b>Pages:&nbsp;</b>6941 - 6952</div><div><br /></div></div>",
            "pubdate": "2022-07-09T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                7,
                9
            ],
            "email_sent": true
        },
        "Evolving Systems, Volume 13, issue 4, August 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/evolving-systems-volume-13-issue-4.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09385-2\">FocusCovid: automated COVID-19 detection using deep learning with chest X-ray images</a></div><div><b>Author(s): </b>Tarun Agrawal, Prakash Choudhary</div><div><b>Pages: </b>519 - 533</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09392-3\">Multichannel convolutional neural network-based fuzzy active contour model for medical image segmentation</a></div><div><b>Author(s):&nbsp;</b>Qingwu Shi, Shoulin Yin...Hang Li</div><div><b>Pages:&nbsp;</b>535 - 549</div><div><br /></div><div><b>3)</b> Cubic graph representation of concept lattice and its decomposition</div><div><b>Author(s):&nbsp;</b>Prem Kumar Singh</div><div><b>Pages:&nbsp;</b>551 - 562</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09401-5\">Battle royale optimizer for training multi-layer perceptron</a></div><div><b>Author(s):&nbsp;</b>Saeid Agahian, Taymaz Akan</div><div><b>Pages:&nbsp;</b>563 - 575</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09404-2\">Extreme gradient boosting model based on improved Jaya optimizer applied to forecasting energy consumption in residential buildings</a></div><div><b>Author(s):&nbsp;</b>Jo\u00e3o Sauer, Viviana Cocco Mariani...Mirco Rampazzo</div><div><b>Pages:&nbsp;</b>577 - 588</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09407-z\">Optimization of FRP jacket by fractional\u2011order pathfinder algorithm to improve the reinforced concrete frames' seismic response</a></div><div><b>Author(s):&nbsp;</b>Chengliang Wang, Wenrui Li, Dragan Rodriguez</div><div><b>Pages:&nbsp;</b>589 - 601</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09408-y\">EvolveCluster: an evolutionary clustering algorithm for streaming data</a></div><div><b>Author(s):&nbsp;</b>Christian Nordahl, Veselka Boeva...Marie Persson Netz</div><div><b>Pages:&nbsp;</b>603 - 623</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09395-0\">Synthetic samples generator (SYSGEN), an approach to increase the size of incidence samples in coffee leaf rust modelling</a></div><div><b>Author(s):&nbsp;</b>Edwar Javier Gir\u00f3n, David Camilo Corrales...Juan Carlos Corrales</div><div><b>Pages:&nbsp;</b>625 - 636</div><div><br /></div></div>",
            "pubdate": "2022-07-11T21:35:00.001+12:00",
            "pubdate_parsed": [
                2022,
                7,
                11
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 15, August 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/soft-computing-volume-26-issue-15.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07174-w\">Analytic hierarchy process-based regression test case prioritization technique enhancing the fault detection rate</a></div><div><b>Author(s): </b>Soumen Nayak, Chiranjeev Kumar, Sachin Tripathi</div><div><b>Pages: </b>6953 - 6968</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07193-7\">On Annihilators in Hoops</a></div><div><b>Author(s):&nbsp;</b>R. A. Borzooei, M. Aaly Kologani...Y. B. Jun</div><div><b>Pages:&nbsp;</b>6969 - 6980</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07216-3\">Properties of stabilizers in residuated lattices</a></div><div><b>Author(s):&nbsp;</b>Michiro Kondo</div><div><b>Pages:&nbsp;</b>6981 - 6988</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07217-2\">Independent domination polynomial of zero-divisor graphs of commutative rings</a></div><div><b>Author(s):&nbsp;</b>Necla K\u0131rcal\u0131 G\u00fcrsoy, Alper \u00dclker, Arif G\u00fcrsoy</div><div><b>Pages:&nbsp;</b>6989 - 6997</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07223-4\">Quasi-MV* algebras: a generalization of MV*-algebras</a></div><div><b>Author(s):&nbsp;</b>Yingying Jiang, Wenjuan Chen</div><div><b>Pages:&nbsp;</b>6999 - 7015</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06985-1\">Fuzzy filters of IL-algebras</a></div><div><b>Author(s):&nbsp;</b>Safiqul Islam, Arundhati Sanyal, Jayanta Sen</div><div><b>Pages:&nbsp;</b>7017 - 7027</div><div><br /></div><div><b>7) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07004-z\">Broad learning system based ensemble deep model</a></div><div><b>Author(s):&nbsp;</b>Chenglong Zhang, Shifei Ding...Jian Zhang</div><div><b>Pages:&nbsp;</b>7029 - 7041</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07101-z\">On differential lattices</a></div><div><b>Author(s):&nbsp;</b>Aiping Gan, Li Guo</div><div><b>Pages:&nbsp;</b>7043 - 7058</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07132-6\">A faster algorithm for identifying signals using complex fuzzy sets</a></div><div><b>Author(s):&nbsp;</b>Madad Khan, Inamullah Khan...Sohail Iqbal</div><div><b>Pages:&nbsp;</b>7059 - 7079</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07172-y\">Trapezoidal approximation operators preserving the most indicators of fuzzy numbers-relationships and applications</a></div><div><b>Author(s):&nbsp;</b>M. Chehlabi</div><div><b>Pages:&nbsp;</b>7081 - 7105</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07150-4\">An introduction to single-valued neutrosophic soft topological structure</a></div><div><b>Author(s):&nbsp;</b>Yaser Saber, Fahad Alsharari, Florentin Smarandache</div><div><b>Pages:&nbsp;</b>7107 - 7122</div><div><br /></div><div><b>12) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07160-2\">An evidence combination rule based on a new weight assignment scheme</a></div><div><b>Author(s):&nbsp;</b>Yu-Cui Wang, Jian Wang...Ming-Hui Wang</div><div><b>Pages:&nbsp;</b>7123 - 7137</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07188-4\">A note on \u201cDealer using a new trapezoidal cubic hesitant fuzzy TOPSIS method and application to group decision-making program\u201d</a></div><div><b>Author(s):&nbsp;</b>S. S. Appadoo, Mohammadreza Makhan, Amit Kumar</div><div><b>Pages:&nbsp;</b>7139 - 7141</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07191-9\">Distributed energy-efficient clustering routing protocol for wireless sensor networks using affinity propagation and fuzzy logic</a></div><div><b>Author(s):&nbsp;</b>Chu-hang Wang, Huang-shui Hu...Jin-feng Zhang</div><div><b>Pages:&nbsp;</b>7143 - 7158</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07323-1\">Stability and admissibility analysis of T\u2013S descriptive systems and its applications</a></div><div><b>Author(s):&nbsp;</b>Muhammad Shamrooz Aslam, Ma Zhenhua...Abdul Majid</div><div><b>Pages:&nbsp;</b>7159 - 7166</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07190-w\">Quantum-enhanced filter: QFilter</a></div><div><b>Author(s):&nbsp;</b>Parfait Atchade-Adelomou, Guillermo Alonso-Linaje</div><div><b>Pages:&nbsp;</b>7167 - 7174</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07194-6\">Estimation of fault probability in medium voltage feeders through calibration techniques in classification models</a></div><div><b>Author(s):&nbsp;</b>Enrico De Santis, Francesco Arn\u00f2, Antonello Rizzi</div><div><b>Pages:&nbsp;</b>7175 - 7193</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07105-9\">Deep multiobjective design optimization of CFRP isogrid tubes using lichtenberg algorithm</a></div><div><b>Author(s):&nbsp;</b>Jo\u00e3o Luiz Junho Pereira, Matheus Brendon Francisco...Guilherme Ferreira Gomes</div><div><b>Pages:&nbsp;</b>7195 - 7209</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07107-7\">An effective structure of multi-modal deep convolutional neural network with adaptive group teaching optimization</a></div><div><b>Author(s):&nbsp;</b>Vinit Gupta, Santosh Pawar</div><div><b>Pages:&nbsp;</b>7211 - 7232</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07109-5\">An enhanced Harris Hawk optimization algorithm for parameter estimation of single, double and triple diode photovoltaic models</a></div><div><b>Author(s):&nbsp;</b>Abdelhady Ramadan, Salah Kamel...Jose Luis Dom\u00ednguez-Garc\u00eda</div><div><b>Pages:&nbsp;</b>7233 - 7257</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07112-w\">A preference structure in multi-attribute decision making: an algorithmic approach based on hesitant fuzzy sets</a></div><div><b>Author(s):&nbsp;</b>B. K. Mohanty, Eshika Aggarwal</div><div><b>Pages:&nbsp;</b>7259 - 7277</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07124-6\">Optimal autonomic management of service-based business processes in the cloud</a></div><div><b>Author(s):&nbsp;</b>Leila Hadded, Tarek Hamrouni</div><div><b>Pages:&nbsp;</b>7279 - 7291</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07139-z\">A novel hybrid gravitational and pattern search algorithm based MPPT controller with ANN and perturb and observe for photovoltaic system</a></div><div><b>Author(s):&nbsp;</b>Salem Alkhalaf, Ziad M. AliHitoshi Oikawa</div><div><b>Pages:&nbsp;</b>7293 - 7315</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06941-z\">Modeling stock market using new hybrid intelligent method based on MFNN and IBHA</a></div><div><b>Author(s):&nbsp;</b>Wei Gao</div><div><b>Pages:&nbsp;</b>7317 - 7337</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06964-6\">Evaluating and selecting agricultural insurance packages through an AHP-based fuzzy TOPSIS Method</a></div><div><b>Author(s):&nbsp;</b>Ta-Chung Chu, Thi Hong Phuong Le</div><div><b>Pages:&nbsp;</b>7339 - 7354</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07027-6\">Inclusion degree-based multigranulation rough fuzzy set over heterogeneous preference information and application to multiple attribute group decision making</a></div><div><b>Author(s):&nbsp;</b>Xinrui Zhang, Bingzhen Sun</div><div><b>Pages:&nbsp;</b>7355 - 7375</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07299-y\">Research on safety simulation model and algorithm of dynamic system based on artificial neural network</a></div><div><b>Author(s):&nbsp;</b>Guangna Zhang</div><div><b>Pages:&nbsp;</b>7377 - 7386</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07300-8\">Mobile robot path planning using multi-objective genetic algorithm in industrial automation</a></div><div><b>Author(s):&nbsp;</b>K. S. Suresh, R. Venkatesan, S. Venugopal</div><div><b>Pages:&nbsp;</b>7387 - 7400</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06677-2\">Toward in-flight Wi-Fi: a neuro-fuzzy based routing approach for Civil Aeronautical Ad hoc Network</a></div><div><b>Author(s):&nbsp;</b>T. Gurumekala, S. Indira Gandhi</div><div><b>Pages:&nbsp;</b>7401 - 7422</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06688-z\">CoupGAN: Chinese couplet generation via encoder\u2013decoder model and adversarial training under global control</a></div><div><b>Author(s):&nbsp;</b>Qian Qu, Jiancheng Lv...Kexin Yang</div><div><b>Pages:&nbsp;</b>7423 - 7433</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-07-14T10:56:00.001+12:00",
            "pubdate_parsed": [
                2022,
                7,
                13
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 16, August 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/soft-computing-volume-26-issue-16.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07213-6\">Special issue on advances in pattern recognition and computer vision, applications and systems</a></div><div><b>Author(s): </b>M. Irfan Uddin</div><div><b>Pages: </b>7435 - 7436</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06439-0\">Analysis of industry convergence based on improved neural network</a></div><div><b>Author(s):&nbsp;</b>Nan Ma</div><div><b>Pages:&nbsp;</b>7437 - 7448</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06449-y\">Nucleus image segmentation method based on GAN and FCN model</a></div><div><b>Author(s):&nbsp;</b>Kai Zhang, Yang Shi...Hang Yu</div><div><b>Pages:&nbsp;</b>7449 - 7460</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06459-w\">A new color image encryption technique using DNA computing and Chaos-based substitution box</a></div><div><b>Author(s):&nbsp;</b>Fawad Masood, Junaid Masood...Jawad Ahmad</div><div><b>Pages:&nbsp;</b>7461 - 7477</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06465-y\">An evolutionary trajectory planning algorithm for multi-UAV-assisted MEC system</a></div><div><b>Author(s):&nbsp;</b>Muhammad Asim, Wali Khan Mashwani...Samir Brahim Belhaouari</div><div><b>Pages:&nbsp;</b>7479 - 7492</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06478-7\">Research on modeling of government debt risk comprehensive evaluation based on multidimensional data mining</a></div><div><b>Author(s):&nbsp;</b>Li Chao Ying, Wu Xiang Da, Zhao En Hui</div><div><b>Pages:&nbsp;</b>7493 - 7500</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06479-6\">Research on trade data encryption of tobacco enterprises based on adversarial neural network</a></div><div><b>Author(s):&nbsp;</b>Zhang Yi</div><div><b>Pages:&nbsp;</b>7501 - 7508</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06480-z\">Research on intelligent language translation system based on deep learning algorithm</a></div><div><b>Author(s):&nbsp;</b>Chunliu Shi</div><div><b>Pages:&nbsp;</b>7509 - 7518</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06519-1\">Analyzing fibrous tissue pattern in fibrous dysplasia bone images using deep R-CNN networks for segmentation</a></div><div><b>Author(s):&nbsp;</b>A. Saranya, Kottilingam Kottursamy...Ali Kashif Bashir</div><div><b>Pages:&nbsp;</b>7519 - 7533</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06569-5\">Deep learning-based election results prediction using Twitter activity</a></div><div><b>Author(s):&nbsp;</b>Haider Ali, Haleem Farman...Adel Ammar</div><div><b>Pages:&nbsp;</b>7535 - 7543</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06581-9\">Q-method optimization of tunnel surrounding rock classification by fuzzy reasoning model and support vector machine</a></div><div><b>Author(s):&nbsp;</b>Feng Jiang, Peng He...Zhihan Lv</div><div><b>Pages:&nbsp;</b>7545 - 7558</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06585-5\">Analysis of the change of artificial intelligence to online consumption patterns and consumption concepts</a></div><div><b>Author(s):&nbsp;</b>Longyue Bai</div><div><b>Pages:&nbsp;</b>7559 - 7569</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06587-3\">Sparse representation optimization of Gaussian mixed feature of image based on convolution neural network</a></div><div><b>Author(s):&nbsp;</b>Yuguang Ye</div><div><b>Pages:&nbsp;</b>7571 - 7580</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06603-6\">Task-specific image summaries using semantic information and self-supervision</a></div><div><b>Author(s):&nbsp;</b>Deepak Kumar Sharma, Anurag Singh...Jerry Chun-Wei Lin</div><div><b>Pages:&nbsp;</b>7581 - 7594</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06612-5\">The study on life model of MOV based on various parameters and surge history</a></div><div><b>Author(s):&nbsp;</b>Xiaofei Ruan, Shaoyun Jin...Weidong Cheng</div><div><b>Pages:&nbsp;</b>7595 - 7600</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06662-9\">Deep ensembling for perceptual image quality assessment</a></div><div><b>Author(s):&nbsp;</b>Nisar Ahmed, H. M. Shahzad Asif...Atif Khan</div><div><b>Pages:&nbsp;</b>7601 - 7622</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06705-1\">ConTEXT: context-aware adaptive SMS client for drivers to reduce risky driving behaviors</a></div><div><b>Author(s):&nbsp;</b>Inayat Khan, Shah Khusro</div><div><b>Pages:&nbsp;</b>7623 - 7640</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06706-0\">Design of robust deep learning-based object detection and classification model for autonomous driving applications</a></div><div><b>Author(s):&nbsp;</b>Mesfer Al Duhayyim, Fahd N. Al-Wesabi...Ashish Khanna</div><div><b>Pages:&nbsp;</b>7641 - 7652</div><div><br /></div><div><b>19) </b><a href=\"https://link.springer.com/article/10.1007/s00500-021-06707-z\">Gene Ontology GAN (GOGAN): a novel architecture for protein function prediction</a></div><div><b>Author(s):&nbsp;</b>Musadaq Mansoor, Mohammad Nauman...Alfredo Benso</div><div><b>Pages:&nbsp;</b>7653 - 7667</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06715-z\">The control mode study of PPP project financing management information system</a></div><div><b>Author(s):&nbsp;</b>Junli Cao, Lin Li</div><div><b>Pages:&nbsp;</b>7669 - 7675</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06869-4\">Correction to: The control mode study of PPP project financing management information system</a></div><div><b>Author(s):&nbsp;</b>Junli Cao, Lin Li</div><div><b>Pages:&nbsp;</b>7677 - 7677</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06734-4\">A comprehensive overview of AI-enabled music classification and its influence in games</a></div><div><b>Author(s):&nbsp;</b>Tiancheng Yang, Shah Nazir</div><div><b>Pages:&nbsp;</b>7679 - 7693</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06739-z\">Classification of Gurumukhi month\u2019s name images using various convolutional neural network optimizers</a></div><div><b>Author(s):&nbsp;</b>Tajinder Pal Singh, Sheifali Gupta...Atef Zaguia</div><div><b>Pages:&nbsp;</b>7695 - 7707</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06742-4\">Novel semi-supervised learning approach for descriptor generation using artificial neural networks</a></div><div><b>Author(s):&nbsp;</b>Alla Fikrat Alwindawi, Osman Nuri U\u00e7an...Aminu Yusuf</div><div><b>Pages:&nbsp;</b>7709 - 7720</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06750-4\">Hybrid deep-learning model to detect botnet attacks over internet of things environments</a></div><div><b>Author(s):&nbsp;</b>Mohammed Y. Alzahrani, Alwi M. Bamhdi</div><div><b>Pages:&nbsp;</b>7721 - 7735</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06755-z\">Machine health surveillance system by using deep learning sparse autoencoder</a></div><div><b>Author(s):&nbsp;</b>Faizan Ullah, Abdu Salam...Wael Alosaimi</div><div><b>Pages:&nbsp;</b>7737 - 7750</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06762-0\">Diagnosis and classification of Alzheimer's disease by using a convolution neural network algorithm</a></div><div><b>Author(s):&nbsp;</b>Mosleh Hmoud Al-Adhaileh</div><div><b>Pages:&nbsp;</b>7751 - 7762</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06773-x\">Machine learning for fake news classification with optimal feature selection</a></div><div><b>Author(s):&nbsp;</b>Muhammad Fayaz, Atif Khan...Sana Ullah Khan</div><div><b>Pages:&nbsp;</b>7763 - 7771</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06775-9\">Application and Analysis of Computer Network Technology in Electronic Information Engineering</a></div><div><b>Author(s):&nbsp;</b>Wanjie Kang, Jie Xiao</div><div><b>Pages:&nbsp;</b>7773 - 7779</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06781-x\">Research and application of GIS and data mining technology in monitoring and assessment of natural geography environment</a></div><div><b>Author(s):&nbsp;</b>Fuheng Zhang, Guangbin Ji...Guihua Liu</div><div><b>Pages:&nbsp;</b>7781 - 7787</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06794-6\">Extracting built-up areas from spectro-textural information using machine learning</a></div><div><b>Author(s):&nbsp;</b>Ahsen Maqsoom, Bilal Aslam...Muhammad Imran</div><div><b>Pages:&nbsp;</b>7789 - 7808</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06796-4\">The traditional settlement planning and the renovation of residential buildings based on spatial syntax analysis</a></div><div><b>Author(s):&nbsp;</b>Kaifeng Chu, Mengyu Wu</div><div><b>Pages:&nbsp;</b>7809 - 7815</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06804-7\">Efficient facial emotion recognition model using deep convolutional neural network and modified joint trilateral filter</a></div><div><b>Author(s):&nbsp;</b>Naveen Kumari, Rekha Bhatia</div><div><b>Pages:&nbsp;</b>7817 - 7830</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06805-6\">Convolutional neural network based hurricane damage detection using satellite images</a></div><div><b>Author(s):&nbsp;</b>Swapandeep Kaur, Sheifali Gupta...Atef Zaguia</div><div><b>Pages:&nbsp;</b>7831 - 7845</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06806-5\">Fake opinion detection in an e-commerce business based on a long-short memory algorithm</a></div><div><b>Author(s):&nbsp;</b>Nizar Alsharif</div><div><b>Pages:&nbsp;</b>7847 - 7854</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06811-8\">Visual design elements based on digital visualization</a></div><div><b>Author(s):&nbsp;</b>Lei Jiang</div><div><b>Pages:&nbsp;</b>7855 - 7863</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06812-7\">Analysis of transmission line icing prediction based on CNN and data mining technology</a></div><div><b>Author(s):&nbsp;</b>Lixue Li, Da Luo, Wenhao Yao</div><div><b>Pages:&nbsp;</b>7865 - 7870</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06813-6\">Computer application under the management of network information security technology using genetic algorithm</a></div><div><b>Author(s):&nbsp;</b>Xu Jian Qiang</div><div><b>Pages:&nbsp;</b>7871 - 7876</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06830-5\">Software defect prediction employing BiLSTM and BERT-based semantic feature</a></div><div><b>Author(s):&nbsp;</b>Md Nasir Uddin, Bixin Li...Islam Zada</div><div><b>Pages:&nbsp;</b>7877 - 7891</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06845-y\">An effective nonlocal means image denoising framework based on non-subsampled shearlet transform</a></div><div><b>Author(s):&nbsp;</b>Bhawna Goyal, Ayush Dogra, Arun Kumar Sangaiah</div><div><b>Pages:&nbsp;</b>7893 - 7915</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06849-8\">The study of physical education evaluation based on a fuzzy stochastic algorithm</a></div><div><b>Author(s):&nbsp;</b>Xiuyan Su</div><div><b>Pages:&nbsp;</b>7917 - 7923</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06851-0\">Conceptual model construction of building information management system based on BIM architecture</a></div><div><b>Author(s):&nbsp;</b>Jiang Haiying</div><div><b>Pages:&nbsp;</b>7925 - 7931</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06853-y\">Perceptual adversarial non-residual learning for blind image denoising</a></div><div><b>Author(s):&nbsp;</b>Aamir Khan, Weidong Jin, Rizwan Ali Naqvi</div><div><b>Pages:&nbsp;</b>7933 - 7957</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06855-w\">Research on the optimization of communication protocol in network security protocol</a></div><div><b>Author(s):&nbsp;</b>Daoyuan Sun</div><div><b>Pages:&nbsp;</b>7959 - 7966</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06859-6\">Multi-scale local-global architecture for person re-identification</a></div><div><b>Author(s):&nbsp;</b>Jing Liu, Prayag Tiwari...Shahab S. Band</div><div><b>Pages:&nbsp;</b>7967 - 7977</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06900-8\">Optimal feature extraction and ulcer classification from WCE image data using deep learning</a></div><div><b>Author(s):&nbsp;</b>Youssef Masmoudi, Muhammad Ramzan...Mohammed Habib</div><div><b>Pages:&nbsp;</b>7979 - 7992</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06901-7\">A communication security anti-interference decision model using deep learning in intelligent industrial IoT environment</a></div><div><b>Author(s):&nbsp;</b>Lichao Yan, Juan Hu...Jinhong Di</div><div><b>Pages:&nbsp;</b>7993 - 8002</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06990-4\">Smoke removal and image enhancement of laparoscopic images by an artificial multi-exposure image fusion method</a></div><div><b>Author(s):&nbsp;</b>Muhammad Adeel Azam, Khan Bahadar Khan...Sana Ullah Khan</div><div><b>Pages:&nbsp;</b>8003 - 8015</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06996-y\">Predicting the spread of COVID-19 with a machine learning technique and multiplicative calculus</a></div><div><b>Author(s):&nbsp;</b>B\u00fclent Bilgehan, Ali \u00d6zyap\u0131c\u0131...Yusuf Gurefe</div><div><b>Pages:&nbsp;</b>8017 - 8024</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07047-2\">Fusion of multi-modality biomedical images using deep neural networks</a></div><div><b>Author(s):&nbsp;</b>Manish Gupta, Naresh Kumar...Atef Zaguia</div><div><b>Pages:&nbsp;</b>8025 - 8036</div><div><br /></div><div><b>51)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07058-z\">Rotating object detection in remote-sensing environment</a></div><div><b>Author(s):&nbsp;</b>Sixian Chan, Jingcheng Zheng...Kai Fang</div><div><b>Pages:&nbsp;</b>8037 - 8045</div><div><br /></div><div><b>52)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07062-3\">Analyzing the interactions among factors affecting cloud adoption for software testing: a two-stage ISM-ANN approach</a></div><div><b>Author(s):&nbsp;</b>Sikandar Ali, Samad Baseer...Jiwei Huang</div><div><b>Pages:&nbsp;</b>8047 - 8075</div><div><br /></div><div><b>53)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07064-1\">A probabilistic approach toward evaluation of Internet rumor on COVID</a></div><div><b>Author(s):&nbsp;</b>Yancheng Yang, Shah Nazir, Wajeeha Khalil</div><div><b>Pages:&nbsp;</b>8077 - 8088</div><div><br /></div><div><b>54)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07142-4\">Single-image reconstruction using novel super-resolution technique for large-scaled images</a></div><div><b>Author(s):&nbsp;</b>Ramanath Datta, Sekhar Mandal...Jazem Mutared Alanazi</div><div><b>Pages:&nbsp;</b>8089 - 8103</div><div><br /></div><div><b>55)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07155-z\">Hybrid optimisation-based robust watermarking using denoising convolutional neural network</a></div><div><b>Author(s):&nbsp;</b>Dhiran Kumar Mahto, Ashima Anand, Amit Kumar Singh</div><div><b>Pages:&nbsp;</b>8105 - 8116</div><div><br /></div><div><b>56)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07169-7\">Constrained optimization based on hybrid version of superiority of feasibility solution strategy</a></div><div><b>Author(s):&nbsp;</b>Asia Noureen, Wali Khan Mashwani...Muhammad Asim</div><div><b>Pages:&nbsp;</b>8117 - 8132</div><div><br /></div><div><b>57)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07204-7\">An adaptive genetic algorithm-based background elimination model for English text</a></div><div><b>Author(s):&nbsp;</b>Tang Xiaohui</div><div><b>Pages:&nbsp;</b>8133 - 8143</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-07-18T12:16:00.000+12:00",
            "pubdate_parsed": [
                2022,
                7,
                18
            ],
            "email_sent": true
        },
        "IEEE Transactions on Neural Networks and Learning Systems, Volume 33, Issue 7, July 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/ieee-transactions-on-neural-networks.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9311226/\">The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks</a></div><div><b>Author(s): </b>Benjamin Cramer, Yannik Stradmann, Johannes Schemmel, Friedemann Zenke</div><div><b>Pages: </b>2744 - 2757</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9312438/\">Knowledge-Routed Visual Question Reasoning: Challenges for Deep Representation Embedding</a></div><div><b>Author(s):&nbsp;</b>Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, Liang Lin</div><div><b>Pages:&nbsp;</b>2758 - 2767</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9311244/\">Stabilizing Training of Generative Adversarial Nets via Langevin Stein Variational Gradient Descent</a></div><div><b>Author(s):&nbsp;</b>Dong Wang, Xiaoqian Qin, Fengyi Song, Li Cheng</div><div><b>Pages:&nbsp;</b>2768 - 2780</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9316912/\">Reinforcement Learning and Adaptive Optimal Control for Continuous-Time Nonlinear Systems: A Value Iteration Approach</a></div><div><b>Author(s):&nbsp;</b>Tao Bian, Zhong-Ping Jiang</div><div><b>Pages:&nbsp;</b>2781 - 2790</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9314928/\">Parameterized Luenberger-Type H\u221e State Estimator for Delayed Static Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Yongsik Jin, Wookyong Kwon, Sangmoon Lee</div><div><b>Pages:&nbsp;</b>2791 - 2800</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9319553/\">BiCoSS: Toward Large-Scale Cognition Brain With Multigranular Neuromorphic Architecture</a></div><div><b>Author(s):&nbsp;</b>Shuangming Yang, Jiang Wang, Xinyu Hao, Huiyan Li, Xile Wei, Bin Deng, Kenneth A. Loparo</div><div><b>Pages:&nbsp;</b>2801 - 2815</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9317707/\">Causal Discovery in Linear Non-Gaussian Acyclic Model With Multiple Latent Confounders</a></div><div><b>Author(s):&nbsp;</b>Wei Chen, Ruichu Cai, Kun Zhang, Zhifeng Hao</div><div><b>Pages:&nbsp;</b>2816 - 2827</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9319566/\">A Study on Truncated Newton Methods for Linear Classification</a></div><div><b>Author(s):&nbsp;</b>Leonardo Galli, Chih-Jen Lin</div><div><b>Pages:&nbsp;</b>2828 - 2841</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9324979/\">Agglomerative Neural Networks for Multiview Clustering</a></div><div><b>Author(s):&nbsp;</b>Zhe Liu, Yun Li, Lina Yao, Xianzhi Wang, Feiping Nie</div><div><b>Pages:&nbsp;</b>2842 - 2852</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9321210/\">Low-Latency <i>In Situ</i> Image Analytics With FPGA-Based Quantized Convolutional Neural Network</a></div><div><b>Author(s):&nbsp;</b>Maolin Wang, Kelvin C. M. Lee, Bob M. F. Chung, Sharatchandra Varma Bogaraju, Ho-Cheung Ng, Justin S. J. Wong, Ho Cheung Shum, Kevin K. Tsia, Hayden Kwok-Hay So</div><div><b>Pages:&nbsp;</b>2853 - 2866</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9325083/\">Adaptive Optimal Control for Unknown Constrained Nonlinear Systems With a Novel Quasi-Model Network</a></div><div><b>Author(s):&nbsp;</b>Xiumei Han, Xudong Zhao, Hamid Reza Karimi, Ding Wang, Guangdeng Zong</div><div><b>Pages:&nbsp;</b>2867 - 2878</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9316921/\">A Hybrid Residual Dilated LSTM and Exponential Smoothing Model for Midterm Electric Load Forecasting</a></div><div><b>Author(s):&nbsp;</b>Grzegorz Dudek, Pawe\u0142 Pe\u0142ka, Slawek Smyl</div><div><b>Pages:&nbsp;</b>2879 - 2891</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9344620/\">Observer-Based Fixed-Time Neural Control for a Class of Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Yan Zhang, Fang Wang</div><div><b>Pages:&nbsp;</b>2892 - 2902</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9335504/\">Generalized Zero-Shot Learning With Multiple Graph Adaptive Generative Networks</a></div><div><b>Author(s):&nbsp;</b>Guo-Sen Xie, Zheng Zhang, Guoshuai Liu, Fan Zhu, Li Liu, Ling Shao, Xuelong Li</div><div><b>Pages:&nbsp;</b>2903 - 2915</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9319541/\">mCRF and mRD: Two Classification Methods Based on a Novel Multiclass Label Noise Filtering Learning Framework</a></div><div><b>Author(s):&nbsp;</b>Shuyin Xia, Baiyun Chen, Guoyin Wang, Yong Zheng, Xinbo Gao, Elisabeth Giem, Zizhong Chen</div><div><b>Pages:&nbsp;</b>2916 - 2930</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9325091/\">Observer Design for Sampled-Data Systems via Deterministic Learning</a></div><div><b>Author(s):&nbsp;</b>Jingtao Hu, Weiming Wu, Bing Ji, Cong Wang</div><div><b>Pages:&nbsp;</b>2931 - 2939</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9324926/\">Dynamically Weighted Balanced Loss: Class Imbalanced Learning and Confidence Calibration of Deep Neural Networks</a></div><div><b>Author(s):&nbsp;</b>K. Ruwani M. Fernando, Chris P. Tsokos</div><div><b>Pages:&nbsp;</b>2940 - 2951</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9328160/\">Broad Learning With Reinforcement Learning Signal Feedback: Theory and Applications</a></div><div><b>Author(s):&nbsp;</b>Ruiqi Mao, Rongxin Cui, C. L. Philip Chen</div><div><b>Pages:&nbsp;</b>2952 - 2964</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9324947/\">Neural-Network-Based Distributed Asynchronous Event-Triggered Consensus Tracking of a Class of Uncertain Nonlinear Multi-Agent Systems</a></div><div><b>Author(s):&nbsp;</b>Yun Ho Choi, Sung Jin Yoo</div><div><b>Pages:&nbsp;</b>2965 - 2979</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9325069/\">Active Learning With Multiple Kernels</a></div><div><b>Author(s):&nbsp;</b>Songnam Hong, Jeongmin Chae</div><div><b>Pages:&nbsp;</b>2980 - 2994</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9325927/\">Dissipativity-Based Finite-Time Filtering for Uncertain Semi-Markovian Jump Random Systems With Multiple Time Delays and State Constraints</a></div><div><b>Author(s):&nbsp;</b>Shaoxin Sun, Huaguang Zhang, Jian Han, Juan Zhang</div><div><b>Pages:&nbsp;</b>2995 - 3009</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9325537/\">PID Controller-Guided Attention Neural Network Learning for Fast and Effective Real Photographs Denoising</a></div><div><b>Author(s):&nbsp;</b>Ruijun Ma, Bob Zhang, Yicong Zhou, Zhengming Li, Fangyuan Lei</div><div><b>Pages:&nbsp;</b>3010 - 3023</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9325549/\">A Novel Feature Selection Method for High-Dimensional Mixed Decision Tables</a></div><div><b>Author(s):&nbsp;</b>Nguyen Ngoc Thuy, Sartra Wongthanavasu</div><div><b>Pages:&nbsp;</b>3024 - 3037</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9325918/\">Spatio-Spectral Feature Representation for Motor Imagery Classification Using Convolutional Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Ji-Seon Bang, Min-Ho Lee, Siamac Fazli, Cuntai Guan, Seong-Whan Lee</div><div><b>Pages:&nbsp;</b>3038 - 3049</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9366422/\">MetaMixUp: Learning Adaptive Interpolation Policy of MixUp With Metalearning</a></div><div><b>Author(s):&nbsp;</b>Zhijun Mai, Guosheng Hu, Dexiong Chen, Fumin Shen, Heng Tao Shen</div><div><b>Pages:&nbsp;</b>3050 - 3064</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9334415/\">An Efficient Sparse Bayesian Learning Algorithm Based on Gaussian-Scale Mixtures</a></div><div><b>Author(s):&nbsp;</b>Wei Zhou, Hai-Tao Zhang, Jun Wang</div><div><b>Pages:&nbsp;</b>3065 - 3078</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9340559/\">Efficient Approximation of High-Dimensional Functions With Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Patrick Cheridito, Arnulf Jentzen, Florian Rossmannek</div><div><b>Pages:&nbsp;</b>3079 - 3093</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9337198/\">Synaptic Scaling\u2014An Artificial Neural Network Regularization Inspired by Nature</a></div><div><b>Author(s):&nbsp;</b>Martin Hofmann, Patrick M\u00e4der</div><div><b>Pages:&nbsp;</b>3094 - 3108</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9340575/\">Observer-Based Adaptive Synchronization of Multiagent Systems With Unknown Parameters Under Attacks</a></div><div><b>Author(s):&nbsp;</b>Shiping Wen, Xiaoze Ni, Huamin Wang, Song Zhu, Kaibo Shi, Tingwen Huang</div><div><b>Pages:&nbsp;</b>3109 - 3119</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9336287/\">Explicit Duration Recurrent Networks</a></div><div><b>Author(s):&nbsp;</b>Shun-Zheng Yu</div><div><b>Pages:&nbsp;</b>3120 - 3130</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9336267/\">Observer-Based Neuro-Adaptive Optimized Control of Strict-Feedback Nonlinear Systems With State Constraints</a></div><div><b>Author(s):&nbsp;</b>Yongming Li, Yanjun Liu, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>3131 - 3145</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9494037/\">Granger Causality Inference in EEG Source Connectivity Analysis: A State-Space Approach</a></div><div><b>Author(s):&nbsp;</b>Parinthorn Manomaisaowapak, Anawat Nartkulpat, Jitkomut Songsiri</div><div><b>Pages:&nbsp;</b>3146 - 3156</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9410428/\">Rank Consistency Induced Multiview Subspace Clustering via Low-Rank Matrix Factorization</a></div><div><b>Author(s):&nbsp;</b>Jipeng Guo, Yanfeng Sun, Junbin Gao, Yongli Hu, Baocai Yin</div><div><b>Pages:&nbsp;</b>3157 - 3170</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9316932/\">Data-Driven-Based Event-Triggered Control for Nonlinear CPSs Against Jamming Attacks</a></div><div><b>Author(s):&nbsp;</b>Yingchun Wang, Xiaojie Qiu, Huaguang Zhang, Xiangpeng Xie</div><div><b>Pages:&nbsp;</b>3171 - 3177</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9333591/\">Sequence Learning in a Single Trial: A Spiking Neurons Model Based on Hippocampal Circuitry</a></div><div><b>Author(s):&nbsp;</b>Simone Coppolino, Giuseppe Giacopelli, Michele Migliore</div><div><b>Pages:&nbsp;</b>3178 - 3183</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9340604/\">DNN-kWTA With Bounded Random Offset Voltage Drifts in Threshold Logic Units</a></div><div><b>Author(s):&nbsp;</b>Wenhao Lu, Chi-Sing Leung, John Sum, Yi Xiao</div><div><b>Pages:&nbsp;</b>3184 - 3192</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-07-20T15:09:00.000+12:00",
            "pubdate_parsed": [
                2022,
                7,
                20
            ],
            "email_sent": true
        },
        "Upcoming Special Issues": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/upcoming-special-issues.html",
            "description": "<div style=\"text-align: left;\"><div style=\"text-align: left;\"><ul style=\"text-align: left;\"><li><b>IEEE Computational Intelligence Magazine</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-papers/cim/CIM-SI-MLEMO_CFP.pdf\">Machine Learning Assisted Evolutionary Multi-Objective Optimization</a> - <u>September 1, 2022</u></li><li><b>IEEE Transactions on Neural Networks and Learning Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-papers/tnnls/TNNLS_SI_CFP.pdf\">Explainable and Generalizable Deep Learning for Medical Imaging</a> -&nbsp;<u>1 September 2022</u></li><li><b>IEEE Transactions on Neural Networks and Learning Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-papers/tnnls/202202-Explainable_Representation_Learning-based_Intelligent_Inspection_and_Maintenance_of_Complex_Systems.pdf\">Explainable Representation Learning-based Intelligent Inspection and Maintenance of Complex Systems</a> -&nbsp;<u>1 September 2022</u></li><li><b>IEEE Transactions on Cognitive and Developmental Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-special-issues/Cognitive_Learning_of_Multi-Agent_Systems-IEEE_TCDS-CFP-20211210.pdf\">Cognitive Learning of Multi-Agent Systems</a> -&nbsp;<u>September 30, 2022</u></li><li><b>IEEE Transactions on Neural Networks and Learning Systems</b> - <a href=\"https://cis.ieee.org/images/files/Publications/TNNLS/special-issues/proposal_TNNLS_2.27_new.pdf\">Information Theoretic Methods for the Generalization, Robustness and Interpretability of Machine Learning</a> -&nbsp;<u>1 October 2022</u></li><li><b>IEEE Transactions on Neural Networks and Learning Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-papers/tnnls/New_SI.pdf\">Deep Learning for Intelligent Media Computing and Applications</a> -&nbsp;<u>30 October 2022</u></li><li><b>IEEE Transactions on Cognitive and Developmental Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-special-issues/Movement_Sciences_in_Cognitive_Systems_CFP_2022.pdf\">Movement Sciences in Cognitive Systems</a> -&nbsp;<u>6 January 2023</u></li><li><b>IEEE Transactions on Cognitive and Developmental Systems</b> - <a href=\"https://cis.ieee.org/images/files/Documents/call-for-special-issues/Advancing_Machine_Intelligence_with_Neuromorphic_Computing-CFP2022.pdf\">Advancing Machine Intelligence with Neuromorphic Computing</a> -&nbsp;<u>31 January 2023</u></li><li><b>IEEE Transactions on Emerging Topics in Computational Intelligence</b> - <a href=\"https://cis.ieee.org/images/files/Publications/TETCI/SI26_CFP_RSCAI.pdf\">Resource Sustainable Computational and Artificial Intelligence</a> -&nbsp;<u>1 February 2023</u></li></ul></div><div><br /></div></div>",
            "pubdate": "2022-07-21T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                7,
                21
            ],
            "email_sent": true
        },
        "IEEE Transactions on Emerging Topics in Computational Intelligence, Volume 6, Issue 4": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/ieee-transactions-on-emerging-topics-in.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9834992/\">Guest Editorial Special Issue on Computational Intelligence for IoT-based Human Activity Recognition</a></div><div><b>Author(s): </b>Xiaoli Li, Huanhuan Chen, Thomas Ploetz, Min Wu, Zhenghua Chen</div><div><b>Pages: </b>725 - 727</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9546996/\">Evolutionary Dual-Ensemble Class Imbalance Learning for Human Activity Recognition</a></div><div><b>Author(s):&nbsp;</b>Yinan Guo, Yaoqi Chu, Botao Jiao, Jian Cheng, Zekuan Yu, Ning Cui, Lianbo Ma</div><div><b>Pages:&nbsp;</b>728 - 739</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9720134/\">Real-Time Activities of Daily Living Recognition Under Long-Tailed Class Distribution</a></div><div><b>Author(s):&nbsp;</b>Atul Chaudhary, Hari Prabhat Gupta, K. K. Shukla</div><div><b>Pages:&nbsp;</b>740 - 750</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9724169/\">MOCLoc: Emerging Online Collaborative Localization Enhanced by Multidimensional Scaling</a></div><div><b>Author(s):&nbsp;</b>Chanxin Zhou, Bang Wang, Yijun Mo, Zeng Zeng</div><div><b>Pages:&nbsp;</b>751 - 761</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9779097/\">Trends and Prospects of Techniques for Haze Removal From Degraded Images: A Survey</a></div><div><b>Author(s):&nbsp;</b>Geet Sahu, Ayan Seal, Debotosh Bhattacharjee, Mita Nasipuri, Peter Brida, Ondrej Krejcar</div><div><b>Pages:&nbsp;</b>762 - 782</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9686068/\">Evolutionary Architectural Search for Generative Adversarial Networks</a></div><div><b>Author(s):&nbsp;</b>Qiuzhen Lin, Zhixiong Fang, Yi Chen, Kay Chen Tan, Yun Li</div><div><b>Pages:&nbsp;</b>783 - 794</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9600838/\">Generating Black-Box Adversarial Examples in Sparse Domain</a></div><div><b>Author(s):&nbsp;</b>Hadi Zanddizari, Behnam Zeinali, J. Morris Chang</div><div><b>Pages:&nbsp;</b>795 - 804</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9472875/\">A Novel Evolutionary Algorithm Based on Judgment-Rule Evolution Strategy for Structural Balance in Signed Social Networks</a></div><div><b>Author(s):&nbsp;</b>Mingzhou Yang, Xingwei Wang, Min Huang, Lianbo Ma, Qiang He</div><div><b>Pages:&nbsp;</b>805 - 817</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9582815/\">Multiobjective Multitasking Optimization With Subspace Distribution Alignment and Decision Variable Transfer</a></div><div><b>Author(s):&nbsp;</b>Weifeng Gao, Jiangli Cheng, Maoguo Gong, Hong Li, Jin Xie</div><div><b>Pages:&nbsp;</b>818 - 827</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9564251/\">Transfer Clustering Using a Multiple Kernel Metric Learned Under Multi-Instance Weak Supervision</a></div><div><b>Author(s):&nbsp;</b>Avisek Gupta, Swagatam Das</div><div><b>Pages:&nbsp;</b>828 - 838</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9615029/\">Dynamic Path Planning for Unmanned Aerial Vehicles Under Deadline and Sector Capacity Constraints</a></div><div><b>Author(s):&nbsp;</b>Sudharsan Vaidhun, Zhishan Guo, Jiang Bian, Haoyi Xiong, Sajal K. Das</div><div><b>Pages:&nbsp;</b>839 - 851</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9586052/\">A Robust Non-Integer Controller Design for Load Frequency Control in Modern Marine Power Grids</a></div><div><b>Author(s):&nbsp;</b>B. Yildirim, M. Gheisarnejad, M. H. Khooban</div><div><b>Pages:&nbsp;</b>852 - 866</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9586057/\">A Weighted Portfolio Optimization Model Based on the Trend Ratio, Emotion Index, and ANGQTS</a></div><div><b>Author(s):&nbsp;</b>Yao-Hsin Chou, Yu-Chi Jiang, Yi-Rui Hsu, Shu-Yu Kuo, Sy-Yen Kuo</div><div><b>Pages:&nbsp;</b>867 - 882</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9671053/\">Do Models Learn the Directionality of Relations? A New Evaluation: Relation Direction Recognition</a></div><div><b>Author(s):&nbsp;</b>Shengfei Lyu, Xingyu Wu, Jinlong Li, Qiuju Chen, Huanhuan Chen</div><div><b>Pages:&nbsp;</b>883 - 892</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9628017/\">Quaternion Capsule Neural Network With Region Attention for Facial Expression Recognition in Color Images</a></div><div><b>Author(s):&nbsp;</b>Yu Zhou, Lianghai Jin, Guangzhi Ma, Xiangyang Xu</div><div><b>Pages:&nbsp;</b>893 - 912</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9475074/\">Unbalanced Incomplete Multi-View Clustering Via the Scheme of View Evolution: Weak Views are Meat,&nbsp; Strong Views Do Eat</a></div><div><b>Author(s):&nbsp;</b>Xiang Fang, Yuchong Hu, Pan Zhou, Dapeng Oliver Wu</div><div><b>Pages:&nbsp;</b>913 - 927</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9655256/\">EDITH : ECG Biometrics Aided by Deep Learning for Reliable Individual Authentication</a></div><div><b>Author(s):&nbsp;</b>Nabil Ibtehaz, Muhammad E. H. Chowdhury, Amith Khandakar, Serkan Kiranyaz, M. Sohel Rahman, Anas Tahir, Yazan Qiblawey, Tawsifur Rahman</div><div><b>Pages:&nbsp;</b>928 - 940</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9576096/\">Efficient Privacy Preserving Edge Intelligent Computing Framework for Image Classification in IoT</a></div><div><b>Author(s):&nbsp;</b>Omobayode Fagbohungbe, Sheikh Rufsan Reza, Xishuang Dong, Lijun Qian</div><div><b>Pages:&nbsp;</b>941 - 956</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9583676/\">APNet: Adversarial Learning Assistance and Perceived Importance Fusion Network for All-Day RGB-T Salient Object Detection</a></div><div><b>Author(s):&nbsp;</b>Wujie Zhou, Yun Zhu, Jingsheng Lei, Jian Wan, Lu Yu</div><div><b>Pages:&nbsp;</b>957 - 968</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9615378/\">A Novel Test Case Generation Approach for Adaptive Random Testing of Object-Oriented Software Using K-Means Clustering Technique</a></div><div><b>Author(s):&nbsp;</b>Jinfu Chen, Haibo Chen, Yuchi Guo, Minmin Zhou, Rubing Huang, Chengying Mao</div><div><b>Pages:&nbsp;</b>969 - 981</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9772749/\">Data Embedding Scheme for Efficient Program Behavior Modeling With Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Sunwoo Ahn, Hayoon Yi, Ho Bae, Sungroh Yoon, Yunheung Paek</div><div><b>Pages:&nbsp;</b>982 - 993</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9704877/\">Half Quadratic Dual Learning for Fuzzy Multiconcepts of Partially-Observed Images</a></div><div><b>Author(s):&nbsp;</b>Bo-Wei Chen, Kuan-Lin Hou</div><div><b>Pages:&nbsp;</b>994 - 1007</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9712322/\">Obtaining Fuzzy Membership Function of Clusters With the Memristor Hardware Implementation and On-Chip Learning</a></div><div><b>Author(s):&nbsp;</b>Mohammad Javadian, Arian Hejazi, Sajad Haghzad Klidbary</div><div><b>Pages:&nbsp;</b>1008 - 1025</div><div><br /></div>",
            "pubdate": "2022-07-29T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                7,
                29
            ],
            "email_sent": true
        },
        "IEEE Transactions on Artificial Intelligence, Volume 3, Issue 4": {
            "url": "https://computational-intelligence.blogspot.com/2022/07/ieee-transactions-on-artificial.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9591342/\">Balanced Graph Cut With Exponential Inter-Cluster Compactness</a></div><div><b>Author(s): </b>Danyang Wu, Feiping Nie, Jitao Lu, Rong Wang, Xuelong Li</div><div><b>Pages: </b>498 - 505</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9695289/\">Efficient Temporal Piecewise-Linear Numeric Planning With Lazy Consistency Checking</a></div><div><b>Author(s):&nbsp;</b>Josef Bajada, Maria Fox, Derek Long</div><div><b>Pages:&nbsp;</b>506 - 517</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9612034/\">Ignorance is Bliss: Exploring Defenses Against Invariance-Based Attacks on Neural Machine Translation Systems</a></div><div><b>Author(s):&nbsp;</b>Akshay Chaturvedi, Abhisek Chakrabarty, Masao Utiyama, Eiichiro Sumita, Utpal Garain</div><div><b>Pages:&nbsp;</b>518 - 525</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9618852/\">ArcText: A Unified Text Approach to Describing Convolutional Neural Network Architectures</a></div><div><b>Author(s):&nbsp;</b>Yanan Sun, Gary G. Yen, Bing Xue, Mengjie Zhang, Jiancheng Lv</div><div><b>Pages:&nbsp;</b>526 - 540</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9652037/\">Histogram Layers for Texture Analysis</a></div><div><b>Author(s):&nbsp;</b>Joshua Peeples, Weihuang Xu, Alina Zare</div><div><b>Pages:&nbsp;</b>541 - 552</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9613742/\">Traded Control of Human\u2013Machine Systems for Sequential Decision-Making Based on Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Qianqian Zhang, Yu Kang, Yun-Bo Zhao, Pengfei Li, Shiyi You</div><div><b>Pages:&nbsp;</b>553 - 566</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9612040/\">Potential Impacts of Smart Homes on Human Behavior: A Reinforcement Learning Approach</a></div><div><b>Author(s):&nbsp;</b>Shashi Suman, Ali Etemad, Francois Rivest</div><div><b>Pages:&nbsp;</b>567 - 580</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9612064/\">Multiadvisor Reinforcement Learning for Multiagent Multiobjective Smart Home Energy Control</a></div><div><b>Author(s):&nbsp;</b>Andrew Tittaferrante, Abdulsalam Yassine</div><div><b>Pages:&nbsp;</b>581 - 594</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9614997/\">On a Sparse Shortcut Topology of Artificial Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Feng-Lei Fan, Dayang Wang, Hengtao Guo, Qikui Zhu, Pingkun Yan, Ge Wang, Hengyong Yu</div><div><b>Pages:&nbsp;</b>595 - 608</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9594655/\">CSNAS: Contrastive Self-Supervised Learning Neural Architecture Search Via Sequential Model-Based Optimization</a></div><div><b>Author(s):&nbsp;</b>Nam Nguyen, J. Morris Chang</div><div><b>Pages:&nbsp;</b>609 - 624</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9652108/\">Perturbed Composite Attention Model for Macular Optical Coherence Tomography Image Classification</a></div><div><b>Author(s):&nbsp;</b>Sapna S. Mishra, Bappaditya Mandal, Niladri B. Puhan</div><div><b>Pages:&nbsp;</b>625 - 635</div><div><br /></div>",
            "pubdate": "2022-07-30T12:00:00.041+12:00",
            "pubdate_parsed": [
                2022,
                7,
                30
            ],
            "email_sent": true
        },
        "IEEE Transactions on Evolutionary Computation, Volume 26, Issue 4": {
            "url": "https://computational-intelligence.blogspot.com/2022/08/ieee-transactions-on-evolutionary.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9515483/\">A Kernel-Based Indicator for Multi/Many-Objective Optimization</a></div><div><b>Author(s): </b>Xinye Cai, Yushun Xiao, Zhenhua Li, Qi Sun, Hanchuan Xu, Miqing Li, Hisao Ishibuchi</div><div><b>Pages: </b>602 - 615</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9521243/\">Clustering-Guided Particle Swarm Feature Selection Algorithm for High-Dimensional Imbalanced Data With Missing Values</a></div><div><b>Author(s):&nbsp;</b>Yong Zhang, Yan-Hu Wang, Dun-Wei Gong, Xiao-Yan Sun</div><div><b>Pages:&nbsp;</b>616 - 630</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9509584/\">An Ensemble Surrogate-Based Framework for Expensive Multiobjective Evolutionary Optimization</a></div><div><b>Author(s):&nbsp;</b>Qiuzhen Lin, Xunfeng Wu, Lijia Ma, Jianqiang Li, Maoguo Gong, Carlos A. Coello Coello</div><div><b>Pages:&nbsp;</b>631 - 645</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9528854/\">A Voting-Mechanism-Based Ensemble Framework for Constraint Handling Techniques</a></div><div><b>Author(s):&nbsp;</b>Guohua Wu, Xupeng Wen, Ling Wang, Witold Pedrycz, Ponnuthurai Nagaratnam Suganthan</div><div><b>Pages:&nbsp;</b>646 - 660</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9520652/\">Genetic Programming for Manifold Learning: Preserving Local Topology</a></div><div><b>Author(s):&nbsp;</b>Andrew Lensen, Bing Xue, Mengjie Zhang</div><div><b>Pages:&nbsp;</b>661 - 675</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9542966/\">Surrogate-Assisted Autoencoder-Embedded Evolutionary Optimization Algorithm to Solve High-Dimensional Expensive Problems</a></div><div><b>Author(s):&nbsp;</b>Meiji Cui, Li Li, Mengchu Zhou, Abdullah Abusorrah</div><div><b>Pages:&nbsp;</b>676 - 68</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9546930/\">An Online Prediction Approach Based on Incremental Support Vector Machine for Dynamic Multiobjective Optimization</a></div><div><b>Author(s):&nbsp;</b>Dejun Xu, Min Jiang, Weizhen Hu, Shaozi Li, Renhu Pan, Gary G. Yen</div><div><b>Pages:&nbsp;</b>690 - 703</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9594087/\">Region-Focused Memetic Algorithms With Smart Initialization for Real-World Large-Scale Waste Collection Problems</a></div><div><b>Author(s):&nbsp;</b>Wenxing Lan, Ziyuan Ye, Peijun Ruan, Jialin Liu, Peng Yang, Xin Yao</div><div><b>Pages:&nbsp;</b>704 - 718</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9627943/\">A Meta-Knowledge Transfer-Based Differential Evolution for Multitask Optimization</a></div><div><b>Author(s):&nbsp;</b>Jian-Yu Li, Zhi-Hui Zhan, Kay Chen Tan, Jun Zhang</div><div><b>Pages:&nbsp;</b>719 - 734</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9656554/\">An Evolutionary Forest for Regression</a></div><div><b>Author(s):&nbsp;</b>Hengzhe Zhang, Aimin Zhou, Hu Zhang</div><div><b>Pages:&nbsp;</b>735 - 749</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9509298/\">Fast Greedy Subset Selection From Large Candidate Solution Sets in Evolutionary Multiobjective Optimization</a></div><div><b>Author(s):&nbsp;</b>Weiyu Chen, Hisao Ishibuchi, Ke Shang</div><div><b>Pages:&nbsp;</b>750 - 764</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9621218/\">Genetic Programming With Knowledge Transfer and Guided Search for Uncertain Capacitated Arc Routing Problem</a></div><div><b>Author(s):&nbsp;</b>Mazhar Ansari Ardeh, Yi Mei, Mengjie Zhang</div><div><b>Pages:&nbsp;</b>765 - 779</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9559391/\">Surrogate-Assisted Differential Evolution With Region Division for Expensive Optimization Problems With Discontinuous Responses</a></div><div><b>Author(s):&nbsp;</b>Yong Wang, Jianqing Lin, Jiao Liu, Guangyong Sun, Tong Pang</div><div><b>Pages:&nbsp;</b>780 - 792</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9684551/\">Automating Genetic Algorithm Mutations for Molecules Using a Masked Language Model</a></div><div><b>Author(s):&nbsp;</b>Andrew E. Blanchard, Mayanka Chandra Shekar, Shang Gao, John Gounley, Isaac Lyngaas, Jens Glaser, Debsindhu Bhowmik</div><div><b>Pages:&nbsp;</b>793 - 799</div><div><br /></div></div>",
            "pubdate": "2022-08-04T17:35:00.000+12:00",
            "pubdate_parsed": [
                2022,
                8,
                4
            ],
            "email_sent": true
        },
        "IEEE Transactions on Fuzzy Systems, Volume 30, Issue 8": {
            "url": "https://computational-intelligence.blogspot.com/2022/08/ieee-transactions-on-fuzzy-systems.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9366977/\">The Fusion of Deep Learning and Fuzzy Systems: A State-of-the-Art Survey</a></div><div><b>Author(s): </b>Yuanhang Zheng, Zeshui Xu, Xinxin Wang</div><div><b>Pages: </b>2783 - 2799</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9477079/\">A Decade of the Z-Numbers</a></div><div><b>Author(s):&nbsp;</b>Romi Banerjee, Sankar K. Pal, Jayanta Kumar Pal</div><div><b>Pages:&nbsp;</b>2800 - 2812</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9416169/\">Event-Based Adaptive Fixed-Time Fuzzy Control for Active Vehicle Suspension Systems With Time-Varying Displacement Constraint</a></div><div><b>Author(s):&nbsp;</b>Tinghan Jia, Yingnan Pan, Hongjing Liang, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>2813 - 2821</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9416776/\">Event-Triggered Asynchronous Fuzzy Filtering for Vehicle Sideslip Angle Estimation With Data Quantization and Dropouts</a></div><div><b>Author(s):&nbsp;</b>Wenfeng Li, Zhengchao Xie, Pak Kin Wong, Yunfeng Hu, Ge Guo, Jing Zhao</div><div><b>Pages:&nbsp;</b>2822 - 2836</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9416832/\">Design of Syncretic Fuzzy-Neural Control for WWTP</a></div><div><b>Author(s):&nbsp;</b>Honggui Han, Zheng Liu, Jiaming Li, Junfei Qiao</div><div><b>Pages:&nbsp;</b>2837 - 2849</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9417721/\">Detection and Classification of Anomalies in Large Datasets on the Basis of Information Granules</a></div><div><b>Author(s):&nbsp;</b>Adam Kiersztyn, Pawe\u0142 Karczmarek, Krystyna Kiersztyn, Witold Pedrycz</div><div><b>Pages:&nbsp;</b>2850 - 2860</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9477024/\">A Hierarchical Approach to Interpretability of TS Rule-Based Models</a></div><div><b>Author(s):&nbsp;</b>Witold Pedrycz, Adam Gacek, Xianmin Wang</div><div><b>Pages:&nbsp;</b>2861 - 2869</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9476988/\">Fuzzy Adaptive Optimal Consensus Fault-Tolerant Control for Stochastic Nonlinear Multiagent Systems</a></div><div><b>Author(s):&nbsp;</b>Kewen Li, Yongming Li</div><div><b>Pages:&nbsp;</b>2870 - 2885</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9483671/\">A Spectral Feature Selection Approach With Kernelized Fuzzy Rough Sets</a></div><div><b>Author(s):&nbsp;</b>Jinkun Chen, Yaojin Lin, Jusheng Mi, Shaozi Li, Weiping Ding</div><div><b>Pages:&nbsp;</b>2886 - 2901</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9490300/\">An Unsupervised Fuzzy Clustering Approach for Early Screening of COVID-19 From Radiological Images</a></div><div><b>Author(s):&nbsp;</b>Weiping Ding, Shouvik Chakraborty, Kalyani Mali, Sankhadeep Chatterjee, Janmenjoy Nayak, Asit Kumar Das, Soumen Banerjee</div><div><b>Pages:&nbsp;</b>2902 - 2914</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9490345/\">Interval Type-2 Fuzzy Differential Equations and Stability</a></div><div><b>Author(s):&nbsp;</b>Marzieh Najariyan, Li Qiu</div><div><b>Pages:&nbsp;</b>2915 - 2929</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9490328/\">Feature Selection With Fuzzy-Rough Minimum Classification Error Criterion</a></div><div><b>Author(s):&nbsp;</b>Changzhong Wang, Yuhua Qian, Weiping Ding, Xiaodong Fan</div><div><b>Pages:&nbsp;</b>2930 - 2942</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9492324/\">An Enhanced Input-Delay Approach to Sampled-Data Stabilization for Nonlinear Stochastic Singular Systems Based on T-S Fuzzy Models</a></div><div><b>Author(s):&nbsp;</b>Shuangyun Xing, Weixing Zheng, Feiqi Deng, Chunling Chang</div><div><b>Pages:&nbsp;</b>2943 - 2956</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9492811/\">Deep Fuzzy Rule-Based Classification System With Improved Wang\u2013Mendel Method</a></div><div><b>Author(s):&nbsp;</b>Yuangang Wang, Haoran Liu, Wenjuan Jia, Shuo Guan, Xiaodong Liu, Xiaodong Duan</div><div><b>Pages:&nbsp;</b>2957 - 2970</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9492757/\">Observer-Based Fault Reconstruction and Fault-Tolerant Control for Nonlinear Systems Subject to Simultaneous Actuator and Sensor Faults</a></div><div><b>Author(s):&nbsp;</b>Huaguang Zhang, Yunfei Mu, Zhiyun Gao, Wei Wang</div><div><b>Pages:&nbsp;</b>2971 - 2980</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9495277/\">Adaptive Fuzzy Event-Triggered Control of Aerial Refueling Hose System With Actuator Failures</a></div><div><b>Author(s):&nbsp;</b>Zhijie Liu, Jun Shi, Xuena Zhao, Zhijia Zhao, Han-Xiong Li</div><div><b>Pages:&nbsp;</b>2981 - 2992</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9495212/\">Input-to-State Stability of the Nonlinear Fuzzy Systems via Small-Gain Theorem and Decentralized Sliding-Mode Control</a></div><div><b>Author(s):&nbsp;</b>Zhenghong Jin, Zhanxiu Wang, Jiawen Li</div><div><b>Pages:&nbsp;</b>2993 - 3008</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9496162/\">Modeling, Fault Detection, and Fault-Tolerant Control for Nonlinear Singularly Perturbed Systems With Actuator Faults and External Disturbances</a></div><div><b>Author(s):&nbsp;</b>Jinxiang Chen, Chunxiao He</div><div><b>Pages:&nbsp;</b>3009 - 3022</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9496226/\">Fuzzy STUDENT\u2019S T-Distribution Model Based on Richer Spatial Combination</a></div><div><b>Author(s):&nbsp;</b>Tao Lei, Xiaohong Jia, Dinghua Xue, Qi Wang, Hongying Meng, Asoke K. Nandi</div><div><b>Pages:&nbsp;</b>3023 - 3037</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9496225/\">Incomplete Multiple View Fuzzy Inference System With Missing View Imputation and Cooperative Learning</a></div><div><b>Author(s):&nbsp;</b>Wei Zhang, Zhaohong Deng, Te Zhang, Kup-Sze Choi, Jun Wang, Shitong Wang</div><div><b>Pages:&nbsp;</b>3038 - 3051</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9506856/\">Event-Triggered Practical Fixed-Time Fuzzy Containment Control for Stochastic Multiagent Systems</a></div><div><b>Author(s):&nbsp;</b>Dajie Yao, Chunxia Dou, Dong Yue, Xiangpeng Xie</div><div><b>Pages:&nbsp;</b>3052 - 3062</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9506819/\">Interval Type-2 Fuzzy Control for HMM-Based Multiagent Systems via Dynamic Event-Triggered Scheme</a></div><div><b>Author(s):&nbsp;</b>Yuan Wang, Huaicheng Yan, Hao Zhang, Hao Shen, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>3063 - 3073</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9506820/\">Fuzzy Event-Triggered Control for PDE Systems With Pointwise Measurements Based on Relaxed Lyapunov\u2013Krasovskii Functionals</a></div><div><b>Author(s):&nbsp;</b>Xiaona Song, Qiyuan Zhang, Yijun Zhang, Shuai Song</div><div><b>Pages:&nbsp;</b>3074 - 3084</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9511173/\">Quasi-Synchronization of Fuzzy Heterogeneous Complex Networks via Intermittent Discrete-Time State Observations Control</a></div><div><b>Author(s):&nbsp;</b>Tianrui Chen, Wenhua Wang, Yongbao Wu</div><div><b>Pages:&nbsp;</b>3085 - 3097</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9511214/\">Event-Triggered Output Feedback Type-2 Fuzzy Control for Uncertain Steer-By-Wire Systems With Prespecified Tracking Performance</a></div><div><b>Author(s):&nbsp;</b>Bingxin Ma, Yongfu Wang, Tianyou Chai</div><div><b>Pages:&nbsp;</b>3098 - 3112</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9512480/\">H\u221e Fuzzy Dynamic Output Feedback Reliable Control for Markov Jump Nonlinear Systems With PDT Switched Transition Probabilities and Its Application</a></div><div><b>Author(s):&nbsp;</b>Jing Wang, Jiacheng Wu, Jinde Cao, Ju H. Park, Hao Shen</div><div><b>Pages:&nbsp;</b>3113 - 3124</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9512491/\">Distributed Semisupervised Fuzzy Regression With Interpolation Consistency Regularization</a></div><div><b>Author(s):&nbsp;</b>Ye Shi, Leijie Zhang, Zehong Cao, Mohammad Tanveer, Chin-Teng Lin</div><div><b>Pages:&nbsp;</b>3125 - 3137</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9516920/\">Lagrange Stability of Fuzzy Memristive Neural Networks on Time Scales With Discrete Time Varying and Infinite Distributed Delays</a></div><div><b>Author(s):&nbsp;</b>Peng Wan, Zhigang Zeng</div><div><b>Pages:&nbsp;</b>3138 - 3151</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9516953/\">Effective Fuzzy System for Qualifying the Characteristics of Stocks by Random Trading</a></div><div><b>Author(s):&nbsp;</b>Mu-En Wu, Jia-Hao Syu, Jerry Chun-Wei Lin, Jan-Ming Ho</div><div><b>Pages:&nbsp;</b>3152 - 3165</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9516892/\">Fuzzy First-Order and Second Moment Method for Failure Credibility Analysis in the Presence of Fuzzy Uncertainty</a></div><div><b>Author(s):&nbsp;</b>Beixi Jia, Zhenzhou Lu, Jingyu Lei</div><div><b>Pages:&nbsp;</b>3166 - 3175</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9520250/\">A Concise TSK Fuzzy Ensemble Classifier Integrating Dropout and Bagging for High-Dimensional Problems</a></div><div><b>Author(s):&nbsp;</b>Fei Guo, Jiahuan Liu, Maoyuan Li, Tianlun Huang, Yun Zhang, Dequn Li, Huamin Zhou</div><div><b>Pages:&nbsp;</b>3176 - 3190</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9521795/\">Resilient Adaptive Event-Triggered Fuzzy Tracking Control and Filtering for Nonlinear Networked Systems Under Denial-of-Service Attacks</a></div><div><b>Author(s):&nbsp;</b>Ning Zhao, Peng Shi, Wen Xing, Chee Peng Lim</div><div><b>Pages:&nbsp;</b>3191 - 3201</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9522064/\">Nonfragile Sampled-Data Control of T\u2013S Fuzzy Systems With Time Delay</a></div><div><b>Author(s):&nbsp;</b>Yunfei Qiu, Changchun Hua, Yibo Wang</div><div><b>Pages:&nbsp;</b>3202 - 3210</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9522055/\">Integrated State/Fault Estimation and Fault-Tolerant Control Design for Switched T\u2013S Fuzzy Systems With Sensor and Actuator Faults</a></div><div><b>Author(s):&nbsp;</b>Ayyoub Ait Ladel, Abdellah Benzaouia, Rachid Outbib, Mustapha Ouladsine</div><div><b>Pages:&nbsp;</b>3211 - 3223</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9522042/\">Extended-Dissipativity-Based Adaptive Event-Triggered Control for Stochastic Polynomial Fuzzy Singular Systems</a></div><div><b>Author(s):&nbsp;</b>Zhiguang Feng, Yang Yang, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>3224 - 3236</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9522052/\">Computing a Possibility Theory Repair for Partially Preordered Inconsistent Ontologies</a></div><div><b>Author(s):&nbsp;</b>Sihem Belabbes, Salem Benferhat</div><div><b>Pages:&nbsp;</b>3237 - 3246</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9535276/\">Adaptive Fuzzy Control for a Hybrid Spacecraft System With Spatial Motion and Communication Constraints</a></div><div><b>Author(s):&nbsp;</b>Zhiji Han, Zhijie Liu, Linghuan Kong, Liang Ding, Jun-Wei Wang, Wei He</div><div><b>Pages:&nbsp;</b>3247 - 3256</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9535264/\">Robust Asynchronous Filtering for Discrete-Time T\u2013S Fuzzy Complex Dynamical Networks Against Deception Attacks</a></div><div><b>Author(s):&nbsp;</b>Ramalingam Sakthivel, Oh-Min Kwon, Myeong Jin Park, Seong-Gon Choi, Rathinasamy Sakthivel</div><div><b>Pages:&nbsp;</b>3257 - 3269</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9537692/\">Broad Learning Based Dynamic Fuzzy Inference System With Adaptive Structure and Interpretable Fuzzy Rules</a></div><div><b>Author(s):&nbsp;</b>Kaiyuan Bai, Xiaomin Zhu, Shiping Wen, Runtong Zhang, Wenyu Zhang</div><div><b>Pages:&nbsp;</b>3270 - 3283</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9537594/\">LFIC: Identifying Influential Nodes in Complex Networks by Local Fuzzy Information Centrality</a></div><div><b>Author(s):&nbsp;</b>Haotian Zhang, Shen Zhong, Yong Deng, Kang Hao Cheong</div><div><b>Pages:&nbsp;</b>3284 - 3296</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9537698/\">Adjustable Event-Triggered Load Frequency Control of Power Systems Using Control-Performance-Standard-Based Fuzzy Logic</a></div><div><b>Author(s):&nbsp;</b>Xing-Chen Shangguan, Yong He, Chuan-Ke Zhang, Lin Jiang, Min Wu</div><div><b>Pages:&nbsp;</b>3297 - 3311</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9538964/\">Intermittent Event-Triggered Exponential Stabilization for State-Dependent Switched Fuzzy Neural Networks With Mixed Delays</a></div><div><b>Author(s):&nbsp;</b>Xiaofan Li, Nikhil R. Pal, Huiyuan Li, Tingwen Huang</div><div><b>Pages:&nbsp;</b>3312 - 3321</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9541054/\">An Algebraic Fuzzy Pole Placement Approach to Stabilize Nonlinear Mechanical Systems</a></div><div><b>Author(s):&nbsp;</b>Jes\u00fas Alberto Meda-Campa\u00f1a, Rom\u00e1n A. Rodr\u00edguez-Manzanarez, S. Denisse Ontiveros-Paredes, Jos\u00e9 de Jes\u00fas Rubio, Ricardo Tapia-Herrera, Tonatiuh Hern\u00e1ndez-Cort\u00e9s, Guillermo Obreg\u00f3n-Pulido, Carlos Aguilar-Ib\u00e1\u00f1ez</div><div><b>Pages:&nbsp;</b>3322 - 3332</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9541015/\">Consensus Reaching in Multiple Attribute Group Decision Making: A Multi-Stage Optimization Feedback Mechanism With Individual Bounded Confidences</a></div><div><b>Author(s):&nbsp;</b>Quanbo Zha, Yucheng Dong, Francisco Chiclana, Enrique Herrera-Viedma</div><div><b>Pages:&nbsp;</b>3333 - 3346</div><div><br /></div><div><b>45)</b> <a href=\"https://ieeexplore.ieee.org/document/9541019/\">Fuzzy Secure Control for Nonlinear N-D Parabolic PDE-ODE Coupled Systems Under Stochastic Deception Attacks</a></div><div><b>Author(s):&nbsp;</b>Ruimei Zhang, Hongxia Wang, Ju H. Park, Peisong He, Deqiang Zeng, Xiangpeng Xie</div><div><b>Pages:&nbsp;</b>3347 - 3359</div><div><br /></div><div><b>46)</b> M<a href=\"https://ieeexplore.ieee.org/document/9540982/\">ultilinear-Trend Fuzzy Information Granule-Based Short-Term Forecasting for Time Series</a></div><div><b>Author(s):&nbsp;</b>Fang Li, Yuqing Tang, Fusheng Yu, Witold Pedrycz, Yuming Liu, Wenyi Zeng</div><div><b>Pages:&nbsp;</b>3360 - 3372</div><div><br /></div><div><b>47)</b> <a href=\"https://ieeexplore.ieee.org/document/9547686/\">Adaptive Fuzzy Decentralized Dynamic Surface Control for Fractional-Order Nonlinear Large-Scale Systems</a></div><div><b>Author(s):&nbsp;</b>Yongliang Zhan, Shuai Sui, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>3373 - 3383</div><div><br /></div><div><b>48)</b> <a href=\"https://ieeexplore.ieee.org/document/9516933/\">Federated FCM: Clustering Under Privacy Requirements</a></div><div><b>Author(s):&nbsp;</b>Witold Pedrycz</div><div><b>Pages:&nbsp;</b>3384 - 3388</div><div><br /></div><div><b>49)</b> <a href=\"https://ieeexplore.ieee.org/document/9540996/\">Fuzzy Logic on Quantum Annealers</a></div><div><b>Author(s):&nbsp;</b>Amir Pourabdollah, Giovanni Acampora, Roberto Schiattarella</div><div><b>Pages:&nbsp;</b>3389 - 3394</div><div><br /></div>",
            "pubdate": "2022-08-08T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                8,
                8
            ],
            "email_sent": true
        },
        "Complex and Intelligent Systems, Volume 8, Issue 4": {
            "url": "https://computational-intelligence.blogspot.com/2022/08/complex-and-intelligent-systems-volume.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00766-x\">Evolutionary optimization of large complex problems</a></div><div><b>Author(s): </b>Handing Wang, Chaoli Sun...Yew-soon Ong</div><div><b>Pages: </b>2697 - 2698</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-020-00249-x\">Surrogate-assisted evolutionary algorithm for expensive constrained multi-objective discrete optimization problems</a></div><div><b>Author(s):&nbsp;</b>Qinghua Gu, Qian Wang...Lu Chen</div><div><b>Pages:&nbsp;</b>2699 - 2718</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00352-7\">Accelerate the optimization of large-scale manufacturing planning using game theory</a></div><div><b>Author(s):&nbsp;</b>Hui-Ling Zhen, Zhenkun Wang...Jia Zeng</div><div><b>Pages:&nbsp;</b>2719 - 2730</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00374-1\">fSDE: efficient evolutionary optimisation for many-objective aero-engine calibration</a></div><div><b>Author(s):&nbsp;</b>Jialin Liu, Qingquan Zhang...Feng Wu</div><div><b>Pages:&nbsp;</b>2731 - 2747</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00402-0\">Simplified Phasmatodea population evolution algorithm for optimization</a></div><div><b>Author(s):&nbsp;</b>Pei-Cheng Song, Shu-Chuan Chu...Hongmei Yang</div><div><b>Pages:&nbsp;</b>2749 - 2767</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00452-4\">Two-stage improved Grey Wolf optimization algorithm for feature selection on high-dimensional classification</a></div><div><b>Author(s):&nbsp;</b>Chaonan Shen, Kai Zhang</div><div><b>Pages:&nbsp;</b>2769 - 2789</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00510-x\">A hybrid ant lion optimization chicken swarm optimization algorithm for charger placement problem</a></div><div><b>Author(s):&nbsp;</b>Sanchari Deb, Xiao-Zhi Gao</div><div><b>Pages:&nbsp;</b>2791 - 2808</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00773-y\">Robotic dexterous manipulation: from tele-operation to autonomous learning and adaptive control</a></div><div><b>Author(s):&nbsp;</b>Qiang Li, Chao Liu...Helge Ritter</div><div><b>Pages:&nbsp;</b>2809 - 2811</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00341-w\">Lower limb movement intention recognition for rehabilitation robot aided with projected recurrent neural network</a></div><div><b>Author(s):&nbsp;</b>Mei Liu, Bo Peng, Mingsheng Shang</div><div><b>Pages:&nbsp;</b>2813 - 2824</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00333-w\">Approach to hand posture recognition based on hand shape features for human\u2013robot interaction</a></div><div><b>Author(s):&nbsp;</b>Jing Qi, Kun Xu, Xilun Ding</div><div><b>Pages:&nbsp;</b>2825 - 2842</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00350-9\">Development and evaluation of demonstration information recording approach for wheelchair mounted robotic arm</a></div><div><b>Author(s):&nbsp;</b>Mingshan Chi, Yaxin Liu...Ming Zhong</div><div><b>Pages:&nbsp;</b>2843 - 2857</div><div><br /></div><div><b>12) </b><a href=\"https://link.springer.com/article/10.1007/s40747-021-00420-y\">Towards a balancing safety against performance approach in human\u2013robot co-manipulation for door-closing emergencies</a></div><div><b>Author(s):&nbsp;</b>Chuande Liu, Chuang Yu...Adriana Tapus</div><div><b>Pages:&nbsp;</b>2859 - 2871</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00429-3\">DMPs-based skill learning for redundant dual-arm robotic synchronized cooperative manipulation</a></div><div><b>Author(s):&nbsp;</b>Zhenyu Lu, Ning Wang, Donghao Shi</div><div><b>Pages:&nbsp;</b>2873 - 2882</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00418-6\">Incorporating model predictive control with fuzzy approximation for robot manipulation under remote center of motion constraint</a></div><div><b>Author(s):&nbsp;</b>Hang Su, Junhao Zhang...Elena De Momi</div><div><b>Pages:&nbsp;</b>2883 - 2895</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00464-0\">A continuous switching contact model for virtual environment based teleoperation</a></div><div><b>Author(s):&nbsp;</b>Yongqing Fu, Baibo Wu, Weiyang Lin</div><div><b>Pages:&nbsp;</b>1 - 13</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00459-x\">GraspVDN: scene-oriented grasp estimation by learning vector representations of grasps</a></div><div><b>Author(s):&nbsp;</b>Zhipeng Dong, Hongkun Tian...Fei Chen</div><div><b>Pages:&nbsp;</b>2911 - 2922</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00485-9\">Monocular tissue reconstruction via remote center motion for robot-assisted minimally invasive surgery</a></div><div><b>Author(s):&nbsp;</b>Peng Li, Ming Tang...Yunhui Liu</div><div><b>Pages:&nbsp;</b>2923 - 2936</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00499-3\">Hybrid type multi-robot path planning of a serial manipulator and SwarmItFIX robots in sheet metal milling process</a></div><div><b>Author(s):&nbsp;</b>Satheeshkumar Veeramani, Sreekumar Muthuswamy</div><div><b>Pages:&nbsp;</b>2937 - 2954</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00522-7\">A peduncle detection method of tomato for autonomous harvesting</a></div><div>J<b>Author(s):&nbsp;</b>iacheng Rong, Guanglin Dai, Pengbo Wang</div><div><b>Pages:&nbsp;</b>2955 - 2969</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00533-4\">Intent inference in shared-control teleoperation system in consideration of user behavior</a></div><div><b>Author(s):&nbsp;</b>Liangliang Wang, Qiang Li...Zhengyou Zhang</div><div><b>Pages:&nbsp;</b>2971 - 2981</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00546-z\">Bilateral teleoperation with object-adaptive mapping</a></div><div><b>Author(s):&nbsp;</b>Xiao Gao, Jo\u00e3o Silv\u00e9rio...Xiaohui Xiao</div><div><b>Pages:&nbsp;</b>2983 - 2990</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00652-6\">Fault-tolerant motion planning and generation of quadruped robots synthesised by posture optimization and whole body control</a></div><div><b>Author(s):&nbsp;</b>Junwen Cui, Zhan Li...Tianxiao Li</div><div><b>Pages:&nbsp;</b>2991 - 3003</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00777-8\">Editorial of the Special Issue: Brain-like computing for medical applications</a></div><div><b>Author(s):&nbsp;</b>Yu-Dong Zhang, Hong Lin...Steven L. Fernandes</div><div><b>Pages:&nbsp;</b>3005 - 3006</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00321-0\">A decision support system for multimodal brain tumor classification using deep learning</a></div><div><b>Author(s):&nbsp;</b>Muhammad Imran Sharif, Muhammad Attique Khan...Mudassar Raza</div><div><b>Pages:&nbsp;</b>3007 - 3020</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00319-8\">Smart healthcare system-a brain-like computing approach for analyzing the performance of detectron2 and PoseNet models for anomalous action detection in aged people with movement impairments</a></div><div><b>Author(s):&nbsp;</b>R. Divya, J. Dinesh Peter</div><div><b>Pages:&nbsp;</b>3021 - 3040</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00328-7\">3D-semantic segmentation and classification of stomach infections using uncertainty aware deep neural networks</a></div><div><b>Author(s):&nbsp;</b>Javaria Amin, Muhammad Sharif...Ramesh Sunder Nayak</div><div><b>Pages:&nbsp;</b>3041 - 3057</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00336-7\">EEG data augmentation for emotion recognition with a multiple generator conditional Wasserstein GAN</a></div><div><b>Author(s):&nbsp;</b>Aiming Zhang, Lei Su...Shengjin Liang</div><div><b>Pages:&nbsp;</b>3059 - 3071</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00398-7\">Comparative performance analysis of quantum machine learning with deep learning for diabetes prediction</a></div><div><b>Author(s):&nbsp;</b>Himanshu Gupta, Hirdesh Varshney...Om Prakash Verma</div><div><b>Pages:&nbsp;</b>3073 - 3087</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00465-z\">A weighted least squares optimisation strategy for medical image super resolution via multiscale convolutional neural networks for healthcare applications</a></div><div><b>Author(s):&nbsp;</b>Bhawna Goyal, Dawa Chyophel Lepcha...Shui-Hua Wang</div><div><b>Pages:&nbsp;</b>3089 - 3104</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00473-z\">A deep network designed for segmentation and classification of leukemia using fusion of the transfer learning models</a></div><div><b>Author(s):&nbsp;</b>Saba Saleem, Javeria Amin...Shui-Hua Wang</div><div><b>Pages:&nbsp;</b>3105 - 3120</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00474-y\">A federated approach for detecting the chest diseases using DenseNet for multi-label classification</a></div><div><b>Author(s):&nbsp;</b>K. V. Priya, J. Dinesh Peter</div><div><b>Pages:&nbsp;</b>3121 - 3129</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00469-9\">Evolutionary multiple instance boosting framework for weakly supervised learning</a></div><div><b>Author(s):&nbsp;</b>Kamanasish Bhattacharjee, Millie Pant...Shilpa Srivastava</div><div><b>Pages:&nbsp;</b>3131 - 3141</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00564-x\">Categorizing white blood cells by utilizing deep features of proposed 4B-AdditionNet-based CNN network with ant colony optimization</a></div><div><b>Author(s):&nbsp;</b>Asim Shahzad, Mudassar Raza...Ramesh Sunder Nayak</div><div><b>Pages:&nbsp;</b>3143 - 3159</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00563-y\">Brain tumor detection and classification using machine learning: a comprehensive survey</a></div><div><b>Author(s):&nbsp;</b>Javaria Amin, Muhammad Sharif...Ramesh Sundar Nayak</div><div><b>Pages:&nbsp;</b>3161 - 3183</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00771-0\">Person identification from arm\u2019s hair patterns using CT-twofold Siamese network in forensic psychiatric hospitals</a></div><div><b>Author(s):&nbsp;</b>Rohan Don Salins, T. S. Ashwin...Chaitra K. Mallikarjun</div><div><b>Pages:&nbsp;</b>3185 - 3197</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00673-1\">Effective and scalable legal judgment recommendation using pre-learned word embedding</a></div><div><b>Author(s):&nbsp;</b>Jenish Dhanani, Rupa Mehta, Dipti Rana</div><div><b>Pages:&nbsp;</b>3199 - 3213</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00674-0\">An iterative approach to unsupervised outlier detection using ensemble method and distance-based data filtering</a></div><div><b>Author(s):&nbsp;</b>Bodhan Chakraborty, Agneet Chaterjee...Ram Sarkar</div><div><b>Pages:&nbsp;</b>3215 - 3230</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00675-z\">A consensus building model in group decision making with non-reciprocal fuzzy preference relations</a></div><div><b>Author(s):&nbsp;</b>Fang Liu, Tong Liu, Ya-Ru Chen</div><div><b>Pages:&nbsp;</b>3231 - 3245</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00663-3\">Two-factor-based RSA key generation from fingerprint biometrics and password for secure communication</a></div><div><b>Author(s):&nbsp;</b>K. SureshRajarshi Pal, S. R. Balasundaram</div><div><b>Pages:&nbsp;</b>3247 - 3261</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00662-4\">A novel multi-objective bi-level programming problem under intuitionistic fuzzy environment and its application in production planning problem</a></div><div><b>Author(s):&nbsp;</b>V. P. Singh, Kirti Sharma...Ali Ebrahimnejad</div><div><b>Pages:&nbsp;</b>3263 - 3278</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00669-x\">Hesitant T-spherical Dombi fuzzy aggregation operators and their applications in multiple criteria group decision-making</a></div><div><b>Author(s):&nbsp;</b>Faruk Karaaslan, Abdulrasool Hasan Sultan Al-Husseinawi</div><div><b>Pages:&nbsp;</b>3279 - 3297</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00670-4\">Cooperative multi-population Harris Hawks optimization for many-objective optimization</a></div><div><b>Author(s):&nbsp;</b>Na Yang, Zhenzhou Tang...Qian Hu</div><div><b>Pages:&nbsp;</b>3299 - 3332</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00676-y\">Software defect prediction based on nested-stacking and heterogeneous feature selection</a></div><div><b>Author(s):&nbsp;</b>Li-qiong Chen, Can Wang, Shi-long Song</div><div><b>Pages:&nbsp;</b>3333 - 3348</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00678-w\">Product selection based on sentiment analysis of online reviews: an intuitionistic fuzzy TODIM method</a></div><div><b>Author(s):&nbsp;</b>Zhenyu Zhang, Jian Guo...Mengjiao Wang</div><div><b>Pages:&nbsp;</b>3349 - 3362</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00680-2\">Motion magnification multi-feature relation network for facial microexpression recognition</a></div><div><b>Author(s):&nbsp;</b>Jing Zhang, Boyun Yan...Yong Liu</div><div><b>Pages:&nbsp;</b>3363 - 3376</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00682-0\">A novel feature based algorithm for soil type classification</a></div><div><b>Author(s):&nbsp;</b>Machbah Uddin, Md. Rakib Hassan</div><div><b>Pages:&nbsp;</b>3377 - 3393</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00671-3\">Rethinking ResNets: improved stacking strategies with high-order schemes for image classification</a></div><div><b>Author(s):&nbsp;</b>Zhengbo Luo, Zitang Sun...Sei-ichiro Kamata</div><div><b>Pages:&nbsp;</b>3395 - 3407</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00684-y\">Stability analysis based parameter tuning of Social Group Optimization</a></div><div><b>Author(s):&nbsp;</b>Junali Jasmine Jena, Samarendra Chandan Bindu Dash, Suresh Chandra Satapathy</div><div><b>Pages:&nbsp;</b>3409 - 3435</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00677-x\">Control in the loop for synchronization of nonlinear chaotic systems via adaptive intuitionistic neuro-fuzzy: a comparative study</a></div><div><b>Author(s):&nbsp;</b>Salah Helmy, Mohamed Magdy, Mohamed Hamdy</div><div><b>Pages:&nbsp;</b>3437 - 3450</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00679-9\">Eigenvalue-based entropy and spectrum of bipartite digraph</a></div><div><b>Author(s):&nbsp;</b>Yan Sun, Haixing Zhao</div><div><b>Pages:&nbsp;</b>3451 - 3462</div><div><br /></div><div><b>51)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00685-x\">The development trend of China\u2019s aging population: a forecast perspective</a></div><div><b>Author(s):&nbsp;</b>Xuchong Liu, Jianian Zhu, Kai Zou</div><div><b>Pages:&nbsp;</b>3463 - 3478</div><div><br /></div><div><b>52)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00689-7\">Based on neutrosophic fuzzy environment: a new development of FWZIC and FDOSM for benchmarking smart e-tourism applications</a></div><div><b>Author(s):&nbsp;</b>A. H. Alamoodi, R. T. Mohammed...Ali Najm Jasim</div><div><b>Pages:&nbsp;</b>3479 - 3503</div><div><br /></div><div><b>53)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00668-y\">1-Norm random vector functional link networks for classification problems</a></div><div><b>Author(s):&nbsp;</b>Barenya Bikash Hazarika, Deepak Gupta</div><div><b>Pages:&nbsp;</b>3505 - 3521</div><div><br /></div><div><b>54)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00688-8\">Power Muirhead mean in spherical normal fuzzy environment and its applications to multi-attribute decision-making</a></div><div><b>Author(s):&nbsp;</b>Tansu Temel, Salih Berkan Aydemir, Ya\u015far Ho\u015fcan</div><div><b>Pages:&nbsp;</b>3523 - 3541</div><div><br /></div><div><b>55)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00694-w\">Nerve optic segmentation in CT images using a deep learning model and a texture descriptor</a></div><div><b>Author(s):&nbsp;</b>Ramin Ranjbarzadeh, Shadi Dorosti...Malika Bendechache</div><div><b>Pages:&nbsp;</b>3543 - 3557</div><div><br /></div><div><b>56)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00667-z\">Cyber-physical security for IoT networks: a comprehensive review on traditional, blockchain and artificial intelligence based key-security</a></div><div><b>Author(s):&nbsp;</b>Ankit Attkan, Virender Ranga</div><div><b>Pages:&nbsp;</b>3559 - 3591</div><div><br /></div><div><b>57)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00801-x\">Correction to: Evolutionary optimization of large complex problems</a></div><div><b>Author(s):&nbsp;</b>Handing Wang, Chaoli Sun...Yew-soon Ong</div><div><b>Pages:&nbsp;</b>3593 - 3593</div><div><br /></div>",
            "pubdate": "2022-08-09T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                8,
                9
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 17, September 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/08/soft-computing-volume-26-issue-17.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07201-w\">Copper price movement prediction using recurrent neural networks and ensemble averaging</a></div><div><b>Author(s): </b>Jian Ni, Yue Xu...Jun Zhao</div><div><b>Pages: </b>8145 - 8161</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07234-1\">Improving the classification accuracy of melanoma detection by performing feature selection using binary Harris hawks optimization algorithm</a></div><div><b>Author(s):&nbsp;</b>Priti Bansal, Abhishek Vanjani...Sumit Kumar</div><div><b>Pages:&nbsp;</b>8163 - 8181</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07232-3\">Chaotic implanted opposition-based-quantum equipoise state and Ascidiacea algorithms for loss lessening and power permanence enrichment</a></div><div><b>Author(s):&nbsp;</b>Lenin Kanagasabai</div><div><b>Pages:&nbsp;</b>8183 - 8202</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07236-z\">Dominating set-based test prioritization algorithms for regression testing</a></div><div><b>Author(s):&nbsp;</b>Zafer Can Demir, \u015eahin Emrah Amrahov</div><div><b>Pages:&nbsp;</b>8203 - 8220</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07238-x\">(f, g)-derivation in residuated multilattices</a></div><div><b>Author(s):&nbsp;</b>Darline Laure Keubeng Yemene, Luc Emery Diekouam Fotso, Celestin Lele</div><div><b>Pages:&nbsp;</b>8221 - 8228</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07241-2\">RO-implications induced from CL-overlap functions on complete lattices</a></div><div><b>Author(s):&nbsp;</b>Junsheng Qiao</div><div><b>Pages:&nbsp;</b>8229 - 8243</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07256-9\">Partial orders induced by the smallest and greatest nullnorms on bounded lattices</a></div><div><b>Author(s):&nbsp;</b>Zhi-qiang Liu, Xue-ping Wang</div><div><b>Pages:&nbsp;</b>8245 - 8252</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07261-y\">A deep learning approaches and fastai text classification to predict 25 medical diseases from medical speech utterances, transcription and intent</a></div><div><b>Author(s):&nbsp;</b>Yogesh Kumar, Apeksha Koul, Seema Mahajan</div><div><b>Pages:&nbsp;</b>8253 - 8272</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07270-x\">Research on Chinese ancient characters image recognition method based on adaptive receptive field</a></div><div><b>Author(s):&nbsp;</b>Yalin Miao, Li Liang...Guodong Li</div><div><b>Pages:&nbsp;</b>8273 - 8282</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07284-5\">Some similarity measures of generalized trapezoidal cubic numbers with applications</a></div><div><b>Author(s):&nbsp;</b>Mohammed A. Al Shumrani, Muhammad Gulistan</div><div><b>Pages:&nbsp;</b>8283 - 8297</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07212-7\">Sliding window convergence in intuitionistic fuzzy normed spaces for measurable functions</a></div><div><b>Author(s):&nbsp;</b>Rabia Sava\u015f</div><div><b>Pages:&nbsp;</b>8299 - 8306</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07218-1\">Prediction of the lattice constants of pyrochlore compounds using machine learning</a></div><div><b>Author(s):&nbsp;</b>Ibrahim Olanrewaju Alade, Mojeed Opeyemi Oyedeji...Tawfik A. Saleh</div><div><b>Pages:&nbsp;</b>8307 - 8315</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07235-0\">Joint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation</a></div><div><b>Author(s):&nbsp;</b>Chuanbo Qin, Yujie Wu...Xiaozhi Zhang</div><div><b>Pages:&nbsp;</b>8317 - 8334</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07258-7\">Topological IL-algebras</a></div><div><b>Author(s):&nbsp;</b>Safiqul Islam, Arundhati Sanyal, Jayanta Sen</div><div><b>Pages:&nbsp;</b>8335 - 8349</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07271-w\">Pierce sheaves of pseudo EMV-algebras</a></div><div><b>Author(s):&nbsp;</b>Anatolij Dvure\u010denskij, Omid Zahiri</div><div><b>Pages:&nbsp;</b>8351 - 8369</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07207-4\">Finite-time dissipative synchronization of discrete-time semi-Markovian jump complex dynamical networks with actuator faults</a></div><div><b>Author(s):&nbsp;</b>N. Sakthivel, S. Pallavi...V. Vijayakumar</div><div><b>Pages:&nbsp;</b>8371 - 8386</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07208-3\">A multi-criteria group decision-making approach based on revised distance measures under dual hesitant fuzzy setting with unknown weight information</a></div><div><b>Author(s):&nbsp;</b>Jawad Ali, Zia Bashir, Tabasam Rashid</div><div><b>Pages:&nbsp;</b>8387 - 8401</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07221-6\">On the residuation principle of n-dimensional R-implications</a></div><div><b>Author(s):&nbsp;</b>Rosana Zanotelli, Bruno Moura...Benjamin Bedregal</div><div><b>Pages:&nbsp;</b>8403 - 8426</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07229-y\">Improvement of cross-efficiency based on TODIM method</a></div><div><b>Author(s):&nbsp;</b>Meiqin Wu, Xiaoqing Hou, Jianping Fan</div><div><b>Pages:&nbsp;</b>8427 - 8439</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07237-y\">Supervisory adaptive fuzzy sliding mode control with optimal Jaya based fuzzy PID sliding surface for a planer cable robot</a></div><div><b>Author(s):&nbsp;</b>Mohammadhossein Aghaseyedabdollah, Mostafa Abedi, Mahdi Pourgholi</div><div><b>Pages:&nbsp;</b>8441 - 8458</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07239-w\">Multi-attribute group decision-making method based on weighted partitioned Maclaurin symmetric mean operator and a novel score function under neutrosophic cubic environment</a></div><div><b>Author(s):&nbsp;</b>Jianping Fan, Shanshan Zha, Meiqin Wu</div><div><b>Pages:&nbsp;</b>8459 - 8477</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07242-1\">Mehar approach to solve neutrosophic linear programming problems using possibilistic mean</a></div><div><b>Author(s):&nbsp;</b>Tanveen Kaur Bhatia, Amit Kumar...S. S. Appadoo</div><div><b>Pages:&nbsp;</b>8479 - 8495</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07252-z\">An improved solution for the neutrosophic linear programming problems based on Mellin\u2019s transform</a></div><div><b>Author(s):&nbsp;</b>G. Tamilarasi, S. Paulraj</div><div><b>Pages:&nbsp;</b>8497 - 8507</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07151-3\">Low-voltage distribution network topology identification based on constrained least square and graph theory</a></div><div><b>Author(s):&nbsp;</b>Shijie Cui, Peng Zeng...Guangye Li</div><div><b>Pages:&nbsp;</b>8509 - 8519</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07251-0\">Evaluating appropriate communication technology for smart grid by using a comprehensive decision-making approach fuzzy TOPSIS</a></div><div><b>Author(s):&nbsp;</b>Daud Abdul, Jiang Wenqi</div><div><b>Pages:&nbsp;</b>8521 - 8536</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07276-5\">A CEEMD-ARIMA-SVM model with structural breaks to forecast the crude oil prices linked with extreme events</a></div><div><b>Author(s):&nbsp;</b>Yuxiang Cheng, Jiayu Yi...Luis Seco</div><div><b>Pages:&nbsp;</b>8537 - 8551</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07161-1\">Deep learning for volatility forecasting in asset management</a></div><div><b>Author(s):&nbsp;</b>Alessio Petrozziello, Luigi Troiano...Michele La Rocca</div><div><b>Pages:&nbsp;</b>8553 - 8574</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07195-5\">Relationship classification based on dependency parsing and the pretraining model</a></div><div><b>Author(s):&nbsp;</b>Baosheng Yin, Yifei Sun</div><div><b>Pages:&nbsp;</b>8575 - 8583</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07202-9\">A cooperative genetic algorithm based on extreme learning machine for data classification</a></div><div><b>Author(s):&nbsp;</b>Lixia Bai, Hong Li...Jin Xie</div><div><b>Pages:&nbsp;</b>8585 - 8601</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07226-1\">An intermittent fault diagnosis method of analog circuits based on variational modal decomposition and adaptive dynamic density peak clustering</a></div><div><b>Author(s):&nbsp;</b>Jianfeng Qu, Xiaoyu Fang...Jinzhuo Liu</div><div><b>Pages:&nbsp;</b>8603 - 8615</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07230-5\">Residual stacked gated recurrent unit with encoder\u2013decoder architecture and an attention mechanism for temporal traffic prediction</a></div><div><b>Author(s):&nbsp;</b>R. J. Kuo, D. A. Kunarsito</div><div><b>Pages:&nbsp;</b>8617 - 8633</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07244-z\">Code recommendation based on joint embedded attention network</a></div><div><b>Author(s):&nbsp;</b>Wanzhi Wen, Tian Zhao...Deepak Kumar Jain</div><div><b>Pages:&nbsp;</b>8635 - 8645</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07263-w\">Dark convolutional neural network for forest smoke detection and localization based on single image</a></div><div><b>Author(s):&nbsp;</b>Na Lu</div><div><b>Pages:&nbsp;</b>8647 - 8659</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07152-2\">Simulation\u2013optimization approach for the multi-objective production and distribution planning problem in the supply chain: using NSGA-II and Monte Carlo simulation</a></div><div><b>Author(s):&nbsp;</b>Niloofar Nadim Kabiri, Saeed Emami, Abdul Sattar Safaei</div><div><b>Pages:&nbsp;</b>8661 - 8687</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07154-0\">A method of real-temporal object tracking combined the temporal information and spatial information</a></div><div><b>Author(s):&nbsp;</b>Xiaoshuo Jia, Zhihui Li...Shangyou Zeng</div><div><b>Pages:&nbsp;</b>8689 - 8698</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07171-z\">Optimal solution of neutrosophic linear fractional programming problems with mixed constraints</a></div><div><b>Author(s):&nbsp;</b>Sapan Kumar Das, S. A. Edalatpanah</div><div><b>Pages:&nbsp;</b>8699 - 8707</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07182-w\">Software module clustering using grid-based large-scale many-objective particle swarm optimization</a></div><div><b>Author(s):&nbsp;</b>Amarjeet Prajapati</div><div><b>Pages:&nbsp;</b>8709 - 8730</div><div><br /></div><div><b>38) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07196-4\">Process optimization for post disaster reconstruction project based on industrial design structure matrix (DSM)</a></div><div><b>Author(s):&nbsp;</b>Hui Tang, Qingping Zhong, Chuan Chen</div><div><b>Pages:&nbsp;</b>8731 - 8743</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07198-2\">A dynamic space reduction ant colony optimization for capacitated vehicle routing problem</a></div><div><b>Author(s):&nbsp;</b>Jinsi Cai, Peng Wang...Huachao Dong</div><div><b>Pages:&nbsp;</b>8745 - 8756</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07203-8\">Fault location in distribution network by solving the optimization problem using genetic algorithm based on the calculating voltage changes</a></div><div><b>Author(s):&nbsp;</b>Masoud Dashtdar, Mohit Bajaj...H\u00e1m\u00e9d M\u00e9rsh\u00eak\u00e1\u00e9r</div><div><b>Pages:&nbsp;</b>8757 - 8783</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07227-0\">A Scatter Search Algorithm for Multi-Criteria Inventory Classification considering Multi-Objective Optimization</a></div><div><b>Author(s):&nbsp;</b>Ilkay Saracoglu</div><div><b>Pages:&nbsp;</b>8785 - 8806</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07153-1\">The research of innovation path of power monitoring and dispatching under the vision of carbon neutrality based on mobile edge computing</a></div><div><b>Author(s):&nbsp;</b>Jingxuan Dong, Jian Li</div><div><b>Pages:&nbsp;</b>8807 - 8820</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07192-8\">Sustainable supplier selection using HF-DEA-FOCUM-MABAC technique: a case study in the Auto-making industry</a></div><div><b>Author(s):&nbsp;</b>Arunodaya Raj Mishra, Abhijit Saha...Ibrahim M. Hezam</div><div><b>Pages:&nbsp;</b>8821 - 8840</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07225-2\">A note on \u201cPythagorean uncertain linguistic hesitant fuzzy weighted averaging operator and its application in financial group decision making\u201d</a></div><div>S. S. Appadoo, Mohammadreza Makhan, Amit Kumar</div><div><b>Pages:&nbsp;</b>8841 - 8843</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06652-x\">SECOI: an application based on fuzzy soft sets for producing selective-colored images</a></div><div><b>Author(s):&nbsp;</b>Petr Hurtik, Ji\u0159\u00ed Mo\u010dko\u0159</div><div><b>Pages:&nbsp;</b>8845 - 8855</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06661-w\">A novel model based on multiple input factors and variance reciprocal: application on wind speed forecasting</a></div><div><b>Author(s):&nbsp;</b>Zhihao Shang, Min Li...Lian Li</div><div><b>Pages:&nbsp;</b>8857 - 8877</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06692-3\">A variable neighborhood search algorithm with constraint relaxation for the two-echelon vehicle routing problem with simultaneous delivery and pickup demands</a></div><div><b>Author(s):&nbsp;</b>Ran Liu, Shan Jiang</div><div><b>Pages:&nbsp;</b>8879 - 8896</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07296-1\">Reservoir water level forecasting using wavelet support vector regression (WSVR) based on teaching learning-based optimization algorithm (TLBO)</a></div><div><b>Author(s):&nbsp;</b>Mohammad Mahdi Malekpour, Hossein Malekpoor</div><div><b>Pages:&nbsp;</b>8897 - 8909</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06697-y\">Multi-objective casting production scheduling problem by a neighborhood structure enhanced discrete NSGA-II: an application from real-world workshop</a></div><div><b>Author(s):&nbsp;</b>Weihua Tan, Xiaofang Yuan...Lianghong Wu</div><div><b>Pages:&nbsp;</b>8911 - 8928</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06701-5\">Weighted differential evolution-based heuristic computing for identification of Hammerstein systems in electrically stimulated muscle modeling</a></div><div><b>Author(s):&nbsp;</b>Ammara Mehmood, Muhammad Asif Zahoor Raja...Naveed Ishtiaq Chaudhary</div><div><b>Pages:&nbsp;</b>8929 - 8945</div><div><br /></div><div><b>51)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07374-4\">Correction: G-optimal designs for hierarchical linear models: an equivalence theorem and a nature-inspired meta-heuristic algorithm</a></div><div><b>Author(s):&nbsp;</b>Xin Liu, RongXian Yue...Weng Kee Wong</div><div><b>Pages:&nbsp;</b>8947 - 8947</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-08-18T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "IEEE Transactions on Neural Networks and Learning Systems, Volume 33, Issue 8, August 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/08/ieee-transactions-on-neural-networks.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9345705/\">New Generation Deep Learning for Video Object Detection: A Survey</a></div><div><b>Author(s): </b>Licheng Jiao, Ruohan Zhang, Fang Liu, Shuyuan Yang, Biao Hou, Lingling Li, Xu Tang</div><div><b>Pages: </b>3195 - 3215</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9334418/\">Synchronization of Complex Dynamical Networks Subject to Noisy Sampling Interval and Packet Loss</a></div><div><b>Author(s):&nbsp;</b>Zhipei Hu, Hongru Ren, Peng Shi</div><div><b>Pages:&nbsp;</b>3216 - 3226</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9334446/\">Boundary Stabilization of Stochastic Delayed Cohen\u2013Grossberg Neural Networks With Diffusion Terms</a></div><div><b>Author(s):&nbsp;</b>Xiao-Zhen Liu, Kai-Ning Wu, Xiaohua Ding, Weihai Zhang</div><div><b>Pages:&nbsp;</b>3227 - 3237</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9337191/\">Neuron Linear Transformation: Modeling the Domain Shift for Crowd Counting</a></div><div><b>Author(s):&nbsp;</b>Qi Wang, Tao Han, Junyu Gao, Yuan Yuan</div><div><b>Pages:&nbsp;</b>3238 - 3250</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9340584/\">A Hybrid System Based on Dynamic Selection for Time Series Forecasting</a></div><div><b>Author(s):&nbsp;</b>Jo\u00e3o F. L. de Oliveira, Eraylson G. Silva, Paulo S. G. de Mattos Neto</div><div><b>Pages:&nbsp;</b>3251 - 3263</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9336312/\">Encoder-X: Solving Unknown Coefficients Automatically in Polynomial Fitting by Using an Autoencoder</a></div><div><b>Author(s):&nbsp;</b>Guojun Wang, Weijun Li, Liping Zhang, Linjun Sun, Peng Chen, Lina Yu, Xin Ning</div><div><b>Pages:&nbsp;</b>3264 - 3276</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9377649/\">DAMAD: Database, Attack, and Model Agnostic Adversarial Perturbation Detector</a></div><div><b>Author(s):&nbsp;</b>Akshay Agarwal, Gaurav Goswami, Mayank Vatsa, Richa Singh, Nalini K. Ratha</div><div><b>Pages:&nbsp;</b>3277 - 3289</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9336296/\">DQC-ADMM: Decentralized Dynamic ADMM With Quantized and Censored Communications</a></div><div><b>Author(s):&nbsp;</b>Yaohua Liu, Gang Wu, Zhi Tian, Qing Ling</div><div><b>Pages:&nbsp;</b>3290 - 3304</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9340243/\">Generalized Nonconvex Approach for Low-Tubal-Rank Tensor Recovery</a></div><div><b>Author(s):&nbsp;</b>Hailin Wang, Feng Zhang, Jianjun Wang, Tingwen Huang, Jianwen Huang, Xinling Liu</div><div><b>Pages:&nbsp;</b>3305 - 3319</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9339998/\">Deep Attention-Based Imbalanced Image Classification</a></div><div><b>Author(s):&nbsp;</b>Lituan Wang, Lei Zhang, Xiaofeng Qi, Zhang Yi</div><div><b>Pages:&nbsp;</b>3320 - 3330</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9337205/\">Adaptive Neural Network Control for Full-State Constrained Robotic Manipulator With Actuator Saturation and Time-Varying Delays</a></div><div><b>Author(s):&nbsp;</b>Weiwei Sun, You Wu, Xinyu Lv</div><div><b>Pages:&nbsp;</b>3331 - 3342</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9344656/\">Imbalanced Data Classification via Cooperative Interaction Between Classifier and Generator</a></div><div><b>Author(s):&nbsp;</b>Hyun-Soo Choi, Dahuin Jung, Siwon Kim, Sungroh Yoon</div><div><b>Pages:&nbsp;</b>3343 - 3356</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9345987/\">Parkinson\u2019s Disease Classification and Clinical Score Regression via United Embedding and Sparse Learning From Longitudinal Data</a></div><div><b>Author(s):&nbsp;</b>Zhongwei Huang, Haijun Lei, Guoliang Chen, Alejandro F. Frangi, Yanwu Xu, Ahmed Elazab, Jing Qin, Baiying Lei</div><div><b>Pages:&nbsp;</b>3357 - 3371</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9349201/\">Element-Wise Feature Relation Learning Network for Cross-Spectral Image Patch Matching</a></div><div><b>Author(s):&nbsp;</b>Dou Quan, Shuang Wang, Ning Huyan, Jocelyn Chanussot, Ruojing Wang, Xuefeng Liang, Biao Hou, Licheng Jiao</div><div><b>Pages:&nbsp;</b>3372 - 3386</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9344655/\">Temporal Encoding and Multispike Learning Framework for Efficient Recognition of Visual Patterns</a></div><div><b>Author(s):&nbsp;</b>Qiang Yu, Shiming Song, Chenxiang Ma, Jianguo Wei, Shengyong Chen, Kay Chen Tan</div><div><b>Pages:&nbsp;</b>3387 - 3399</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9343714/\">Detachable Second-Order Pooling: Toward High-Performance First-Order Networks</a></div><div><b>Author(s):&nbsp;</b>Lida Li, Jiangtao Xie, Peihua Li, Lei Zhang</div><div><b>Pages:&nbsp;</b>3400 - 3414</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9340597/\">Solving Complex-Valued Time-Varying Linear Matrix Equations via QR Decomposition With Applications to Robotic Motion Tracking and on Angle-of-Arrival Localization</a></div><div><b>Author(s):&nbsp;</b>Vasilios N. Katsikis, Spyridon D. Mourtas, Predrag S. Stanimirovi\u0107, Yunong Zhang</div><div><b>Pages:&nbsp;</b>3415 - 3424</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9340611/\">Underexposed Image Correction via Hybrid Priors Navigated Deep Propagation</a></div><div><b>Author(s):&nbsp;</b>Risheng Liu, Long Ma, Yuxi Zhang, Xin Fan, Zhongxuan Luo</div><div><b>Pages:&nbsp;</b>3425 - 3436</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9352532/\">Theory-Inspired Deep Network for Instantaneous-Frequency Extraction and Subsignals Recovery From Discrete Blind-Source Data</a></div><div><b>Author(s):&nbsp;</b>Ningning Han, H. N. Mhaskar, Charles K. Chui</div><div><b>Pages:&nbsp;</b>3437 - 3447</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9343776/\">GenDet: Meta Learning to Generate Detectors From Few Shots</a></div><div><b>Author(s):&nbsp;</b>Liyang Liu, Bochao Wang, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin Liao, Wayne Zhang</div><div><b>Pages:&nbsp;</b>3448 - 3460</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9344864/\">Distributed Group Coordination of Multiagent Systems in Cloud Computing Systems Using a Model-Free Adaptive Predictive Control Strategy</a></div><div><b>Author(s):&nbsp;</b>Haoran Tan, Yaonan Wang, Min Wu, Zhiwu Huang, Zhiqiang Miao</div><div><b>Pages:&nbsp;</b>3461 - 3473</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9343685/\">Adaptive NN-Based Consensus for a Class of Nonlinear Multiagent Systems With Actuator Faults and Faulty Networks</a></div><div><b>Author(s):&nbsp;</b>Xiaozheng Jin, Shaoyu L\u00fc, Jiguo Yu</div><div><b>Pages:&nbsp;</b>3474 - 3486</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9350111/\">Data-Driven Adaptive Consensus Learning From Network Topologies</a></div><div><b>Author(s):&nbsp;</b>Ronghu Chi, Yu Hui, Biao Huang, Zhongsheng Hou, Xuhui Bu</div><div><b>Pages:&nbsp;</b>3487 - 3497</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9344821/\">Adversarial Learning of Disentangled and Generalizable Representations of Visual Attributes</a></div><div><b>Author(s):&nbsp;</b>James Oldfield, Yannis Panagakis, Mihalis A. Nicolaou</div><div><b>Pages:&nbsp;</b>3498 - 3509</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9350110/\">Comparative Convolutional Dynamic Multi-Attention Recommendation Model</a></div><div><b>Author(s):&nbsp;</b>Juan Ni, Zhenhua Huang, Chang Yu, Dongdong Lv, Cheng Wang</div><div><b>Pages:&nbsp;</b>3510 - 3521</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9347827/\">Link Prediction Based on Stochastic Information Diffusion</a></div><div><b>Author(s):&nbsp;</b>Didier A. Vega-Oliveros, Liang Zhao, Anderson Rocha, Lilian Berton</div><div><b>Pages:&nbsp;</b>3522 - 3532</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9346019/\">A Convex Model for Support Vector Distance Metric Learning</a></div><div><b>Author(s):&nbsp;</b>Yibang Ruan, Yanshan Xiao, Zhifeng Hao, Bo Liu</div><div><b>Pages:&nbsp;</b>3533 - 3546</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9350205/\">RGB-D Point Cloud Registration Based on Salient Object Detection</a></div><div><b>Author(s):&nbsp;</b>Teng Wan, Shaoyi Du, Wenting Cui, Runzhao Yao, Yuyan Ge, Ce Li, Yue Gao, Nanning Zheng</div><div><b>Pages:&nbsp;</b>3547 - 3559</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9345932/\">Hierarchical-Bayesian-Based Sparse Stochastic Configuration Networks for Construction of Prediction Intervals</a></div><div><b>Author(s):&nbsp;</b>Jun Lu, Jinliang Ding, Changxin Liu, Tianyou Chai</div><div><b>Pages:&nbsp;</b>3560 - 3571</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9346050/\">Abnormal Event Detection and Localization via Adversarial Event Prediction</a></div><div><b>Author(s):&nbsp;</b>Jongmin Yu, Younkwan Lee, Kin Choong Yow, Moongu Jeon, Witold Pedrycz</div><div><b>Pages:&nbsp;</b>3572 - 3586</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9349966/\">Improving EEG Decoding via Clustering-Based Multitask Feature Learning</a></div><div><b>Author(s):&nbsp;</b>Yu Zhang, Tao Zhou, Wei Wu, Hua Xie, Hongru Zhu, Guoxu Zhou, Andrzej Cichocki</div><div><b>Pages:&nbsp;</b>3587 - 3597</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9350115/\">Neighborhood Geometric Structure-Preserving Variational Autoencoder for Smooth and Bounded Data Sources</a></div><div><b>Author(s):&nbsp;</b>Xingyu Chen, Chunyu Wang, Xuguang Lan, Nanning Zheng, Wenjun Zeng</div><div><b>Pages:&nbsp;</b>3598 - 3611</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9352534/\">Optimizing Attention for Sequence Modeling via Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Hao Fei, Yue Zhang, Yafeng Ren, Donghong Ji</div><div><b>Pages:&nbsp;</b>3612 - 3621</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9349207/\">Periodic Event-Triggered Synchronization for Discrete-Time Complex Dynamical Networks</a></div><div><b>Author(s):&nbsp;</b>Sanbo Ding, Zhanshan Wang, Xiangpeng Xie</div><div><b>Pages:&nbsp;</b>3622 - 3633</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9350196/\">Fast Unsupervised Projection for Large-Scale Data</a></div><div><b>Author(s):&nbsp;</b>Jingyu Wang, Lin Wang, Feiping Nie, Xuelong Li</div><div><b>Pages:&nbsp;</b>3634 - 3644</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9386270/\">Relaxed Block-Diagonal Dictionary Pair Learning With Locality Constraint for Image Recognition</a></div><div><b>Author(s):&nbsp;</b>Zhe Chen, Xiao-Jun Wu, Josef Kittler</div><div><b>Pages:&nbsp;</b>3645 - 3659</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9351670/\">Multiperspective Progressive Structure Adaptation for JPEG Steganography Detection Across Domains</a></div><div><b>Author(s):&nbsp;</b>Ju Jia, Meng Luo, Jinshuo Liu, Weixiang Ren, Lina Wang</div><div><b>Pages:&nbsp;</b>3660 - 3674</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9364887/\">Optimal Scale Combination Selection Integrating Three-Way Decision With Hasse Diagram</a></div><div><b>Author(s):&nbsp;</b>Qinghua Zhang, Yunlong Cheng, Fan Zhao, Guoyin Wang, Shuyin Xia</div><div><b>Pages:&nbsp;</b>3675 - 3689</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9350116/\">Networked Multiagent Systems: Antagonistic Interaction, Constraint, and its Application</a></div><div><b>Author(s):&nbsp;</b>Wentao Zhang, Zhiqiang Zuo, Yijing Wang</div><div><b>Pages:&nbsp;</b>3690 - 3699</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9350109/\">Dynamic Learning From Adaptive Neural Control for Discrete-Time Strict-Feedback Systems</a></div><div><b>Author(s):&nbsp;</b>Min Wang, Haotian Shi, Cong Wang, Jun Fu</div><div><b>Pages:&nbsp;</b>3700 - 3712</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9349163/\">Transductive Semisupervised Deep Hashing</a></div><div><b>Author(s):&nbsp;</b>Weiwei Shi, Yihong Gong, Badong Chen, Xinhong Hei</div><div><b>Pages:&nbsp;</b>3713 - 3726</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9349967/\">Surrogate-Assisted Particle Swarm Optimization for Evolving Variable-Length Transferable Blocks for Image Classification</a></div><div><b>Author(s):&nbsp;</b>Bin Wang, Bing Xue, Mengjie Zhang</div><div><b>Pages:&nbsp;</b>3727 - 3740</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9351698/\">Target Tracking Control of a Biomimetic Underwater Vehicle Through Deep Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Yu Wang, Chong Tang, Shuo Wang, Long Cheng, Rui Wang, Min Tan, Zengguang Hou</div><div><b>Pages:&nbsp;</b>3741 - 3752</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9353398/\">Variational Inference and Learning of Piecewise Linear Dynamical Systems</a></div><div><b>Author(s):&nbsp;</b>Xavier Alameda-Pineda, Vincent Drouard, Radu Patrice Horaud</div><div><b>Pages:&nbsp;</b>3753 - 3764</div><div><br /></div><div><b>45)</b> <a href=\"https://ieeexplore.ieee.org/document/9352557/\">CRL: Collaborative Representation Learning by Coordinating Topic Modeling and Network Embeddings</a></div><div><b>Author(s):&nbsp;</b>Junyang Chen, Zhiguo Gong, Wei Wang, Weiwen Liu, Xiao Dong</div><div><b>Pages:&nbsp;</b>3765 - 3777</div><div><br /></div><div><b>46)</b> <a href=\"https://ieeexplore.ieee.org/document/9356334/\">Beneficial Perturbation Network for Designing General Adaptive Artificial Intelligence Systems</a></div><div><b>Author(s):&nbsp;</b>Shixian Wen, Amanda Rios, Yunhao Ge, Laurent Itti</div><div><b>Pages:&nbsp;</b>3778 - 3791</div><div><br /></div><div><b>47)</b> <a href=\"https://ieeexplore.ieee.org/document/9352543/\">A Plug-in Method for Representation Factorization in Connectionist Models</a></div><div><b>Author(s):&nbsp;</b>Jee Seok Yoon, Myung-Cheol Roh, Heung-Il Suk</div><div><b>Pages:&nbsp;</b>3792 - 3803</div><div><br /></div><div><b>48)</b> <a href=\"https://ieeexplore.ieee.org/document/9354049/\">Event-Based Design of Finite-Time Adaptive Control of Uncertain Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Yuan-Xin Li, Zhongsheng Hou, Wei-Wei Che, Zheng-Guang Wu</div><div><b>Pages:&nbsp;</b>3804 - 3813</div><div><br /></div><div><b>49)</b> <a href=\"https://ieeexplore.ieee.org/document/9352495/\">Motion Planning and Adaptive Neural Tracking Control of an Uncertain Two-Link Rigid\u2013Flexible Manipulator With Vibration Amplitude Constraint</a></div><div><b>Author(s):&nbsp;</b>Qingxin Meng, Xuzhi Lai, Ze Yan, Chun-Yi Su, Min Wu</div><div><b>Pages:&nbsp;</b>3814 - 3828</div><div><br /></div><div><b>50)</b> <a href=\"https://ieeexplore.ieee.org/document/9349196/\">Sampled-Data Synchronization of Stochastic Markovian Jump Neural Networks With Time-Varying Delay</a></div><div><b>Author(s):&nbsp;</b>Guoliang Chen, Jianwei Xia, Ju H. Park, Hao Shen, Guangming Zhuang</div><div><b>Pages:&nbsp;</b>3829 - 3841</div><div><br /></div><div><b>51)</b> <a href=\"https://ieeexplore.ieee.org/document/9350193/\">A Novel Sparse Graph-Regularized Singular Value Decomposition Model and Its Application to Genomic Data Analysis</a></div><div><b>Author(s):&nbsp;</b>Wenwen Min, Xiang Wan, Tsung-Hui Chang, Shihua Zhang</div><div><b>Pages:&nbsp;</b>3842 - 3856</div><div><br /></div><div><b>52)</b> <a href=\"https://ieeexplore.ieee.org/document/9352556/\">Concept Drift-Tolerant Transfer Learning in Dynamic Environments</a></div><div><b>Author(s):&nbsp;</b>Cuie Yang, Yiu-Ming Cheung, Jinliang Ding, Kay Chen Tan</div><div><b>Pages:&nbsp;</b>3857 - 3871</div><div><br /></div><div><b>53)</b> <a href=\"https://ieeexplore.ieee.org/document/9354489/\">Data-Based Optimal Consensus Control for Multiagent Systems With Policy Gradient Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Xindi Yang, Hao Zhang, Zhuping Wang</div><div><b>Pages:&nbsp;</b>3872 - 3883</div><div><br /></div><div><b>54)</b> <a href=\"https://ieeexplore.ieee.org/document/9354502/\">Directional Deep Embedding and Appearance Learning for Fast Video Object Segmentation</a></div><div><b>Author(s):&nbsp;</b>Yingjie Yin, De Xu, Xingang Wang, Lei Zhang</div><div><b>Pages:&nbsp;</b>3884 - 3894</div><div><br /></div><div><b>55) </b><a href=\"https://ieeexplore.ieee.org/document/9358980/\">Multi-Manifold Optimization for Multi-View Subspace Clustering</a></div><div><b>Author(s):&nbsp;</b>Aparajita Khan, Pradipta Maji</div><div><b>Pages:&nbsp;</b>3895 - 3907</div><div><br /></div><div><b>56)</b> <a href=\"https://ieeexplore.ieee.org/document/9352535/\">Remote State Estimation of Nonlinear Systems Over Fading Channels via Recurrent Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Songfu Cai, Vincent K. N. Lau</div><div><b>Pages:&nbsp;</b>3908 - 3922</div><div><br /></div><div><b>57)</b> <a href=\"https://ieeexplore.ieee.org/document/9352501/\">Convolutional Neural Network for Behavioral Modeling and Predistortion of Wideband Power Amplifiers</a></div><div><b>Author(s):&nbsp;</b>Xin Hu, Zhijun Liu, Xiaofei Yu, Yulong Zhao, Wenhua Chen, Biao Hu, Xuekun Du, Xiang Li, Mohamed Helaoui, Weidong Wang, Fadhel M. Ghannouchi</div><div><b>Pages:&nbsp;</b>3923 - 3937</div><div><br /></div><div><b>58)</b> <a href=\"https://ieeexplore.ieee.org/document/9352485/\">Finite-Time Synchronization of Complex-Valued Memristive-Based Neural Networks via Hybrid Control</a></div><div><b>Author(s):&nbsp;</b>Tianhu Yu, Jinde Cao, Leszek Rutkowski, Yi-Ping Luo</div><div><b>Pages:&nbsp;</b>3938 - 3947</div><div><br /></div><div><b>59)</b> <a href=\"https://ieeexplore.ieee.org/document/9360311/\">Margin Distribution Analysis</a></div><div><b>Author(s):&nbsp;</b>Jun Wang, Zhi-Hua Zhou</div><div><b>Pages:&nbsp;</b>3948 - 3960</div><div><br /></div><div><b>60)</b> <a href=\"https://ieeexplore.ieee.org/document/9359364/\">Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks</a></div><div><b>Author(s):&nbsp;</b>Zhifei Li, Hai Liu, Zhaoli Zhang, Tingting Liu, Neal N. Xiong</div><div><b>Pages:&nbsp;</b>3961 - 3973</div><div><br /></div><div><b>61)</b> <a href=\"https://ieeexplore.ieee.org/document/9354062/\">Toward Full-Stack Acceleration of Deep Convolutional Neural Networks on FPGAs</a></div><div><b>Author(s):&nbsp;</b>Shuanglong Liu, Hongxiang Fan, Martin Ferianc, Xinyu Niu, Huifeng Shi, Wayne Luk</div><div><b>Pages:&nbsp;</b>3974 - 3987</div><div><br /></div><div><b>62)</b> <a href=\"https://ieeexplore.ieee.org/document/9353400/\">Toward the Optimal Design and FPGA Implementation of Spiking Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Wenzhe Guo, Hasan Erdem Yant\u0131r, Mohammed E. Fouda, Ahmed M. Eltawil, Khaled Nabil Salama</div><div><b>Pages:&nbsp;</b>3988 - 4002</div><div><br /></div><div><b>63)</b> <a href=\"https://ieeexplore.ieee.org/document/9353402/\">Lifelong Incremental Reinforcement Learning With Online Bayesian Inference</a></div><div><b>Author(s):&nbsp;</b>Zhi Wang, Chunlin Chen, Daoyi Dong</div><div><b>Pages:&nbsp;</b>4003 - 4016</div><div><br /></div><div><b>64)</b> <a href=\"https://ieeexplore.ieee.org/document/9354503/\">Massive-Scale Aerial Photo Categorization by Cross-Resolution Visual Perception Enhancement</a></div><div><b>Author(s):&nbsp;</b>Luming Zhang, Xiaoqin Zhang, Mingliang Xu, Ling Shao</div><div><b>Pages:&nbsp;</b>4017 - 4030</div><div><br /></div><div><b>65)</b> <a href=\"https://ieeexplore.ieee.org/document/9353404/\">Global Negative Correlation Learning: A Unified Framework for Global Optimization of Ensemble Models</a></div><div><b>Author(s):&nbsp;</b>Carlos Perales-Gonz\u00e1lez, Francisco Fern\u00e1ndez-Navarro, Mariano Carbonero-Ruz, Javier P\u00e9rez-Rodr\u00edguez</div><div><b>Pages:&nbsp;</b>4031 - 4042</div><div><br /></div><div><b>66)</b> <a href=\"https://ieeexplore.ieee.org/document/9354488/\">Optimal Tracking Control of Nonlinear Multiagent Systems Using Internal Reinforce Q-Learning</a></div><div><b>Author(s):&nbsp;</b>Zhinan Peng, Rui Luo, Jiangping Hu, Kaibo Shi, Sing Kiong Nguang, Bijoy Kumar Ghosh</div><div><b>Pages:&nbsp;</b>4043 - 4055</div><div><br /></div><div><b>67)</b> <a href=\"https://ieeexplore.ieee.org/document/9369104/\">Multi-Task Weakly-Supervised Attention Network for Dementia Status Estimation With Structural MRI</a></div><div><b>Author(s):&nbsp;</b>Chunfeng Lian, Mingxia Liu, Li Wang, Dinggang Shen</div><div><b>Pages:&nbsp;</b>4056 - 4068</div><div><br /></div><div><b>68)</b> <a href=\"https://ieeexplore.ieee.org/document/9354493/\">FPGA-Based High-Throughput CNN Hardware Accelerator With High Computing Resource Utilization Ratio</a></div><div><b>Author(s):&nbsp;</b>Wenjin Huang, Huangtao Wu, Qingkun Chen, Conghui Luo, Shihao Zeng, Tianrui Li, Yihua Huang</div><div><b>Pages:&nbsp;</b>4069 - 4083</div><div><br /></div><div><b>69)</b> <a href=\"https://ieeexplore.ieee.org/document/9357939/\">Convolutional Ordinal Regression Forest for Image Ordinal Estimation</a></div><div><b>Author(s):&nbsp;</b>Haiping Zhu, Hongming Shan, Yuheng Zhang, Lingfu Che, Xiaoyang Xu, Junping Zhang, Jianbo Shi, Fei-Yue Wang</div><div><b>Pages:&nbsp;</b>4084 - 4095</div><div><br /></div><div><b>70)</b> <a href=\"https://ieeexplore.ieee.org/document/9353392/\">Spiking Neural Network Regularization With Fixed and Adaptive Drop-Keep Probabilities</a></div><div><b>Author(s):&nbsp;</b>Junhong Zhao, Jie Yang, Jun Wang, Wei Wu</div><div><b>Pages:&nbsp;</b>4096 - 4109</div><div><br /></div><div><b>71)</b> <a href=\"https://ieeexplore.ieee.org/document/9372892/\">Adversarial Binary Mutual Learning for Semi-Supervised Deep Hashing</a></div><div><b>Author(s):&nbsp;</b>Guan\u2019An Wang, Qinghao Hu, Yang Yang, Jian Cheng, Zeng-Guang Hou</div><div><b>Pages:&nbsp;</b>4110 - 4124</div><div><br /></div><div><b>72)</b> <a href=\"https://ieeexplore.ieee.org/document/9334410/\">Scalable Inverse Reinforcement Learning Through Multifidelity Bayesian Optimization</a></div><div><b>Author(s):&nbsp;</b>Mahdi Imani, Seyede Fatemeh Ghoreishi</div><div><b>Pages:&nbsp;</b>4125 - 4132</div><div><br /></div><div><b>73)</b> <a href=\"https://ieeexplore.ieee.org/document/9350113/\">Fixed-Time Synchronization of Competitive Neural Networks With Multiple Time Scales</a></div><div><b>Author(s):&nbsp;</b>Wu Yang, Yan-Wu Wang, Irinel-Constantin Mor\u01cerescu, Xiao-Kang Liu, Yuehua Huang</div><div><b>Pages:&nbsp;</b>4133 - 4138</div><div><br /></div><div><b>74)</b> <a href=\"https://ieeexplore.ieee.org/document/9345983/\">Online Reinforcement Learning Control by Direct Heuristic Dynamic Programming: From Time-Driven to Event-Driven</a></div><div><b>Author(s):&nbsp;</b>Qingtao Zhao, Jennie Si, Jian Sun</div><div><b>Pages:&nbsp;</b>4139 - 4144</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-08-19T12:00:00.235+12:00",
            "pubdate_parsed": [
                2022,
                8,
                19
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 18, September 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/09/soft-computing-volume-26-issue-18.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07292-5\">Specified keywords search scheme for EHR sharing</a></div><div><b>Author(s): </b>Shufen Niu, Fei Yu...Caifen Wang</div><div><b>Pages: </b>8949 - 8960</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07293-4\">Research on path planning algorithm of mobile robot based on reinforcement learning</a></div><div><b>Author(s):&nbsp;</b>Guoqian Pan, Yong Xiang...Xinzhi Zhou</div><div><b>Pages:&nbsp;</b>8961 - 8970</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07294-3\">Lattice-theoretic three-way formal contexts and their concepts</a></div><div><b>Author(s):&nbsp;</b>Ninghua Gao, Zixuan Cao...Haojie Jiang</div><div><b>Pages:&nbsp;</b>8971 - 8985</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07306-2\">Elitable GE-filters of bordered GE-algebras</a></div><div><b>Author(s):&nbsp;</b>Jeong-Gon Lee, Manzoor Kaleem Shaik...Young Bae Jun</div><div><b>Pages:&nbsp;</b>8987 - 8996</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07313-3\">Soft partial metric spaces</a></div><div><b>Author(s):&nbsp;</b>\u0130smet Alt\u0131nta\u015f, Kemal Ta\u015fk\u00f6pr\u00fc, Peyil Esengul kyzy</div><div><b>Pages:&nbsp;</b>8997 - 9010</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07254-x\">Fuzzy superior mandelbrot sets</a></div><div><b>Author(s):&nbsp;</b>Tahir Mahmood, Zeeshan Ali</div><div><b>Pages:&nbsp;</b>9011 - 9020</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07260-z\">Are finite affine topological systems worthy of study?</a></div><div><b>Author(s):&nbsp;</b>Jeffrey T. Denniston, Sergey A. Solovyov</div><div><b>Pages:&nbsp;</b>9021 - 9033</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07279-2\">Fuzzy membership function-dependent switched control for nonlinear systems with memory sampled-data information</a></div><div><b>Author(s):&nbsp;</b>B. Visakamoorthi, K. Subramanian, P. Muthukumar</div><div><b>Pages:&nbsp;</b>9035 - 9048</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07304-4\">Literature review on type-2 fuzzy set theory</a></div><div><b>Author(s):&nbsp;</b>Arnab Kumar De, Debjani Chakraborty, Animesh Biswas</div><div><b>Pages:&nbsp;</b>9049 - 9068</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07305-3\">On the shortest path problem of uncertain random digraphs</a></div><div><b>Author(s):&nbsp;</b>Hao Li, Kun Zhang</div><div><b>Pages:&nbsp;</b>9069 - 9081</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07307-1\">Dombi operations for linguistic T-spherical fuzzy number: an approach for selection of the best variety of maize</a></div><div><b>Author(s):&nbsp;</b>Shahid Hussain Gurmani, Huayou Chen, Yuhang Bai</div><div><b>Pages:&nbsp;</b>9083 - 9100</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07309-z\">Fuzzy entropy and hesitancy entropy in probabilistic hesitant fuzzy information and their applications</a></div><div><b>Author(s):&nbsp;</b>Ting-Ting Xu, Hui Zhang, Bo-Quan Li</div><div><b>Pages:&nbsp;</b>9101 - 9115</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07310-6\">A study on average run length of fuzzy EWMA control chart</a></div><div><b>Author(s):&nbsp;</b>Muhammad Zahir Khan, Muhammad Farid Khan...Abdur Razzaque Mughal</div><div><b>Pages:&nbsp;</b>9117 - 9124</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07346-8\">Fuzzy matrix game: A fast approach using artificial hybrid neural-net logic-gate switching circuit</a></div><div><b>Author(s):&nbsp;</b>Ankan Bhaumik, Sankar Kumar Roy</div><div><b>Pages:&nbsp;</b>9125 - 9135</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07347-7\">A Note on \u201cSolution of matrix games with payoffs of single-valued trapezoidal neutrosophic numbers\u201d</a></div><div><b>Author(s):&nbsp;</b>M. G. Brikaa</div><div><b>Pages:&nbsp;</b>9137 - 9139</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07265-8\">Detecting potholes on Indian roads using Haar feature-based cascade classifier, convolutional neural network, and instance segmentation</a></div><div><b>Author(s):&nbsp;</b>Satish Kumar Satti, K. Suganya Devi...P. Srinivasan</div><div><b>Pages:&nbsp;</b>9141 - 9153</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07275-6\">A deep learning approach for classification and diagnosis of Parkinson\u2019s disease</a></div><div><b>Author(s):&nbsp;</b>Monika Jyotiyana, Nishtha Kesswani...Munish Kumar</div><div><b>Pages:&nbsp;</b>9155 - 9165</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07277-4\">Repair missing data to improve corporate credit risk prediction accuracy with multi-layer perceptron</a></div><div><b>Author(s):&nbsp;</b>Mei Yang, Ming K. Lim...Du Ni</div><div><b>Pages:&nbsp;</b>9167 - 9178</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07331-1\">Back-propagation extreme learning machine</a></div><div><b>Author(s):&nbsp;</b>Weidong Zou, Yuanqing Xia, Weipeng Cao</div><div><b>Pages:&nbsp;</b>9179 - 9188</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07333-z\">Discriminative aging subspace learning for age estimation</a></div><div><b>Author(s):&nbsp;</b>Manisha Sawant, Kishor M. Bhurchandi</div><div><b>Pages:&nbsp;</b>9189 - 9198</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07343-x\">A fault identification method based on an ensemble deep neural network and a correlation coefficient</a></div><div><b>Author(s):&nbsp;</b>Yanli Yang, Yichuan He</div><div><b>Pages:&nbsp;</b>9199 - 9214</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07354-8\">Novel non-Kernel quadratic surface support vector machines based on optimal margin distribution</a></div><div><b>Author(s):&nbsp;</b>Jingyue Zhou, Ye Tian...Qianru Zhai</div><div><b>Pages:&nbsp;</b>9215 - 9227</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07219-0\">Modified grasshopper optimization algorithm-based genetic algorithm for global optimization problems: the system of nonlinear equations case study</a></div><div><b>Author(s):&nbsp;</b>Hala A. Omar, M. A. El-Shorbagy</div><div><b>Pages:&nbsp;</b>9229 - 9245</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07228-z\">Four adaptive grey prediction evolution algorithms with different types of parameters setting techniques</a></div><div><b>Author(s):&nbsp;</b>Cong Gao, Zhongbo Hu...Qinghua Su</div><div><b>Pages:&nbsp;</b>9247 - 9271</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07231-4\">The cost and CO2 emission optimization of reinforced concrete frames with non-prismatic members</a></div><div><b>Author(s):&nbsp;</b>A. KavehL. Mottaghi, R. A. Izadifard</div><div><b>Pages:&nbsp;</b>9273 - 9286</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07245-y\">Energy-aware and carbon-efficient VM placement optimization in cloud datacenters using evolutionary computing methods</a></div><div><b>Author(s):&nbsp;</b>Tahereh Abbasi-khazaeiMohammad Hossein Rezvani</div><div><b>Pages:&nbsp;</b>9287 - 9322</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07250-1\">S-GWO-FH: sparsity-based grey wolf optimization algorithm for face hallucination</a></div><div><b>Author(s):&nbsp;</b>Shyam Singh Rajput</div><div><b>Pages:&nbsp;</b>9323 - 9338</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07255-w\">An evolutionary optimization-based design of Smith delay compensator for cascade control of MIMO time-delay industrial process</a></div><div><b>Author(s):&nbsp;</b>Neelbrata Roy, Anindita Sengupta, Ashoke Sutradhar</div><div><b>Pages:&nbsp;</b>9339 - 9348</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07280-9\">Self-adaptive salp swarm algorithm for optimization problems</a></div><div><b>Author(s):&nbsp;</b>Sofian Kassaymeh, Salwani Abdullah...Zalinda Othman</div><div><b>Pages:&nbsp;</b>9349 - 9368</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07282-7\">Optimal operation and scheduling of a multi-generation microgrid using grasshopper optimization algorithm with cost reduction</a></div><div><b>Author(s):&nbsp;</b>Ziad M. Ali, Mujahed Al-Dhaifallah, Tetsuya Komikawa</div><div><b>Pages:&nbsp;</b>9369 - 9384</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07249-8\">Uncertain random portfolio optimization model with tail value-at-risk</a></div><div><b>Author(s):&nbsp;</b>Qiqi Li, Zhongfeng Qin, Yingchen Yan</div><div><b>Pages:&nbsp;</b>9385 - 9394</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07257-8\">Evaluating chaotic functions with flower pollination algorithm for modelling an optimized low complexity neural network based NAV predictor model</a></div><div><b>Author(s):&nbsp;</b>Smita Mohanty, Rajashree Dash</div><div><b>Pages:&nbsp;</b>9395 - 9417</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07259-6\">Intuitionistic fuzzy decision support based on EDAS and grey relational degree for historic bridges reconstruction priority</a></div><div><b>Author(s):&nbsp;</b>Katarina Rogulj, Jelena Kili\u0107 Pamukovi\u0107...Edmundas Kazimieras Zavadskas</div><div><b>Pages:&nbsp;</b>9419 - 9444</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07268-5\">A VIKOR-based group decision-making approach to software reliability evaluation</a></div><div><b>Author(s):&nbsp;</b>Chuan Yue</div><div><b>Pages:&nbsp;</b>9445 - 9464</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07272-9\">Multivariate uncertain risk aversion with application to accounts receivables pricing</a></div><div><b>Author(s):&nbsp;</b>Ke Wang, Xiaolin Huang...Jian Zhou</div><div><b>Pages:&nbsp;</b>9465 - 9480</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06704-2\">Prediction of the degree of steel corrosion damage in reinforced concrete using field-based data by multi-gene genetic programming approach</a></div><div><b>Author(s):&nbsp;</b>Zahra Rajabi, Mahdi Eftekhari...Hadi Beirami</div><div><b>Pages:&nbsp;</b>9481 - 9496</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06729-7\">Fuzzy dynamic parameter adaptation in the bird swarm algorithm for neural network optimization</a></div><div><b>Author(s):&nbsp;</b>Patricia Melin, Ivette Miramontes...German Prado-Arechiga</div><div><b>Pages:&nbsp;</b>9497 - 9514</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06731-7\">Techno-economic analysis for fertilizer house using hybrid power stations</a></div><div><b>Author(s):&nbsp;</b>Shiv Prakash Bihari, Pradip Kumar Sadhu</div><div><b>Pages:&nbsp;</b>9515 - 9525</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06735-3\">Creep parameter inversion for high CFRDs based on improved BP neural network response surface method</a></div><div><b>Author(s):&nbsp;</b>Xinjie Zhou, Xinjian Sun...Pengtao Zhang</div><div><b>Pages:&nbsp;</b>9527 - 9541</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06738-0\">Applying the Delphi and fuzzy DEMATEL methods for identification and prioritization of the variables affecting Iranian citrus exports to Russia</a></div><div><b>Author(s):&nbsp;</b>Seyyed Mehdi Hosseini, Yazdan Soltanpour, Mohammad Mahdi Paydar</div><div><b>Pages:&nbsp;</b>9543 - 9556</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06751-3\">Combining intrinsic dimension and local tangent space for manifold spectral clustering image segmentation</a></div><div><b>Author(s):&nbsp;</b>Xiaoling Yao, Rongguo Zhang...Jian Zhao</div><div><b>Pages:&nbsp;</b>9557 - 9572</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06777-7\">A novel application of fuzzy inference system optimized with particle swarm optimization and genetic algorithm for PM10 prediction</a></div><div><b>Author(s):&nbsp;</b>Jagriti Saini, Maitreyee Dutta, Gon\u00e7alo Marques</div><div><b>Pages:&nbsp;</b>9573 - 9586</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06786-6\">Computational personality: a survey</a></div><div><b>Author(s):&nbsp;</b>Liang Yang, Shuqun Li...Hongfei Lin</div><div><b>Pages:&nbsp;</b>9587 - 9605</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06789-3\">Stochastic coordination of the wind and solar energy using energy storage system based on real-time pricing</a></div><div><b>Author(s):&nbsp;</b>Sajjad Saeedi, S. M. Hassan Hosseini</div><div><b>Pages:&nbsp;</b>9607 - 9620</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06793-7\">Self-Attention Networks and Adaptive Support Vector Machine for aspect-level sentiment classification</a></div><div><b>Author(s):&nbsp;</b>Meizhen Liu, FengYu Zhou...HongChang Sun</div><div><b>Pages:&nbsp;</b>9621 - 9634</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06815-4\">Using deep reinforcement learning to search reachability properties in systems specified through graph transformation</a></div><div><b>Author(s):&nbsp;</b>Mohammad Javad Mehrabi, Vahid Rafe</div><div><b>Pages:&nbsp;</b>9635 - 9663</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06826-1\">A self-adaptive level-based learning artificial bee colony algorithm for feature selection on high-dimensional classification</a></div><div><b>Author(s):&nbsp;</b>Jing Wang, Yuanzi Zhang...Shiguo Huang</div><div><b>Pages:&nbsp;</b>9665 - 9687</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06828-z\">Creative DNA computing: splicing systems for music composition</a></div><div><b>Author(s):&nbsp;</b>Roberto De Prisco, Rocco Zaccagnino</div><div><b>Pages:&nbsp;</b>9689 - 9706</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06836-z\">A new statistical image watermark detector in RHFMs domain using beta-exponential distribution</a></div><div><b>Author(s):&nbsp;</b>Xiang-yang Wang, Pan-pan Niu...Jia-lin Tian</div><div><b>Pages:&nbsp;</b>9707 - 9727</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07211-8\">Quick flower pollination algorithm (QFPA) and its performance on neural network training</a></div><div><b>Author(s):&nbsp;</b>Ebubekir Kaya</div><div><b>Pages:&nbsp;</b>9729 - 9750</div><div><br /></div></div>",
            "pubdate": "2022-09-06T12:00:00.001+12:00",
            "pubdate_parsed": [
                2022,
                9,
                6
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 20, October 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/09/soft-computing-volume-26-issue-20.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07390-4\">Editorial on decision support system for development of intelligent applications</a></div><div><b>Author(s): </b>Shah Nazir, Habib Ullah Khan...Iv\u00e1n Garc\u00eda-Magari\u00f1o</div><div><b>Pages: </b>10547 - 10551</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06482-x\">A hybrid deep learning and ensemble learning mechanism for damaged power line detection in smart grids</a></div><div><b>Author(s):&nbsp;</b>Yangyang Tian, Qi Wang...Jian Zhao</div><div><b>Pages:&nbsp;</b>10553 - 10561</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06578-4\">EmoPercept: EEG-based emotion classification through perceiver</a></div><div><b>Author(s):&nbsp;</b>Aadam, Abdallah Tubaishat...Fawad Qayum</div><div><b>Pages:&nbsp;</b>10563 - 10570</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06659-4\">Design of enterprise human resources decision support system based on data mining</a></div><div><b>Author(s):&nbsp;</b>Li Jian</div><div><b>Pages:&nbsp;</b>10571 - 10580</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06689-y\">PlantNet: transfer learning-based fine-grained network for high-throughput plants recognition</a></div><div><b>Author(s):&nbsp;</b>Ziying Yang, Wenyan He...Tardi Tjahjadi</div><div><b>Pages:&nbsp;</b>10581 - 10590</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06709-x\">An intelligent deep learning-enabled recommendation algorithm for teaching music students</a></div><div><b>Author(s):&nbsp;</b>Changfei Tang, Jun Zhang</div><div><b>Pages:&nbsp;</b>10591 - 10598</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06720-2\">A network security posture assessment model based on binary semantic analysis</a></div><div><b>Author(s):&nbsp;</b>Dasheng Wu</div><div><b>Pages:&nbsp;</b>10599 - 10606</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06725-x\">Short-term prediction of wind power based on BiLSTM\u2013CNN\u2013WGAN-GP</a></div><div><b>Author(s):&nbsp;</b>Ling Huang, Linxia Li...Dongsheng Zhang</div><div><b>Pages:&nbsp;</b>10607 - 10621</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06747-z\">Application of machine learning in wire damage detection for safety procedure</a></div><div><b>Author(s):&nbsp;</b>Zhimin Guo, Chao Wang...Shaoguang Yuan</div><div><b>Pages:&nbsp;</b>10623 - 10631</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06780-y\">Heart disease diagnosis using deep learning and cardiac color doppler ultrasound</a></div><div><b>Author(s):&nbsp;</b>Jing Wang, Jing Li...Yan Huang</div><div><b>Pages:&nbsp;</b>10633 - 10642</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06788-4\">Tunable control of internet of things information hacking by application of the induced chiral atomic medium</a></div><div><b>Author(s):&nbsp;</b>Syed Muhammad Arif, Bakht Amin Bacha...Muhammad Haneef</div><div><b>Pages:&nbsp;</b>10643 - 10650</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06798-2\">Intrusion detection in networks using cuckoo search optimization</a></div><div><b>Author(s):&nbsp;</b>Muhammad Imran, Sangeen Khan...Sajid Anwar</div><div><b>Pages:&nbsp;</b>10651 - 10663</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06800-x\">Human resource labor dispatch model using an improved genetic algorithm</a></div><div><b>Author(s):&nbsp;</b>Qi Feng, Xingren Su, Qiang Li</div><div><b>Pages:&nbsp;</b>10665 - 10676</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06819-0\">Data news dissemination strategy for decision making using new media platform</a></div><div><b>Author(s):&nbsp;</b>Lu Li</div><div><b>Pages:&nbsp;</b>10677 - 10685</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06847-w\">Extended ICA and M-CSP with BiLSTM towards improved classification of EEG signals</a></div><div><b>Author(s):&nbsp;</b>Atta Ur Rahman, Abdallah Tubaishat...Fawad Qayum</div><div><b>Pages:&nbsp;</b>10687 - 10698</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06848-9\">Using neural network for the evaluation of physical education teaching in colleges and universities</a></div><div><b>Author(s):&nbsp;</b>Qiuhong Han</div><div><b>Pages:&nbsp;</b>10699 - 10705</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06875-6\">A reliable wireless communication mechanisms and decision support system for the IoT networks</a></div><div><b>Author(s):&nbsp;</b>Bo Jin, Fazlullah Khan...Mohammed Abdulaziz Ikram</div><div><b>Pages:&nbsp;</b>10707 - 1071</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06902-6\">Germ integrity detection for rice using a combination of germ color image features and deep learning</a></div><div><b>Author(s):&nbsp;</b>Jin Li, Shuofeng Li...Bin Liu</div><div><b>Pages:&nbsp;</b>10717 - 10727</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06936-w\">Design and realization of CNC machine tool management system using Internet of things</a></div><div><b>Author(s):&nbsp;</b>Pei Shicong, Wu Guocheng, Tao Fuqiang</div><div><b>Pages:&nbsp;</b>10729 - 10739</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06957-5\">A decision support system for assessing the role of the 5G network and AI in situational teaching research in higher education</a></div><div><b>Author(s):&nbsp;</b>Xiaoshuang Liu, Mohammad Faisal, Abdullah Alharbi</div><div><b>Pages:&nbsp;</b>10741 - 10752</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06962-8\">Construction of business innovation model for sports industry using a deep learning algorithm</a></div><div><b>Author(s):&nbsp;</b>Chenchen Lv, Yifeng Wang, Yin Ma</div><div><b>Pages:&nbsp;</b>10753 - 10763</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06983-3\">Evaluation of physical education classes in colleges and universities using machine learning</a></div><div><b>Author(s):&nbsp;</b>Chendi Hu</div><div><b>Pages:&nbsp;</b>10765 - 10773</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06992-2\">Decision support system for evaluating the role of music in network-based game for sustaining effectiveness</a></div><div><b>Author(s):&nbsp;</b>Yanli Yu, Dong Wang...Sumaira Johar</div><div><b>Pages:&nbsp;</b>10775 - 10788</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06993-1\">A comprehensive review on the role of online media in sustainable business development and decision making</a></div><div><b>Author(s):&nbsp;</b>Haiyu He</div><div><b>Pages:&nbsp;</b>10789 - 10803</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07003-0\">Research on evaluation system of ideological and political education of college students based on decision system</a></div><div><b>Author(s):&nbsp;</b>Zhang Rui</div><div><b>Pages:&nbsp;</b>10805 - 10812</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07018-7\">A deep neural network-based decision support system for intelligent geospatial data analysis in intelligent agriculture system</a></div><div><b>Author(s):&nbsp;</b>Chunying Zeng, Fan Zhang, Mingzhong Luo</div><div><b>Pages:&nbsp;</b>10813 - 10826</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07019-6\">Optimized design of oil well pump plunger using bionic structure of decision system</a></div><div><b>Author(s):&nbsp;</b>Bo Wang, Mengji Chen...Jinfeng Wei</div><div><b>Pages:&nbsp;</b>10827 - 10836</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07046-3\">Prediction of thawing settlement coefficient of frozen soil using 5G communication</a></div><div><b>Author(s):&nbsp;</b>Yueming Yin, Chaoqun Wei...Qinglu Deng</div><div><b>Pages:&nbsp;</b>10837 - 10852</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07082-z\">The role of big data in network public opinion within the colleges and universities</a></div><div><b>Author(s):&nbsp;</b>Bin Xu, Ying Liu</div><div><b>Pages:&nbsp;</b>10853 - 10862</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07083-y\">Prediction algorithm and simulation of tennis impact area based on semantic analysis of prior knowledge</a></div><div><b>Author(s):&nbsp;</b>Yong Ke, Zhen Liu, Sai Liu</div><div><b>Pages:&nbsp;</b>10863 - 10870</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07086-9\">An assisted teaching algorithm for basketball shooting based on object decomposition</a></div><div><b>Author(s):&nbsp;</b>Xixiao Liu, Xuyun Xi</div><div><b>Pages:&nbsp;</b>10871 - 10878</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07106-8\">Improved YOLOv5 network method for remote sensing image-based ground objects recognition</a></div><div><b>Author(s):&nbsp;</b>Jie Xue, Yongguo Zheng...Muhammad Yasir</div><div><b>Pages:&nbsp;</b>10879 - 10889</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07144-2\">Teaching English as a foreign language using the Internet and multimedia environment</a></div><div><b>Author(s):&nbsp;</b>Lina Liang</div><div><b>Pages:&nbsp;</b>10891 - 10902</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07156-y\">Designing an interactive teaching model of English language using Internet of Things</a></div><div><b>Author(s):&nbsp;</b>Wei Gao</div><div><b>Pages:&nbsp;</b>10903 - 10913</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07197-3\">Decision support system for real-time segmentation and identification algorithm for wires in mobile terminals using fuzzy AHP method</a></div><div><b>Author(s):&nbsp;</b>Lei Wang, Mingyue Chu...Guanlong Gao</div><div><b>Pages:&nbsp;</b>10915 - 10926</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07209-2\">AI-based production and application of English multimode online reading using multi-criteria decision support system</a></div><div><b>Author(s):&nbsp;</b>Yifan Dong, Xinyu Yu...Sultan Ahmad</div><div><b>Pages:&nbsp;</b>10927 - 10937</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07210-9\">Football training evaluation using machine learning and decision support system</a></div><div><b>Author(s):&nbsp;</b>Qiangqiang Xu, Xin He</div><div><b>Pages:&nbsp;</b>10939 - 10946</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07240-3\">Key technologies of human\u2013computer interaction for immersive somatosensory interactive games using VR technology</a></div><div><b>Author(s):&nbsp;</b>Peng Gao</div><div><b>Pages:&nbsp;</b>10947 - 10956</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07247-w\">Information technology-based revolution in music education using AHP and TOPSIS</a></div><div><b>Author(s):&nbsp;</b>Yi Fu, Mengjia Zhang...Aman Singh</div><div><b>Pages:&nbsp;</b>10957 - 10970</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07295-2\">Application of deep learning in video target tracking of soccer players</a></div><div><b>Author(s):&nbsp;</b>Xin He</div><div><b>Pages:&nbsp;</b>10971 - 10979</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07317-z\">Joint training method for transmission defects based on component hierarchy</a></div><div><b>Author(s):&nbsp;</b>Wang Chao, Tian Yangyang...Tan Qiyun</div><div><b>Pages:&nbsp;</b>10981 - 10992</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07352-w\">Cost estimate in scrum project with the decision-based effort estimation technique</a></div><div><b>Author(s):&nbsp;</b>Fahad H. Alshammari</div><div><b>Pages:&nbsp;</b>10993 - 11005</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07396-y\">A decision support system based on multi-sources information to predict piRNA\u2013disease associations using stacked autoencoder</a></div><div><b>Author(s):&nbsp;</b>Kai Zheng, Ying Liang...Ping Wang</div><div><b>Pages:&nbsp;</b>11007 - 11016</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07398-w\">Sustainable configuration paths of marine eco-efficiency: based on fuzzy-set qualitative comparative analysis of 11 coastal areas in China</a></div><div><b>Author(s):&nbsp;</b>Zhenyu Huang</div><div><b>Pages:&nbsp;</b>11017 - 11032</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07399-9\">Decision support system based on spatial and temporal pattern evolution of ecological environmental quality in the Yellow River Delta from 2000 to 2020</a></div><div><b>Author(s):&nbsp;</b>Xin Zhao, Ping Wang...Zhan Liu</div><div><b>Pages:&nbsp;</b>11033 - 11044</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07400-5\">A comprehensive analysis of the impact of online media and newsprint on advertising sales in the information society</a></div><div><b>Author(s):&nbsp;</b>Keyan Xu, Mengjun Xie...Noha Alnazzawi</div><div><b>Pages:&nbsp;</b>11045 - 11062</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07401-4\">A decision-support system for assessing the function of machine learning and artificial intelligence in music education for network games</a></div><div><b>Author(s):&nbsp;</b>Zou Hong Yun, Yasser Alshehri...Neelam Gohar</div><div><b>Pages:&nbsp;</b>11063 - 11075</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07405-0\">Adapting recurrent neural networks for classifying public discourse on COVID-19 symptoms in Twitter content</a></div><div><b>Author(s):&nbsp;</b>Samina Amin, Abdullah Alharbi...Hashem Alyami</div><div><b>Pages:&nbsp;</b>11077 - 11089</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07475-0\">TinyLFU-based semi-stream cache join for near-real-time data warehousing</a></div><div><b>Author(s):&nbsp;</b>M. Asif Naeem, Wasiullah Waqar...Ali Tahir</div><div><b>Pages:&nbsp;</b>11091 - 11103</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07493-y\">Performance evaluation of state-owned enterprises based on fuzzy neural network combination model</a></div><div><b>Author(s):&nbsp;</b>Wenhao Jiao</div><div><b>Pages:&nbsp;</b>11105 - 11113</div><div><br /></div></div>",
            "pubdate": "2022-09-28T17:22:00.000+13:00",
            "pubdate_parsed": [
                2022,
                9,
                28
            ],
            "email_sent": true
        },
        "IEEE Transactions on Emerging Topics in Computational Intelligence, Volume 6, Issue 5, October 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/09/ieee-transactions-on-emerging-topics-in.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9900111/\">Guest Editorial Special Issue on Computational Intelligence in Big Graph Data Management</a></div><div><b>Author(s): </b>Guanfeng Liu, An Liu, Qing Li, Huanhuan Chen, Deepak Puthal</div><div><b>Pages: </b>1027 - 1029</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9581056/\">Optimal Searching Time Allocation for Information Collection Under Cooperative Path Planning of Multiple UAVs</a></div><div><b>Author(s):&nbsp;</b>Yanmin Li, Lihua Liu, Jibing Wu, Mao Wang, Haohao Zhou, Hongbin Huang</div><div><b>Pages:&nbsp;</b>1030 - 1043</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9548676/\">Compactness Preserving Community Computation Via a Network Generative Process</a></div><div><b>Author(s):&nbsp;</b>Jie Cao, Yuyao Wang, Zhan Bu, Youquan Wang, Haicheng Tao, Guixiang Zhu</div><div><b>Pages:&nbsp;</b>1044 - 1056</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9691281/\">Interval-Valued Intuitionistic Fuzzy Decision With Graph Pattern in Big Graph</a></div><div><b>Author(s):&nbsp;</b>Lei Li, Lan Jiang, Chenyang Bu, Yi Zhu, Xindong Wu</div><div><b>Pages:&nbsp;</b>1057 - 1067</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9695487/\">Wavefront-Based Multiple Rumor Sources Identification by Multi-Task Learning</a></div><div><b>Author(s):&nbsp;</b>Ming Dong, Bolong Zheng, Guohui Li, Chenliang Li, Kai Zheng, Xiaofang Zhou</div><div><b>Pages:&nbsp;</b>1068 - 1078</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9739062/\">Attraction and Repulsion: Unsupervised Domain Adaptive Graph Contrastive Learning Network</a></div><div><b>Author(s):&nbsp;</b>Man Wu, Shirui Pan, Xingquan Zhu</div><div><b>Pages:&nbsp;</b>1079 - 1091</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9735156/\">Understand Me, if You Refer to Aspect Knowledge: Knowledge-Aware Gated Recurrent Memory Network</a></div><div><b>Author(s):&nbsp;</b>Bowen Xing, Ivor W. Tsang</div><div><b>Pages:&nbsp;</b>1092 - 1102</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9714726/\">Self-Consistent Learning of Neural Dynamical Systems From Noisy Time Series</a></div><div><b>Author(s):&nbsp;</b>Zhe Wang, Claude Guet</div><div><b>Pages:&nbsp;</b>1103 - 1112</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9713759/\">Heterogeneous Semi-Asynchronous Federated Learning in Internet of Things: A Multi-Armed Bandit Approach</a></div><div><b>Author(s):&nbsp;</b>Shuai Chen, Xiumin Wang, Pan Zhou, Weiwei Wu, Weiwei Lin, Zhenyu Wang</div><div><b>Pages:&nbsp;</b>1113 - 1124</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9712301/\">Behavior Reasoning for Opponent Agents in Multi-Agent Learning Systems</a></div><div><b>Author(s):&nbsp;</b>Yaqing Hou, Mingyang Sun, Wenxuan Zhu, Yifeng Zeng, Haiyin Piao, Xuefeng Chen, Qiang Zhang</div><div><b>Pages:&nbsp;</b>1125 - 1136</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9674744/\">Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning</a></div><div><b>Author(s):&nbsp;</b>Isaac J. Sledge, Darshan W. Bryner, Jos\u00e9 C. Pr\u00edncipe</div><div><b>Pages:&nbsp;</b>1137 - 1156</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9749857/\">RSAC: A Robust Deep Reinforcement Learning Strategy for Dimensionality Perturbation</a></div><div><b>Author(s):&nbsp;</b>Surbhi Gupta, Gaurav Singal, Deepak Garg, Swagatam Das</div><div><b>Pages:&nbsp;</b>1157 - 1166</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9669910/\">Triple Cross-Domain Attention on Human Activity Recognition Using Wearable Sensors</a></div><div><b>Author(s):&nbsp;</b>Yin Tang, Lei Zhang, Qi Teng, Fuhong Min, Aiguo Song</div><div><b>Pages:&nbsp;</b>1167 - 1176</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9714727/\">Motor Imagery BCI Classification Based on Multivariate Variational Mode Decomposition</a></div><div><b>Author(s):&nbsp;</b>Muhammad Tariq Sadiq, Xiaojun Yu, Zhaohui Yuan, Muhammad Zulkifal Aziz, Naveed ur Rehman, Weiping Ding, Gaoxi Xiao</div><div><b>Pages:&nbsp;</b>1177 - 1189</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9343762/\">Multi-Crop Convolutional Neural Networks for Fast Lung Nodule Segmentation</a></div><div><b>Author(s):&nbsp;</b>Quan Chen, Wei Xie, Pan Zhou, Chuansheng Zheng, Dapeng Wu</div><div><b>Pages:&nbsp;</b>1190 - 1200</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9711563/\">Feature Pyramid Network With Level-Aware Attention for Meningioma Segmentation</a></div><div><b>Author(s):&nbsp;</b>Wei Huang, Xin Shu, Zizhou Wang, Lei Zhang, Chaoyue Chen, Jianguo Xu, Zhang Yi</div><div><b>Pages:&nbsp;</b>1201 - 1210</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9631874/\">Attributes Guided Feature Learning for Vehicle Re-Identification</a></div><div><b>Author(s):&nbsp;</b>Hongchao Li, Xianmin Lin, Aihua Zheng, Chenglong Li, Bin Luo, Ran He, Amir Hussain</div><div><b>Pages:&nbsp;</b>1211 - 1221</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9762457/\">Local Visual and Global Deep Features Based Blind Stitched Panoramic Image Quality Evaluation Using Ensemble Learning</a></div><div><b>Author(s):&nbsp;</b>Yueli Cui, Gangyi Jiang, Mei Yu, Yang Song</div><div><b>Pages:&nbsp;</b>1222 - 1236</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9803821/\">Unsupervised Monocular Depth Estimation in Highly Complex Environments</a></div><div><b>Author(s):&nbsp;</b>Chaoqiang Zhao, Yang Tang, Qiyu Sun</div><div><b>Pages:&nbsp;</b>1237 - 1246</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9638990/\">Minable Data Publication Based on Sensitive Association Rule Hiding</a></div><div><b>Author(s):&nbsp;</b>Fan Yang, Xinyu Lei, Junqing Le, Nankun Mu, Xiaofeng Liao</div><div><b>Pages:&nbsp;</b>1247 - 1257</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9726514/\">ES Attack: Model Stealing Against Deep Neural Networks Without Data Hurdles</a></div><div><b>Author(s):&nbsp;</b>Xiaoyong Yuan, Leah Ding, Lan Zhang, Xiaolin Li, Dapeng Oliver Wu</div><div><b>Pages:&nbsp;</b>1258 - 1270</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9718120/\">Adaptive Evolution Strategies for Stochastic Zeroth-Order Optimization</a></div><div><b>Author(s):&nbsp;</b>Xiaoyu He, Zibin Zheng, Zefeng Chen, Yuren Zhou</div><div><b>Pages:&nbsp;</b>1271 - 1285</div><div><br /></div></div>",
            "pubdate": "2022-09-29T21:24:00.000+13:00",
            "pubdate_parsed": [
                2022,
                9,
                29
            ],
            "email_sent": true
        },
        "Complex & Intelligent Systems. Volume 8, Issue 5, October 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/10/complex-intelligent-systems-volume-8.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00821-7\">Intelligent mobile edge computing for IoT big data</a></div><div><b>Author(s): </b>Gwanggil Jeon, Marcelo Albertini...Abdellah Chehri</div><div><b>Pages: </b>3595 - 3601</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00393-y\">Span identification and technique classification of propaganda in news articles</a></div><div><b>Author(s):&nbsp;</b>Wei Li, Shiqian Li...Shiping Wen</div><div><b>Pages:&nbsp;</b>3603 - 3612</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00397-8\">Evaluation of deep learning algorithms for semantic segmentation of car parts</a></div><div><b>Author(s):&nbsp;</b>Kitsuchart Pasupa, Phongsathorn Kittiworapanya...Kuntpong Woraratpanya</div><div><b>Pages:&nbsp;</b>3613 - 3625</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00419-5\">License plate identification and recognition in a non-standard environment using neural pattern matching</a></div><div><b>Author(s):&nbsp;</b>Imran Shafi, Imtiaz Hussain...Sadia Din</div><div><b>Pages:&nbsp;</b>3627 - 3639</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00434-6\">Implementation analysis of IoT-based offloading frameworks on cloud/edge computing for sensor generated big data</a></div><div><b>Author(s):&nbsp;</b>Karan Bajaj, Bhisham Sharma, Raman Singh</div><div><b>Pages:&nbsp;</b>3641 - 3658</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00447-1\">Supporting autism spectrum disorder screening and intervention with machine learning and wearables: a systematic literature review</a></div><div><b>Author(s):&nbsp;</b>Rita Francese, Xiaomin Yang</div><div><b>Pages:&nbsp;</b>3659 - 3674</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00455-1\">DRAC: a delta recurrent neural network-based arithmetic coding algorithm for edge computing</a></div><div><b>Author(s):&nbsp;</b>Bowei Shan, Yong Fang</div><div><b>Pages:&nbsp;</b>3675 - 3681</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00483-x\">A survey on computation resource allocation in IoT enabled vehicular edge computing</a></div><div><b>Author(s):&nbsp;</b>Naren, Abhishek Kumar Gaurav...Vinay Chamola</div><div><b>Pages:&nbsp;</b>3683 - 3705</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00466-y\">Blockchain-based green big data visualization: BGbV</a></div><div><b>Author(s):&nbsp;</b>Iqra Shahzad, Ayesha Maqbool...Sadia Din</div><div><b>Pages:&nbsp;</b>3707 - 3718</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00498-4\">An edge based hybrid intrusion detection framework for mobile edge computing</a></div><div><b>Author(s):&nbsp;</b>Ashish Singh, Kakali Chatterjee, Suresh Chandra Satapathy</div><div><b>Pages:&nbsp;</b>3719 - 3746</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00518-3\">A novel trust management model for edge computing</a></div><div><b>Author(s):&nbsp;</b>Rabia Latif, Malik Uzair Ahmed...Awais Ahmad</div><div><b>Pages:&nbsp;</b>3747 - 3763</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00544-1\">Spatio-temporal joint aberrance suppressed correlation filter for visual tracking</a></div><div><b>Author(s):&nbsp;</b>Libin Xu, Pyoungwon Kim...Mingliang Gao</div><div><b>Pages:&nbsp;</b>3765 - 3777</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00578-5\">SHANN: an IoT and machine-learning-assisted edge cross-layered routing protocol using spotted hyena optimizer</a></div><div><b>Author(s):&nbsp;</b>Gaurav Dhiman, Rohit Sharma</div><div><b>Pages:&nbsp;</b>3779 - 3787</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00581-w\">Cloud\u2013edge cooperation for meteorological radar big data: a review of data quality control</a></div><div><b>Author(s):&nbsp;</b>Zhichen Hu, Xiaolong Xu...Mohammad R. Khosravi</div><div><b>Pages:&nbsp;</b>3789 - 3803</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00582-9\">Smart healthcare IoT applications based on fog computing: architecture, applications and challenges</a></div><div><b>Author(s):&nbsp;</b>Vu Khanh Quy, Nguyen Van Hau...Le Anh Ngoc</div><div><b>Pages:&nbsp;</b>3805 - 3815</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00576-7\">Secure transmission technique for data in IoT edge computing infrastructure</a></div><div><b>Author(s):&nbsp;</b>Rohit Sharma, Rajeev Arya</div><div><b>Pages:&nbsp;</b>3817 - 3832</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00618-0\">Semantic segmentation of large-scale point clouds based on dilated nearest neighbors graph</a></div><div><b>Author(s):&nbsp;</b>Lei Wang, Jiaji Wu...Jun Cheng</div><div><b>Pages:&nbsp;</b>3833 - 3845</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00659-z\">Decision-making of IoT device operation based on intelligent-task offloading for improving environmental optimization</a></div><div><b>Author(s):&nbsp;</b>Wenquan Jin, Sunhwan Lim...Dohyeun Kim</div><div><b>Pages:&nbsp;</b>3847 - 3866</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00666-0\">Energy-saving service management technology of internet of things using edge computing and deep learning</a></div><div><b>Author(s):&nbsp;</b>Defeng Li, Mingming Lan, Yuan Hu</div><div><b>Pages:&nbsp;</b>3867 - 3879</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00683-z\">Ship feature recognition methods for deep learning in complex marine environments</a></div><div><b>Author(s):&nbsp;</b>Xiang Wang, Jingxian Liu...Quan Ouyang</div><div><b>Pages:&nbsp;</b>3881 - 3897</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00664-2\">Counseling (ro)bot as a use case for 5G/6G</a></div><div><b>Author(s):&nbsp;</b>Yoshio Taniguchi, Yukino Ikegami...Setsuo Tsuruta</div><div><b>Pages:&nbsp;</b>3899 - 3917</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00765-y\">Authorization schemes for internet of things: requirements, weaknesses, future challenges and trends</a></div><div><b>Author(s):&nbsp;</b>Abid Khan, Awais Ahmad...Marco Anisetti</div><div><b>Pages:&nbsp;</b>3919 - 3941</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00735-4\">Research on energy saving technology at mobile edge networks of IoTs based on big data analysis</a></div><div><b>Author(s):&nbsp;</b>Chaochen Xie, Qiaozhi Hua...Lixia Guo</div><div><b>Pages:&nbsp;</b>3943 - 3952</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00775-w\">A methodology to engineering continuous monitoring of intrinsic capacity for elderly people</a></div><div><b>Author(s):&nbsp;</b>Valerio Bellandi, Paolo Ceravolo...Mircea Dan Marzan</div><div><b>Pages:&nbsp;</b>3953 - 3971</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00635-z\">Tourism route optimization based on improved knowledge ant colony algorithm</a></div><div><b>Author(s):&nbsp;</b>Sidi Li, Tianyu Luo...Teng Ren</div><div><b>Pages:&nbsp;</b>3973 - 3988</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00640-2\">An estimation of distribution algorithm with clustering for scenario-based robust financial optimization</a></div><div><b>Author(s):&nbsp;</b>Wen Shi, Xiao-Min Hu, Wei-Neng Chen</div><div><b>Pages:&nbsp;</b>3989 - 4003</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00692-y\">Joint metric learning of local and global features for vehicle re-identification</a></div><div><b>Author(s):&nbsp;</b>Junge Shen, Jian Sun...Zhaoyong Mao</div><div><b>Pages:&nbsp;</b>4005 - 4020</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00687-9\">Fuzzy efficiency evaluation in relational network data envelopment analysis: application in gas refineries</a></div><div><b>Author(s):&nbsp;</b>Somayeh Tabatabaei, Mohammad Reza Mozaffari...Farhad Hosseinzadeh Lotfi</div><div><b>Pages:&nbsp;</b>4021 - 4049</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00697-7\">A conjugate gradient-assisted multi-objective evolutionary algorithm for fluence map optimization in radiotherapy treatment</a></div><div><b>Author(s):&nbsp;</b>Ruifen Cao, Langchun Si...Xingyi Zhang</div><div><b>Pages:&nbsp;</b>4051 - 4077</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00698-6\">A new neutrosophic model using DUS-Weibull transformation with application</a></div><div><b>Author(s):&nbsp;</b>B. M. Nayana, K. K. Anakha...Mohammed Albassam</div><div><b>Pages:&nbsp;</b>4079 - 4088</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00702-z\">Manifold learning for novelty detection and its application in gesture recognition</a></div><div><b>Author(s):&nbsp;</b>Yang Luo, Yibiao Yuan...Xiaohui Mo</div><div><b>Pages:&nbsp;</b>4089 - 4100</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00704-x\">Topological approach to generate new rough set models</a></div><div><b>Author(s):&nbsp;</b>Tareq M. Al-shami</div><div><b>Pages:&nbsp;</b>4101 - 4113</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00686-w\">Carbon mechanism on sustainable multi-objective solid transportation problem for waste management in Pythagorean hesitant fuzzy environment</a></div><div><b>Author(s):&nbsp;</b>Shyamali Ghosh, Karl-Heinz K\u00fcfer...Gerhard-Wilhelm Weber</div><div><b>Pages:&nbsp;</b>4115 - 4143</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00700-1\">Predicting vacant parking space availability zone-wisely: a hybrid deep learning approach</a></div><div><b>Author(s):&nbsp;</b>Yajing Feng, Yingying Xu...Zhenzhou Tang</div><div><b>Pages:&nbsp;</b>4145 - 4161</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00709-6\">Joint uneven channel information network with blend metric loss for person re-identification</a></div><div><b>Author(s):&nbsp;</b>Zhi Yu, Zhiyong Huang...Daming Sun</div><div><b>Pages:&nbsp;</b>4163 - 4175</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00710-z\">A multi-attribute approach to ranking departments based on performance: a balanced scorecard pilot study</a></div><div><b>Author(s):&nbsp;</b>U\u011fur Tahsin \u015eenel, Babak Daneshvar Rouyendegh, Sercan Demir</div><div><b>Pages:&nbsp;</b>4177 - 4185</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00691-z\">Evaluation and selection of green suppliers for papermaking enterprises using the interval basic probability assignment-based intuitionistic fuzzy set</a></div><div><b>Author(s):&nbsp;</b>Xin Kang, Xiangjun Xu...Zaoli Yang</div><div><b>Pages:&nbsp;</b>4187 - 4203</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00695-9\">Anomaly detection for high-dimensional space using deep hypersphere fused with probability approach</a></div><div><b>Author(s):&nbsp;</b>Jian Zheng, Jingyi Li...Hongling Liu</div><div><b>Pages:&nbsp;</b>4205 - 4220</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00705-w\">A federated learning framework for cyberattack detection in vehicular sensor networks</a></div><div><b>Author(s):&nbsp;</b>Maha Driss, Iman Almomani...Jawad Ahmad</div><div><b>Pages:&nbsp;</b>4221 - 4235</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00713-w\">A DCRNN-based ensemble classifier for speech emotion recognition in Odia language</a></div><div><b>Author(s):&nbsp;</b>Monorama Swain, Bubai Maji...Aurobinda Routray</div><div><b>Pages:&nbsp;</b>4237 - 4249</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00712-x\">Interactive spatio-temporal feature learning network for video foreground detection</a></div><div><b>Author(s):&nbsp;</b>Hongrui Zhang, Huan Li</div><div><b>Pages:&nbsp;</b>4251 - 4263</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00711-y\">Hierarchical edge-aware network for defocus blur detection</a></div><div><b>Author(s):&nbsp;</b>Zijian Zhao, Hang Yang, Huiyuan Luo</div><div><b>Pages:&nbsp;</b>4265 - 4276</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00706-9\">A knee point-driven many-objective pigeon-inspired optimization algorithm</a></div><div><b>Author(s):&nbsp;</b>Lihong Zhao, Yeqing Ren...Wensheng Zhang</div><div><b>Pages:&nbsp;</b>4277 - 4299</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00715-8\">Multiobjective portfolio optimization via Pareto front evolution</a></div><div><b>Author(s):&nbsp;</b>Yi Chen, Aimin Zhou</div><div><b>Pages:&nbsp;</b>4301 - 4317</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00690-0\">Time-sequential hesitant fuzzy set and its application to multi-attribute decision making</a></div><div><b>Author(s):&nbsp;</b>Lingyu Meng, Liangqun Li</div><div><b>Pages:&nbsp;</b>4319 - 4338</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00717-6\">An adaptive model switch-based surrogate-assisted evolutionary algorithm for noisy expensive multi-objective optimization</a></div><div><b>Author(s):&nbsp;</b>Nan Zheng, Handing Wang, Bo Yuan</div><div><b>Pages:&nbsp;</b>4339 - 4356</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00708-7\">Source code auto-completion using various deep learning models under limited computing resources</a></div><div><b>Author(s):&nbsp;</b>Madhab Sharma, Tapas Kumar Mishra, Arun Kumar</div><div><b>Pages:&nbsp;</b>4357 - 4368</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00696-8\">Implicit optimal variational collaborative filtering</a></div><div><b>Author(s):&nbsp;</b>Joojo Walker, Fan Zhou...Fengli Zhang</div><div><b>Pages:&nbsp;</b>4369 - 4384</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00789-4\">Design of sampling-based noniterative algorithms for centroid type-reduction of general type-2 fuzzy logic systems</a></div><div><b>Author(s):&nbsp;</b>Yang Chen</div><div><b>Pages:&nbsp;</b>4385 - 4402</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00603-7\">Distributed ledger technologies in vehicular mobile edge computing: a survey</a></div><div><b>Author(s):&nbsp;</b>Ming Jiang, Xingsheng Qin</div><div><b>Pages:&nbsp;</b>4403 - 4419</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-10-02T17:43:00.000+13:00",
            "pubdate_parsed": [
                2022,
                10,
                2
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 26, issue 21, November 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/10/soft-computing-volume-26-issue-21.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07318-y\">A derived least square extreme learning machine</a></div><div><b>Author(s): </b>Shuang Hou, Yi Wang...Xiaosheng Wang</div><div><b>Pages: </b>11115 - 11127</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07407-y\">A novel distance between single valued neutrosophic sets and its application in pattern recognition</a></div><div><b>Author(s):&nbsp;</b>Minxia Luo, Guofeng Zhang, Lixian Wu</div><div><b>Pages:&nbsp;</b>11129 - 11137</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07469-y\">Hardening secure search in encrypted database: a KGA-resistance conjunctive searchable encryption scheme from lattice</a></div><div><b>Author(s):&nbsp;</b>Xiaoling Yu, Chungen Xu...Lin Mei</div><div><b>Pages:&nbsp;</b>11139 - 11151</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07246-x\">Deep transfer learning techniques with hybrid optimization in early prediction and diagnosis of different types of oral cancer</a></div><div><b>Author(s):&nbsp;</b>Khushboo Bansal, R. K. Bathla, Yogesh Kumar</div><div><b>Pages:&nbsp;</b>11153 - 11184</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07430-z\">Use of calibration constraints and linear moments for variance estimation under stratified adaptive cluster sampling</a></div><div><b>Author(s):&nbsp;</b>Usman Shahzad, Ishfaq Ahmad...Troon J. Benedict</div><div><b>Pages:&nbsp;</b>11185 - 11196</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07439-4\">Hybrid data selection with preservation rough sets</a></div><div><b>Author(s):&nbsp;</b>Yenny Villuendas-Rey</div><div><b>Pages:&nbsp;</b>11197 - 11223</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07467-0\">A note on multi-criteria decision-making using a complete ranking of generalized trapezoidal fuzzy numbers</a></div><div><b>Author(s):&nbsp;</b>S Jeevaraj</div><div><b>Pages:&nbsp;</b>11225 - 11230</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07473-2\">Matroidal structures on S-approximation spaces</a></div><div><b>Author(s):&nbsp;</b>Xiaonan Li, Yue Chen</div><div><b>Pages:&nbsp;</b>11231 - 11242</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07502-0\">Entropy measurement for a hybrid information system with images: an application in attribute reduction</a></div><div><b>Author(s):&nbsp;</b>Zhaowen Li, Yiying Chen...Ningxin Xie</div><div><b>Pages:&nbsp;</b>11243 - 11263</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07351-x\">Maximum entropy of random permutation set</a></div><div><b>Author(s):&nbsp;</b>Jixiang Deng, Yong Deng</div><div><b>Pages:&nbsp;</b>11265 - 11275</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07372-6\">Skew polynomial superrings</a></div><div><b>Author(s):&nbsp;</b>Surdive Atamewoue Tsafack, Shiping Wen...Yuming Feng</div><div><b>Pages:&nbsp;</b>11277 - 11286</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07382-4\">On level spaces of fuzzy bitopological spaces</a></div><div><b>Author(s):&nbsp;</b>M. Kameswari, S. A. Mohiuddine...N. Anbazhagan</div><div><b>Pages:&nbsp;</b>11287 - 11293</div><div><br /></div><div><b>13) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07381-5\">A way to proper generalization of \u03d5-divergence based on Choquet derivatives</a></div><div><b>Author(s):&nbsp;</b>Zuzana Ontkovi\u010dov\u00e1, Jozef Kise\u013e\u00e1k</div><div><b>Pages:&nbsp;</b>11295 - 11314</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07419-8\">An elite approach for enhancement of LVRT in doubly fed induction generator (DFIG)-based wind energy conversion system (WECS): a FAMSANFIS approach</a></div><div><b>Author(s):&nbsp;</b>Gangikunta Manohar, Sonnati Venkateshwarlu, Askani JayaLaxmi</div><div><b>Pages:&nbsp;</b>11315 - 11337</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07423-y\">Kinematics and trajectory planning analysis based on hybrid optimization algorithms for an industrial robotic manipulators</a></div><div><b>Author(s):&nbsp;</b>Gurjeet Singh, Vijay Kumar Banga</div><div><b>Pages:&nbsp;</b>11339 - 11372</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07429-6\">Some fuzzy Korovkin type approximation theorems via power series summability method</a></div><div><b>Author(s):&nbsp;</b>B. Baxhaku, P. N. Agrawal, R. Shukla</div><div><b>Pages:&nbsp;</b>11373 - 11379</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07434-9\">An extensive operational law for monotone functions of LR fuzzy intervals with applications to fuzzy optimization</a></div><div><b>Author(s):&nbsp;</b>Mingxuan Zhao, Yulin Han...Jian Zhou</div><div><b>Pages:&nbsp;</b>11381 - 11401</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07438-5\">Intuitionistic fuzzy c control charts based on intuitionistic fuzzy ranking method for TIFNs</a></div><div><b>Author(s):&nbsp;</b>G\u00fcltekin Atalik, Sevil Senturk</div><div><b>Pages:&nbsp;</b>11403 - 11407</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07321-3\">An incremental algorithm for concept lattice based on structural similarity index</a></div><div><b>Author(s):&nbsp;</b>Yu Hu, Yan Zhu Hu...Jia Feng Chai</div><div><b>Pages:&nbsp;</b>11409 - 11423</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07474-1\">Interval-based non-dimensionalization method (IBNM) and its application</a></div><div><b>Author(s):&nbsp;</b>Tianjiao Xu, Shihong Chen...Huaping Guan</div><div><b>Pages:&nbsp;</b>11425 - 11434</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07420-1\">Prediction of gestational diabetes based on explainable deep learning and fog computing</a></div><div><b>Author(s):&nbsp;</b>Nora El-Rashidy, Nesma E. ElSayed...Fatma M. Talaat</div><div><b>Pages:&nbsp;</b>11435 - 11450</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07422-z\">An APPSO\u2013SVM approach building the monitoring model of dam safety</a></div><div><b>Author(s):&nbsp;</b>Zhiping Wen, Zhendong Fan, Huaizhi Su</div><div><b>Pages:&nbsp;</b>11451 - 11459</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07432-x\">Coarse-to-fine spatial-channel-boundary attention network for image copy-move forgery detection</a></div><div><b>Author(s):&nbsp;</b>Jun-Liu Zhong, Ji-Xiang Yang...Hua Zeng</div><div><b>Pages:&nbsp;</b>11461 - 11478</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07440-x\">A comprehensive social matrix factorization for recommendations with prediction and feedback mechanisms by fusing trust relationships and social tags</a></div><div><b>Author(s):&nbsp;</b>Rui Chen, Jian-wei Zhang...Hui Liang</div><div><b>Pages:&nbsp;</b>11479 - 11496</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07441-w\">OKC classifier: an efficient approach for classification of imbalanced dataset using hybrid methodology</a></div><div><b>Author(s):&nbsp;</b>Ashok Kumar Bathla, Shally Bansal, Munish Kumar</div><div><b>Pages:&nbsp;</b>11497 - 11503</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07389-x\">A balanced butterfly optimization algorithm for numerical optimization and feature selection</a></div><div><b>Author(s):&nbsp;</b>Wen Long, Jianjun Jiao...Shaohong Cai</div><div><b>Pages:&nbsp;</b>11505 - 11523</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07408-x\">Mehar approach to solve fuzzy linear fractional transportation problems</a></div><div><b>Author(s):&nbsp;</b>Tanveen Kaur Bhatia, Amit Kumar, Mahesh Kumar Sharma</div><div><b>Pages:&nbsp;</b>11525 - 11551</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07409-w\">Maritime anomaly detection based on a support vector machine</a></div><div><b>Author(s):&nbsp;</b>Zhaokun Wei, Xinlian Xie, Xiaoju Zhang</div><div><b>Pages:&nbsp;</b>11553 - 11566</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07425-w\">Parameter estimation of three diode solar PV cell using chaotic dragonfly algorithm</a></div><div><b>Author(s):&nbsp;</b>Manish Kumar Singla, Parag Nijhawan, Amandeep Singh Oberoi</div><div><b>Pages:&nbsp;</b>11567 - 11598</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07427-8\">Visualization of occipital lobe and zygomatic arch of brain region through non-linear perspective projection using DCO algorithm</a></div><div><b>Author(s):&nbsp;</b>R. Partheepan, J. Raja Paul Perinbam...B. Chinthamani</div><div><b>Pages:&nbsp;</b>11599 - 11610</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07442-9\">Fixed-charge solid transportation problem with budget constraints based on carbon emission in neutrosophic environment</a></div><div><b>Author(s):&nbsp;</b>Shyamali Ghosh, Sankar Kumar Roy, Jos\u00e9 Luis Verdegay</div><div><b>Pages:&nbsp;</b>11611 - 11625</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07459-0\">On the exact l1 penalty function method for convex nonsmooth optimization problems with fuzzy objective function</a></div><div><b>Author(s):&nbsp;</b>Tadeusz Antczak</div><div><b>Pages:&nbsp;</b>11627 - 11643</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07466-1\">Comparing the performances of six nature-inspired algorithms on a real-world discrete optimization problem</a></div><div><b>Author(s):&nbsp;</b>Huseyin Hakli, Harun Uguz, Zeynep Ortacay</div><div><b>Pages:&nbsp;</b>11645 - 11667</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07470-5\">Opposition-based learning multi-verse optimizer with disruption operator for optimization problems</a></div><div><b>Author(s):&nbsp;</b>Mohammad Shehab, Laith Abualigah</div><div><b>Pages:&nbsp;</b>11669 - 11693</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07489-8\">A hybrid genetic-particle swarm optimization algorithm for multi-constraint optimization problems</a></div><div><b>Author(s):&nbsp;</b>Bosong Duan, Chuangqiang Guo, Hong Liu</div><div><b>Pages:&nbsp;</b>11695 - 11711</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07339-7\">Integrating prospect theory with variable reference point into the conversion-based framework for linear ordinal ranking aggregation</a></div><div><b>Author(s):&nbsp;</b>Nana Liu, Zeshui Xu...Peijia Ren</div><div><b>Pages:&nbsp;</b>11713 - 11732</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07369-1\">The online website privacy disclosure behavior of users based on concerns-outcomes model</a></div><div><b>Author(s):&nbsp;</b>X. I. E. WeihongZhang Qian</div><div><b>Pages:&nbsp;</b>11733 - 11747</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07380-6\">Public health events emergency management supervision strategy considering citizens\u2019 and new media\u2019s different ways of participation</a></div><div><b>Author(s):&nbsp;</b>Bingjie Lu, Lilong Zhu</div><div><b>Pages:&nbsp;</b>11749 - 11769</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07426-9\">A Multi-criteria approach for public tenders. ELECTRE III and Parsimonious AHP: a comparative study</a></div><div><b>Author(s):&nbsp;</b>Gerarda Fattoruso, Gabriella Marcarelli</div><div><b>Pages:&nbsp;</b>11771 - 11781</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07428-7\">Recognition of DDoS attacks based on images correlation analysis within deep learning framework</a></div><div><b>Author(s):&nbsp;</b>Hengchang Jing, Jian Wang</div><div><b>Pages:&nbsp;</b>11783 - 11794</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06951-x\">MS-ADR: predicting drug\u2013drug adverse reactions base on multi-source heterogeneous convolutional signed network</a></div><div><b>Author(s):&nbsp;</b>Luhe Zhuang, Hong Wang...Hui Zhang</div><div><b>Pages:&nbsp;</b>11795 - 11807</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06959-3\">Self-learning differential evolution algorithm for scheduling of internal tasks in cross-docking</a></div><div><b>Author(s):&nbsp;</b>Dollaya Buakum, Warisa Wisittipanich</div><div><b>Pages:&nbsp;</b>11809 - 11826</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06963-7\">Neural-network-based interval grey prediction models with applications to forecasting the demand of printed circuit boards</a></div><div><b>Author(s):&nbsp;</b>Yi-Chung Hu</div><div><b>Pages:&nbsp;</b>11827 - 11838</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06966-4\">Application of Mamdani model-based fuzzy inference system in water consumption estimation using time series</a></div><div><b>Author(s):&nbsp;</b>H. J. Surendra, P. C. Deka, H. N. Rajakumara</div><div><b>Pages:&nbsp;</b>11839 - 11847</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06981-5\">Decision-making analysis based on hesitant fuzzy N-soft ELECTRE-I approach</a></div><div><b>Author(s):&nbsp;</b>Arooj Adeel, Muhammad Akram, Naim \u00c7a\u01e7man</div><div><b>Pages:&nbsp;</b>11849 - 11863</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06987-z\">Modified hybrid combination synchronization of chaotic fractional order systems</a></div><div><b>Author(s):&nbsp;</b>Kayode S. Ojo, Samuel T. Ogunjo, Ibiyinka A. Fuwape</div><div><b>Pages:&nbsp;</b>11865 - 11872</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07001-2\">Compressive strength evaluation of concrete confined with spiral stirrups by using adaptive neuro-fuzzy inference system (ANFIS)</a></div><div><b>Author(s):&nbsp;</b>Wei Chang, Wenzhong Zheng</div><div><b>Pages:&nbsp;</b>11873 - 11889</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07007-w\">An enhanced approach for optimizing mathematical and structural problems by combining PSO, GSA and gradient directions</a></div><div><b>Author(s):&nbsp;</b>Farsad Salajegheh, Eysa Salajegheh, Saeed Shojaee</div><div><b>Pages:&nbsp;</b>11891 - 11913</div><div><br /></div><div><b>49) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07020-z\">Stratified hyperparameters optimization of feed-forward neural network for social network spam detection (SON2S)</a></div><div><b>Author(s):&nbsp;</b>E. Elakkiya, S. Selvakumar</div><div><b>Pages:&nbsp;</b>11915 - 11934</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07028-5\">Wavelet packet and fuzzy logic theory for automatic fault detection in induction motor</a></div><div><b>Author(s):&nbsp;</b>Hicham Talhaoui, Tarek Ameid...Abdelhalim Kessal</div><div><b>Pages:&nbsp;</b>11935 - 11949</div><div><br /></div></div>",
            "pubdate": "2022-10-14T14:32:00.000+13:00",
            "pubdate_parsed": [
                2022,
                10,
                14
            ],
            "email_sent": true
        },
        "IEEE Transactions on Neural Networks and Learning Systems, Volume 33, Issue 10, October 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/10/ieee-transactions-on-neural-networks.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9410437/\">A Survey of Deep Learning on CPUs: Opportunities and Co-Optimizations</a></div><div><b>Author(s): </b>Sparsh Mittal, Poonam Rajput, Sreenivas Subramoney</div><div><b>Pages: </b>5095 - 5115</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9399692/\">Hash Bit Selection via Collaborative Neurodynamic Optimization With Discrete Hopfield Networks</a></div><div><b>Author(s):&nbsp;</b>Xinqi Li, Jun Wang, Sam Kwong</div><div><b>Pages:&nbsp;</b>5116 - 5124</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9404857/\">Neural Time-Aware Sequential Recommendation by Jointly Modeling Preference Dynamics and Explicit Feature Couplings</a></div><div><b>Author(s):&nbsp;</b>Qi Zhang, Longbing Cao, Chongyang Shi, Zhendong Niu</div><div><b>Pages:&nbsp;</b>5125 - 5137</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9395183/\">Two-Stage Bayesian Optimization for Scalable Inference in State-Space Models</a></div><div><b>Author(s):&nbsp;</b>Mahdi Imani, Seyede Fatemeh Ghoreishi</div><div><b>Pages:&nbsp;</b>5138 - 5149</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9397867/\">Multigraph Transformer for Free-Hand Sketch Recognition</a></div><div><b>Author(s):&nbsp;</b>Peng Xu, Chaitanya K. Joshi, Xavier Bresson</div><div><b>Pages:&nbsp;</b>5150 - 5161</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9397437/\">Supervised Learning for Nonsequential Data: A Canonical Polyadic Decomposition Approach</a></div><div><b>Author(s):&nbsp;</b>Alexandros Haliassos, Kriton Konstantinidis, Danilo P. Mandic</div><div><b>Pages:&nbsp;</b>5162 - 5176</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9399655/\">Multiview Subspace Clustering via Co-Training Robust Data Representation</a></div><div><b>Author(s):&nbsp;</b>Jiyuan Liu, Xinwang Liu, Yuexiang Yang, Xifeng Guo, Marius Kloft, Liangzhong He</div><div><b>Pages:&nbsp;</b>5177 - 5189</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9399171/\">Training-Free Deep Generative Networks for Compressed Sensing of Neural Action Potentials</a></div><div><b>Author(s):&nbsp;</b>Biao Sun, Chaoxu Mu, Zexu Wu, Xinshan Zhu</div><div><b>Pages:&nbsp;</b>5190 - 5199</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9404318/\">Effective Collaborative Representation Learning for Multilabel Text Categorization</a></div><div><b>Author(s):&nbsp;</b>Hao Wu, Shaowei Qin, Rencan Nie, Jinde Cao, Sergey Gorbachev</div><div><b>Pages:&nbsp;</b>5200 - 5214</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9400417/\">Spike-Timing-Dependent Plasticity With Activation-Dependent Scaling for Receptive Fields Development</a></div><div><b>Author(s):&nbsp;</b>Marcin Bia\u0142as, Jacek Ma\u0144dziuk</div><div><b>Pages:&nbsp;</b>5215 - 5228</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9404328/\">Reinforcement Learning-Based Cooperative Optimal Output Regulation via Distributed Adaptive Internal Model</a></div><div><b>Author(s):&nbsp;</b>Weinan Gao, Mohammed Mynuddin, Donald C. Wunsch, Zhong-Ping Jiang</div><div><b>Pages:&nbsp;</b>5229 - 5240</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9399168/\">Event-Triggered Adaptive Neural Network Sensor Failure Compensation for Switched Interconnected Nonlinear Systems With Unknown Control Coefficients</a></div><div><b>Author(s):&nbsp;</b>Jing Zhang, Zhengrong Xiang</div><div><b>Pages:&nbsp;</b>5241 - 5252</div><div><br /></div><div><b>13)</b> <a href=\"https://computational-intelligence.blogspot.com/feeds/posts/Publication Year: 2022,Page(s):https:/ieeexplore.ieee.org/document/9399174/\">General Bitwidth Assignment for Efficient Deep Convolutional Neural Network Quantization</a></div><div><b>Author(s):&nbsp;</b>Wen Fei, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</div><div><b>Pages:&nbsp;</b>5253-5267</div><div><br /></div><div><b>14) </b><a href=\"https://ieeexplore.ieee.org/document/9399170/\">Finite-Time Synchronization of Markovian Coupled Neural Networks With Delays via Intermittent Quantized Control: Linear Programming Approach</a></div><div><b>Author(s):&nbsp;</b>Rongqiang Tang, Housheng Su, Yi Zou, Xinsong Yang</div><div><b>Pages:&nbsp;</b>5268 - 5278</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9399169/\">EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Hojjat Salehinejad, Shahrokh Valaee</div><div><b>Pages:&nbsp;</b>5279 - 5292</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9399689/\">Multi-Source Contribution Learning for Domain Adaptation</a></div><div><b>Author(s):&nbsp;</b>Keqiuyin Li, Jie Lu, Hua Zuo, Guangquan Zhang</div><div><b>Pages:&nbsp;</b>5293 - 5307</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9404309/\">Prototype-Based Multisource Domain Adaptation</a></div><div><b>Author(s):&nbsp;</b>Lihua Zhou, Mao Ye, Dan Zhang, Ce Zhu, Luping Ji</div><div><b>Pages:&nbsp;</b>5308 - 5320</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9404313/\">Uniform Stability of Complex-Valued Neural Networks of Fractional Order With Linear Impulses and Fixed Time Delays</a></div><div><b>Author(s):&nbsp;</b>Hui Li, Yonggui Kao, Haibo Bao, Yangquan Chen</div><div><b>Pages:&nbsp;</b>5321 - 5331</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9404315/\">Discriminative Multi-View Dynamic Image Fusion for Cross-View 3-D Action Recognition</a></div><div><b>Author(s):&nbsp;</b>Yancheng Wang, Yang Xiao, Junyi Lu, Bo Tan, Zhiguo Cao, Zhenjun Zhang, Joey Tianyi Zhou</div><div><b>Pages:&nbsp;</b>5332 - 5345</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9404323/\">Face Sketch Synthesis Using Regularized Broad Learning System</a></div><div><b>Author(s):&nbsp;</b>Ping Li, Bin Sheng, C. L. Philip Chen</div><div><b>Pages:&nbsp;</b>5346 - 5360</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9432789/\">Hierarchical Multiagent Reinforcement Learning for Allocating Guaranteed Display Ads</a></div><div><b>Author(s):&nbsp;</b>Lu Wang, Lei Han, Xinru Chen, Chengchang Li, Junzhou Huang, Weinan Zhang, Wei Zhang, Xiaofeng He, Dijun Luo</div><div><b>Pages:&nbsp;</b>5361 - 5373</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9410247/\">Multiagent Meta-Reinforcement Learning for Adaptive Multipath Routing Optimization</a></div><div><b>Author(s):&nbsp;</b>Long Chen, Bin Hu, Zhi-Hong Guan, Lian Zhao, Xuemin Shen</div><div><b>Pages:&nbsp;</b>5374 - 5386</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9404870/\">Nonblind Image Deblurring via Deep Learning in Complex Field</a></div><div><b>Author(s):&nbsp;</b>Yuhui Quan, Peikang Lin, Yong Xu, Yuesong Nan, Hui Ji</div><div><b>Pages:&nbsp;</b>5387 - 5400</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9408408/\">Joint Learning of Neural Transfer and Architecture Adaptation for Image Recognition</a></div><div><b>Author(s):&nbsp;</b>Guangrun Wang, Liang Lin, Rongcong Chen, Guangcong Wang, Jiqi Zhang</div><div><b>Pages:&nbsp;</b>5401 - 5415</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9404321/\">Adaptive NN Finite-Time Resilient Control for Nonlinear Time-Delay Systems With Unknown False Data Injection and Actuator Faults</a></div><div><b>Author(s):&nbsp;</b>Shuai Song, Ju H. Park, Baoyong Zhang, Xiaona Song</div><div><b>Pages:&nbsp;</b>5416 - 5428</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9404854/\">Deep Learning-Based 2-D Frequency Estimation of Multiple Sinusoidals</a></div><div><b>Author(s):&nbsp;</b>Pingping Pan, Yunjian Zhang, Zhenmiao Deng, Wei Qi</div><div><b>Pages:&nbsp;</b>5429 - 5440</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9404312/\">Simultaneous State and Unknown Input Estimation for Complex Networks With Redundant Channels Under Dynamic Event-Triggered Mechanisms</a></div><div><b>Author(s):&nbsp;</b>Qi Li, Zidong Wang, Jun Hu, Weiguo Sheng</div><div><b>Pages:&nbsp;</b>5441 - 5451</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9406189/\">Continuation Multiple Instance Learning for Weakly and Fully Supervised Object Detection</a></div><div><b>Author(s):&nbsp;</b>Qixiang Ye, Fang Wan, Chang Liu, Qingming Huang, Xiangyang Ji</div><div><b>Pages:&nbsp;</b>5452 - 5466</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9404856/\">A Separation-Based Methodology to Consensus Tracking of Switched High-Order Nonlinear Multiagent Systems</a></div><div><b>Author(s):&nbsp;</b>Maolong Lv, Wenwu Yu, Jinde Cao, Simone Baldi</div><div><b>Pages:&nbsp;</b>5467 - 5479</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9404310/\">Degradation Alignment in Remaining Useful Life Prediction Using Deep Cycle-Consistent Learning</a></div><div><b>Author(s):&nbsp;</b>Xiang Li, Wei Zhang, Hui Ma, Zhong Luo, Xu Li</div><div><b>Pages:&nbsp;</b>5480 - 5491</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9406172/\">Adaptive Observation-Based Efficient Reinforcement Learning for Uncertain Systems</a></div><div><b>Author(s):&nbsp;</b>Maopeng Ran, Lihua Xie</div><div><b>Pages:&nbsp;</b>5492 - 5503</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9406180/\">Learning-Based Distributed Resilient Fault-Tolerant Control Method for Heterogeneous MASs Under Unknown Leader Dynamic</a></div><div><b>Author(s):&nbsp;</b>Chao Deng, Xiao-Zheng Jin, Wei-Wei Che, Hai Wang</div><div><b>Pages:&nbsp;</b>5504 - 5513</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9403414/\">A High-Efficient Hybrid Physics-Informed Neural Networks Based on Convolutional Neural Network</a></div><div><b>Author(s):&nbsp;</b>Zhiwei Fang</div><div><b>Pages:&nbsp;</b>5514 - 5526</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9409779/\">Design and Analysis of Data-Driven Learning Control: An Optimization-Based Approach</a></div><div><b>Author(s):&nbsp;</b>Deyuan Meng, Jingyao Zhang</div><div><b>Pages:&nbsp;</b>5527 - 5541</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9404859/\">Improved Results on Fixed-/Preassigned-Time Synchronization for Memristive Complex-Valued Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Qintao Gan, Liangchen Li, Jing Yang, Yan Qin, Mingqiang Meng</div><div><b>Pages:&nbsp;</b>5542 - 5556</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9404853/\">Multipixel Anomaly Detection With Unknown Patterns for Hyperspectral Imagery</a></div><div><b>Author(s):&nbsp;</b>Jun Liu, Zengfu Hou, Wei Li, Ran Tao, Danilo Orlando, Hongbin Li</div><div><b>Pages:&nbsp;</b>5557 - 5567</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9405431/\">A Novel Formulation of Trace Ratio Linear Discriminant Analysis</a></div><div><b>Author(s):&nbsp;</b>Jingyu Wang, Lin Wang, Feiping Nie, Xuelong Li</div><div><b>Pages:&nbsp;</b>5568 - 5578</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9406173/\">Consensus-Based Cooperative Algorithms for Training Over Distributed Data Sets Using Stochastic Gradients</a></div><div><b>Author(s):&nbsp;</b>Zhongguo Li, Bo Liu, Zhengtao Ding</div><div><b>Pages:&nbsp;</b>5579 - 5589</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9420275/\">Maximum A Posteriori Approximation of Hidden Markov Models for Proportional Sequential Data Modeling With Simultaneous Feature Selection</a></div><div><b>Author(s):&nbsp;</b>Samr Ali, Nizar Bouguila</div><div><b>Pages:&nbsp;</b>5590 - 5601</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9410407/\">Large-Scale Multiagent System Tracking Control Using Mean Field Games</a></div><div><b>Author(s):&nbsp;</b>Zejian Zhou, Hao Xu</div><div><b>Pages:&nbsp;</b>5602 - 5610</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9406190/\">Heterogeneous Face Interpretable Disentangled Representation for Joint Face Recognition and Synthesis</a></div><div><b>Author(s):&nbsp;</b>Decheng Liu, Xinbo Gao, Chunlei Peng, Nannan Wang, Jie Li</div><div><b>Pages:&nbsp;</b>5611 - 5625</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9416240/\">Class-Imbalanced Deep Learning via a Class-Balanced Ensemble</a></div><div><b>Author(s):&nbsp;</b>Zhi Chen, Jiang Duan, Li Kang, Guoping Qiu</div><div><b>Pages:&nbsp;</b>5626 - 5640</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9404848/\">Domain Adaptation Preconceived Hashing for Unconstrained Visual Retrieval</a></div><div><b>Author(s):&nbsp;</b>Fuxiang Huang, Lei Zhang, Xinbo Gao</div><div><b>Pages:&nbsp;</b>5641 - 5655</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9423873/\">Unified Analysis on the Global Dissipativity and Stability of Fractional-Order Multidimension-Valued Memristive Neural Networks With Time Delay</a></div><div><b>Author(s):&nbsp;</b>Jianying Xiao, Shouming Zhong, Shiping Wen</div><div><b>Pages:&nbsp;</b>5656 - 5665</div><div><br /></div><div><b>45)</b> <a href=\"https://ieeexplore.ieee.org/document/9420270/\">Learning Deep Context-Sensitive Decomposition for Low-Light Image Enhancement</a></div><div><b>Author(s):&nbsp;</b>Long Ma, Risheng Liu, Jiaao Zhang, Xin Fan, Zhongxuan Luo</div><div><b>Pages:&nbsp;</b>5666 - 5680</div><div><br /></div><div><b>46)</b> <a href=\"https://ieeexplore.ieee.org/document/9410431/\">A Decoder-Free Variational Deep Embedding for Unsupervised Clustering</a></div><div><b>Author(s):&nbsp;</b>Qiang Ji, Yanfeng Sun, Junbin Gao, Yongli Hu, Baocai Yin</div><div><b>Pages:&nbsp;</b>5681 - 5693</div><div><br /></div><div><b>47)</b> <a href=\"https://ieeexplore.ieee.org/document/9404864/\">Data-Driven Designs of Fault Detection Systems via Neural Network-Aided Learning</a></div><div><b>Author(s):&nbsp;</b>Hongtian Chen, Zheng Chai, Oguzhan Dogru, Bin Jiang, Biao Huang</div><div><b>Pages:&nbsp;</b>5694 - 5705</div><div><br /></div><div><b>48)</b> <a href=\"https://ieeexplore.ieee.org/document/9406178/\">Prediction With Unpredictable Feature Evolution</a></div><div><b>Author(s):&nbsp;</b>Bo-Jian Hou, Lijun Zhang, Zhi-Hua Zhou</div><div><b>Pages:&nbsp;</b>5706 - 5715</div><div><br /></div><div><b>49)</b> <a href=\"https://ieeexplore.ieee.org/document/9408232/\">Spherical Formation Tracking Control of Nonlinear Second-Order Agents With Adaptive Neural Flow Estimate</a></div><div><b>Author(s):&nbsp;</b>Yang-Yang Chen, Rong Huang, Yanteng Ge, Ya Zhang</div><div><b>Pages:&nbsp;</b>5716 - 5727</div><div><br /></div><div><b>50)</b> <a href=\"https://ieeexplore.ieee.org/document/9405414/\">Learning Gaussian\u2013Bernoulli RBMs Using Difference of Convex Functions Optimization</a></div><div><b>Author(s):&nbsp;</b>Vidyadhar Upadhya, P. S. Sastry</div><div><b>Pages:&nbsp;</b>5728 - 5738</div><div><br /></div><div><b>51)</b> <a href=\"https://ieeexplore.ieee.org/document/9406169/\">Knowledge-Based Prediction of Network Controllability Robustness</a></div><div><b>Author(s):&nbsp;</b>Yang Lou, Yaodong He, Lin Wang, Kim Fung Tsang, Guanrong Chen</div><div><b>Pages:&nbsp;</b>5739 - 5750</div><div><br /></div><div><b>52)</b> <a href=\"https://ieeexplore.ieee.org/document/9405447/\">Deep Feature Aggregation Framework Driven by Graph Convolutional Network for Scene Classification in Remote Sensing</a></div><div><b>Author(s):&nbsp;</b>Kejie Xu, Hong Huang, Peifang Deng, Yuan Li</div><div><b>Pages:&nbsp;</b>5751 - 5765</div><div><br /></div><div><b>53)</b> <a href=\"https://ieeexplore.ieee.org/document/9410288/\">On the Rates of Convergence From Surrogate Risk Minimizers to the Bayes Optimal Classifier</a></div><div><b>Author(s):&nbsp;</b>Jingwei Zhang, Tongliang Liu, Dacheng Tao</div><div><b>Pages:&nbsp;</b>5766 - 5774</div><div><br /></div><div><b>54)</b> <a href=\"https://ieeexplore.ieee.org/document/9411671/\">An L1-and-L2-Norm-Oriented Latent Factor Model for Recommender Systems</a></div><div><b>Author(s):&nbsp;</b>Di Wu, Mingsheng Shang, Xin Luo, Zidong Wang</div><div><b>Pages:&nbsp;</b>5775 - 5788</div><div><br /></div><div><b>55)</b> <a href=\"https://ieeexplore.ieee.org/document/9408405/\">Deep Mixture Generative Autoencoders</a></div><div><b>Author(s):&nbsp;</b>Fei Ye, Adrian G. Bors</div><div><b>Pages:&nbsp;</b>5789 - 5803</div><div><br /></div><div><b>56)</b> <a href=\"https://ieeexplore.ieee.org/document/9406188/\">Stability and Synchronization of Nonautonomous Reaction\u2013Diffusion Neural Networks With General Time-Varying Delays</a></div><div><b>Author(s):&nbsp;</b>Hao Zhang, Zhigang Zeng</div><div><b>Pages:&nbsp;</b>5804 - 5817</div><div><br /></div><div><b>57)</b> <a href=\"https://ieeexplore.ieee.org/document/9408409/\">Weakly Supervised Domain Adaptation for Aspect Extraction via Multilevel Interaction Transfer</a></div><div><b>Author(s):&nbsp;</b>Tao Liang, Wenya Wang, Fengmao Lv</div><div><b>Pages:&nbsp;</b>5818 - 5829</div><div><br /></div><div><b>58)</b> <a href=\"https://ieeexplore.ieee.org/document/9406192/\">Decentralized Event-Driven Constrained Control Using Adaptive Critic Designs</a></div><div><b>Author(s):&nbsp;</b>Xiong Yang, Yuanheng Zhu, Na Dong, Qinglai Wei</div><div><b>Pages:&nbsp;</b>5830 - 5844</div><div><br /></div><div><b>59)</b> <a href=\"https://ieeexplore.ieee.org/document/9411732/\">Whitening-Net: A Generalized Network to Diagnose the Faults Among Different Machines and Conditions</a></div><div><b>Author(s):&nbsp;</b>Jie Li, Yu Wang, Yanyang Zi, Zhijie Zhang</div><div><b>Pages:&nbsp;</b>5845 - 5858</div><div><br /></div><div><b>60)</b> <a href=\"https://ieeexplore.ieee.org/document/9410401/\">Adaptive Data Structure Regularized Multiclass Discriminative Feature Selection</a></div><div><b>Author(s):&nbsp;</b>Mingyu Fan, Xiaoqin Zhang, Jie Hu, Nannan Gu, Dacheng Tao</div><div><b>Pages:&nbsp;</b>5859 - 5872</div><div><br /></div><div><b>61)</b> <a href=\"https://ieeexplore.ieee.org/document/9424995/\">Reinforcement Learning Control of Robotic Knee With Human-in-the-Loop by Flexible Policy Iteration</a></div><div><b>Author(s):&nbsp;</b>Xiang Gao, Jennie Si, Yue Wen, Minhan Li, He Huang</div><div><b>Pages:&nbsp;</b>5873 - 5887</div><div><br /></div><div><b>62)</b> <a href=\"https://ieeexplore.ieee.org/document/9410432/\">Explicit Metric-Based Multiconcept Multi-Instance Learning With Triplet and Superbag</a></div><div><b>Author(s):&nbsp;</b>Ziqiu Chi, Zhe Wang, Wenli Du</div><div><b>Pages:&nbsp;</b>5888 - 5897</div><div><br /></div><div><b>63)</b> <a href=\"https://ieeexplore.ieee.org/document/9424959/\">Learning With Label Proportions by Incorporating Unmarked Data</a></div><div><b>Author(s):&nbsp;</b>Jing Chai, Ivor W. Tsang</div><div><b>Pages:&nbsp;</b>5898 - 5912</div><div><br /></div><div><b>64)</b> <a href=\"https://ieeexplore.ieee.org/document/9409784/\">Graph-Based Bayesian Optimization for Large-Scale Objective-Based Experimental Design</a></div><div><b>Author(s):&nbsp;</b>Mahdi Imani, Seyede Fatemeh Ghoreishi</div><div><b>Pages:&nbsp;</b>5913 - 5925</div><div><br /></div><div><b>65)</b> <a href=\"https://ieeexplore.ieee.org/document/9410433/\">Neuro-Evolutionary Direct Policy Search for Multiobjective Optimal Control</a></div><div><b>Author(s):&nbsp;</b>Marta Zaniolo, Matteo Giuliani, Andrea Castelletti</div><div><b>Pages:&nbsp;</b>5926 - 5938</div><div><br /></div><div><b>66)</b> <a href=\"https://ieeexplore.ieee.org/document/9416238/\">Temporal Coding in Spiking Neural Networks With Alpha Synaptic Function: Learning With Backpropagation</a></div><div><b>Author(s):&nbsp;</b>Iulia-Maria Com\u015fa, Krzysztof Potempa, Luca Versari, Thomas Fischbacher, Andrea Gesmundo, Jyrki Alakuijala</div><div><b>Pages:&nbsp;</b>5939 - 5952</div><div><br /></div><div><b>67)</b> <a href=\"https://ieeexplore.ieee.org/document/9411738/\">Noise Robust Face Hallucination Based on Smooth Correntropy Representation</a></div><div><b>Author(s):&nbsp;</b>Licheng Liu, Qiying Feng, C. L. Philip Chen, Yaonan Wang</div><div><b>Pages:&nbsp;</b>5953 - 5965</div><div><br /></div><div><b>68)</b> <a href=\"https://ieeexplore.ieee.org/document/9422177/\">Memory-Efficient Class-Incremental Learning for Image Classification</a></div><div><b>Author(s):&nbsp;</b>Hanbin Zhao, Hui Wang, Yongjian Fu, Fei Wu;</div><div>Xi Li</div><div><b>Pages:&nbsp;</b>5966 - 5977</div><div><br /></div><div><b>69)</b> <a href=\"https://ieeexplore.ieee.org/document/9496282/\">IVS-Caffe\u2014Hardware-Oriented Neural Network Model Development</a></div><div><b>Author(s):&nbsp;</b>Chia-Chi Tsai, Jiun-In Guo</div><div><b>Pages:&nbsp;</b>5978 - 5992</div><div><br /></div><div><b>70)</b> <a href=\"https://ieeexplore.ieee.org/document/9411670/\">Inferring Effective Connectivity Networks From fMRI Time Series With a Temporal Entropy-Score</a></div><div><b>Author(s):&nbsp;</b>Jinduo Liu, Junzhong Ji, Guangxu Xun, Aidong Zhang</div><div><b>Pages:&nbsp;</b>5993 - 6006</div><div><br /></div><div><b>71)</b> <a href=\"https://ieeexplore.ieee.org/document/9399651/\">Synchronization of Chaotic Neural Networks: Average-Delay Impulsive Control</a></div><div><b>Author(s):&nbsp;</b>Bangxin Jiang, Jungang Lou, Jianquan Lu, Kaibo Shi</div><div><b>Pages:&nbsp;</b>6007 - 6012</div><div><br /></div><div><b>72)</b> <a href=\"https://ieeexplore.ieee.org/document/9399673/\">Fully Decoupled Neural Network Learning Using Delayed Gradients</a></div><div><b>Author(s):&nbsp;</b>Huiping Zhuang, Yi Wang, Qinglai Liu, Zhiping Lin</div><div><b>Pages:&nbsp;</b>6013 - 6020</div><div><br /></div><div><b>73)</b> <a href=\"https://ieeexplore.ieee.org/document/9419682/\">Neural Embedding Singular Value Decomposition for Collaborative Filtering</a></div><div><b>Author(s):&nbsp;</b>Tianlin Huang, Rujie Zhao, Lvqing Bi, Defu Zhang, Chao Lu</div><div><b>Pages:&nbsp;</b>6021 - 6029</div><div><br /></div><div><b>74)</b> <a href=\"https://ieeexplore.ieee.org/document/9425429/\">Event-Triggered Adaptive Control of Uncertain Nonlinear Systems With Composite Condition</a></div><div><b>Author(s):&nbsp;</b>Xinglan Liu, Bin Xu, Yingxin Shou, Quan-Yong Fan, Yingxue Chen</div><div><b>Pages:&nbsp;</b>6030 - 6037</div><div><br /></div><div><b>75)</b> <a href=\"https://ieeexplore.ieee.org/document/9774865/\">Fast Rates of Gaussian Empirical Gain Maximization With Heavy-Tailed Noise</a></div><div><b>Author(s):&nbsp;</b>Shouyou Huang, Yunlong Feng, Qiang Wu</div><div><b>Pages:&nbsp;</b>6038 - 6043</div><div><br /></div></div>",
            "pubdate": "2022-10-22T19:42:00.000+13:00",
            "pubdate_parsed": [
                2022,
                10,
                22
            ],
            "email_sent": true
        },
        "Evolving Systems, Volume 13, Issue 6, December 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/10/evolving-systems-volume-13-issue-6.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09409-x\">A new optimal prediction technique for energy demand based on CNN and improved water strider algorithm: a study on socio-economic-climatic parameters</a></div><div><b>Author(s): </b>Shimin Liao, Giorgos Jimenez</div><div><b>Pages: </b>759 - 775</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09410-4\">Varying combination of feature extraction and modified support vector machines based prediction of myocardial infarction</a></div><div><b>Author(s):&nbsp;</b>A. Razia Sulthana, A. K. Jaithunbi</div><div><b>Pages:&nbsp;</b>777 - 794</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09413-1\">Dynamics of multi-point singular fifth-order Lane\u2013Emden system with neuro-evolution heuristics</a></div><div><b>Author(s):&nbsp;</b>Zulqurnain Sabir, Mohamed R. Ali...Dumitru Baleanu</div><div><b>Pages:&nbsp;</b>795 - 806</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09426-4\">Deep learning system applicability for rapid glaucoma prediction from fundus images across various data sets</a></div><div><b>Author(s):&nbsp;</b>Law Kumar Singh, Pooja...Munish Khanna</div><div><b>Pages:&nbsp;</b>807 - 836</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-021-09417-x\">A semi-supervised deep rule-based classifier for robust finger knuckle-print verification</a></div><div><b>Author(s):&nbsp;</b>Mounir Benmalek, Abdelouahab Attia...M. Hassaballah</div><div><b>Pages:&nbsp;</b>837 - 848</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09418-4\">Application of hybrid metaheuristic with Levenberg-Marquardt algorithm for 6-dimensional magnetic localization</a></div><div><b>Author(s):&nbsp;</b>Memduh Suveren, Rustu Akay...Muzaffer Kanaan</div><div><b>Pages:&nbsp;</b>849 - 867</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09423-7\">Correlation-based modified long short-term memory network approach for software defect prediction</a></div><div><b>Author(s):&nbsp;</b>Suresh Kumar Pemmada, H. S. Behera...Bighnaraj Naik</div><div><b>Pages:&nbsp;</b>869 - 887</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09425-5\">Nature-inspired optimization algorithms and their significance in multi-thresholding image segmentation: an inclusive review</a></div><div><b>Author(s):&nbsp;</b>Rebika Rai, Arunita Das, Krishna Gopal Dhal</div><div><b>Pages:&nbsp;</b>889 - 945</div><div><br /></div></div>",
            "pubdate": "2022-10-23T21:37:00.000+13:00",
            "pubdate_parsed": [
                2022,
                10,
                23
            ],
            "email_sent": true
        },
        "IEEE Transactions on Fuzzy Systems, Volume 30, Issue 11, November 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/11/ieee-transactions-on-fuzzy-systems.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9933713/\">Guest Editorial: Recent Advances in Fuzzy-Based Intelligent IoT and Cyber-Physical Systems</a></div><div><b>Author(s): </b>Mainak Adhikari, Varun G. Menon, Ju H. Park, Danda B. Rawat</div><div><b>Pages: </b>4541 - 4542</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9626570/\">Fuz-Spam: Label Smoothing-Based Fuzzy Detection of Spammers in Internet of Things</a></div><div><b>Author(s):&nbsp;</b>Zhiwei Guo, Keping Yu, Alireza Jolfaei, Feng Ding, Ning Zhang</div><div><b>Pages:&nbsp;</b>4543 - 4554</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9730093/\">Fast Nonsingular Fixed-Time Fuzzy Fault-Tolerant Control for HFVs With Guaranteed Time-Varying Flight State Constraints</a></div><div><b>Author(s):&nbsp;</b>Maolong Lv, Yinghong Li, Lujun Wan, Jiangbin Dai, Jing Chang</div><div><b>Pages:&nbsp;</b>4555 - 4567</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9756858/\">Data-Driven Fuzzy Target-Side Representation for Intelligent Translation System</a></div><div><b>Author(s):&nbsp;</b>Kehai Chen, Muyun Yang, Tiejun Zhao, Min Zhang</div><div><b>Pages:&nbsp;</b>4568 - 4577</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9714825/\">FuzzyAct: A Fuzzy-Based Framework for Temporal Activity Recognition in IoT Applications Using RNN and 3D-DWT</a></div><div><b>Author(s):&nbsp;</b>Fayaz Ali Dharejo, Muhammad Zawish, Yuanchun Zhou, Steven Davy, Kapal Dev, Sunder Ali Khowaja, Yanjie Fu, Nawab Muhammad Faseeh Qureshi</div><div><b>Pages:&nbsp;</b>4578 - 4592</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9737450/\">Game Theory for Distributed IoV Task Offloading With Fuzzy Neural Network in Edge Computing</a></div><div><b>Author(s):&nbsp;</b>Xiaolong Xu, Qinting Jiang, Peiming Zhang, Xuefei Cao, Mohammad R. Khosravi, Linss T. Alex, Lianyong Qi, Wanchun Dou</div><div><b>Pages:&nbsp;</b>4593 - 4604</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9750892/\">Online Intrusion Detection for Internet of Things Systems With Full Bayesian Possibilistic Clustering and Ensembled Fuzzy Classifiers</a></div><div><b>Author(s):&nbsp;</b>Fang-Qi Li, Rui-Jie Zhao, Shi-Lin Wang, Li-Bo Chen, Alan Wee-Chung Liew, Weiping Ding</div><div><b>Pages:&nbsp;</b>4605 - 4617</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9756835/\">Fuzzy Active Learning to Detect OpenCL Kernel Heterogeneous Machines in Cyber Physical Systems</a></div><div><b>Author(s):&nbsp;</b>Usman Ahmed, Jerry Chun-Wei Lin, Gautam Srivastava, M S Mekala, Ho-Youl Jung</div><div><b>Pages:&nbsp;</b>4618 - 4629</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9745612/\">Multiobjective Multiple Mobile Sink Scheduling via Evolutionary Fuzzy Rough Neural Network for Wireless Sensor Networks</a></div><div><b>Author(s):&nbsp;</b>Jianwei Zhao, Bin Cao, Xin Liu, Peng Yang, Amit Kumar Singh, Zhihan Lv</div><div><b>Pages:&nbsp;</b>4630 - 4641</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9772948/\">Side-Channel Fuzzy Analysis-Based AI Model Extraction Attack With Information-Theoretic Perspective in Intelligent IoT</a></div><div><b>Author(s):&nbsp;</b>Qianqian Pan, Jun Wu, Ali Kashif Bashir, Jianhua Li, Jie Wu</div><div><b>Pages:&nbsp;</b>4642 - 4656</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9732224/\">Cloud Risk Management With OWA-LSTM and Fuzzy Linguistic Decision Making</a></div><div><b>Author(s):&nbsp;</b>Walayat Hussain, Muhammad Raheel Raza, Mian Ahmad Jan, Jos\u00e9 M. Merig\u00f3, Honghao Gao</div><div><b>Pages:&nbsp;</b>4657 - 4666</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9721584/\">An Observer-Based Fuzzy Adaptive Consensus Control Method for Nonlinear Multiagent Systems</a></div><div><b>Author(s):&nbsp;</b>Yongming Li, Kewen Li, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>4667 - 4678</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9729498/\">Fuzzy-Model-Based Asynchronous Fault Detection for Markov Jump Systems With Partially Unknown Transition Probabilities: An Adaptive Event-Triggered Approach</a></div><div><b>Author(s):&nbsp;</b>Guangtao Ran, Jian Liu, Chuanjiang Li, Hak-Keung Lam, Dongyu Li, Hongtian Chen</div><div><b>Pages:&nbsp;</b>4679 - 4689</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9729427/\">Unknown Input Functional Observer Design for Discrete-Time Interval Type-2 Takagi\u2013Sugeno Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Yueyang Li, Ming Yuan, Mohammed Chadli, Zi-Peng Wang, Dong Zhao</div><div><b>Pages:&nbsp;</b>4690 - 4701</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9729629/\">Small-Gain Approach to Fuzzy Adaptive Control for Interconnected Systems With Unmodeled Dynamics</a></div><div><b>Author(s):&nbsp;</b>Bo Xu, Yuan-Xin Li, Choon Ki Ahn</div><div><b>Pages:&nbsp;</b>4702 - 4716</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9732219/\">On Fractional Tikhonov Regularization: Application to the Adaptive Network-Based Fuzzy Inference System for Regression Problems</a></div><div><b>Author(s):&nbsp;</b>Stefania Tomasiello, Witold Pedrycz, Vincenzo Loia</div><div><b>Pages:&nbsp;</b>4717 - 4727</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9732660/\">Subpredictor-Based Fuzzy Control Design for a Reaction\u2013Diffusion Equation</a></div><div><b>Author(s):&nbsp;</b>Wen Kang, Jing Zhang, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>4728 - 4740</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9732700/\">Trust-Consensus Multiplex Networks by Combining Trust Social Network Analysis and Consensus Evolution Methods in Group Decision-Making</a></div><div><b>Author(s):&nbsp;</b>Tong Wu, Xinwang Liu, Jindong Qin, Francisco Herrera</div><div><b>Pages:&nbsp;</b>4741 - 4753</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9735344/\">C-Loss Based Higher Order Fuzzy Inference Systems for Identifying DNA N4-Methylcytosine Sites</a></div><div><b>Author(s):&nbsp;</b>Yijie Ding, Prayag Tiwari, Quan Zou, Fei Guo, Hari Mohan Pandey</div><div><b>Pages:&nbsp;</b>4754 - 4765</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9736626/\">Improved Admissibility Analysis of Takagi\u2013Sugeno Fuzzy Singular Systems With Time-Varying Delays</a></div><div><b>Author(s):&nbsp;</b>Yang Li, Yong He</div><div><b>Pages:&nbsp;</b>4766 - 4774</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9736585/\">Adaptive Fuzzy Backstepping Asymptotic Disturbance Rejection of Multiagent Systems With Unknown Model Dynamics</a></div><div><b>Author(s):&nbsp;</b>Xin Hu, Yue Long, Tieshan Li, C. L. Philip Chen</div><div><b>Pages:&nbsp;</b>4775 - 4787</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9737428/\">Admissible Orders on Fuzzy Numbers</a></div><div><b>Author(s):&nbsp;</b>Nicol\u00e1s Zumelzu, Benjam\u00edn Bedregal, Edmundo Mansilla, Humberto Bustince, Roberto D\u00edaz</div><div><b>Pages:&nbsp;</b>4788 - 4799</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9738446/\">Neural-Observer-Based Terminal Sliding Mode Control: Design and Application</a></div><div><b>Author(s):&nbsp;</b>Shixi Hou, Cheng Wang, Yundi Chu, Juntao Fei</div><div><b>Pages:&nbsp;</b>4800 - 4814</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9740453/\">Large-Scale Fuzzy Least Squares Twin SVMs for Class Imbalance Learning</a></div><div><b>Author(s):&nbsp;</b>M. A. Ganaie, M. Tanveer, Chin-Teng Lin</div><div><b>Pages:&nbsp;</b>4815 - 4827</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9740425/\">Fuzzy Adaptive Output-Feedback Constrained Trajectory Tracking Control for HFVs With Fixed-Time Convergence</a></div><div><b>Author(s):&nbsp;</b>Renwei Zuo, Yinghui Li, Maolong Lv, Zongcheng Liu, Fan Zhang</div><div><b>Pages:&nbsp;</b>4828 - 4840</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9740434/\">Granularity Selection for Hierarchical Classification Based on Uncertainty Measure</a></div><div><b>Author(s):&nbsp;</b>Shuai Li, Jie Yang, Guoyin Wang, Qinghua Zhang, Jun Hu</div><div><b>Pages:&nbsp;</b>4841 - 4855</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9741321/\">Delay-Variation-Dependent Criteria on Stability and Stabilization for Discrete-Time T\u2013S Fuzzy Systems With Time-Varying Delays</a></div><div><b>Author(s):&nbsp;</b>Wen-Hu Chen, Chuan-Ke Zhang, Ke-You Xie, Cui Zhu, Yong He</div><div><b>Pages:&nbsp;</b>4856 - 4866</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9741378/\">Generalized Fuzzy Extended State Observer Design for Uncertain Nonlinear Systems: An Improved Dynamic Event-Triggered Approach</a></div><div><b>Author(s):&nbsp;</b>Zhichen Li, Huaicheng Yan, Hao Zhang, Meng Wang, Lu Zeng</div><div><b>Pages:&nbsp;</b>4867 - 4875</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9743321/\">Fuzzy Control of Switched Systems With Unknown Backlash and Nonconstant Control Gain: A Parameterized Smooth Inverse</a></div><div><b>Author(s):&nbsp;</b>Yanxian Chen, Zhi Liu, C. L. Philip Chen, Yun Zhang</div><div><b>Pages:&nbsp;</b>4876 - 4890</div><div><br /></div><div><b>30)</b> <a href=\"https://ieeexplore.ieee.org/document/9745804/\">Fuzzy Adaptive Finite-Time Consensus Control for High-Order Nonlinear Multiagent Systems Based on Event-Triggered</a></div><div><b>Author(s):&nbsp;</b>Haodong Zhou, Shuai Sui, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>4891 - 4904</div><div><br /></div><div><b>31)</b> <a href=\"https://ieeexplore.ieee.org/document/9745830/\">Robust Fuzzy Feedback Control for Nonlinear Systems With Input Quantization</a></div><div><b>Author(s):&nbsp;</b>Ting-Ting Pan, Xiao-Heng Chang, Yi Liu</div><div><b>Pages:&nbsp;</b>4905 - 4914</div><div><br /></div><div><b>32)</b> <a href=\"https://ieeexplore.ieee.org/document/9748988/\">Adaptive Fuzzy Control for an Uncertain Axially Moving Slung-Load Cable System of a Hovering Helicopter With Actuator Fault</a></div><div><b>Author(s):&nbsp;</b>Yong Ren, Zhijia Zhao, Choon Ki Ahn, Han-Xiong Li</div><div><b>Pages:&nbsp;</b>4915 - 4925</div><div><br /></div><div><b>33)</b> <a href=\"https://ieeexplore.ieee.org/document/9748990/\">Finite-Time Dynamic Event-Triggered Fuzzy Output Fault-Tolerant Control for Interval Type-2 Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Xiaomei Li, Wenting Song, Yongming Li, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>4926 - 4938</div><div><br /></div><div><b>34)</b> <a href=\"https://ieeexplore.ieee.org/document/9748983/\">Adaptive Fuzzy Control of Nonlinear Systems With Function Constraints Based on Time-Varying IBLFs</a></div><div><b>Author(s):&nbsp;</b>Tianqi Yu, Yan-Jun Liu, Lei Liu, Shaocheng Tong</div><div><b>Pages:&nbsp;</b>4939 - 4952</div><div><br /></div><div><b>35)</b> <a href=\"https://ieeexplore.ieee.org/document/9749908/\">Adaptive Image Steganography Using Fuzzy Enhancement and Grey Wolf Optimizer</a></div><div><b>Author(s):&nbsp;</b>Jialiang Xie, Honghui Wang, Dongrui Wu</div><div><b>Pages:&nbsp;</b>4953 - 4964</div><div><br /></div><div><b>36)</b> <a href=\"https://ieeexplore.ieee.org/document/9749939/\">Enhanced Multiview Fuzzy Clustering Using Double Visible-Hidden View Cooperation and Network LASSO Constraint</a></div><div><b>Author(s):&nbsp;</b>Zhaohong Deng, Ling Liang, Hongtan Yang, Wei Zhang, Qiongdan Lou, Kup-Sze Choi, Te Zhang, Jin Zhou, Shitong Wang</div><div><b>Pages:&nbsp;</b>4965 - 4979</div><div><br /></div><div><b>37)</b> <a href=\"https://ieeexplore.ieee.org/document/9749839/\">Reachable Set Estimation for T\u2013S Fuzzy Markov Jump Systems With Time-Varying Delays via Membership Function Dependent Performance</a></div><div><b>Author(s):&nbsp;</b>B. Visakamoorthi, P. Muthukumar, H. Trinh</div><div><b>Pages:&nbsp;</b>4980 - 4990</div><div><br /></div><div><b>38)</b> <a href=\"https://ieeexplore.ieee.org/document/9750865/\">Consensus Reaching With Minimum Cost of Informed Individuals and Time Constraints in Large-Scale Group Decision-Making</a></div><div><b>Author(s):&nbsp;</b>Haiming Liang, Gang Kou, Yucheng Dong, Francisco Chiclana, Enrique Herrera-Viedma</div><div><b>Pages:&nbsp;</b>4991 - 5004</div><div><br /></div><div><b>39)</b> <a href=\"https://ieeexplore.ieee.org/document/9750872/\">Sliding Mode Control for Networked Interval Type-2 Fuzzy Systems via Random Multiaccess Protocols</a></div><div><b>Author(s):&nbsp;</b>Yekai Yang, Yugang Niu, Hak-Keung Lam</div><div><b>Pages:&nbsp;</b>5005 - 5018</div><div><br /></div><div><b>40)</b> <a href=\"https://ieeexplore.ieee.org/document/9751415/\">Nonfragile Dissipative Fuzzy PID Control With Mixed Fading Measurements</a></div><div><b>Author(s):&nbsp;</b>Yezheng Wang, Zidong Wang, Lei Zou, Hongli Dong</div><div><b>Pages:&nbsp;</b>5019 - 5033</div><div><br /></div><div><b>41)</b> <a href=\"https://ieeexplore.ieee.org/document/9760164/\">A Complementary Study on General Interval Type-2 Fuzzy Sets</a></div><div><b>Author(s):&nbsp;</b>Pablo Hern\u00e1ndez, Susana Cubillo, Carmen Torres-Blanc</div><div><b>Pages:&nbsp;</b>5034 - 5043</div><div><br /></div><div><b>42)</b> <a href=\"https://ieeexplore.ieee.org/document/9697430/\">Relaxed Resilient Fuzzy Stabilization of Discrete-Time Takagi\u2013Sugeno Systems via a Higher Order Time-Variant Balanced Matrix Method</a></div><div><b>Author(s):&nbsp;</b>Xiangpeng Xie, Cong Wei, Zhou Gu, Kaibo Shi</div><div>5044 - 5050</div><div><br /></div><div><b>43)</b> <a href=\"https://ieeexplore.ieee.org/document/9739907/\">Finite-Time Composite Antidisturbance Control for T\u2013S Fuzzy Nonhomogeneous Markovian Jump Systems via Asynchronous Disturbance Observer</a></div><div><b>Author(s):&nbsp;</b>Yao Wang, Shengyuan Xu, Choon Ki Ahn</div><div><b>Pages:&nbsp;</b>5051 - 5057</div><div><br /></div><div><b>44)</b> <a href=\"https://ieeexplore.ieee.org/document/9743322/\">Reduced-Order Extended Dissipative Filtering for Nonlinear Systems With Sensor Saturation via Interval Type-2 Fuzzy Model</a></div><div><b>Author(s):&nbsp;</b>Yi Zeng, Hak-Keung Lam, Bo Xiao, Ligang Wu, Ming Chen</div><div><b>Pages:&nbsp;</b>5058 - 5064</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-11-03T18:55:00.000+13:00",
            "pubdate_parsed": [
                2022,
                11,
                3
            ],
            "email_sent": true
        },
        "Complex & Intelligent Systems, Volume 8, issue 6, December 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/11/complex-intelligent-systems-volume-8.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00843-1\">Guest editorial on \u201cdata-driven operations management\u201d</a></div><div><b>Author(s): </b>Dujuan Wang, Yugang Yu...Yunqiang Yin</div><div><b>Pages: </b>4421 - 4424</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00410-0\">Hybrid evolutionary optimization for takeaway order selection and delivery path planning utilizing habit data</a></div><div><b>Author(s):&nbsp;</b>Min-Xia Zhang, Jia-Yu Wu...Yu-Jun Zheng</div><div><b>Pages:&nbsp;</b>4425 - 4440</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00443-5\">DP-TABU: an algorithm to solve single-depot multi-line vehicle scheduling problem</a></div><div><b>Author(s):&nbsp;</b>Zhao Xinchao, Sun Hao...Li Zhiyu</div><div><b>Pages:&nbsp;</b>4441 - 4451</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00415-9\">Robust programming for basin-level water allocation with uncertain water availability and policy-driven scenario analysis</a></div><div><b>Author(s):&nbsp;</b>Liming Yao, Zerui Su, Shuhua Hou</div><div><b>Pages:&nbsp;</b>4453 - 4473</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00479-7\">EHEFT-R: multi-objective task scheduling scheme in cloud computing</a></div><div><b>Author(s):&nbsp;</b>Honglin Zhang, Yaohua Wu, Zaixing Sun</div><div><b>Pages:&nbsp;</b>4475 - 4482</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00481-z\">Using context-dependent DEA to analyze the efficiency of highly funded scientists in China</a></div><div><b>Author(s):&nbsp;</b>Keyu Xiang, Haiming Liang...Yucheng Dong</div><div><b>Pages:&nbsp;</b>4483 - 4495</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00478-8\">An enhanced group teaching optimization algorithm for multi-product disassembly line balancing problems</a></div><div><b>Author(s):&nbsp;</b>Pei Liang, Yaping Fu...Hao Sun</div><div><b>Pages:&nbsp;</b>4497 - 4512</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00492-w\">Strategic rationing and freshness keeping of perishable products under transportation disruptions and demand learning</a></div><div><b>Author(s):&nbsp;</b>Shanshan Li, Yong He, Melissza Salling</div><div><b>Pages:&nbsp;</b>4513 - 4527</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00493-9\">Portfolio optimization model with uncertain returns based on prospect theory</a></div><div><b>Author(s):&nbsp;</b>Yufeng Li, Bing Zhou, Yingxue Tan</div><div><b>Pages:&nbsp;</b>4529 - 4542</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00520-9\">Dynamic sourcing strategies for supply disruptions under consumer stockpiling</a></div><div><b>Author(s):&nbsp;</b>Shanshan Li, Yong He, Li Zhou</div><div><b>Pages:&nbsp;</b>4543 - 4555</div><div><br /></div><div><b>11)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00554-z\">Nursing rescheduling problem with multiple rescheduling methods under uncertainty</a></div><div><b>Author(s):&nbsp;</b>Zhiren Long, Xianxiu Wen...Yongjian Yang</div><div><b>Pages:&nbsp;</b>4557 - 4569</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00605-5\">A metaheuristic-based framework for index tracking with practical constraints</a></div><div><b>Author(s):&nbsp;</b>Man-Chung Yuen, Sin-Chun Ng...Hangjun Che</div><div><b>Pages:&nbsp;</b>4571 - 4586</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00597-2\">Storage assignment optimization for fishbone robotic mobile fulfillment systems</a></div><div><b>Author(s):&nbsp;</b>Yanyan Wang, Rongjun Man...Hong Zhao</div><div><b>Pages:&nbsp;</b>4587 - 4602</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00776-9\">Optimal scheduling in cloud healthcare system using Q-learning algorithm</a></div><div><b>Author(s):&nbsp;</b>Yafei Li, Hongfeng Wang...Tianhong Zhang</div><div><b>Pages:&nbsp;</b>4603 - 4618</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00791-w\">A two-stage stacked-based heterogeneous ensemble learning for cancer survival prediction</a></div><div><b>Author(s):&nbsp;</b>Fangzhou Yan, Yi Feng</div><div><b>Pages:&nbsp;</b>4619 - 4639</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00844-0\">Dynamic scheduling for semiconductor manufacturing systems with uncertainties using convolutional neural networks and reinforcement learning</a></div><div><b>Author(s):&nbsp;</b>Juan Liu, Fei Qiao...Birgit Vogel-Heuser</div><div><b>Pages:&nbsp;</b>4641 - 4662</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-021-00608-2\">ETHOS: a multi-label hate speech detection dataset</a></div><div><b>Author(s):&nbsp;</b>Ioannis Mollas, Zoe Chrysopoulou...Grigorios Tsoumakas</div><div><b>Pages:&nbsp;</b>4663 - 4678</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00716-7\">Multi-colony ant optimization with dynamic collaborative mechanism and cooperative game</a></div><div><b>Author(s):&nbsp;</b>Yadong Mo, Xiaoming You, Sheng Liu</div><div><b>Pages:&nbsp;</b>4679 - 4696</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00714-9\">A competitive swarm optimizer with probabilistic criteria for many-objective optimization problems</a></div><div><b>Author(s):&nbsp;</b>Chao He, Ming Li...Junhua Li</div><div><b>Pages:&nbsp;</b>4697 - 4725</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00727-4\">A system for electric vehicle\u2019s energy-aware routing in a transportation network through real-time prediction of energy consumption</a></div><div><b>Author(s):&nbsp;</b>Shatrughan Modi, Jhilik Bhattacharya</div><div><b>Pages:&nbsp;</b>4727 - 4751</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00722-9\">MSAt-GAN: a generative adversarial network based on multi-scale and deep attention mechanism for infrared and visible light image fusion</a></div><div><b>Author(s):&nbsp;</b>Junwu Li, Binhua Li...Weiwei Cai</div><div><b>Pages:&nbsp;</b>4753 - 4781</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00725-6\">Fuzzy acceptance sampling plan for transmuted Weibull distribution</a></div><div><b>Author(s):&nbsp;</b>Muhammad Zahir Khan, Muhammad Farid Khan...Abdur Razzaque Mughal</div><div><b>Pages:&nbsp;</b>4783 - 4795</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00728-3\">Self-attention-guided scale-refined detector for pedestrian detection</a></div><div><b>Author(s):&nbsp;</b>Xinchen Lin, Chaoqiang Zhao...Feng Qian</div><div><b>Pages:&nbsp;</b>4797 - 4809</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00730-9\">A hybrid multi-objective bi-level interactive fuzzy programming method for solving ECM-DWTA problem</a></div><div><b>Author(s):&nbsp;</b>Luda Zhao, Zongxu An...Yihua Hu</div><div><b>Pages:&nbsp;</b>4811 - 4829</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00733-6\">A wavelet convolutional capsule network with modified super resolution generative adversarial network for fault diagnosis and classification</a></div><div><b>Author(s):&nbsp;</b>Happy Nkanta Monday, Jianping Li...Ariyo Oluwasanmi</div><div><b>Pages:&nbsp;</b>4831 - 4847</div><div><br /></div><div><b>26)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00738-1\">A Lagrangian dual-based theory-guided deep neural network</a></div><div><b>Author(s):&nbsp;</b>Miao Rong, Dongxiao Zhang, Nanzhe Wang</div><div><b>Pages:&nbsp;</b>4849 - 4862</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00739-0\">Strengthening intrusion detection system for adversarial attacks: improved handling of imbalance classification problem</a></div><div><b>Author(s):&nbsp;</b>Chutipon Pimsarn, Tossapon Boongoen...Longzhi Yang</div><div><b>Pages:&nbsp;</b>4863 - 4880</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00720-x\">Parameters and reliability estimation for the weibull distribution based on intuitionistic fuzzy lifetime data</a></div><div><b>Author(s):&nbsp;</b>Zahra Roohanizadeh, Ezzatallah Baloui Jamkhaneh, Einolah Deiri</div><div><b>Pages:&nbsp;</b>4881 - 4896</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00741-6\">Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text</a></div><div><b>Author(s):&nbsp;</b>Mai A. Shaaban, Yasser F. Hassan, Shawkat K. Guirguis</div><div><b>Pages:&nbsp;</b>4897 - 4909</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00718-5\">A new method for image processing using generalized linguistic neutrosophic cubic aggregation operator</a></div><div><b>Author(s):&nbsp;</b>Gagandeep Kaur, Harish Garg</div><div><b>Pages:&nbsp;</b>4911 - 4937</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00719-4\">Modeling stochastic service time for complex on-demand food delivery</a></div><div><b>Author(s):&nbsp;</b>Jie Zheng, Ling Wang...Xuetao Ding</div><div><b>Pages:&nbsp;</b>4939 - 4953</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00721-w\">New extension of ordinal priority approach for multiple attribute decision-making problems: design and analysis</a></div><div><b>Author(s):&nbsp;</b>Mohamed Abdel-Basset, Mai Mohamed...Mohamed Abd Elfattah</div><div><b>Pages:&nbsp;</b>4955 - 4970</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00746-1\">An improved artificial bee colony algorithm based on Bayesian estimation</a></div><div><b>Author(s):&nbsp;</b>Chunfeng Wang, Pengpeng Shang, Peiping Shen</div><div><b>Pages:&nbsp;</b>4971 - 4991</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00752-3\">Automatic data volley: game data acquisition with temporal-spatial filters</a></div><div><b>Author(s):&nbsp;</b>Xina Cheng, Linzi Liang, Takeshi Ikenaga</div><div><b>Pages:&nbsp;</b>4993 - 5010</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00723-8\">A novel method to estimate incomplete PLTS information based on knowledge-match degree with reliability and its application in LSGDM problem</a></div><div><b>Author(s):&nbsp;</b>Huimin Xiao, Shouwen Wu, Liu Wang</div><div><b>Pages:&nbsp;</b>5011 - 5026</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00726-5\">New bag-of-feature for histopathology image classification using reinforced cat swarm algorithm and weighted Gaussian mixture modelling</a></div><div><b>Author(s):&nbsp;</b>Surbhi Vijh, Sumit Kumar, Mukesh Saraswat</div><div><b>Pages:&nbsp;</b>5027 - 5046</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00751-4\">A two-stage infill strategy and surrogate-ensemble assisted expensive many-objective optimization</a></div><div><b>Author(s):&nbsp;</b>Yi Zhao, Jian Zhao...Ying Tan</div><div><b>Pages:&nbsp;</b>5047 - 5063</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00755-0\">End-to-end human inspired learning based system for dynamic obstacle avoidance</a></div><div><b>Author(s):&nbsp;</b>S. M. Haider Jafri, Rahul Kala</div><div><b>Pages:&nbsp;</b>5065 - 5086</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00737-2\">Assessing cloud manufacturing applications using an optimally rectified FAHP approach</a></div><div><b>Author(s):&nbsp;</b>Tin-Chih Toly Chen, Chi-Wei Lin</div><div><b>Pages:&nbsp;</b>5087 - 5099</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00740-7\">Design and implementation of fault-tolerant control strategies for a real underactuated manipulator robot</a></div><div><b>Author(s):&nbsp;</b>Claudio Urrea, John Kern, Exequiel \u00c1lvarez</div><div><b>Pages:&nbsp;</b>5101 - 5123</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00744-3\">Star topology convolution for graph representation learning</a></div><div><b>Author(s):&nbsp;</b>Chong Wu, Zhenan Feng...Hong Yan</div><div><b>Pages:&nbsp;</b>5125 - 5141</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00731-8\">Student achievement prediction using deep neural network from multi-source campus data</a></div><div><b>Author(s):&nbsp;</b>Xiaoyong Li, Yong Zhang...Baocai Yin</div><div><b>Pages:&nbsp;</b>5143 - 5156</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00747-0\">A decomposition-based many-objective evolutionary algorithm with optional performance indicators</a></div><div><b>Author(s):&nbsp;</b>Hao Wang, Chaoli Sun...Xiaobo Li</div><div><b>Pages:&nbsp;</b>5157 - 5176</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00754-1\">Hall effect on MHD Jeffrey fluid flow with Cattaneo\u2013Christov heat flux model: an application of stochastic neural computing</a></div><div><b>Author(s):&nbsp;</b>Muhammad Awais, Huma Rehman...Muhammad Yousaf Malik</div><div><b>Pages:&nbsp;</b>5177 - 5201</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00732-7\">Complex system health condition estimation using tree-structured simple recurrent unit networks</a></div><div><b>Author(s):&nbsp;</b>Weijie Kang, Jiyang Xiao, Junjie Xue</div><div><b>Pages:&nbsp;</b>5203 - 5221</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00743-4\">Fermatean fuzzy copula aggregation operators and similarity measures-based complex proportional assessment approach for renewable energy source selection</a></div><div><b>Author(s):&nbsp;</b>Arunodaya Raj Mishra, Pratibha Rani...Ronald R. Yager</div><div><b>Pages:&nbsp;</b>5223 - 5248</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00745-2\">Deep multi-layer perceptron-based evolutionary algorithm for dynamic multiobjective optimization</a></div><div><b>Author(s):&nbsp;</b>Zhen Zhu, Yanpeng Yang...Yingfeng Cai</div><div><b>Pages:&nbsp;</b>5249 - 5264</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00757-y\">An associative knowledge network model for interpretable semantic representation of noun context</a></div><div><b>Author(s):&nbsp;</b>Yulin Li, Zhenping Xie, Fanyu Wang</div><div><b>Pages:&nbsp;</b>5265 - 5285</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00759-w\">Machine learning-based framework to cover optimal Pareto-front in many-objective optimization</a></div><div><b>Author(s):&nbsp;</b>Azam Asilian Bidgoli, Shahryar Rahnamayan...Ali Grami</div><div><b>Pages:&nbsp;</b>5287 - 5308</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00763-0\">Online group streaming feature selection using entropy-based uncertainty measures for fuzzy neighborhood rough sets</a></div><div><b>Author(s):&nbsp;</b>Jiucheng Xu, Yuanhao Sun...Qinchen Hou</div><div><b>Pages:&nbsp;</b>5309 - 5328</div><div><br /></div><div><b>51)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00699-5\">Gradient-supervised person re-identification based on dense feature pyramid network</a></div><div><b>Author(s):&nbsp;</b>Shaoqi Hou, Kangning Yin...Guangqiang Yin</div><div><b>Pages:&nbsp;</b>5329 - 5342</div><div><br /></div><div><b>52)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00758-x\">Generalized fuzzy ideals in ordered semirings</a></div><div><b>Author(s):&nbsp;</b>G. Muhiuddin, Ahsan Mahboob, N. Abughazalah</div><div><b>Pages:&nbsp;</b>5343 - 5353</div><div><br /></div><div><b>53)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00761-2\">A self-organizing map approach for constrained multi-objective optimization problems</a></div><div><b>Author(s):&nbsp;</b>Chao He, Ming Li...Junhua Li</div><div><b>Pages:&nbsp;</b>5355 - 5375</div><div><br /></div><div><b>54)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00762-1\">Discriminative and efficient non-local attention network for league of legends highlight detection</a></div><div><b>Author(s):&nbsp;</b>Qian Wan, Aruna Wang...Jiaji Wu</div><div><b>Pages:&nbsp;</b>5377 - 5386</div><div><br /></div><div><b>55)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00748-z\">A novel kinematics and statics correction algorithm of semi-cylindrical foot end structure for 3-DOF LHDS of legged robots</a></div><div><b>Author(s):&nbsp;</b>Kai-xian Ba, Yan-he Song...Xiang-dong Kong</div><div><b>Pages:&nbsp;</b>5387 - 5407</div><div><br /></div><div><b>56)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00749-y\">Complex Pythagorean uncertain linguistic group decision-making model based on Heronian mean aggregation operator considering uncertainty, interaction and interrelationship</a></div><div><b>Author(s):&nbsp;</b>Haolun Wang, Faming Zhang</div><div><b>Pages:&nbsp;</b>5409 - 5438</div><div><br /></div><div><b>57)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00769-8\">Trajectory prediction based on conditional Hamiltonian generative network for incomplete observation image sequences</a></div><div><b>Author(s):&nbsp;</b>Kui Qian, Lei Tian, Aiguo Song</div><div><b>Pages:&nbsp;</b>5439 - 5448</div><div><br /></div><div><b>58)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00772-z\">Cyberbullying detection using deep transfer learning</a></div><div><b>Author(s):&nbsp;</b>Pradeep Kumar Roy, Fenish Umeshbhai Mali</div><div><b>Pages:&nbsp;</b>5449 - 5467</div><div><br /></div><div><b>59)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00764-z\">Diagnosis of rotor demagnetization and eccentricity faults for IPMSM based on deep CNN and image recognition</a></div><div><b>Author(s):&nbsp;</b>Zhiyuan Li, Qinmu Wu...Xiangping Chen</div><div><b>Pages:&nbsp;</b>5469 - 5488</div><div><br /></div><div><b>60)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00774-x\">A novel ensemble feature selection method for pixel-level segmentation of HER2 overexpression</a></div><div><b>Author(s):&nbsp;</b>Ana Aguilera, Raquel Pezoa, Andrea Rodr\u00edguez-Delherbe</div><div><b>Pages:&nbsp;</b>5489 - 5510</div><div><br /></div><div><b>61)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00770-1\">Distributed neurodynamic approaches to nonsmooth optimization problems with inequality and set constraints</a></div><div><b>Author(s):&nbsp;</b>Linhua Luan, Xingnan Wen, Sitian Qin</div><div><b>Pages:&nbsp;</b>5511 - 5530</div><div><br /></div><div><b>62)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00750-5\">Survey on clothing image retrieval with cross-domain</a></div><div><b>Author(s):&nbsp;</b>Chen Ning, Yang Di, Li Menglu</div><div><b>Pages:&nbsp;</b>5531 - 5544</div><div><br /></div><div><b>63)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00724-7\">The use of deep learning methods in low-dose computed tomography image reconstruction: a systematic review</a></div><div><b>Author(s):&nbsp;</b>Minghan Zhang, Sai Gu, Yuhui Shi</div><div><b>Pages:&nbsp;</b>5545 - 5561</div><div><br /></div><div><b>64)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00753-2\">Correction to: Control in the loop for synchronization of nonlinear chaotic systems via adaptive intuitionistic neuro-fuzzy: a comparative study</a></div><div><b>Author(s):&nbsp;</b>Salah Helmy, Mohamed Magdy, Mohamed Hamdy</div><div><b>Pages:&nbsp;</b>5563 - 5563</div><div><br /></div><div><b>65)</b> <a href=\"https://link.springer.com/article/10.1007/s40747-022-00768-9\">Correction to: Joint metric learning of local and global features for vehicle re-identification</a></div><div><b>Author(s):&nbsp;</b>Junge Shen, Jian Sun...Zhaoyong Mao</div><div><b>Pages:&nbsp;</b>5565 - 5565</div><div><br /></div><div><br /></div></div>",
            "pubdate": "2022-11-04T13:21:00.000+13:00",
            "pubdate_parsed": [
                2022,
                11,
                4
            ],
            "email_sent": true
        },
        "AI Conferences 2023 - Part 2": {
            "url": "https://computational-intelligence.blogspot.com/2022/11/ai-conferences-2023-part-2.html",
            "description": "<div style=\"text-align: left;\"><div><div>This post follows <a href=\"https://computational-intelligence.blogspot.com/2022/09/ai-conferences-in-2023.html\">an earlier one</a>, and lists additional AI conferences being offered in 2023.</div><div><br /></div><div><b><u>12th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2023)</u></b></div><div>22-24 February 2023</div><div><a href=\"https://en.wikipedia.org/wiki/Lisbon\">Lisbon, Portugal</a></div><div>General Chair: Ana Fred</div><div>Website: <a href=\"https://icpram.scitevents.org\">https://icpram.scitevents.org</a></div><div><br /></div><div><b><u>15th International Conference on Agents and Artificial Intelligence (ICAART 2023)</u></b></div><div>22-24 February 2023</div><div><a href=\"https://en.wikipedia.org/wiki/Lisbon\">Lisbon, Portugal</a></div><div>General Chair: Jaap van den Herik</div><div>Website: <a href=\"https://icaart.scitevents.org\">https://icaart.scitevents.org</a></div><div><br /></div></div><div><b><u>2023 IEEE Swiss Conference on Data Science (IEEE SDS 2023)</u></b></div><div>22-23 June 2023</div><div><a href=\"https://en.wikipedia.org/wiki/Z%C3%BCrich\">Zurich, Switzerland</a>&nbsp;</div><div>General Chair: Melanie Geiger</div><div>Website: <a href=\"https://sds2023.ch/\">https://sds2023.ch/</a></div><div><br /></div><div><b><u>2023 IEEE Smart World Congress (IEEE SWC 2023)</u></b></div><div>25-28 August 2023</div><div><a href=\"https://en.wikipedia.org/wiki/Portsmouth\">Portsmouth, UK</a>&nbsp;</div><div>General chairs: Hui Yu and Man Lin</div><div>Website:&nbsp;<a href=\"https://ieee-smart-world-congress.org/\">https://ieee-smart-world-congress.org/</a></div><div><br /></div><div><b><u>2023 IEEE Symposium Series on Computational Intelligence (IEEE SSCI 2023)</u></b></div><div>6-8 December 2023</div><div><a href=\"https://en.wikipedia.org/wiki/Mexico_City\">Mexico City, Mexico</a>&nbsp;</div><div>General Chair: Wen Yu&nbsp;</div><div>Website: <a href=\"https://attend.ieee.org/ssci-2023/\">https://attend.ieee.org/ssci-2023/</a></div><div><br /></div></div>",
            "pubdate": "2022-11-08T13:55:00.000+13:00",
            "pubdate_parsed": [
                2022,
                11,
                8
            ],
            "email_sent": true
        },
        "IEEE Transactions on Artificial Intelligence, Volume 3, Issue 6, December 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/11/ieee-transactions-on-artificial.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9645355/\">Recent Advances in Trustworthy Explainable Artificial Intelligence: Status, Challenges, and Perspectives</a></div><div><b>Author(s): </b>Atul Rawal, James McCoy, Danda B. Rawat, Brian M. Sadler, Robert St. Amant</div><div><b>Pages: </b>852 - 866</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9736644/\">An Overview of Emotion in Artificial Intelligence</a></div><div><b>Author(s):&nbsp;</b>Gustavo Assun\u00e7\u00e3o, Bruno Patr\u00e3o, Miguel Castelo-Branco, Paulo Menezes</div><div><b>Pages:&nbsp;</b>867 - 886</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9762529/\">The New Generation Brain-Inspired Sparse Learning: A Comprehensive Survey</a></div><div><b>Author(s):&nbsp;</b>Licheng Jiao, Yuting Yang, Fang Liu, Shuyuan Yang, Biao Hou</div><div><b>Pages:&nbsp;</b>887 - 907</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9645219/\">Intellectual Property Protection for Deep Learning Models: Taxonomy, Methods, Attacks, and Evaluations</a></div><div><b>Author(s):&nbsp;</b>Mingfu Xue, Yushu Zhang, Jian Wang, Weiqiang Liu</div><div><b>Pages:&nbsp;</b>908 - 923</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9878189/\">Integrating Constraints Into Dimensionality Reduction for Visualization: A Survey</a></div><div><b>Author(s):&nbsp;</b>Viet Minh Vu, Adrien Bibal, Beno\u00eet Fr\u00e9nay</div><div><b>Pages:&nbsp;</b>944 - 962</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9645169/\">Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness</a></div><div><b>Author(s):&nbsp;</b>Yuwei Sun, Hideya Ochiai, Hiroshi Esaki</div><div><b>Pages:&nbsp;</b>963 - 972</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9738474/\">On Supervised Class-Imbalanced Learning: An Updated Perspective and Some Key Challenges</a></div><div><b>Author(s):&nbsp;</b>Swagatam Das, Sankha Subhra Mullick, Ivan Zelinka</div><div><b>Pages:&nbsp;</b>973 - 993</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9893751/\">A Survey on Siamese Network: Methodologies, Applications, and Opportunities</a></div><div><b>Author(s):&nbsp;</b>Yikai Li, C. L. Philip Chen, Tong Zhang</div><div><b>Pages:&nbsp;</b>994 - 1014</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9850361/\">Semisupervised Deep Learning for Image Classification With Distribution Mismatch: A Survey</a></div><div><b>Author(s):&nbsp;</b>Saul Calderon-Ramirez, Shengxiang Yang, David Elizondo</div><div><b>Pages:&nbsp;</b>1015 - 1029</div><div><br /></div>",
            "pubdate": "2022-11-24T21:45:00.000+13:00",
            "pubdate_parsed": [
                2022,
                11,
                24
            ],
            "email_sent": true
        },
        "IEEE Transactions on Emerging Topics in Computational Intelligence, Volume 6, Issue 6, December 2022": {
            "url": "https://computational-intelligence.blogspot.com/2022/12/ieee-transactions-on-emerging-topics-in.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9965783/\">Guest Editorial Special Issue on Computational Intelligence for Perception and Decision-Making of Autonomous Systems</a></div><div><b>Author(s): </b>Yang Tang, Gary G. Yen, J\u00fcrgen Kurths</div><div><b>Pages: </b>1287 - 1289</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9776555/\">Monitoring Social Distancing With Single Image Depth Estimation</a></div><div><b>Author(s):&nbsp;</b>Alessio Mingozzi, Andrea Conti, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia</div><div><b>Pages:&nbsp;</b>1290 - 1301</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9756199/\">Pharmaceutical Foreign Particle Detection: An Efficient Method Based on Adaptive Convolution and Multiscale Attention</a></div><div><b>Author(s):&nbsp;</b>Junfei Yi, Hui Zhang, Jianxu Mao, Yurong Chen, Hang Zhong, Yaonan Wang</div><div><b>Pages:&nbsp;</b>1302 - 1313</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9730833/\">Efficient Brain Decoding Based on Adaptive EEG Channel Selection and Transformation</a></div><div><b>Author(s):&nbsp;</b>Jiaxing Wang, Lei Shi, Weiqun Wang, Zeng-Guang Hou</div><div><b>Pages:&nbsp;</b>1314 - 1323</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9687093/\">Optimal Actor-Critic Policy With Optimized Training Datasets</a></div><div><b>Author(s):&nbsp;</b>Chayan Banerjee, Zhiyong Chen, Nasimul Noman, Mohsen Zamani</div><div><b>Pages:&nbsp;</b>1324 - 1334</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9775710/\">Decision Making in Monopoly Using a Hybrid Deep Reinforcement Learning Approach</a></div><div><b>Author(s):&nbsp;</b>Trevor Bonjour, Marina Haliem, Aala Alsalem, Shilpa Thomas, Hongyu Li, Vaneet Aggarwal, Mayank Kejriwal, Bharat Bhargava</div><div><b>Pages:&nbsp;</b>1335 - 1344</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9754220/\">Motion Planning and Cooperative Manipulation for Mobile Robots With Dual Arms</a></div><div><b>Author(s):&nbsp;</b>Fuchun Sun, Yang Chen, Yangyang Wu, Linxiang Li, Xiaolei Ren</div><div><b>Pages:&nbsp;</b>1345 - 1356</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9628154/\">Multi-Robot Learning Dynamic Obstacle Avoidance in Formation With Information-Directed Exploration</a></div><div><b>Author(s):&nbsp;</b>Junjie Cao, Yujie Wang, Yong Liu, Xuesong Ni</div><div><b>Pages:&nbsp;</b>1357 - 1367</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9770581/\">FedPCC: Parallelism of Communication and Computation for Federated Learning in Wireless Networks</a></div><div><b>Author(s):&nbsp;</b>Hong Zhang, Hao Tian, Mianxiong Dong, Kaoru Ota,&nbsp;</div><div>Juncheng Jia</div><div><b>Pages:&nbsp;</b>1368 - 1377</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9715148/\">Dynamic Network Structure: Doubly Stacking Broad Learning Systems With Residuals and Simpler Linear Model Transmission</a></div><div><b>Author(s):&nbsp;</b>Runshan Xie, Chi-Man Vong, C. L. Philip Chen, Shitong Wang</div><div><b>Pages:&nbsp;</b>1378 - 1395</div><div><br /></div><div><b>11) </b><a href=\"https://ieeexplore.ieee.org/document/9768859/\">A Deep 1-D CNN and Bidirectional LSTM Ensemble Model With Arbitration Mechanism for LDDoS Attack Detection</a></div><div><b>Author(s):&nbsp;</b>Zengguang Liu, Jiguo Yu, Biwei Yan, Guijuan Wang</div><div><b>Pages:&nbsp;</b>1396 - 1410</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9806713/\">Distributed Continuous and Discrete Time Projection Neurodynamic Approaches for Sparse Recovery</a></div><div><b>Author(s):&nbsp;</b>You Zhao, Xiaofeng Liao, Xing He</div><div><b>Pages:&nbsp;</b>1411 - 1426</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9826434/\">Taking Away Both Model and Data: Remember Training Data by Parameter Combinations</a></div><div><b>Author(s):&nbsp;</b>Wenjian Luo, Licai Zhang, Peiyi Han, Chuanyi Liu, Rongfei Zhuang</div><div><b>Pages:&nbsp;</b>1427 - 1437</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9778848/\">A Buffer-Based Ant Colony System Approach for Dynamic Cold Chain Logistics Scheduling</a></div><div><b>Author(s):&nbsp;</b>Li-Jiao Wu, Lin Shi, Zhi-Hui Zhan, Kuei-Kuei Lai, Jun Zhang</div><div><b>Pages:&nbsp;</b>1438 - 1452</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9956871/\">Unsupervised Feature Selection Using Iterative Shrinking and Expansion Algorithm</a></div><div><b>Author(s):&nbsp;</b>Tapas Bhadra, Ujjwal Maulik</div><div><b>Pages:&nbsp;</b>1453 - 1462</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9805994/\">A Deep Integrated Framework for Predicting SARS-CoV2\u2013Human Protein-Protein Interaction</a></div><div><b>Author(s):&nbsp;</b>Sumanta Ray, Snehalika Lall, Sanghamitra Bandyopadhyay</div><div><b>Pages:&nbsp;</b>1463 - 1472</div><div><br /></div></div>",
            "pubdate": "2022-12-02T15:03:00.000+13:00",
            "pubdate_parsed": [
                2022,
                12,
                2
            ],
            "email_sent": true
        },
        "IEEE Transactions on Fuzzy Systems, Volume 31, Issue 1, January 2023": {
            "url": "https://computational-intelligence.blogspot.com/2023/01/ieee-transactions-on-fuzzy-systems.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://ieeexplore.ieee.org/document/9788034/\">Stability and Stabilization of Polynomial Fuzzy-Model-Based Switched Nonlinear Systems Under MDADT Switching Signal</a></div><div><b>Author(s): </b>Zhiyong Bao, Hak-Keung Lam, Yong Peng, Fucai Liu, Kamyar Mehran</div><div><b>Pages: </b>1 - 13</div><div><br /></div><div><b>2)</b> <a href=\"https://ieeexplore.ieee.org/document/9787996/\">Adaptive Fuzzy Sliding Exact Tracking Control Based on High-Order Log-Type Time-Varying BLFs for High-Order Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Qiang Zhang, Dakuo He</div><div><b>Pages:&nbsp;</b>14 - 24</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9789396/\">Two Integral Models and Applications of Hesitant Fuzzy Information Fusion</a></div><div><b>Author(s):&nbsp;</b>Jie Gao, Zeshui Xu, Zhilei Liang, Yunshu Mao</div><div><b>Pages:&nbsp;</b>25 - 39</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9792631/\">Membership-Function-Dependent Design of Quantized Fuzzy Sampled-Data Controller for Semi-Markovian Jump Systems With Actuator Faults</a></div><div><b>Author(s):&nbsp;</b>R. Vijay Aravind, P. Balasubramaniam</div><div><b>Pages:&nbsp;</b>40 - 52</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9792630/\">Robust Pricing for a Dual-Channel Green Supply Chain Under Fuzzy Demand Ambiguity</a></div><div><b>Author(s):&nbsp;</b>Huili Pei, Yankui Liu, Hongliang Li</div><div><b>Pages:&nbsp;</b>53 - 66</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9792632/\">Event-Triggered Fixed-Time Practical Tracking Control for Flexible-Joint Robot</a></div><div><b>Author(s):&nbsp;</b>Yingkang Xie, Qian Ma, Jason Gu, Guopeng Zhou</div><div><b>Pages:&nbsp;</b>67 - 76</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9795240/\">Fuzzy Mutual Information-Based Multilabel Feature Selection With Label Dependency and Streaming Labels</a></div><div><b>Author(s):&nbsp;</b>Jinghua Liu, Yaojin Lin, Weiping Ding, Hongbo Zhang, Jixiang Du</div><div><b>Pages:&nbsp;</b>77 - 91</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9795258/\">An Intelligent Digital Redesign Approach to the Sampled-Data Fuzzy Observer Design</a></div><div><b>Author(s):&nbsp;</b>Yong Hoon Jang, Kwangil Lee, Han Sol Kim</div><div><b>Pages:&nbsp;</b>92 - 103</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9795905/\">Adaptive Fuzzy Tracking Control for Uncertain Nonlinear Systems With Multiple Actuators and Sensors Faults</a></div><div><b>Author(s):&nbsp;</b>Dengxiu Yu, Ming Yang, Yan-Jun Liu, Zhen Wang, C. L. Philip Chen</div><div><b>Pages:&nbsp;</b>104 - 116</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9795884/\">Finite-Time Adaptive Fuzzy Event-Triggered Control of Constrained Nonlinear Systems via Bounded Command Filter</a></div><div><b>Author(s):&nbsp;</b>Zhibao Song, Ping Li, Zongyao Sun, Zhen Wang</div><div><b>Pages:&nbsp;</b>117 - 128</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9795868/\">Stability Analysis of Time-Varying Delay T\u2013S Fuzzy Systems via Quadratic-Delay-Product Method</a></div><div><b>Author(s):&nbsp;</b>Yunfei Qiu, Ju H. Park, Changchun Hua, Xijuan Wang</div><div><b>Pages:&nbsp;</b>129 - 137</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9797877/\">Adaptive Event-Triggered Control of Stochastic Nonlinear Systems With Unknown Dead Zone</a></div><div><b>Author(s):&nbsp;</b>Tong Wang, Nan Wang, Jianbin Qiu, Concettina Buccella, Carlo Cecati</div><div><b>Pages:&nbsp;</b>138 - 147</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9797865/\">Decentralized Event-Triggered Adaptive Control for Interconnected Nonlinear Systems With Actuator Failures</a></div><div><b>Author(s):&nbsp;</b>Lin-Xing Xu, Yu-Long Wang, Xiaofan Wang, Chen Peng</div><div><b>Pages:&nbsp;</b>148 - 159</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9799748/\">dCF-Integrals: Generalizing CF-Integrals by Means of Restricted Dissimilarity Functions</a></div><div><b>Author(s):&nbsp;</b>Jonata Wieczynski, Giancarlo Lucca, Gra\u00e7aliz P. Dimuro, Eduardo N. Borges, Jos\u00e9 A. Sanz, Tiago da Cruz Asmus, Javier Fern\u00e1ndez, Humberto Bustince</div><div><b>Pages:&nbsp;</b>160 - 173</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9799735/\">Nonsingular Fixed-Time Fault-Tolerant Fuzzy Control for Switched Uncertain Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Di Cui, Zhengrong Xiang</div><div><b>Pages:&nbsp;</b>174 - 183</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9802687/\">Disturbance-Observer-Based Adaptive Fuzzy Tracking Control for Unmanned Autonomous Helicopter With Flight Boundary Constraints</a></div><div><b>Author(s):&nbsp;</b>Haoxiang Ma, Mou Chen, Gang Feng, Qingxian Wu</div><div><b>Pages:&nbsp;</b>184 - 198</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9802711/\">Fuzzy Markov Decision-Making Model for Interference Effects</a></div><div><b>Author(s):&nbsp;</b>Xiaozhuan Gao, Lipeng Pan, Danilo Pelusi, Yong Deng</div><div><b>Pages:&nbsp;</b>199 - 212</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9804219/\">Feature Grouping and Selection With Graph Theory in Robust Fuzzy Rough Approximation Space</a></div><div><b>Author(s):&nbsp;</b>Jihong Wan, Hongmei Chen, Tianrui Li, Binbin Sang, Zhong Yuan</div><div><b>Pages:&nbsp;</b>213 - 225</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9804229/\">Command Filter-Based Adaptive Fuzzy Finite-Time Tracking Control for Uncertain Fractional-Order Nonlinear Systems</a></div><div><b>Author(s):&nbsp;</b>Xingxing You, Songyi Dian, Kai Liu, Bin Guo, Guofei Xiang, Yuqi Zhu</div><div><b>Pages:&nbsp;</b>226 - 240</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9804222/\">Fuzzy-Based Optimization and Control of a Soft Exosuit for Compliant Robot\u2013Human\u2013Environment Interaction</a></div><div><b>Author(s):&nbsp;</b>Qinjian Li, Wen Qi, Zhijun Li, Haisheng Xia, Yu Kang, Lin Cheng</div><div><b>Pages:&nbsp;</b>241 - 253</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9804236/\">Layer Normalization for TSK Fuzzy System Optimization in Regression Problems</a></div><div><b>Author(s):&nbsp;</b>Yuqi Cui, Yifan Xu, Ruimin Peng, Dongrui Wu</div><div><b>Pages:&nbsp;</b>254 - 264</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9804850/\">Further Stability Criteria for Sampled-Data-Based Interval Type-2 Fuzzy Systems via a Refined Two-Side Looped-Functional Method</a></div><div><b>Author(s):&nbsp;</b>Zheng You, Huaicheng Yan, Hao Zhang, Lu Zeng, Meng Wang</div><div><b>Pages:&nbsp;</b>265 - 277</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9804802/\">Resilient Adaptive Event-Triggered H\u221e Fuzzy Filtering for Cyber-Physical Systems Under Stochastic-Sampling and Denial-of-Service Attacks</a></div><div><b>Author(s):&nbsp;</b>Xuhuan Xie, Songlin Hu, Yonggui Liu, Qinxue Li</div><div><b>Pages:&nbsp;</b>278 - 292</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9806311/\">Consensus Reaching Process With Multiobjective Optimization for Large-Scale Group Decision Making With Cooperative Game</a></div><div><b>Author(s):&nbsp;</b>Peng Wu, Fengen Li, Jie Zhao, Ligang Zhou, Luis Mart\u00ednez</div><div><b>Pages:&nbsp;</b>293 - 306</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9806378/\">Opinion Dynamics and Minimum Adjustment-Driven Consensus Model for Multi-Criteria Large-Scale Group Decision Making Under a Novel Social Trust Propagation Mechanism</a></div><div><b>Author(s):&nbsp;</b>Peide Liu, Yueyuan Li, Peng Wang</div><div><b>Pages:&nbsp;</b>307 - 321</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9810512/\">Stock Selection System Through Suitability Index and Fuzzy-Based Quantitative Characteristics</a></div><div><b>Author(s):&nbsp;</b>Jia-Hao Syu, Jerry Chun-Wei Lin, Chi-Jen Wu, Jan-Ming Ho</div><div><b>Pages:&nbsp;</b>322 - 334</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9795253/\">Characterization of Nested Lattice Codes Through Quotient Groups Relative to Normal Fuzzy Subgroups for Channel Quantization</a></div><div><b>Author(s):&nbsp;</b>Cibele Cristina Trinca, Ricardo Augusto Watanabe, Estev\u00e3o Esmi</div><div><b>Pages:&nbsp;</b>335 - 342</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9795256/\">On a Parametric Measure of Vagueness</a></div><div><b>Author(s):&nbsp;</b>J\u00f3zsef Dombi, Tam\u00e1s J\u00f3n\u00e1s</div><div><b>Pages:&nbsp;</b>343 - 347</div><div><br /></div><div><b>29)</b> <a href=\"https://ieeexplore.ieee.org/document/9795351/\">Constrained Control for a Class of TS Fuzzy Systems</a></div><div><b>Author(s):&nbsp;</b>Souhail Tiko, Fouad Mesquine</div><div><b>Pages:&nbsp;</b>348 - 353</div><div><br /></div></div>",
            "pubdate": "2023-01-06T18:27:00.000+13:00",
            "pubdate_parsed": [
                2023,
                1,
                6
            ],
            "email_sent": true
        },
        "Soft Computing, Volume 27, Issue 1": {
            "url": "https://computational-intelligence.blogspot.com/2023/01/soft-computing-volume-27-issue-1.html",
            "description": "<div style=\"text-align: left;\"><div><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07548-0\">On Alexandrov L-fuzzy nearness (II)</a></div><div><b>Author(s): </b>Enas H. Elkordy, Ahmed A. Ramadan, Reham M. Ahmed</div><div><b>Pages: </b>1 - 16</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07547-1\">A study on the normality of Wijsman topology of a fuzzy metric space</a></div><div><b>Author(s):&nbsp;</b>Zia Bashir, Asad Ullah</div><div><b>Pages:&nbsp;</b>17 - 23</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07550-6\">An alternative approach to quadratic scoring rules using continuous-valued logic</a></div><div><b>Author(s):&nbsp;</b>J\u00f3zsef Dombi, Tam\u00e1s J\u00f3n\u00e1s</div><div><b>Pages:&nbsp;</b>25 - 46</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07601-y\">Some new generalized difference of sequences for fuzzy numbers</a></div><div><b>Author(s):&nbsp;</b>Abdulkadir Karaka\u015f</div><div><b>Pages:&nbsp;</b>47 - 55</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07521-x\">On separation axioms of topological rough groups</a></div><div><b>Author(s):&nbsp;</b>Pi-Yu Li, Wen-Li Liu...Zhi-Fang Guo</div><div><b>Pages:&nbsp;</b>57 - 61</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07522-w\">Ship detection based on deep learning using SAR imagery: a systematic literature review</a></div><div><b>Author(s):&nbsp;</b>Muhammad Yasir, Wan Jianhua...Md Sakaouth Hossain</div><div><b>Pages:&nbsp;</b>63 - 84</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07566-y\">A variable precision multigranulation rough set model and attribute reduction</a></div><div><b>Author(s):&nbsp;</b>Jiayue Chen. Ping Zhu</div><div><b>Pages:&nbsp;</b>85 - 106</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07567-x\">An adaptive replica configuration mechanism based on predictive file popularity and queue balance in mobile edge computing environment</a></div><div><b>Author(s):&nbsp;</b>Mao-Lun Chiang, Hui-Ching Hsieh...Hong-Wei Chen</div><div><b>Pages:&nbsp;</b>107 - 129</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07583-x\">Mp- and purified residuated lattices</a></div><div><b>Author(s):&nbsp;</b>Saeed Rasouli, Amin Dehghani</div><div><b>Pages:&nbsp;</b>131 - 148</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07555-1\">Assessing the best art design based on artificial intelligence and machine learning using GTMA</a></div><div><b>Author(s):&nbsp;</b>Xu Wenjing, Zilu Cai</div><div><b>Pages:&nbsp;</b>149 - 156</div><div><br /></div><div><b>11) </b><a href=\"https://link.springer.com/article/10.1007/s00500-022-07556-0\">Using the fuzzy analytical hierarchy process to prioritize the impact of visual communication based on artificial intelligence for long-term learning</a></div><div><b>Author(s):&nbsp;</b>Yadi Liu, Abdullah A. Al-Atawi...Qamar Zaman</div><div><b>Pages:&nbsp;</b>157 - 168</div><div><br /></div><div><b>12)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07569-9\">Industry 4.0 implementation challenges in small- and medium-sized enterprises: an approach integrating interval type-2 fuzzy BWM and DEMATEL</a></div><div><b>Author(s):&nbsp;</b>Moslem Alimohammadlou, Sahar Sharifian</div><div><b>Pages:&nbsp;</b>169 - 186</div><div><br /></div><div><b>13)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07570-2\">Strengthened conditional distributivity of semi-t-operators over uninorms</a></div><div><b>Author(s):&nbsp;</b>Dragan Jo\u010di\u0107, Ivana \u0160tajner-Papuga</div><div><b>Pages:&nbsp;</b>187 - 200</div><div><br /></div><div><b>14)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07575-x\">Asynchronous robust dynamic output feedback \ud835\udc3b\u221e control for fuzzy stochastic hybrid systems subject to time-varying delays and hidden Markov model</a></div><div><b>Author(s):&nbsp;</b>Yuqian Lin, Guangming Zhuang...Wei Sun</div><div><b>Pages:&nbsp;</b>201 - 218</div><div><br /></div><div><b>15)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07576-w\">Mixing of capacity preserving dynamical systems</a></div><div><b>Author(s):&nbsp;</b>Lixin Guo, Guo Wei, Zhiming Li</div><div><b>Pages:&nbsp;</b>219 - 225</div><div><br /></div><div><b>16)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07580-0\">Evidential global linguistic terms entropy</a></div><div><b>Author(s):&nbsp;</b>Jinyan Su, Yong Deng, Nam-Van Huynh</div><div><b>Pages:&nbsp;</b>227 - 237</div><div><br /></div><div><b>17)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07584-w\">Examples, properties and applications of fuzzy inner product spaces</a></div><div><b>Author(s):&nbsp;</b>Jian-Zhong Xiao, Ying Lu, Feng-Qin Zhu</div><div><b>Pages:&nbsp;</b>239 - 256</div><div><br /></div><div><b>18)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07591-x\">An integrated Fuzzy MCDM approach for modelling and prioritising the enablers of responsiveness in automotive supply chain using Fuzzy DEMATEL, Fuzzy AHP and Fuzzy TOPSIS</a></div><div><b>Author(s):&nbsp;</b>Rinu Sathyan, P. Parthiban...M. S. Sachin</div><div><b>Pages:&nbsp;</b>257 - 277</div><div><br /></div><div><b>19)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07278-3\">The unit generalized half-normal quantile regression model: formulation, estimation, diagnostics, and numerical applications</a></div><div><b>Author(s):&nbsp;</b>Josmar Mazucheli, Mustafa \u00c7. Korkmaz...V\u00edctor Leiva</div><div><b>Pages:&nbsp;</b>279 - 295</div><div><br /></div><div><b>20)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07633-4\">Artificial neural network-based secured communication strategy for vehicular ad hoc network</a></div><div><b>Author(s):&nbsp;</b>B. V. D. S. Sekhar, Pamula Udayaraju...M. S. S. S. Srinivas</div><div><b>Pages:&nbsp;</b>297 - 309</div><div><br /></div><div><b>21)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07617-4\">Multi-layered network model for text summarization using feature representation</a></div><div><b>Author(s):&nbsp;</b>G. Malarselvi, A. Pandian</div><div><b>Pages:&nbsp;</b>311 - 322</div><div><br /></div><div><b>22)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07618-3\">Machine learning and IoTs for forecasting prediction of smart road traffic flow</a></div><div><b>Author(s):&nbsp;</b>Sun Chuanxia, Zhang Han, Yin Peixuan</div><div><b>Pages:&nbsp;</b>323 - 335</div><div><br /></div><div><b>23)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07560-4\">ACO-based approach for integrating product lifecycle management with MRO services in aviation industry</a></div><div><b>Author(s):&nbsp;</b>Ahmet Muhammed Guraksin, Alper Ozcan</div><div><b>Pages:&nbsp;</b>337 - 361</div><div><br /></div><div><b>24)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05572-0\">Deep belief network-based probabilistic generative model for detection of robotic manipulator failure execution</a></div><div><b>Author(s):&nbsp;</b>Pandit Byomakesha Dash, Bighnaraj Naik...S. Vimal</div><div><b>Pages:&nbsp;</b>363 - 375</div><div><br /></div><div><b>25)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05586-8\">Detection of shilling attack in recommender system for YouTube video statistics using machine learning techniques</a></div><div><b>Author(s):&nbsp;</b>Shalli Rani, Manpreet Kaur...Jnyana Ranjan Mohanty</div><div><b>Pages:&nbsp;</b>377 - 389</div><div><br /></div><div><b>26) </b><a href=\"https://link.springer.com/article/10.1007/s00500-021-05613-8\">An agent architecture for autonomous UAV flight control in object classification and recognition missions</a></div><div><b>Author(s):&nbsp;</b>Salama A. Mostafa, Aida Mustapha...Seifedine Kadry</div><div><b>Pages:&nbsp;</b>391 - 404</div><div><br /></div><div><b>27)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05649-w\">Rule-based multi-view human activity recognition system in real time using skeleton data from RGB-D sensor</a></div><div><b>Author(s):&nbsp;</b>Neeraj Varshney, Brijesh Bakariya...Manish Khare</div><div><b>Pages:&nbsp;</b>405 - 421</div><div><br /></div><div><b>28)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05650-3\">Remote sensing image processing technology based on mobile augmented reality technology in surveying and mapping engineering</a></div><div><b>Author(s):&nbsp;</b>Wei Lu, LiJun Zhao, Rong Xu</div><div><b>Pages:&nbsp;</b>423 - 433</div><div><br /></div><div><b>29)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05700-w\">Performance enhancement of generative adversarial network for photograph\u2013sketch identification</a></div><div><b>Author(s):&nbsp;</b>M. S. Sannidhan, G. Ananth Prabhu...Jnyana Ranjan Mohanty</div><div><b>Pages:&nbsp;</b>435 - 452</div><div><br /></div><div><b>30)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05701-9\">Pedestrian identification using motion-controlled deep neural network in real-time visual surveillance</a></div><div><b>Author(s):&nbsp;</b>Muhammad Zahid, Muhammad Attique Khan...Jnyana Ranjan Mohanty</div><div><b>Pages:&nbsp;</b>453 - 469</div><div><br /></div><div><b>31)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05727-z\">Manifold fitting algorithm of noisy manifold data based on variable-scale spectral graph</a></div><div><b>Author(s):&nbsp;</b>Tao Yang, Jintao Meng</div><div><b>Pages:&nbsp;</b>471 - 482</div><div><br /></div><div><b>32)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05735-z\">Intelligent recommendation method integrating knowledge graph and Bayesian network</a></div><div><b>Author(s):&nbsp;</b>Hailan Pan, Xiaohuan Yang</div><div><b>Pages:&nbsp;</b>483 - 492</div><div><br /></div><div><b>33)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05815-0\">An improved and low-complexity neural network model for curved lane detection of autonomous driving system</a></div><div><b>Author(s):&nbsp;</b>Safwan Ghanem, Priyadarshi Kanungo...Pritee Parwekar</div><div><b>Pages:&nbsp;</b>493 - 504</div><div><br /></div><div><b>34)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05816-z\">Efficiently harvesting deep web interfaces based on adaptive learning using two-phase data crawler framework</a></div><div><b>Author(s):&nbsp;</b>Madhusudhan Rao Murugudu, L. S. S. Reddy</div><div><b>Pages:&nbsp;</b>505 - 515</div><div><br /></div><div><b>35)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05856-5\">Investigation on broad beam pattern synthesis</a></div><div><b>Author(s):&nbsp;</b>M. Chandrasekhar</div><div><b>Pages:&nbsp;</b>517 - 527</div><div><br /></div><div><b>36)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05857-4\">Blind signal-to-interference-plus-noise ratio estimation of OFDM/OQAM system in radio frequency interference environment</a></div><div><b>Author(s):&nbsp;</b>J. Tarun Kumar, C. H. Sridevi, V. Sandeep Kumar</div><div><b>Pages:&nbsp;</b>529 - 536</div><div><br /></div><div><b>37)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05879-y\">Sector multi-beam space optimal bit error rate enhancement in wireless 5G using power domain NOMA</a></div><div><b>Author(s):&nbsp;</b>K. Murali, S. SivaPerumal</div><div><b>Pages:&nbsp;</b>537 - 545</div><div><br /></div><div><b>38)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-06120-6\">A defensive design for control application based on networked systems</a></div><div><b>Author(s):&nbsp;</b>Ahmed Raza Rajput, Muhammad Shamrooz Aslam, Shahab Ahmad Niazi</div><div><b>Pages:&nbsp;</b>547 - 558</div><div><br /></div><div><b>39)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06950-y\">Securing medical data by role-based user policy with partially homomorphic encryption in AWS cloud</a></div><div><b>Author(s):&nbsp;</b>M. D. Boomija, S. V. Kasmir Raja</div><div><b>Pages:&nbsp;</b>559 - 568</div><div><br /></div><div><b>40)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06986-0\">Empirical evidence of effects of stringency amid Covid-19 pandemic spread</a></div><div><b>Author(s):&nbsp;</b>R. I. Minu, G. Nagarajan, T. R. Saravanan</div><div><b>Pages:&nbsp;</b>569 - 577</div><div><br /></div><div><b>41)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06989-x\">Hybrid high performance intelligent computing approach of CACNN and RNN for skin cancer image grading</a></div><div><b>Author(s):&nbsp;</b>S. Manimurugan</div><div><b>Pages:&nbsp;</b>579 - 589</div><div><br /></div><div><b>42)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07135-3\">Stacking optimized with artificial bee colony for driving style classification by feature reconstruction from OBD II data</a></div><div><b>Author(s):&nbsp;</b>G. Priyadharshini, M. Ferni Ukrit</div><div><b>Pages:&nbsp;</b>591 - 603</div><div><br /></div><div><b>43)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-06938-8\">Dynamic multi-variant relational scheme-based intelligent ETL framework for healthcare management</a></div><div><b>Author(s):&nbsp;</b>Vijayalakshmi Manickam, Minu Rajasekaran Indra</div><div><b>Pages:&nbsp;</b>605 - 614</div><div><br /></div><div><b>44)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-021-05871-6\">RETRACTED ARTICLE: COVID-19 pandemic and unemployment rate: A hybrid unemployment rate prediction approach for developed and developing countries of Asia</a></div><div><b>Author(s):&nbsp;</b>Han Lai, Yousaf Ali Khan, Syed Zaheer Abbas</div><div><b>Pages:&nbsp;</b>615 - 615</div><div><br /></div><div><b>45)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07713-5\">Retraction Note: The prediction of the lifetime of the new coronavirus in the USA using mathematical models</a></div><div><b>Author(s):&nbsp;</b>K. Selvakumar, S. Lokesh</div><div><b>Pages:&nbsp;</b>617 - 617</div><div><br /></div><div><b>46)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07708-2\">Retraction Note: Data processing model and performance analysis of cognitive computing based on machine learning in Internet environment</a></div><div><b>Author(s):&nbsp;</b>Hu Jin</div><div><b>Pages:&nbsp;</b>619 - 619</div><div><br /></div><div><b>47)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07709-1\">Retraction Note: Building the electronic evidence analysis model based on association rule mining and FP-growth algorithm</a></div><div><b>Author(s):&nbsp;</b>Yilan Wu, Jing Zhang</div><div><b>Pages:&nbsp;</b>621 - 621</div><div><br /></div><div><b>48)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07710-8\">Retraction Note: Visual communication design elements of Internet of Things based on cloud computing applied in graffiti art schema</a></div><div><b>Author(s):&nbsp;</b>Haotian Wu, Guangan Li</div><div><b>Pages:&nbsp;</b>623 - 623</div><div><br /></div><div><b>49)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07711-7\">Retraction Note: Application of cloud-based visual communication design in Internet of Things image</a></div><div><b>Author(s):&nbsp;</b>Xixia Liu</div><div><b>Pages:&nbsp;</b>625 - 625</div><div><br /></div><div><b>50)</b> <a href=\"https://link.springer.com/article/10.1007/s00500-022-07712-6\">Retraction Note: Analysis on the construction of ideological and political education system for college students based on mobile artificial intelligence terminal</a></div><div><b>Author(s):&nbsp;</b>Yuting Wang</div><div><b>Pages:&nbsp;</b>627 - 627</div><div><br /></div></div>",
            "pubdate": "2023-01-07T22:23:00.000+13:00",
            "pubdate_parsed": [
                2023,
                1,
                7
            ],
            "email_sent": true
        },
        "IEEE Transactions on Cognitive and Developmental Systems, Volume 15, Issue 1, March 2023": {
            "url": "https://computational-intelligence.blogspot.com/2023/03/ieee-transactions-on-cognitive-and.html",
            "description": "<div style=\"text-align: left;\"><div><b>1) </b><a href=\"https://ieeexplore.ieee.org/document/10061513/\">Editorial IEEE Transactions on Cognitive and Developmental Systems</a></div><div><b>Author(s): </b>Huajin Tang</div><div><b>Pages:</b> 2</div><div><br /></div><div><b>2</b>) <a href=\"https://ieeexplore.ieee.org/document/9667107/\">Vision-and-Language Navigation Based on Cross-Modal Feature Fusion in Indoor Environment</a></div><div><b>Author(s):&nbsp;</b>Shuhuan Wen, Xiaohan Lv, F. Richard Yu, Simeng Gong</div><div><b>Pages:</b>3 - 15</div><div><br /></div><div><b>3)</b> <a href=\"https://ieeexplore.ieee.org/document/9669200/\">One-Pass Online Learning Based on Gradient Descent for Multilayer Spiking Neural Networks</a></div><div><b>Author(s):&nbsp;</b>Xianghong Lin, Tiandou Hu, Xiangwen Wang</div><div><b>Pages:</b>16 - 31</div><div><br /></div><div><b>4)</b> <a href=\"https://ieeexplore.ieee.org/document/9937202/\">Adaptation Through Prediction: Multisensory Active Inference Torque Control</a></div><div><b>Author(s):&nbsp;</b>Cristian Meo, Giovanni Franzese, Corrado Pezzato, Max Spahn, Pablo Lanillos</div><div><b>Pages:</b>32 - 41</div><div><br /></div><div><b>5)</b> <a href=\"https://ieeexplore.ieee.org/document/9675829/\">A Weakly Supervised-Guided Soft Attention Network for Classification of Intracranial Hemorrhage</a></div><div><b>Author(s):&nbsp;</b>Long Zhang, Wenlong Miao, Chuang Zhu, Yuanyuan Wang, Yihao Luo, Ruoning Song, Lian Liu, Jie Yang</div><div><b>Pages:</b>42 - 53</div><div><br /></div><div><b>6)</b> <a href=\"https://ieeexplore.ieee.org/document/9676712/\">Reinforcement-Learning-Based Dynamic Opinion Maximization Framework in Signed Social Networks</a></div><div><b>Author(s):&nbsp;</b>Qiang He, Yingjie Lv, Xingwei Wang, Jianhua Li, Min Huang, Lianbo Ma, Yuliang Cai</div><div><b>Pages:</b>54 - 64</div><div><br /></div><div><b>7)</b> <a href=\"https://ieeexplore.ieee.org/document/9690949/\">Self-Attention Pooling-Based Long-Term Temporal Network for Action Recognition</a></div><div><b>Author(s):&nbsp;</b>Huifang Li, Jingwei Huang, Mengchu Zhou, Qisong Shi, Qing Fei</div><div><b>Pages:</b>65 - 77</div><div><br /></div><div><b>8)</b> <a href=\"https://ieeexplore.ieee.org/document/9690946/\">Collision-Free Navigation in Human-Following Task Using a Cognitive Robotic System on Differential Drive Vehicles</a></div><div><b>Author(s):&nbsp;</b>Chien Van Dang, Heungju Ahn, Jong-Wook Kim, Sang C. Lee</div><div><b>Pages:</b>78 - 87</div><div><br /></div><div><b>9)</b> <a href=\"https://ieeexplore.ieee.org/document/9698104/\">Quantized Reservoir Computing for Spectrum Sensing With Knowledge Distillation</a></div><div><b>Author(s):&nbsp;</b>Shiya Liu, Lingjia Liu, Yang Yi</div><div><b>Pages:</b>88 - 99</div><div><br /></div><div><b>10)</b> <a href=\"https://ieeexplore.ieee.org/document/9690943/\">Isokinetic Muscle Strength Training Strategy of an Ankle Rehabilitation Robot Based on Adaptive Gain and Cascade PID Control</a></div><div><b>Author(s):&nbsp;</b>Jianfeng Li, Yu Zhou, Mingjie Dong, Xi Rong</div><div><b>Pages:</b>100 - 110</div><div><br /></div><div><b>11)</b> <a href=\"https://ieeexplore.ieee.org/document/9693979/\">Optimizing Deep Neural Networks Through Neuroevolution With Stochastic Gradient Descent</a></div><div><b>Author(s):&nbsp;</b>Haichao Zhang, Kuangrong Hao, Lei Gao, Bing Wei, Xuesong Tang</div><div><b>Pages:</b>111 - 121</div><div><br /></div><div><b>12)</b> <a href=\"https://ieeexplore.ieee.org/document/9695493/\">Sensing and Navigation of Wearable Assistance Cognitive Systems for the Visually Impaired</a></div><div><b>Author(s):&nbsp;</b>Guoxin Li, Jiaqi Xu, Zhijun Li, Chao Chen, Zhen Kan</div><div><b>Pages:</b>122 - 133</div><div><br /></div><div><b>13)</b> <a href=\"https://ieeexplore.ieee.org/document/9709561/\">Flexible Behavioral Decision Making of Mobile Robot in Dynamic Environment</a></div><div><b>Author(s):&nbsp;</b>Jinbiao Zhu, Dongshu Wang, Jikai Si</div><div><b>Pages:</b>134 - 149</div><div><br /></div><div><b>14)</b> <a href=\"https://ieeexplore.ieee.org/document/9706313/\">A Cerebellum-Inspired Network Model and Learning Approaches for Solving Kinematic Tracking Control of Redundant Manipulators</a></div><div><b>Author(s):&nbsp;</b>Ning Tan, Peng Yu, Fenglei Ni</div><div><b>Pages:</b>150 - 162</div><div><br /></div><div><b>15)</b> <a href=\"https://ieeexplore.ieee.org/document/9707481/\">Emotion Recognition Based on EEG Brain Rhythm Sequencing Technique</a></div><div><b>Author(s):&nbsp;</b>Jia Wen Li, Shovan Barma, Sio Hang Pun, Mang I. Vai, Peng Un Mak</div><div><b>Pages:</b>163 - 174</div><div><br /></div><div><b>16)</b> <a href=\"https://ieeexplore.ieee.org/document/9708412/\">Facial Expression Recognition Through Cross-Modality Attention Fusion</a></div><div><b>Author(s):&nbsp;</b>Rongrong Ni, Biao Yang, Xu Zhou, Angelo Cangelosi, Xiaofeng Liu</div><div><b>Pages:</b>175 - 185</div><div><br /></div><div><b>17)</b> <a href=\"https://ieeexplore.ieee.org/document/9714272/\">Six-Dimensional Target Pose Estimation for Robot Autonomous Manipulation: Methodology and Verification</a></div><div><b>Author(s):&nbsp;</b>Rui Wang, Congjia Su, Hao Yu, Shuo Wang</div><div><b>Pages:</b>186 - 197</div><div><br /></div><div><b>18)</b> <a href=\"https://ieeexplore.ieee.org/document/9714486/\">Implicit Robot Control Using Error-Related Potential-Based Brain\u2013Computer Interface</a></div><div><b>Author(s):&nbsp;</b>Xiaofei Wang, Hsiang-Ting Chen, Yu-Kai Wang, Chin-Teng Lin</div><div><b>Pages:</b>198 - 209</div><div><br /></div><div><b>19)</b> <a href=\"https://ieeexplore.ieee.org/document/9714489/\">C-GRAIL: Autonomous Reinforcement Learning of Multiple and Context-Dependent Goals</a></div><div><b>Author(s):&nbsp;</b>Vieri Giuliano Santucci, Davide Montella, Gianluca Baldassarre</div><div><b>Pages:</b>210 - 222</div><div><br /></div><div><b>20)</b> <a href=\"https://ieeexplore.ieee.org/document/9716077/\">Self-Supervised Learning of Depth and Ego-Motion From Videos by Alternative Training and Geometric Constraints from 3-D to 2-D</a></div><div><b>Author(s):&nbsp;</b>Jiaojiao Fang, Guizhong Liu</div><div><b>Pages:</b>223 - 233</div><div><br /></div><div><b>21)</b> <a href=\"https://ieeexplore.ieee.org/document/9718550/\">Fusing Attention Network Based on Dilated Convolution for Superresolution</a></div><div><b>Author(s):&nbsp;</b>Zhaoyang Song, Xiaoqiang Zhao, Yongyong Hui, Hongmei Jiang</div><div><b>Pages:</b>234 - 241</div><div><br /></div><div><b>22)</b> <a href=\"https://ieeexplore.ieee.org/document/9731518/\">Fall-Perceived Action Recognition of Persons With Neurological Disorders Using Semantic Supervision</a></div><div><b>Author(s):&nbsp;</b>Nitika Nigam, Tanima Dutta, Deepali Verma</div><div><b>Pages:</b>242 - 251</div><div><br /></div><div><b>23)</b> <a href=\"https://ieeexplore.ieee.org/document/9717290/\">Are Nonimage Data Really Necessary for Disease Prediction With Graph Convolutional Networks?</a></div><div><b>Author(s):&nbsp;</b>Gen Shi, Yifan Zhu, Zhensen Chen, Jinyan Liu, Xuesong Li</div><div><b>Pages:</b>252 - 260</div><div><br /></div><div><b>24)</b> <a href=\"https://ieeexplore.ieee.org/document/9718542/\">Multistream 3-D Convolution Neural Network With Parameter Sharing for Human State Estimation</a></div><div><b>Author(s):&nbsp;</b>Chin-Teng Lin, Jia Liu, Chieh-Ning Fang, Shih-Ying Hsiao, Yu-Cheng Chang, Yu-Kai Wang</div><div><b>Pages:</b>261 - 271</div><div><br /></div><div><b>25)</b> <a href=\"https://ieeexplore.ieee.org/document/9733988/\">Convolutional Multiple Instance Learning for Sleep Spindle Detection With Label Refinement</a></div><div><b>Author(s):&nbsp;</b>Xuyun Sun, Yu Qi, Yueming Wang, Gang Pan</div><div><b>Pages:</b>272 - 284</div><div><br /></div><div><b>26)</b> <a href=\"https://ieeexplore.ieee.org/document/9729895/\">Blurry Facial-Image Deconvolution via Model-Guided Deep Neural Network Inspired From Edge Regularization</a></div><div><b>Author(s):&nbsp;</b>Xiaoyuan Yu, Wei Xie</div><div><b>Pages:</b>285 - 297</div><div><br /></div><div><b>27)</b> <a href=\"https://ieeexplore.ieee.org/document/9729878/\">3-D Facial Feature Reconstruction and Learning Network for Facial Expression Recognition in the Wild</a></div><div><b>Author(s):&nbsp;</b>Ning Sun, Jianglong Tao, Jixin Liu, Haian Sun, Guang Han</div><div><b>Pages:</b>298 - 309</div><div><br /></div><div><b>28)</b> <a href=\"https://ieeexplore.ieee.org/document/9733936/\">Obscenity Detection in Videos Through a Sequential ConvNet Pipeline Classifier</a></div><div><b>Author(s):&nbsp;</b>Neil Gautam, Dinesh Kumar Vishwakarma</div><div><b>Pages:</b>310 - 318</div><div><br /></div></div>",
            "pubdate": "2023-03-13T13:04:00.000+13:00",
            "pubdate_parsed": [
                2023,
                3,
                13
            ],
            "email_sent": true
        },
        "Evolving Systems, Volume 14, issue 2, April 2023": {
            "url": "https://computational-intelligence.blogspot.com/2023/03/evolving-systems-volume-14-issue-2.html",
            "description": "<div style=\"text-align: left;\"><b>1)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09441-5\">Reducing the number of trees in a forest using noisy features</a></div><div><b>Author(s): </b>Youness Manzali, Yassine Akhiat...Ahmed Zinedine</div><div><b>Pages: </b>157 - 174</div><div><br /></div><div><b>2)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09442-4\">Social network security using genetic algorithm</a></div><div><b>Author(s):&nbsp;</b>Benyamin MazhariSefat, Soodeh Hosseini</div><div><b>Pages:&nbsp;</b>175 - 190</div><div><br /></div><div><b>3)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09445-1\">Sleep apnea detection from ECG signal using deep CNN-based structures</a></div><div><b>Author(s):&nbsp;</b>Ahmad Ayatollahi, Sajjad Afrakhteh...Ehsan Saleh</div><div><b>Pages:&nbsp;</b>191 - 206</div><div><br /></div><div><b>4)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09447-z\">Min max kurtosis distance based improved initial centroid selection approach of K-means clustering for big data mining on gene expression data</a></div><div><b>Author(s):&nbsp;</b>Kamlesh Kumar Pandey, Diwakar Shukla</div><div><b>Pages:&nbsp;</b>207 - 244</div><div><br /></div><div><b>5)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09450-4\">Comparative analysis with topic modeling and word embedding methods after the Aegean Sea earthquake on Twitter</a></div><div><b>Author(s):&nbsp;</b>Nazmiye Elig\u00fczel, Cihan \u00c7etinkaya, T\u00fcrkay Dereli</div><div><b>Pages:&nbsp;</b>245 - 261</div><div><br /></div><div><b>6)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09454-0\">Combined analysis on AGC and ELD of a hybrid power system with D-WCA designed Gaussian type-2 fuzzy controller</a></div><div><b>Author(s):&nbsp;</b>Krushna Keshab Baral, Prakash Chandra Sahu...Banaja Mohanty</div><div><b>Pages:&nbsp;</b>263 - 280</div><div><br /></div><div><b>7)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09457-x\">Very deep fully convolutional encoder\u2013decoder network based on wavelet transform for art image fusion in cloud computing environment</a></div><div><b>Author(s):&nbsp;</b>Tong Chen, Juan Yang</div><div><b>Pages:&nbsp;</b>281 - 293</div><div><br /></div><div><b>8)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09458-w\">A player unknown's battlegrounds ranking based optimization technique for power system optimization problem</a></div><div><b>Author(s):&nbsp;</b>Kapil Deo Bodha, V. Mukherjee, Vinod Kumar Yadav</div><div><b>Pages:&nbsp;</b>295 - 317</div><div><br /></div><div><b>9)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09455-z\">Evolving fuzzy neural classifier that integrates uncertainty from human-expert feedback</a></div><div><b>Author(s):&nbsp;</b>Paulo Vitor de Campos Souza, Edwin Lughofer</div><div><b>Pages:&nbsp;</b>319 - 341</div><div><br /></div><div><b>10)</b> <a href=\"https://link.springer.com/article/10.1007/s12530-022-09448-y\">A review of online supervised learning</a></div><div><b>Author(s):&nbsp;</b>Charanjeet Singh, Anuj Sharma</div><div><b>Pages:&nbsp;</b>343 - 364</div><div><br /></div><div><br /></div>",
            "pubdate": "2023-03-28T14:55:00.000+13:00",
            "pubdate_parsed": [
                2023,
                3,
                28
            ],
            "email_sent": true
        }
    },
    "TopBots Blog": {
        "Amazon Releases 51-Language Dataset for Language Understanding": {
            "url": "https://www.topbots.com/amazon-massive-dataset/",
            "description": "<p>MASSIVE dataset and Massively Multilingual NLU (MMNLU-22) competition and workshop will help researchers scale natural-language-understanding technology to every language on Earth.</p>\n<p>The post <a href=\"https://www.topbots.com/amazon-massive-dataset/\" rel=\"nofollow\">Amazon Releases 51-Language Dataset for Language Understanding</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Wed, 20 Apr 2022 15:00:07 +0000",
            "pubdate_parsed": [
                2022,
                4,
                20
            ],
            "email_sent": true
        },
        "Unconstrained Chatbots Condone Self-Harm": {
            "url": "https://www.topbots.com/unconstrained-chatbots-condone-self-harm/",
            "description": "<p>WARNING. This post contains references to self-harm and suicide. It includes conversations between a human and DialoGPT, with the sole purpose of surfacing the danger of uncontrolled AI. If you or a loved one are dealing or have dealt with suicidal thoughts, I suggest skipping this article. In the context of an accelerating&#160;mental health crisis, [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/unconstrained-chatbots-condone-self-harm/\" rel=\"nofollow\">Unconstrained Chatbots Condone Self-Harm</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Tue, 31 May 2022 14:02:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                31
            ],
            "email_sent": true
        },
        "Similarity-Based Image Search for Visual Art": {
            "url": "https://www.topbots.com/similarity-based-image-search-for-visual-art/",
            "description": "<p>Similarity-based image search, also known as content-based image retrieval, has historically been a challenging computer vision task. This problem is&#160;especially difficult for visual art, because it is less obvious as to what a metric of \u201csimilarity\u201d should be defined as and who should set that standard for art. For example, when I upload a photo [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/similarity-based-image-search-for-visual-art/\" rel=\"nofollow\">Similarity-Based Image Search for Visual Art</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Wed, 01 Jun 2022 14:09:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                1
            ],
            "email_sent": true
        },
        "Data Capitalism: Innovation, Extraction, Social Conscience": {
            "url": "https://www.topbots.com/data-capitalism/",
            "description": "<p>How to advance social capital in a world increasingly governed by surveillance capitalism? About a year ago, I read&#160;Atlas of AI&#160;by Kate Crawford, a brilliant analysis of the extractive processes that govern the field of machine learning, from environmental resource allocation to the harvesting of our political fabric to data privacy infringements. This article reflects [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/data-capitalism/\" rel=\"nofollow\">Data Capitalism: Innovation, Extraction, Social Conscience</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Thu, 09 Jun 2022 14:57:20 +0000",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "10 Leading Language Models For NLP In 2022": {
            "url": "https://www.topbots.com/leading-nlp-language-models-2020/",
            "description": "<p>The introduction of transfer learning and pretrained language models in natural language processing (NLP) pushed forward the limits of language understanding and generation. Transfer learning and applying transformers to different downstream NLP tasks have become the main trend of the latest research advances. At the same time, there is a controversy in the NLP community [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/leading-nlp-language-models-2020/\" rel=\"nofollow\">10 Leading Language Models For NLP In 2022</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Fri, 17 Jun 2022 11:14:55 +0000",
            "pubdate_parsed": [
                2022,
                6,
                17
            ],
            "email_sent": true
        },
        "Transformers And Multimodal: The Same Key For All Data Types": {
            "url": "https://www.topbots.com/transformers-and-multimodal/",
            "description": "<p>The world of Machine Learning is undoubtedly fascinating, constantly growing, and capable of touching the most diverse sectors, from medicine to space racing, from catering to big manufacturing. There are countless fields of application for this technology and just as many techniques that have been developed over the decades, but they all have one thing [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/transformers-and-multimodal/\" rel=\"nofollow\">Transformers And Multimodal: The Same Key For All Data Types</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Thu, 14 Jul 2022 22:38:13 +0000",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "DALLE 2, Explained: The Promise And Limitations Of A Revolutionary AI": {
            "url": "https://www.topbots.com/dalle-2-explained/",
            "description": "<p>DALL\u00b7E 2 is the newest AI model by OpenAI. If you\u2019ve seen some of its creations and think they\u2019re amazing, keep reading to understand why you\u2019re totally right \u2014 but also wrong. OpenAI published a&#160;blog post&#160;and a paper entitled \u201cHierarchical Text-Conditional Image Generation with CLIP Latents\u201d on DALL\u00b7E 2. The post is fine if you [&#8230;]</p>\n<p>The post <a href=\"https://www.topbots.com/dalle-2-explained/\" rel=\"nofollow\">DALL\u00b7E 2, Explained: The Promise And Limitations Of A Revolutionary AI</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Fri, 15 Jul 2022 15:47:52 +0000",
            "pubdate_parsed": [
                2022,
                7,
                15
            ],
            "email_sent": true
        },
        "The Evolution Of Science: From Descartes To Generative AI": {
            "url": "https://www.topbots.com/the-evolution-of-science-from-descartes-to-generative-ai/",
            "description": "<p>Machine learning became the best tool in history for mathematical manipulation through the use of algorithms for pattern recognition.</p>\n<p>The post <a href=\"https://www.topbots.com/the-evolution-of-science-from-descartes-to-generative-ai/\" rel=\"nofollow\">The Evolution Of Science: From Descartes To Generative AI</a> appeared first on <a href=\"https://www.topbots.com\" rel=\"nofollow\">TOPBOTS</a>.</p>",
            "pubdate": "Thu, 02 Mar 2023 13:45:26 +0000",
            "pubdate_parsed": [
                2023,
                3,
                2
            ],
            "email_sent": true
        }
    },
    "Sebastian Raschka Blog": {
        "Machine Learning with PyTorch and Scikit-Learn": {
            "url": "https://sebastianraschka.com/blog/2022/ml-pytorch-book.html",
            "description": "Machine Learning with PyTorch and Scikit-Learn has been a long time in the making, and I am excited to finally get to talk about the release of my new book. Initially, this project started as the 4th edition of Python Machine Learning. However, we made so many changes to the book that we thought it deserved a new title to reflect that. So, what's new, you may wonder? In this post, I am excited to tell you all about it.",
            "pubdate": "Fri, 25 Feb 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                2,
                25
            ],
            "email_sent": true
        },
        "TorchMetrics": {
            "url": "https://sebastianraschka.com/blog/2022/torchmetrics.html",
            "description": "TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It's designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows. This iterative computation is useful if we want to track a model during iterative training or evaluation on minibatches (and optionally across on multiple GPUs). In deep learning, that's essentially *all the time*. However, when using TorchMetrics, one common question is whether we should use `.update()` or `.forward()`? (And that's also a question I certainly had when I started using it.). Here's a hands-on example and explanation.",
            "pubdate": "Thu, 24 Mar 2022 13:00:00 +0000",
            "pubdate_parsed": [
                2022,
                3,
                24
            ],
            "email_sent": true
        },
        "Losses Learned": {
            "url": "https://sebastianraschka.com/blog/2022/losses-learned-part1.html",
            "description": "The cross-entropy loss is our go-to loss for training deep learning-based classifiers. In this article, I am giving you a quick tour of how we usually compute the cross-entropy loss and how we compute it in PyTorch. There are two parts to it, and here we will look at a binary classification context first. You may wonder why bother writing this article; computing the cross-entropy loss should be relatively straightforward!? Yes and no. We can compute the cross-entropy loss in one line of code, but there's a common gotcha due to numerical optimizations under the hood. (And yes, when I am not careful, I sometimes make this mistake, too.) So, in this article, let me tell you a bit about deep learning jargon, improving numerical performance, and what could go wrong.",
            "pubdate": "Mon, 04 Apr 2022 15:00:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                4
            ],
            "email_sent": true
        },
        "Creating Confidence Intervals for Machine Learning Classifiers": {
            "url": "https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html",
            "description": "Developing good predictive models hinges upon accurate performance evaluation and comparisons. However, when evaluating machine learning models, we typically have to work around many constraints, including limited data, independence violations, and sampling biases. Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. This article outlines different methods for creating confidence intervals for machine learning models. Note that these methods also apply to deep learning.",
            "pubdate": "Mon, 25 Apr 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                4,
                25
            ],
            "email_sent": true
        },
        "Running PyTorch on the M1 GPU": {
            "url": "https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html",
            "description": "Today, PyTorch officially introduced GPU support for Apple's ARM M1 chips. This is an exciting day for Mac users out there, so I spent a few minutes trying it out in practice. In this short blog post, I will summarize my experience and thoughts with the M1 chip for deep learning tasks.",
            "pubdate": "Wed, 18 May 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "Taking Datasets, DataLoaders, and PyTorchs New DataPipes for a Spin": {
            "url": "https://sebastianraschka.com/blog/2022/datapipes.html",
            "description": "The PyTorch team recently announced TorchData, a prototype library focused on implementing composable and reusable data loading utilities for PyTorch. In particular, the TorchData library is centered around DataPipes, which are meant to be a DataLoader-compatible replacement for the existing Dataset class.",
            "pubdate": "Sun, 12 Jun 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                12
            ],
            "email_sent": true
        },
        "Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App": {
            "url": "https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html",
            "description": "In this post, we will build a Lightning App. Why? Because it is 2022, and it is time to explore a more modern take on interacting with, presenting, and sharing our deep learning models. We are going to tackle this in three parts. In this first part, we will learn what a Lightning App is and how we build a Super Resolution GAN demo.",
            "pubdate": "Fri, 17 Jun 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                17
            ],
            "email_sent": true
        },
        "Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud": {
            "url": "https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html",
            "description": "In this article, we will take deploy a Super Resolution App on the cloud using lightning.ai. The primary goal here is to see how easy it is to create and share a research demo. However, the cloud is for more than just model sharing: we will also learn how we can tap into additional GPU resources for model training.",
            "pubdate": "Thu, 30 Jun 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                6,
                30
            ],
            "email_sent": true
        },
        "No, We Don't Have to Choose Batch Sizes As Powers Of 2": {
            "url": "https://sebastianraschka.com/blog/2022/batch-size-2.html",
            "description": "Regarding neural network training, I think we are all guilty of doing this: we choose our batch sizes as powers of 2, that is, 64, 128, 256, 512, 1024, and so forth. There are some valid theoretical justifications for this, but how does it pan out in practice? We had some discussions about that in the last couple of days, and here I want to write down some of the take-aways so I can reference them in the future. I hope you'll find this helpful as well!",
            "pubdate": "Tue, 05 Jul 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                7,
                5
            ],
            "email_sent": true
        },
        "A Short Chronology Of Deep Learning For Tabular Data": {
            "url": "https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html",
            "description": "Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional methods or counterexamples. So, with this short post, I aim to briefly summarize the major papers on deep tabular learning I am currently aware of. However, I want to emphasize that no matter how interesting or promising deep tabular methods look, I still recommend using a conventional machine learning method as a baseline. There is a reason why I cover conventional machine learning before deep learning in my books.",
            "pubdate": "Sun, 24 Jul 2022 07:00:00 +0000",
            "pubdate_parsed": [
                2022,
                7,
                24
            ],
            "email_sent": true
        }
    },
    "Distill Machine Learning Blog": {},
    "Assembly AI Blog": {
        "Deep Learning Paper Recap - Streaming ASR and Summarization": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-streaming-asr-summarization/",
            "description": "This week\u2019s Deep Learning Paper Recaps are Bridging the gap between streaming and non-streaming ASR systems by distilling ensembles of CTC and RNN-T models and BRIO: Bringing Order to Abstractive Summarization",
            "pubdate": "Wed, 22 Jun 2022 15:24:10 GMT",
            "pubdate_parsed": [
                2022,
                6,
                22
            ],
            "email_sent": true
        },
        "How Imagen Actually Works": {
            "url": "https://www.assemblyai.com/blog/how-imagen-actually-works/",
            "description": "Given a brief description of a scene, Imagen can generate photorealistic, high-resolution images of the scene. Learn everything you need to know about Imagen and how it works in this easy-to-follow guide.",
            "pubdate": "Thu, 23 Jun 2022 14:13:40 GMT",
            "pubdate_parsed": [
                2022,
                6,
                23
            ],
            "email_sent": true
        },
        "Content Moderation: What It Is, How It Works, and the Best APIs": {
            "url": "https://www.assemblyai.com/blog/content-moderation-what-it-is-how-it-works-best-apis-2/",
            "description": "This article will look at what Content Moderation is, how it works, some of the best APIs for performing Content Moderation, and a few of its top use cases.",
            "pubdate": "Mon, 27 Jun 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                27
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recap - Language Models": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-language-models/",
            "description": "This week\u2019s Deep Learning Paper Recap is Prune Once For All: Sparse Pre-Trained Language Models",
            "pubdate": "Thu, 07 Jul 2022 14:37:49 GMT",
            "pubdate_parsed": [
                2022,
                7,
                7
            ],
            "email_sent": true
        },
        "Our $30M Series B": {
            "url": "https://www.assemblyai.com/blog/our-30m-series-b/",
            "description": "Today, we\u2019re excited to share that we\u2019ve raised another $30M in our Series B round led by global software investor Insight Partners.",
            "pubdate": "Thu, 14 Jul 2022 14:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                14
            ],
            "email_sent": true
        },
        "Creating Top Hiring Intelligence Platforms with ASR, NLP, and NLU Tools": {
            "url": "https://www.assemblyai.com/blog/creating-top-hiring-intelligence-platforms-with-asr-nlp-and-nlu-tools/",
            "description": "This article looks at how top ASR, NLP, and NLU tools can be integrated into Hiring Intelligence Platforms.",
            "pubdate": "Tue, 19 Jul 2022 14:50:24 GMT",
            "pubdate_parsed": [
                2022,
                7,
                19
            ],
            "email_sent": true
        },
        "AssemblyAI Named a G2 High Performer and Momentum Leader for Summer 2022": {
            "url": "https://www.assemblyai.com/blog/assemblyai-named-a-g2-high-performer-and-momentum-leader-for-summer-2022/",
            "description": "We are thrilled to announce that AssemblyAI has been named a Summer 2022 High Performer and Momentum Leader in the Voice Recognition Software category by G2",
            "pubdate": "Wed, 20 Jul 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                20
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recaps - Modality Matching and Masked Autoencoders": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recaps-modality-matching-and-masked-autoencoders/",
            "description": "This week\u2019s Deep Learning Paper Recaps are MAESTRO: Matched Speech Text Representations through Modality Matching and Masked Autoencoders that Listen.",
            "pubdate": "Wed, 27 Jul 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                27
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recap - Automatic Speech Recognition": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-automatic-speech-recognition/",
            "description": "This week\u2019s Deep Learning Paper Recaps are Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition and Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition",
            "pubdate": "Wed, 03 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                3
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recap - Transfer Learning": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-transfer-learning/",
            "description": "This week\u2019s Deep Learning Paper Review is Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis.",
            "pubdate": "Wed, 10 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Build Your Own Imagen Text-to-Image Model": {
            "url": "https://www.assemblyai.com/blog/build-your-own-imagen-text-to-image-model/",
            "description": "Text-to-Image models have made great strides this year, from DALL-E 2 to the more recent Imagen model. In this tutorial learn how to build a minimal Imagen implementation - MinImagen.",
            "pubdate": "Wed, 17 Aug 2022 15:00:02 GMT",
            "pubdate_parsed": [
                2022,
                8,
                17
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recap - Redundancy Reduction and Sparse MoEs": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-redundancy-reduction-and-sparse-moes/",
            "description": "This week\u2019s Deep Learning Paper Reviews is Barlow Twins: Self-Supervised Learning via Redundancy Reduction and Sparse MoEs Meet Efficient Ensembles",
            "pubdate": "Wed, 17 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                17
            ],
            "email_sent": true
        },
        "How to Run Stable Diffusion Locally to Generate Images": {
            "url": "https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/",
            "description": "Stable Diffusion is a text-to-image model with recently-released open-sourced weights. Learn how to generate an image of a scene given only a description of it in this simple tutorial.",
            "pubdate": "Tue, 23 Aug 2022 12:57:39 GMT",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Deep Learning Paper Recap - Diffusion and Transformer Models": {
            "url": "https://www.assemblyai.com/blog/deep-learning-paper-recap-diffusion-and-transformer-models/",
            "description": "This week\u2019s Deep Learning Paper Reviews is Diffusion-LM Improves Controllable Text Generation and Sparsifying Transformer Models with Trainable Representation Pooling.",
            "pubdate": "Wed, 24 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                24
            ],
            "email_sent": true
        },
        "Why Product Teams at Top Call Tracking Solutions are Turning to AI": {
            "url": "https://www.assemblyai.com/blog/why-product-teams-at-top-call-tracking-solutions-are-turning-to-ai/",
            "description": "This article will look at some of the top AI models for product teams to integrate into Call Tracking Solutions, including what they are and how they work.",
            "pubdate": "Thu, 25 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                25
            ],
            "email_sent": true
        },
        "What are the Top PII Redaction APIs and AI Models for 2023?": {
            "url": "https://www.assemblyai.com/blog/what-are-the-top-pii-redaction-apis-and-ai-models/",
            "description": "This article will explore what Personally Identifiable Information (PII) Redaction APIs are, how they work, and the top use cases.",
            "pubdate": "Tue, 30 Aug 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                30
            ],
            "email_sent": true
        },
        "Introducing the AssemblyAI Creators Program": {
            "url": "https://www.assemblyai.com/blog/introducing-assemblyai-creators-program/",
            "description": "We are excited to announce our AssemblyAI Creators program, a community of creators in the AI space who grow together and give back to the developer community.",
            "pubdate": "Wed, 14 Sep 2022 12:58:00 GMT",
            "pubdate_parsed": [
                2022,
                9,
                14
            ],
            "email_sent": true
        },
        "AI Research Review - Multistream CNN": {
            "url": "https://www.assemblyai.com/blog/ai-research-review-multistream-cnn/",
            "description": "This week\u2019s AI Research Review is Multistream CNN For Robust Acoustic Modeling",
            "pubdate": "Wed, 21 Sep 2022 10:00:00 GMT",
            "pubdate_parsed": [
                2022,
                9,
                21
            ],
            "email_sent": true
        },
        "AssemblyAI Recognized as G2 High Performer, Momentum Leader for Fall 2022": {
            "url": "https://www.assemblyai.com/blog/assemblyai-recognized-as-g2-high-performer-momentum-leader-for-fall-2022/",
            "description": "We are proud to share that AssemblyAI has once again been named a G2 High Performer and Momentum Leader in Voice Recognition Software for Fall 2022!",
            "pubdate": "Wed, 28 Sep 2022 14:13:06 GMT",
            "pubdate_parsed": [
                2022,
                9,
                28
            ],
            "email_sent": true
        },
        "How Aloware Shipped AI-powered Smart Transcription and QA in 6 Weeks": {
            "url": "https://www.assemblyai.com/blog/aloware-shipped-ai-powered-smart-transcription-qa-6-weeks/",
            "description": "Leveraging AssemblyAI, Aloware now offers fast, accurate call transcription and smarter Quality Assurance tools to its customers.",
            "pubdate": "Thu, 29 Sep 2022 13:36:23 GMT",
            "pubdate_parsed": [
                2022,
                9,
                29
            ],
            "email_sent": true
        },
        "Transcribe audio or video files right from your terminal": {
            "url": "https://www.assemblyai.com/blog/transcribe-audio-or-video-files-right-from-your-terminal/",
            "description": "We built the AssemblyAI CLI to help developers quickly test our latest models, right from your terminal, with minimal installation required.",
            "pubdate": "Wed, 19 Oct 2022 13:43:36 GMT",
            "pubdate_parsed": [
                2022,
                10,
                19
            ],
            "email_sent": true
        },
        "AI for product managers: Todays top terms to stay in the know": {
            "url": "https://www.assemblyai.com/blog/ai-for-product-managers-todays-top-terms-to-stay-in-the-know/",
            "description": "Gaining a basic understanding of newer AI terminology is critical for any field, including product management. This guide will serve as an entry point into the current key AI terminology to know now.",
            "pubdate": "Mon, 07 Nov 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                7
            ],
            "email_sent": true
        },
        "AI research review - Merging Models Modulo Permutation Symmetries": {
            "url": "https://www.assemblyai.com/blog/ai-research-review-merging-models-modulo-permutation-symmetries/",
            "description": "This week\u2019s AI Research Review is Git Re-Basin: Merging Models Modulo Permutation Symmetries.",
            "pubdate": "Wed, 16 Nov 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                16
            ],
            "email_sent": true
        },
        "DeepMind's AlphaTensor Explained": {
            "url": "https://www.assemblyai.com/blog/deepminds-alphatensor-explained/",
            "description": "AlphaTensor is a novel AI solution to discover mathematical algorithms with Reinforcement Learning. Learn everything you need to know about AlphaTensor in this comprehensive introduction.",
            "pubdate": "Tue, 22 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                22
            ],
            "email_sent": true
        },
        "Build standout call coaching features with AI Summarization": {
            "url": "https://www.assemblyai.com/blog/build-standout-call-coaching-features-ai-summarization/",
            "description": "Revenue intelligence platforms are adopting an AI-first approach to build competitive call coaching features. This includes embedding AI Summarization models that are purpose-built for understanding conversations.",
            "pubdate": "Mon, 12 Dec 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                12
            ],
            "email_sent": true
        },
        "7 best practices for building better products with AI": {
            "url": "https://www.assemblyai.com/blog/7-best-practices-for-building-better-products-with-ai/",
            "description": "This guide serves to examine the best practices for building better products with AI to demystify the process, and to smooth and accelerate the path to deployment.",
            "pubdate": "Thu, 22 Dec 2022 11:26:04 GMT",
            "pubdate_parsed": [
                2022,
                12,
                22
            ],
            "email_sent": true
        },
        "How ChatGPT actually works": {
            "url": "https://www.assemblyai.com/blog/how-chatgpt-actually-works/",
            "description": "Since its release, the public has been playing with ChatGPT and seeing what it can do, but how does ChatGPT actually work? While the details of its inner workings have not been published, we can piece together its functioning principles from recent research.",
            "pubdate": "Fri, 23 Dec 2022 08:43:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                23
            ],
            "email_sent": true
        },
        "2022 at AssemblyAI - A Year in Review": {
            "url": "https://www.assemblyai.com/blog/2022-at-assemblyai-a-year-in-review/",
            "description": "The end of 2022 is quickly approaching, and what a year it has been! As we get closer to 2023, we wanted to take a moment to look back and reflect on some of the highlights of the past year.",
            "pubdate": "Thu, 29 Dec 2022 06:00:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                29
            ],
            "email_sent": true
        },
        "AI research review  Locating and Editing Factual Associations in GPT": {
            "url": "https://www.assemblyai.com/blog/ai-research-review-locating-and-editing-factual-associations-in-gpt/",
            "description": "This week\u2019s AI Research Review is Locating and Editing Factual Associations in GPT.",
            "pubdate": "Wed, 18 Jan 2023 06:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                18
            ],
            "email_sent": true
        },
        "How CallRail doubled its Conversation Intelligence customers by building with a trusted AI partner": {
            "url": "https://www.assemblyai.com/blog/how-callrail-doubled-its-conversation-intelligence-customers-by-building-with-a-trusted-ai-partner/",
            "description": "With AssemblyAI\u2019s Conversational Summarization Model, CallRail helps its customers easily review calls at scale.",
            "pubdate": "Tue, 14 Feb 2023 06:00:33 GMT",
            "pubdate_parsed": [
                2023,
                2,
                14
            ],
            "email_sent": true
        },
        "6 Best AI playgrounds in 2023": {
            "url": "https://www.assemblyai.com/blog/6-best-ai-playgrounds/",
            "description": "If you\u2019re looking for ways to play around with AI in 2023, these are the six best AI playgrounds to try.",
            "pubdate": "Wed, 08 Mar 2023 13:17:46 GMT",
            "pubdate_parsed": [
                2023,
                3,
                8
            ],
            "email_sent": true
        },
        "AI trends in 2023: Graph Neural Networks": {
            "url": "https://www.assemblyai.com/blog/ai-trends-graph-neural-networks/",
            "description": "From fundamental research to productionized AI models, let\u2019s discover how this cutting-edge technology is powering production applications and may be shaping the future of AI.",
            "pubdate": "Tue, 21 Mar 2023 10:23:28 GMT",
            "pubdate_parsed": [
                2023,
                3,
                21
            ],
            "email_sent": true
        },
        "How AI-powered transcription helped a hiring intelligence platform cut time spent on manual tasks by 90% for its customers": {
            "url": "https://www.assemblyai.com/blog/how-ai-powered-transcription-helped-a-hiring-intelligence-platform-cut-time-spent-on-manual-tasks-by-90-for-its-customers/",
            "description": "Hiring intelligence platform Screenloop builds features using AI-powered transcription that reduce time-to-hire, limit candidate drop-off, and increase offer acceptance for end users.",
            "pubdate": "Tue, 11 Apr 2023 12:27:23 GMT",
            "pubdate_parsed": [
                2023,
                4,
                11
            ],
            "email_sent": true
        }
    },
    "Javier ML Blog": {},
    "Data School": {
        "Building a dataset of Python versions with regular expressions": {
            "url": "https://www.dataschool.io/web-scraping-with-regex/",
            "description": "Learn how to use pandas, requests, and regular expressions  (\"regex\") to create a dataset of every Python version and its release date!",
            "pubdate": "Wed, 12 Apr 2023 13:29:32 GMT",
            "pubdate_parsed": [
                2023,
                4,
                12
            ],
            "email_sent": true
        }
    },
    "Lil'Log": {
        "Learning with not Enough Data Part 2: Active Learning": {
            "url": "https://lilianweng.github.io/posts/2022-02-20-active-learning/",
            "description": "This is part 2 of what to do when facing a limited amount of labeled data for supervised learning tasks. This time we will get some amount of human labeling work involved, but within a budget limit, and therefore we need to be smart when selecting which samples to label.\nNotations    Symbol Meaning     $K$ Number of unique class labels.   $(\\mathbf{x}^l, y) \\sim \\mathcal{X}, y \\in \\{0, 1\\}^K$ Labeled dataset.",
            "pubdate": "Sun, 20 Feb 2022 00:00:00 +0000",
            "pubdate_parsed": [
                2022,
                2,
                20
            ],
            "email_sent": true
        },
        "Learning with not Enough Data Part 3: Data Generation": {
            "url": "https://lilianweng.github.io/posts/2022-04-15-data-gen/",
            "description": "Here comes the Part 3 on learning with not enough data (Previous: Part 1 and Part 2). Let\u2019s consider two approaches for generating synthetic data for training.\n Augmented data. Given a set of existing training samples, we can apply a variety of augmentation, distortion and transformation to derive new data points without losing the key attributes. We have covered a bunch of augmentation methods on text and images in a previous post on contrastive learning.",
            "pubdate": "Fri, 15 Apr 2022 15:10:30 -0700",
            "pubdate_parsed": [
                2022,
                4,
                15
            ],
            "email_sent": true
        },
        "Generalized Visual Language Models": {
            "url": "https://lilianweng.github.io/posts/2022-06-09-vlm/",
            "description": "Processing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained generalized language models to be capable of consuming visual signals.",
            "pubdate": "Thu, 09 Jun 2022 15:10:30 -0700",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        }
    },
    "Hugging Face Blog": {
        "Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker": {
            "url": "https://huggingface.co/blog/gptj-sagemaker",
            "description": null,
            "pubdate": "Mon, 10 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                10
            ],
            "email_sent": true
        },
        "Boost Wav2Vec2 with n-gram LM in  Transformers": {
            "url": "https://huggingface.co/blog/wav2vec2-with-ngram",
            "description": null,
            "pubdate": "Tue, 11 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                11
            ],
            "email_sent": true
        },
        "Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs": {
            "url": "https://huggingface.co/blog/infinity-cpu-performance",
            "description": null,
            "pubdate": "Wed, 12 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                12
            ],
            "email_sent": true
        },
        "Welcome Stable-baselines3 to the Hugging Face Hub ": {
            "url": "https://huggingface.co/blog/sb3",
            "description": null,
            "pubdate": "Thu, 20 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                20
            ],
            "email_sent": true
        },
        "Supercharged Searching on the Hugging Face Hub": {
            "url": "https://huggingface.co/blog/searching-the-hub",
            "description": null,
            "pubdate": "Mon, 24 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                24
            ],
            "email_sent": true
        },
        "Making automatic speech recognition work on large files with Wav2Vec2 in  Transformers": {
            "url": "https://huggingface.co/blog/asr-chunking",
            "description": null,
            "pubdate": "Mon, 31 Jan 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                1,
                31
            ],
            "email_sent": true
        },
        "Getting Started with Sentiment Analysis using Python": {
            "url": "https://huggingface.co/blog/sentiment-analysis-python",
            "description": null,
            "pubdate": "Tue, 01 Feb 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                1
            ],
            "email_sent": true
        },
        "Fine-Tune ViT for Image Classification with  Transformers": {
            "url": "https://huggingface.co/blog/fine-tune-vit",
            "description": null,
            "pubdate": "Thu, 10 Feb 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                2,
                10
            ],
            "email_sent": true
        },
        "BERT 101  State Of The Art NLP Model Explained": {
            "url": "https://huggingface.co/blog/bert-101",
            "description": null,
            "pubdate": "Tue, 01 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                1
            ],
            "email_sent": true
        },
        "Guiding Text Generation with Constrained Beam Search in  Transformers": {
            "url": "https://huggingface.co/blog/constrained-beam-search",
            "description": null,
            "pubdate": "Thu, 10 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                10
            ],
            "email_sent": true
        },
        "Accelerate BERT inference with Hugging Face Transformers and AWS inferentia": {
            "url": "https://huggingface.co/blog/bert-inferentia-sagemaker",
            "description": null,
            "pubdate": "Tue, 15 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                15
            ],
            "email_sent": true
        },
        "Image search with  datasets": {
            "url": "https://huggingface.co/blog/image-search-datasets",
            "description": null,
            "pubdate": "Tue, 15 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                15
            ],
            "email_sent": true
        },
        "Fine-Tune a Semantic Segmentation Model with a Custom Dataset": {
            "url": "https://huggingface.co/blog/fine-tune-segformer",
            "description": null,
            "pubdate": "Wed, 16 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                16
            ],
            "email_sent": true
        },
        "Announcing the  AI Research Residency Program": {
            "url": "https://huggingface.co/blog/ai-residency",
            "description": null,
            "pubdate": "Mon, 21 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                21
            ],
            "email_sent": true
        },
        "Machine Learning Experts - Meg Mitchell Interview": {
            "url": "https://huggingface.co/blog/meg-mitchell-interview",
            "description": null,
            "pubdate": "Tue, 22 Mar 2022 23:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                22
            ],
            "email_sent": true
        },
        "Introducing Decision Transformers on Hugging Face ": {
            "url": "https://huggingface.co/blog/decision-transformers",
            "description": null,
            "pubdate": "Sun, 27 Mar 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                3,
                27
            ],
            "email_sent": true
        },
        "Don't repeat yourself -  Transformers Design Philosophy": {
            "url": "https://huggingface.co/blog/transformers-design-philosophy",
            "description": null,
            "pubdate": "Mon, 04 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                4
            ],
            "email_sent": true
        },
        "Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training": {
            "url": "https://huggingface.co/blog/habana",
            "description": null,
            "pubdate": "Mon, 11 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                11
            ],
            "email_sent": true
        },
        "Machine Learning Experts - Lewis Tunstall Interview": {
            "url": "https://huggingface.co/blog/lewis-tunstall-interview",
            "description": null,
            "pubdate": "Tue, 12 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                12
            ],
            "email_sent": true
        },
        "CO2 Emissions and the  Hub: Leading the Charge": {
            "url": "https://huggingface.co/blog/carbon-emissions-on-the-hub",
            "description": null,
            "pubdate": "Thu, 21 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                21
            ],
            "email_sent": true
        },
        "Introducing Hugging Face for Education": {
            "url": "https://huggingface.co/blog/education",
            "description": null,
            "pubdate": "Sun, 24 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                24
            ],
            "email_sent": true
        },
        "Supercharged Customer Service with Machine Learning": {
            "url": "https://huggingface.co/blog/supercharge-customer-service-with-machine-learning",
            "description": null,
            "pubdate": "Sun, 24 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                24
            ],
            "email_sent": true
        },
        "Getting Started with Transformers on Habana Gaudi": {
            "url": "https://huggingface.co/blog/getting-started-habana",
            "description": null,
            "pubdate": "Mon, 25 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                25
            ],
            "email_sent": true
        },
        "Director of Machine Learning Insights [Series]": {
            "url": "https://huggingface.co/blog/ml-director-insights",
            "description": null,
            "pubdate": "Tue, 26 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                26
            ],
            "email_sent": true
        },
        "Opinion Classification with Kili and HuggingFace AutoTrain": {
            "url": "https://huggingface.co/blog/opinion-classification-with-kili",
            "description": null,
            "pubdate": "Wed, 27 Apr 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                4,
                27
            ],
            "email_sent": true
        },
        "Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel": {
            "url": "https://huggingface.co/blog/pytorch-fsdp",
            "description": null,
            "pubdate": "Sun, 01 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                1
            ],
            "email_sent": true
        },
        "An Introduction to Deep Reinforcement Learning": {
            "url": "https://huggingface.co/blog/deep-rl-intro",
            "description": null,
            "pubdate": "Tue, 03 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                3
            ],
            "email_sent": true
        },
        "Welcome fastai to the Hugging Face Hub": {
            "url": "https://huggingface.co/blog/fastai",
            "description": null,
            "pubdate": "Thu, 05 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                5
            ],
            "email_sent": true
        },
        "We Raised $100 Million for Open & Collaborative Machine Learning ": {
            "url": "https://huggingface.co/blog/series-c",
            "description": null,
            "pubdate": "Sun, 08 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                8
            ],
            "email_sent": true
        },
        "Accelerated Inference with Optimum and Transformers Pipelines": {
            "url": "https://huggingface.co/blog/optimum-inference",
            "description": null,
            "pubdate": "Mon, 09 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                9
            ],
            "email_sent": true
        },
        "Director of Machine Learning Insights [Part 2: SaaS Edition]": {
            "url": "https://huggingface.co/blog/ml-director-insights-2",
            "description": null,
            "pubdate": "Thu, 12 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "Student Ambassador Program's call for applications is open!": {
            "url": "https://huggingface.co/blog/ambassadors",
            "description": null,
            "pubdate": "Thu, 12 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                12
            ],
            "email_sent": true
        },
        "Gradio 3.0 is Out!": {
            "url": "https://huggingface.co/blog/gradio-blocks",
            "description": null,
            "pubdate": "Sun, 15 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                15
            ],
            "email_sent": true
        },
        "Machine Learning Experts - Sasha Luccioni Interview": {
            "url": "https://huggingface.co/blog/sasha-luccioni-interview",
            "description": null,
            "pubdate": "Mon, 16 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                16
            ],
            "email_sent": true
        },
        "Announcing the Hugging Face Fellowship Program": {
            "url": "https://huggingface.co/blog/fellowship",
            "description": null,
            "pubdate": "Mon, 16 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                16
            ],
            "email_sent": true
        },
        "An Introduction to Q-Learning Part 1": {
            "url": "https://huggingface.co/blog/deep-rl-q-part1",
            "description": null,
            "pubdate": "Tue, 17 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                17
            ],
            "email_sent": true
        },
        "How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap": {
            "url": "https://huggingface.co/blog/sempre-health-eap-case-study",
            "description": null,
            "pubdate": "Wed, 18 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "Putting ethical principles at the core of research lifecycle": {
            "url": "https://huggingface.co/blog/ethical-charter-multimodal",
            "description": null,
            "pubdate": "Wed, 18 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                18
            ],
            "email_sent": true
        },
        "An Introduction to Q-Learning Part 2": {
            "url": "https://huggingface.co/blog/deep-rl-q-part2",
            "description": null,
            "pubdate": "Thu, 19 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                19
            ],
            "email_sent": true
        },
        "Efficient Table Pre-training without Real Data: An Introduction to TAPEX": {
            "url": "https://huggingface.co/blog/tapex",
            "description": null,
            "pubdate": "Sun, 22 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                22
            ],
            "email_sent": true
        },
        "Hugging Face Collaborates with Microsoft to Launch Hugging Face Endpoints on Azure": {
            "url": "https://huggingface.co/blog/hugging-face-endpoints-on-azure",
            "description": null,
            "pubdate": "Mon, 23 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                23
            ],
            "email_sent": true
        },
        "Introducing Pull Requests and Discussions ": {
            "url": "https://huggingface.co/blog/community-update",
            "description": null,
            "pubdate": "Tue, 24 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                24
            ],
            "email_sent": true
        },
        "Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers": {
            "url": "https://huggingface.co/blog/graphcore-update",
            "description": null,
            "pubdate": "Wed, 25 May 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                5,
                25
            ],
            "email_sent": true
        },
        "The Annotated Diffusion Model": {
            "url": "https://huggingface.co/blog/annotated-diffusion",
            "description": null,
            "pubdate": "Mon, 06 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                6
            ],
            "email_sent": true
        },
        "Deep Q-Learning with Atari": {
            "url": "https://huggingface.co/blog/deep-rl-dqn",
            "description": null,
            "pubdate": "Mon, 06 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                6
            ],
            "email_sent": true
        },
        "Code generation with Hugging Face": {
            "url": "https://huggingface.co/spaces/loubnabnl/code-generation-models",
            "description": null,
            "pubdate": "Tue, 07 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                7
            ],
            "email_sent": true
        },
        "Using Sentence Transformers for semantic search": {
            "url": "https://huggingface.co/spaces/sentence-transformers/Sentence_Transformers_for_semantic_search",
            "description": null,
            "pubdate": "Thu, 09 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                9
            ],
            "email_sent": true
        },
        "Director of Machine Learning Insights [Part 3: Finance Edition]": {
            "url": "https://huggingface.co/blog/ml-director-insights-3",
            "description": null,
            "pubdate": "Mon, 13 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                13
            ],
            "email_sent": true
        },
        "Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration": {
            "url": "https://huggingface.co/blog/intel",
            "description": null,
            "pubdate": "Tue, 14 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                14
            ],
            "email_sent": true
        },
        "Convert Transformers to ONNX with Hugging Face Optimum": {
            "url": "https://huggingface.co/blog/convert-transformers-to-onnx",
            "description": null,
            "pubdate": "Tue, 21 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                21
            ],
            "email_sent": true
        },
        "Getting Started With Embeddings": {
            "url": "https://huggingface.co/blog/getting-started-with-embeddings",
            "description": null,
            "pubdate": "Wed, 22 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                22
            ],
            "email_sent": true
        },
        "Accelerate Large Model Training using DeepSpeed": {
            "url": "https://huggingface.co/blog/accelerate-deepspeed",
            "description": null,
            "pubdate": "Mon, 27 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                27
            ],
            "email_sent": true
        },
        "Announcing Evaluation on the Hub": {
            "url": "https://huggingface.co/blog/eval-on-the-hub",
            "description": null,
            "pubdate": "Mon, 27 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                27
            ],
            "email_sent": true
        },
        "Liftoff! How to get started with your first ML project ": {
            "url": "https://huggingface.co/blog/your-first-ml-project",
            "description": null,
            "pubdate": "Tue, 28 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                28
            ],
            "email_sent": true
        },
        "Policy Gradient with PyTorch": {
            "url": "https://huggingface.co/blog/deep-rl-pg",
            "description": null,
            "pubdate": "Wed, 29 Jun 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                6,
                29
            ],
            "email_sent": true
        },
        "Getting Started with Sentiment Analysis on Twitter": {
            "url": "https://huggingface.co/blog/sentiment-analysis-twitter",
            "description": null,
            "pubdate": "Wed, 06 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                6
            ],
            "email_sent": true
        },
        "Introducing The World's Largest Open Multilingual Language Model: BLOOM": {
            "url": "https://huggingface.co/blog/bloom",
            "description": null,
            "pubdate": "Mon, 11 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                11
            ],
            "email_sent": true
        },
        "Building a Playlist Generator with Sentence Transformers": {
            "url": "https://huggingface.co/blog/playlist-generator",
            "description": null,
            "pubdate": "Tue, 12 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                12
            ],
            "email_sent": true
        },
        "The Technology Behind BLOOM Training": {
            "url": "https://huggingface.co/blog/bloom-megatron-deepspeed",
            "description": null,
            "pubdate": "Wed, 13 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                13
            ],
            "email_sent": true
        },
        "How to train your model dynamically using adversarial data": {
            "url": "https://huggingface.co/blog/mnist-adversarial",
            "description": null,
            "pubdate": "Fri, 15 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                15
            ],
            "email_sent": true
        },
        "Advantage Actor Critic (A2C)": {
            "url": "https://huggingface.co/blog/deep-rl-a2c",
            "description": null,
            "pubdate": "Thu, 21 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                21
            ],
            "email_sent": true
        },
        "Deploying TensorFlow Vision Models in Hugging Face with TF Serving": {
            "url": "https://huggingface.co/blog/tf-serving-vision",
            "description": null,
            "pubdate": "Sun, 24 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                24
            ],
            "email_sent": true
        },
        "Faster Text Generation with TensorFlow and XLA": {
            "url": "https://huggingface.co/blog/tf-xla-generate",
            "description": null,
            "pubdate": "Tue, 26 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                26
            ],
            "email_sent": true
        },
        "Introducing new audio and vision documentation in  Datasets": {
            "url": "https://huggingface.co/blog/datasets-docs-update",
            "description": null,
            "pubdate": "Wed, 27 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                27
            ],
            "email_sent": true
        },
        "AI Policy @: Comments on U.S. National AI Research Resource Interim Report": {
            "url": "https://huggingface.co/blog/us-national-ai-research-resource",
            "description": null,
            "pubdate": "Sun, 31 Jul 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                7,
                31
            ],
            "email_sent": true
        },
        "Nystrmformer, Approximating self-attention in linear time and memory via the Nystrm method": {
            "url": "https://huggingface.co/blog/nystromformer",
            "description": null,
            "pubdate": "Mon, 01 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                1
            ],
            "email_sent": true
        },
        "Introducing the Private Hub: A New Way to Build With Machine Learning": {
            "url": "https://huggingface.co/blog/introducing-private-hub",
            "description": null,
            "pubdate": "Tue, 02 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                2
            ],
            "email_sent": true
        },
        "Proximal Policy Optimization (PPO)": {
            "url": "https://huggingface.co/blog/deep-rl-ppo",
            "description": null,
            "pubdate": "Thu, 04 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                4
            ],
            "email_sent": true
        },
        "Train and Fine-Tune Sentence Transformers Models": {
            "url": "https://huggingface.co/blog/how-to-train-sentence-transformers",
            "description": null,
            "pubdate": "Tue, 09 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                9
            ],
            "email_sent": true
        },
        "Deploying  ViT on Kubernetes with TF Serving": {
            "url": "https://huggingface.co/blog/deploy-tfserving-kubernetes",
            "description": null,
            "pubdate": "Wed, 10 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                10
            ],
            "email_sent": true
        },
        "Introducing Skops": {
            "url": "https://huggingface.co/blog/skops",
            "description": null,
            "pubdate": "Thu, 11 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                11
            ],
            "email_sent": true
        },
        "Hugging Face's TensorFlow Philosophy": {
            "url": "https://huggingface.co/blog/tensorflow-philosophy",
            "description": null,
            "pubdate": "Thu, 11 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                11
            ],
            "email_sent": true
        },
        "A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes": {
            "url": "https://huggingface.co/blog/hf-bitsandbytes-integration",
            "description": null,
            "pubdate": "Tue, 16 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                16
            ],
            "email_sent": true
        },
        "Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore": {
            "url": "https://huggingface.co/blog/vision-transformers",
            "description": null,
            "pubdate": "Wed, 17 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                17
            ],
            "email_sent": true
        },
        "Deploying  ViT on Vertex AI": {
            "url": "https://huggingface.co/blog/deploy-vertex-ai",
            "description": null,
            "pubdate": "Thu, 18 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                18
            ],
            "email_sent": true
        },
        "Stable Diffusion with  Diffusers": {
            "url": "https://huggingface.co/blog/stable_diffusion",
            "description": null,
            "pubdate": "Sun, 21 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                21
            ],
            "email_sent": true
        },
        "Pre-Train BERT with Hugging Face Transformers and Habana Gaudi": {
            "url": "https://huggingface.co/blog/pretraining-bert",
            "description": null,
            "pubdate": "Sun, 21 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                21
            ],
            "email_sent": true
        },
        "Visualize proteins on Hugging Face Spaces": {
            "url": "https://huggingface.co/blog/spaces_3dmoljs",
            "description": null,
            "pubdate": "Tue, 23 Aug 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                8,
                23
            ],
            "email_sent": true
        },
        "Very Large Language Models and How to Evaluate Them": {
            "url": "https://huggingface.co/blog/zero-shot-eval-on-the-hub",
            "description": null,
            "pubdate": "Sun, 02 Oct 2022 22:00:00 GMT",
            "pubdate_parsed": [
                2022,
                10,
                2
            ],
            "email_sent": true
        },
        "Getting started with Hugging Face Inference Endpoints": {
            "url": "https://huggingface.co/blog/inference-endpoints",
            "description": null,
            "pubdate": "Fri, 14 Oct 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                10,
                14
            ],
            "email_sent": true
        },
        "Accelerate your models with  Optimum Intel and OpenVINO": {
            "url": "https://huggingface.co/blog/openvino",
            "description": null,
            "pubdate": "Wed, 02 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                2
            ],
            "email_sent": true
        },
        "Introducing our new pricing": {
            "url": "https://huggingface.co/blog/pricing-update",
            "description": null,
            "pubdate": "Tue, 08 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                8
            ],
            "email_sent": true
        },
        "An Overview of Inference Solutions on Hugging Face": {
            "url": "https://huggingface.co/blog/inference-update",
            "description": null,
            "pubdate": "Mon, 21 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                21
            ],
            "email_sent": true
        },
        "Director of Machine Learning Insights [Part 4]": {
            "url": "https://huggingface.co/blog/ml-director-insights-4",
            "description": null,
            "pubdate": "Wed, 23 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                23
            ],
            "email_sent": true
        },
        "VQ Diffusion with  Diffusers": {
            "url": "https://huggingface.co/blog/vq-diffusion",
            "description": null,
            "pubdate": "Wed, 30 Nov 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                11,
                30
            ],
            "email_sent": true
        },
        "Probabilistic Time Series Forecasting with  Transformers": {
            "url": "https://huggingface.co/blog/time-series-transformers",
            "description": null,
            "pubdate": "Thu, 01 Dec 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                1
            ],
            "email_sent": true
        },
        "From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community": {
            "url": "https://huggingface.co/blog/elixir-bumblebee",
            "description": null,
            "pubdate": "Fri, 09 Dec 2022 00:00:00 GMT",
            "pubdate_parsed": [
                2022,
                12,
                9
            ],
            "email_sent": true
        },
        "Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1": {
            "url": "https://huggingface.co/blog/intel-sapphire-rapids",
            "description": null,
            "pubdate": "Mon, 02 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                2
            ],
            "email_sent": true
        },
        "Introduction to Graph Machine Learning": {
            "url": "https://huggingface.co/blog/intro-graphml",
            "description": null,
            "pubdate": "Tue, 03 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                3
            ],
            "email_sent": true
        },
        "AI for Game Development: Creating a Farming Game in 5 Days. Part 2": {
            "url": "https://huggingface.co/blog/ml-for-games-2",
            "description": null,
            "pubdate": "Mon, 09 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                9
            ],
            "email_sent": true
        },
        "Image Similarity with Hugging Face Datasets and Transformers": {
            "url": "https://huggingface.co/blog/image-similarity",
            "description": null,
            "pubdate": "Mon, 16 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                16
            ],
            "email_sent": true
        },
        "Welcome PaddlePaddle to the Hugging Face Hub": {
            "url": "https://huggingface.co/blog/paddlepaddle",
            "description": null,
            "pubdate": "Tue, 17 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                17
            ],
            "email_sent": true
        },
        "Using LoRA for Efficient Stable Diffusion Fine-Tuning": {
            "url": "https://huggingface.co/blog/lora",
            "description": null,
            "pubdate": "Thu, 26 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                26
            ],
            "email_sent": true
        },
        "The State of Computer Vision at Hugging Face ": {
            "url": "https://huggingface.co/blog/cv_state",
            "description": null,
            "pubdate": "Mon, 30 Jan 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                1,
                30
            ],
            "email_sent": true
        },
        "A Dive into Pretraining Strategies for Vision-Language Models": {
            "url": "https://huggingface.co/blog/vision_language_pretraining",
            "description": null,
            "pubdate": "Fri, 03 Feb 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                2,
                3
            ],
            "email_sent": true
        },
        "Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2": {
            "url": "https://huggingface.co/blog/intel-sapphire-rapids-inference",
            "description": null,
            "pubdate": "Mon, 06 Feb 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                2,
                6
            ],
            "email_sent": true
        },
        "Speech Synthesis, Recognition, and More With SpeechT5": {
            "url": "https://huggingface.co/blog/speecht5",
            "description": null,
            "pubdate": "Wed, 08 Feb 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                2,
                8
            ],
            "email_sent": true
        },
        " PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware": {
            "url": "https://huggingface.co/blog/peft",
            "description": null,
            "pubdate": "Fri, 10 Feb 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                2,
                10
            ],
            "email_sent": true
        },
        "Leveraging Hugging Face for complex text classification use cases": {
            "url": "https://huggingface.co/blog/classification-use-cases",
            "description": null,
            "pubdate": "Wed, 01 Mar 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                3,
                1
            ],
            "email_sent": true
        },
        "Ethical guidelines for developing the Diffusers library": {
            "url": "https://huggingface.co/blog/ethics-diffusers",
            "description": null,
            "pubdate": "Thu, 02 Mar 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                3,
                2
            ],
            "email_sent": true
        },
        "Using Machine Learning to Aid Survivors and Race through Time": {
            "url": "https://huggingface.co/blog/using-ml-for-disasters",
            "description": null,
            "pubdate": "Fri, 03 Mar 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                3,
                3
            ],
            "email_sent": true
        },
        "ControlNet in Diffusers ": {
            "url": "https://huggingface.co/blog/controlnet",
            "description": null,
            "pubdate": "Fri, 03 Mar 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                3,
                3
            ],
            "email_sent": true
        },
        "Accelerating Stable Diffusion Inference on Intel CPUs": {
            "url": "https://huggingface.co/blog/stable-diffusion-inference-intel",
            "description": null,
            "pubdate": "Tue, 28 Mar 2023 00:00:00 GMT",
            "pubdate_parsed": [
                2023,
                3,
                28
            ],
            "email_sent": true
        }
    },
    "Sorta Insightful": {
        "A Prelude to the Inevitable Long Post About MIT Mystery Hunt 2023": {
            "url": "http://www.alexirpan.com/2023/01/19/mh-2023-prelude.html",
            "description": "<p>The first time I ever wrote for a puzzlehunt was Mystery Hunt 2013. I had joined Manic Sages in 2012 out of the Canada/USA Mathcamp pipeline. They won that year, and I figured, hey, I\u2019ll help write, why not. Writing sounds fun! I helped out on two puzzles. One was...",
            "pubdate": "Thu, 19 Jan 2023 23:32:00 -0800",
            "pubdate_parsed": [
                2023,
                1,
                20
            ],
            "email_sent": true
        }
    },
    "Azure-MLOps": {}
}